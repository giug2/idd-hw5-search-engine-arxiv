{
    "S4.T1": {
        "source_file": "Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering",
        "caption": "Table 1: \nLexicon quality (%) on LibriSpeech dev-clean when different SSL features are used in\ntwo representative systems, one using continuous and the other discrete features.",
        "body": "Features\n\n\nNED\n\n\n\n\nPurity\n\n\nV-measure\n\n\n\nContinuous + average + kk-means\n\n\n\nWavLM Large\n\n\n7.4\n\n\n\n\n89.3\n\n\n83.7\n\n\nHuBERT Large\n\n\n9.3\n\n\n\n\n89.0\n\n\n83.6\n\n\nHuBERT Soft\n\n\n10.0\n\n\n\n\n85.0\n\n\n83.1\n\n\nmHuBERT\n\n\n10.8\n\n\n\n\n83.4\n\n\n82.2\n\n\nDiscrete + edit distance + graph clustering\n\n\nWavLM Large\n\n\n7.3\n\n\n\n\n83.3\n\n\n88.6\n\n\nHuBERT Large\n\n\n7.8\n\n\n\n\n85.0\n\n\n89.8\n\n\nHuBERT Soft\n\n\n23.5\n\n\n\n\n59.6\n\n\n78.9\n\n\nmHuBERT\n\n\n29.7\n\n\n\n\n61.0\n\n\n79.1",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Features</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">NED</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">Purity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">V-measure</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Continuous + average + </span><math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">-means</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">83.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">HuBERT Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">HuBERT Soft</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">mHuBERT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">82.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Discrete + edit distance + graph clustering</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">HuBERT Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">85.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">HuBERT Soft</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">mHuBERT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "quality",
            "features",
            "hubert",
            "librispeech",
            "ned",
            "used",
            "large",
            "purity",
            "graph",
            "clustering",
            "distance",
            "one",
            "wavlm",
            "ssl",
            "continuous",
            "soft",
            "lexicon",
            "vmeasure",
            "representative",
            "discrete",
            "average",
            "systems",
            "mhubert",
            "kkmeans",
            "edit",
            "different",
            "two",
            "devclean",
            "when"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Before comparing the various combinations of representations and clustering methods, we start by selecting the best-performing SSL features.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports English development scores using two representative systems: continuous averaged features which are </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustered (top), and discrete unit sequences which are graph clustered using edit distance (bottom).</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Systems using WavLM Large features perform well, especially in its continuous form (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-top) where it outperforms all other models.\nIn the discrete graph clustering system, HuBERT Large also performs well.\nIn contrast, mHuBERT generally results in worse lexicons, indicating that diluting English pre-training data with other languages worsens performance compared to using English-only models.\nAlthough HuBERT Soft is competitive in the continuous setting, the discretised version (bottom) performs much worse.\nBased on these results, we use WavLM Large features for the subsequent lexicon learning experiments on English data.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels.\nDespite progress, the induced lexicons are still far from perfect.\nIn an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types.\nWe combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means, hierarchical, graph-based) on English and Mandarin data.\nThe best system uses graph clustering with dynamic time warping on continuous features.\nFaster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences.\nThrough controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type&#8212;rather than clustering&#8212;is the primary factor limiting performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "distance",
                    "features",
                    "continuous",
                    "edit",
                    "different",
                    "discrete",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unsupervised word discovery aims to discover word-like units from unlabelled speech by locating word boundaries and clustering the resulting segments into hypothesised lexical categories&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis is difficult because, unlike text, speech is continuous and lacks explicit word delimiters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nMoreover, different instances of the same word exhibit substantial acoustic variability, even when produced by the same speaker.\nDespite these challenges, human infants can discriminate between words in their native language within their first year </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDeveloping computational models that mimic this ability could therefore both inform theories of language acquisition </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and support the development of low-resource speech technology </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "clustering",
                    "different",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Full-coverage systems can typically be broken into three components: an unsupervised word boundary detection component, a feature extraction method for representing word-like segments, and a clustering method used to group the hypothesised segments into a lexicon of word-like types.\nTo assess the influence of boundary detection, Malan et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> performed word discovery using ground-truth word boundaries.\nEven in this idealised setting, the resulting lexicon is still far from perfect.\nSince true boundaries were used, the errors stem either from the representation or clustering method.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "clustering",
                    "used",
                    "lexicon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper investigates these limitations by revisiting the task of unsupervised lexicon learning, where unlabelled word segments with true boundaries need to be clustered into hypothesised word types.\nWe combine a range of representations from self-supervised learned (SSL) speech models with different clustering approaches.\nWe consider both discrete and continuous frame-level features.\nThese are either kept in sequence form to represent word segments, with distances between sequences used for clustering, or averaged to obtain fixed-dimensional acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor clustering, we consider </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, agglomerative, BIRCH, and graph clustering.\nIn experiments on English and Mandarin, our best system uses graph clustering with DTW over continuous sequences.\nWhile producing a better lexicon than previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the result remains imperfect, with word-level purity below 90%.\nThis system is also much slower than those using averaged embeddings or discrete sequences.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "ssl",
                    "features",
                    "continuous",
                    "lexicon",
                    "different",
                    "discrete",
                    "used",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To definitively answer whether it is the representations or clustering that limits performance, we conduct two controlled experiments.\nFirst, we reduce variability in clustering by forcing all instances of a word into the same cluster at initialisation.\nSecond, we eliminate representational inconsistency by artificially fixing all instances of the same word type to near-identical representations.\nThis allows the influence of speech representations and clustering methods to be assessed individually.\nThe results show that, despite substantial improvements, current speech representations, not clustering methods, constrain lexicon quality.\nThus, further research on SSL features is needed to make progress on unsupervised word discovery.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "ssl",
                    "quality",
                    "features",
                    "lexicon",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We investigate several combinations of speech representation and clustering methods to understand the current landscape of unsupervised lexicon learning.\nThe systems are shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nApart from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who implicitly looked at lexicon learning, work on this task is limited. One exception is&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied different clustering approaches on acoustic word embeddings. But this was done using conventional features in a time before neural representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "features",
                    "one",
                    "lexicon",
                    "different",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given ground truth word segments (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-a), we start by extracting continuous features from intermediate layers of SSL speech models (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i).\nWe consider a range of SSL models, all trained using a masked prediction pretext task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe different SSL models are described in detail in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAs an alternative to continuous features, discretised representations are obtained by quantising continuous features, producing a lower-bitrate encoding which reduces downstream computational load (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii).\nPrevious work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showed that quantisation can remove speaker-specific information from continuous features, potentially improving consistency in representations used for lexicon learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "features",
                    "continuous",
                    "lexicon",
                    "different",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While some clustering approaches can be applied directly to feature sequences, others require word segments to be represented as single fixed-dimensional vectors.\nA common approach is to use an acoustic word embedding obtained\nby averaging the features of a word segment across the temporal dimension&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhile simple, several studies have shown that averaging appropriate SSL features results in\nrobust acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib30\" title=\"\">30</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "ssl",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We couple the different representations with a range of clustering methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c), resulting in six lexicon learning systems.\nThe first four cluster averaged acoustic word embeddings using one of the following methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c): </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering; BIRCH clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an efficient tree-based algorithm; agglomerative hierarchical clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; or graph clustering, where word segments are represented as nodes and edges are weighted by the cosine similarity between their embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "one",
                    "lexicon",
                    "different",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The two remaining systems also use graph clustering, but operate on feature sequences, retaining temporal information.\nThe distance metric used for graph construction corresponds to the type of representations.\nFor continuous features, pairwise distances are computed using DTW, which aligns sequences of varying lengths and calculates the cost of the optimal alignment (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i to Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor discrete unit sequences, edit distance is used, which measures the minimum number of operations required to convert one sequence into another (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii to Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor all graph clustering systems, a graph is incrementally constructed with edges inserted between nodes when the similarity exceeds a predefined threshold.\nThe graph is then partitioned using the efficient Leiden algorithm </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with the constant Potts model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as clustering objective.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "distance",
                    "features",
                    "one",
                    "continuous",
                    "edit",
                    "two",
                    "discrete",
                    "systems",
                    "when",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use LibriSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dev-clean for development and test-clean for final evaluations.\nEach set contains 5.4 hours of English speech from 40 speakers.\nWord boundaries are obtained from forced alignments&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the Mandarin evaluation, we use data from Track&#160;2 of the ZeroSpeech challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, consisting of 2.5 hours of speech from 12 speakers with word alignments.\nFor the Mandarin experiments, the optimal hyperparameters from the English development experiments are retained as is.</span>\n</p>\n\n",
                "matched_terms": [
                    "devclean",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate lexicon quality using a range of standard metrics.\nNormalised edit distance&#160;(NED)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures the phonetic purity of clusters by calculating the length-normalised edit distance between phonetic transcriptions (from forced alignments) of all word segment pairs within a cluster.\nA single average over all clusters gives the final NED; lower is better.\nPurity measures how homogeneous each cluster is with respect to the true word labels.\nFor every cluster, the most frequent word type is identified, and the number of segments belonging to that type is counted.\nThese are summed over all clusters and divided by the total number of segments in&#160;the&#160;dataset.\nV-measure&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the harmonic mean of purity (also called homogeneity) and completeness, where the latter measures the degree to&#160;which segments of the same type are assigned to the same cluster.\nHigher purity and V-measure are better.\nBitrate is the average data rate of&#160;the encoded output, measured in bits per second (bits/s); lower is better.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "distance",
                    "quality",
                    "edit",
                    "lexicon",
                    "vmeasure",
                    "ned",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Purity and NED can be artificially improved by increasing the number of clusters.\nTo enable a fair comparison between methods, and since we are operating in an idealised setting, we fix the number of clusters to the true number of word types in each dataset.\nThe number of clusters for LibriSpeech dev-clean and test-clean is 8,216 and 8,006, respectively, while for Mandarin it is 8,871.\nGraph clustering does not take the number of clusters as an explicit input but instead infers it from the data; we tune the hyperparameters to get approximately the desired number of clusters.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "clustering",
                    "librispeech",
                    "devclean",
                    "ned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare speech representations from a range of SSL models, all producing 20 ms frames.\nTo test state-of-the-art features&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nA HuBERT Base model fine-tuned for voice-conversion, HuBERT Soft&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, is also considered for its ability to preserve phonetic content while suppressing speaker-specific information: its continuous features are extracted from a linear projection layer that predicts a distribution over discrete units, thereby providing a middle-ground between continuous and discrete features.\nTo assess the effects of pre-training language, we use the multilingual HuBERT (mHuBERT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pre-trained on 90k hours across 147 languages (including English and Mandarin).\nFor the Mandarin tests, we include a HuBERT Large model pre-trained on 10k hours of Mandarin.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/TencentGameMate/chinese-hubert-large</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor all large SSL models, 1,024-dimensional features are extracted from the 21st layer based on its ability to capture word information&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFollowing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 768-dimensional mHuBERT features are extracted from the eighth&#160;layer.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "features",
                    "continuous",
                    "hubert",
                    "soft",
                    "mhubert",
                    "discrete",
                    "wavlm",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All these features are subsequently mean and variance normalised, and then projected to 350 dimensions using principal component analysis (PCA).\nThis gave the best performance on dev-clean, and improved on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where 250 dimensions were used.\nFor HuBERT Soft, features from the 12th layer are pushed through the projection layer, resulting in 256-dimensional soft features, which are mean and variance normalised.\nFor each SSL model, acoustic word embeddings are obtained by averaging the different low-dimensional features and then normalising to the unit sphere.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "features",
                    "soft",
                    "hubert",
                    "different",
                    "devclean",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Discrete features are obtained by applying </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering with 500 clusters on the different SSL representations (without normalisation or PCA).\nThese unit extraction models are trained on 50 hours of audio from LibriSpeech train-clean for English data, or on the complete dataset for the Mandarin data.\nFollowing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib41\" title=\"\">41</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use duration-penalised dynamic programming to smooth the resulting sequences by discouraging rapid unit changes.\nThe unit sequences are not deduplicated, which worked better in development experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "ssl",
                    "features",
                    "different",
                    "librispeech",
                    "discrete",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first describe the clustering methods that learn a lexicon on averaged acoustic word embeddings.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, we use the efficient FAISS library.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/facebookresearch/faiss</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nBut unlike&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we switch the default random initialisation to </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means++ initialisation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis selects the first centroid at random, with each subsequent centroid chosen from the remaining data points with probability proportional to the squared distance from the closest existing centroid.\nThis alternative initialisation gave large improvements over&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on development data.\nBIRCH and agglomerative clustering are implemented using scikit-learn.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://scikit-learn.org</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply BIRCH with a distance threshold of 0.25.\nAgglomerative clustering uses Ward linkage&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which merges clusters by attempting to minimise the total within-cluster variance.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "distance",
                    "lexicon",
                    "large",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Graph clustering can be used with either averaged acoustic embeddings or feature sequences.\nWe use the efficient igraph library </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDistance-based thresholds are applied globally: nodes with no edges remain isolated. Thresholds are tuned based on memory constraints and development performance, with values of 0.65 for edit distance graphs, 0.4 for cosine distance graphs, and 0.35 for DTW graphs.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "distance",
                    "edit",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the lexicon learning performance on English test data of the different representation&#8211;clustering systems using WavLM Large features (continuous or discrete).\nContinuous averaged features with graph clustering achieves the best purity (89.6%), V-measure (90.3%), and bitrate (35.6).\nThe continuous features with DTW and graph clustering yield the lowest NED (5.2%), but at a substantial computational cost, being around 250 times slower than alternative systems.\nThese results all improve on the continuous averaged </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means approach from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieves a NED of 17.3%, purity of 78.2%, V-measure of 80.0%, and a bitrate of 41.4 (after altering the experimental setup to exactly match the protocol here).</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "clustering",
                    "features",
                    "continuous",
                    "lexicon",
                    "vmeasure",
                    "different",
                    "ned",
                    "discrete",
                    "wavlm",
                    "systems",
                    "large",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In general, continuous representations outperform discrete units on purity and NED, with the discrete graph system only being competitive in V-measure (88.4%).\nOf the different clustering methods, graph-based clustering consistently yields higher V-measure scores than simpler methods like </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means or agglomerative clustering.\nBitrate indicates that graph clustering produces a more compact encoding output, especially when used with the averaged embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "continuous",
                    "vmeasure",
                    "different",
                    "ned",
                    "discrete",
                    "when",
                    "used",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although all systems deliver competitive lexicons and improve on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, none of them achieve a perfect lexicon, even when provided with a ground-truth word segmentation: NED shows that the clustered sequences still have phonetic differences, and none of the systems achieve perfect purity or completeness.\nBefore we investigate what limits performance, we see if this is also true on another language, Mandarin.\nApart from being in a different language family, the SSL representations available in Mandarin are also different from those available in English.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "ssl",
                    "lexicon",
                    "different",
                    "ned",
                    "systems",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows how well our systems developed on English data generalise to Mandarin data.\nWe also evaluate the effect of an SSL model&#8217;s pre-training language by testing with models trained on different amounts of Mandarin data.\nConcretely, we compare the English-only WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the multilingual mHuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> including limited Mandarin data, and the Mandarin HuBERT Large trained entirely on Mandarin (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for details).\nWe show results for the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system representative of previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with the two systems that worked best on English (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "hubert",
                    "different",
                    "mhubert",
                    "two",
                    "representative",
                    "wavlm",
                    "systems",
                    "large",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Across the different systems, the English-only WavLM performs worst, the mHuBERT being exposed to some Mandarin gives intermediate results, and the Mandarin HuBERT performs the best by a substantial margin.\nIn particular, Mandarin HuBERT features achieve a NED of 4.9% when averaged and graph clustered.\nThis results in a lexicon of comparable quality to the best English lexicon (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and shows that including more of the target language in pre-training improves lexicon quality.\nThis underscores the importance of language-specific SSL representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "ssl",
                    "quality",
                    "features",
                    "hubert",
                    "lexicon",
                    "different",
                    "ned",
                    "wavlm",
                    "systems",
                    "when",
                    "mhubert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although we show that quality lexicons can be obtained on a different language, the results here corroborate those on English where even the strongest systems produce lexicons with a NED of around 5% and word-level purity of between 80% and 90%.\nBelow we examine the source of this limitation.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "quality",
                    "different",
                    "ned",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given that we are using true word boundaries, lexicon imperfections stem\nfrom clustering methods that fail to group instances of the same word together and/or representations that fail to capture similarity across different instances of the same word.\nIn Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we present controlled experiments on LibriSpeech test-clean where we idealise either the clustering method or representations in two representative systems.\nThe baseline results are from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "lexicon",
                    "different",
                    "librispeech",
                    "two",
                    "representative",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the second row of each section in the table, the two clustering algorithms are initialised so that all instances of the same word type are assigned to the same cluster.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means (top section), this means that centroids are initialised at the mean embeddings of segments corresponding to the same word type.\nFor graph clustering (bottom), segments are initially assigned to clusters based on their true word types.\nAt initialisation, both systems have a purity and V-measure of 100%.\nHowever, after the methods are run from their perfect starting points, the resulting lexicons are far from ideal: purity and V-measure drop to baseline levels.\nFrom the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system we conclude that embeddings of the same word type are not close to all the other instances of that word.\nSimilarly, from the discrete system we see that unit sequences are variable across instances of the same word type.</span>\n</p>\n\n",
                "matched_terms": [
                    "other",
                    "purity",
                    "clustering",
                    "graph",
                    "vmeasure",
                    "two",
                    "discrete",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next we consider the case where representations are ideal and clustering is performed as usual.\nFor continuous features (top section), each word type is represented by its mean acoustic word embedding with a small amount of random noise added.\nFor the discrete unit sequences (bottom), a single sequence is randomly selected to represent all instances of that word type.\nResults are given in the third row of each section in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe see that, in both cases, a perfect lexicon is produced with word-level purities and V-measures of 100%.\nIn both cases, NED is worse than the baselines, dropping from roughly 8% to 12%.\nThis is because, although all of the same words are clustered together, some of their phonetic transcriptions (used to calculate NED) differ due to differences in how they are produced.\nThis highlights the danger of using NED as the only metric for lexicon evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "features",
                    "continuous",
                    "lexicon",
                    "ned",
                    "discrete",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Pronunciation and co-articulation not only affects NED but also lead to differences in the representations, which in part explains the poor performance when initialising the clustering methods perfectly (the second rows in the two sections).\nBut in the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means systems we see that perfect initialisation actually leads to worse performance than the non-ideal baseline, e.g. purity dropping from 88.4% to 81.5%.\nTogether these findings suggest that clustering methods, particularly graph clustering, perform robustly, but inconsistency in the representation of spoken words hinder effective lexicon learning.\nIf consistent representations aligned with word types were available, perfect word-level purity and V-measure would be achievable.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "lexicon",
                    "vmeasure",
                    "two",
                    "ned",
                    "systems",
                    "when",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper compared six systems for lexicon learning that combine different feature representations and clustering approaches, achieving clear improvements over previous methods.\nThese results summarise the current landscape of lexicon learning in terms of accuracy and computational cost.\nIn controlled experiments, we showed that if representations are idealised, existing\nclustering methods can learn perfect lexicons.\nThe current bottleneck is therefore not in clustering, but in the consistency of feature representations.\nFuture work will explore how these insights can be applied to the training of self-supervised speech models to improve the resulting representations.\nWe will also consider how our best systems can be extended for true word discovery, where boundaries are&#160;unknown.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "clustering",
                    "lexicon",
                    "different"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering",
        "caption": "Table 2: \nLexicon quality on LibriSpeech test-clean in terms of NED (%), purity (%), V-measure (%), bitrate (bits/second), and runtime (seconds) for six different representationclustering systems.",
        "body": "System\n\n\nNED\n\n\n\n\nPurity\n\n\n\n\nV-m\n\n\n\n\nBitrate\n\n\nRuntime\n\n\n\n\n\ncontinu + avg + kk-means\n\n\n\n8.6\n\n\n\n\n88.4\n\n\n\n\n83.6\n\n\n\n\n40.9\n\n\n281.0\n\n\ncontinu + avg + BIRCH\n\n\n6.8\n\n\n\n\n89.5\n\n\n\n\n84.1\n\n\n\n\n41.0\n\n\n415.0\n\n\ncontinu + avg + agglom\n\n\n6.8\n\n\n\n\n89.5\n\n\n\n\n84.1\n\n\n\n\n40.9\n\n\n433.0\n\n\ncontinu + avg + graph\n\n\n6.7\n\n\n\n\n89.6\n\n\n\n\n90.3\n\n\n\n\n35.6\n\n\n484.0\n\n\ncontinu + DTW + graph\n\n\n5.2\n\n\n\n\n89.3\n\n\n\n\n89.1\n\n\n\n\n36.6\n\n\n123,630.9\n\n\ndiscrete + edit + graph\n\n\n7.9\n\n\n\n\n83.0\n\n\n\n\n88.4\n\n\n\n\n36.9\n\n\n1,526.6",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">System</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">NED</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">Purity</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">V-m</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">Bitrate</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Runtime</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">continu + avg + </span><math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">-means</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">281.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">continu + avg + BIRCH</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">415.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">continu + avg + agglom</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">433.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">continu + avg + graph</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">35.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">484.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">continu + DTW + graph</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">123,630.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">discrete + edit + graph</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,526.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "quality",
            "agglom",
            "seconds",
            "librispeech",
            "dtw",
            "avg",
            "testclean",
            "ned",
            "purity",
            "graph",
            "six",
            "bitssecond",
            "system",
            "lexicon",
            "vmeasure",
            "continu",
            "discrete",
            "systems",
            "kkmeans",
            "terms",
            "bitrate",
            "edit",
            "different",
            "representationclustering",
            "runtime",
            "birch"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the lexicon learning performance on English test data of the different representation&#8211;clustering systems using WavLM Large features (continuous or discrete).\nContinuous averaged features with graph clustering achieves the best purity (89.6%), V-measure (90.3%), and bitrate (35.6).\nThe continuous features with DTW and graph clustering yield the lowest NED (5.2%), but at a substantial computational cost, being around 250 times slower than alternative systems.\nThese results all improve on the continuous averaged </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means approach from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieves a NED of 17.3%, purity of 78.2%, V-measure of 80.0%, and a bitrate of 41.4 (after altering the experimental setup to exactly match the protocol here).</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows how well our systems developed on English data generalise to Mandarin data.\nWe also evaluate the effect of an SSL model&#8217;s pre-training language by testing with models trained on different amounts of Mandarin data.\nConcretely, we compare the English-only WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the multilingual mHuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> including limited Mandarin data, and the Mandarin HuBERT Large trained entirely on Mandarin (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for details).\nWe show results for the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system representative of previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with the two systems that worked best on English (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Across the different systems, the English-only WavLM performs worst, the mHuBERT being exposed to some Mandarin gives intermediate results, and the Mandarin HuBERT performs the best by a substantial margin.\nIn particular, Mandarin HuBERT features achieve a NED of 4.9% when averaged and graph clustered.\nThis results in a lexicon of comparable quality to the best English lexicon (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and shows that including more of the target language in pre-training improves lexicon quality.\nThis underscores the importance of language-specific SSL representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given that we are using true word boundaries, lexicon imperfections stem\nfrom clustering methods that fail to group instances of the same word together and/or representations that fail to capture similarity across different instances of the same word.\nIn Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we present controlled experiments on LibriSpeech test-clean where we idealise either the clustering method or representations in two representative systems.\nThe baseline results are from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels.\nDespite progress, the induced lexicons are still far from perfect.\nIn an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types.\nWe combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means, hierarchical, graph-based) on English and Mandarin data.\nThe best system uses graph clustering with dynamic time warping on continuous features.\nFaster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences.\nThrough controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type&#8212;rather than clustering&#8212;is the primary factor limiting performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "system",
                    "edit",
                    "different",
                    "discrete",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Full-coverage systems can typically be broken into three components: an unsupervised word boundary detection component, a feature extraction method for representing word-like segments, and a clustering method used to group the hypothesised segments into a lexicon of word-like types.\nTo assess the influence of boundary detection, Malan et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> performed word discovery using ground-truth word boundaries.\nEven in this idealised setting, the resulting lexicon is still far from perfect.\nSince true boundaries were used, the errors stem either from the representation or clustering method.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "lexicon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper investigates these limitations by revisiting the task of unsupervised lexicon learning, where unlabelled word segments with true boundaries need to be clustered into hypothesised word types.\nWe combine a range of representations from self-supervised learned (SSL) speech models with different clustering approaches.\nWe consider both discrete and continuous frame-level features.\nThese are either kept in sequence form to represent word segments, with distances between sequences used for clustering, or averaged to obtain fixed-dimensional acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor clustering, we consider </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, agglomerative, BIRCH, and graph clustering.\nIn experiments on English and Mandarin, our best system uses graph clustering with DTW over continuous sequences.\nWhile producing a better lexicon than previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the result remains imperfect, with word-level purity below 90%.\nThis system is also much slower than those using averaged embeddings or discrete sequences.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "system",
                    "lexicon",
                    "different",
                    "dtw",
                    "discrete",
                    "birch",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To definitively answer whether it is the representations or clustering that limits performance, we conduct two controlled experiments.\nFirst, we reduce variability in clustering by forcing all instances of a word into the same cluster at initialisation.\nSecond, we eliminate representational inconsistency by artificially fixing all instances of the same word type to near-identical representations.\nThis allows the influence of speech representations and clustering methods to be assessed individually.\nThe results show that, despite substantial improvements, current speech representations, not clustering methods, constrain lexicon quality.\nThus, further research on SSL features is needed to make progress on unsupervised word discovery.</span>\n</p>\n\n",
                "matched_terms": [
                    "lexicon",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We investigate several combinations of speech representation and clustering methods to understand the current landscape of unsupervised lexicon learning.\nThe systems are shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nApart from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who implicitly looked at lexicon learning, work on this task is limited. One exception is&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied different clustering approaches on acoustic word embeddings. But this was done using conventional features in a time before neural representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "lexicon",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given ground truth word segments (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-a), we start by extracting continuous features from intermediate layers of SSL speech models (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i).\nWe consider a range of SSL models, all trained using a masked prediction pretext task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe different SSL models are described in detail in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAs an alternative to continuous features, discretised representations are obtained by quantising continuous features, producing a lower-bitrate encoding which reduces downstream computational load (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii).\nPrevious work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showed that quantisation can remove speaker-specific information from continuous features, potentially improving consistency in representations used for lexicon learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "lexicon",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We couple the different representations with a range of clustering methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c), resulting in six lexicon learning systems.\nThe first four cluster averaged acoustic word embeddings using one of the following methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c): </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering; BIRCH clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an efficient tree-based algorithm; agglomerative hierarchical clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; or graph clustering, where word segments are represented as nodes and edges are weighted by the cosine similarity between their embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "six",
                    "lexicon",
                    "different",
                    "systems",
                    "birch",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The two remaining systems also use graph clustering, but operate on feature sequences, retaining temporal information.\nThe distance metric used for graph construction corresponds to the type of representations.\nFor continuous features, pairwise distances are computed using DTW, which aligns sequences of varying lengths and calculates the cost of the optimal alignment (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i to Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor discrete unit sequences, edit distance is used, which measures the minimum number of operations required to convert one sequence into another (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii to Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor all graph clustering systems, a graph is incrementally constructed with edges inserted between nodes when the similarity exceeds a predefined threshold.\nThe graph is then partitioned using the efficient Leiden algorithm </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with the constant Potts model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as clustering objective.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "edit",
                    "dtw",
                    "discrete",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use LibriSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dev-clean for development and test-clean for final evaluations.\nEach set contains 5.4 hours of English speech from 40 speakers.\nWord boundaries are obtained from forced alignments&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the Mandarin evaluation, we use data from Track&#160;2 of the ZeroSpeech challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, consisting of 2.5 hours of speech from 12 speakers with word alignments.\nFor the Mandarin experiments, the optimal hyperparameters from the English development experiments are retained as is.</span>\n</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate lexicon quality using a range of standard metrics.\nNormalised edit distance&#160;(NED)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures the phonetic purity of clusters by calculating the length-normalised edit distance between phonetic transcriptions (from forced alignments) of all word segment pairs within a cluster.\nA single average over all clusters gives the final NED; lower is better.\nPurity measures how homogeneous each cluster is with respect to the true word labels.\nFor every cluster, the most frequent word type is identified, and the number of segments belonging to that type is counted.\nThese are summed over all clusters and divided by the total number of segments in&#160;the&#160;dataset.\nV-measure&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the harmonic mean of purity (also called homogeneity) and completeness, where the latter measures the degree to&#160;which segments of the same type are assigned to the same cluster.\nHigher purity and V-measure are better.\nBitrate is the average data rate of&#160;the encoded output, measured in bits per second (bits/s); lower is better.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "bitrate",
                    "quality",
                    "edit",
                    "lexicon",
                    "vmeasure",
                    "ned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Purity and NED can be artificially improved by increasing the number of clusters.\nTo enable a fair comparison between methods, and since we are operating in an idealised setting, we fix the number of clusters to the true number of word types in each dataset.\nThe number of clusters for LibriSpeech dev-clean and test-clean is 8,216 and 8,006, respectively, while for Mandarin it is 8,871.\nGraph clustering does not take the number of clusters as an explicit input but instead infers it from the data; we tune the hyperparameters to get approximately the desired number of clusters.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "librispeech",
                    "testclean",
                    "ned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Discrete features are obtained by applying </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering with 500 clusters on the different SSL representations (without normalisation or PCA).\nThese unit extraction models are trained on 50 hours of audio from LibriSpeech train-clean for English data, or on the complete dataset for the Mandarin data.\nFollowing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib41\" title=\"\">41</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use duration-penalised dynamic programming to smooth the resulting sequences by discouraging rapid unit changes.\nThe unit sequences are not deduplicated, which worked better in development experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "librispeech",
                    "discrete",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first describe the clustering methods that learn a lexicon on averaged acoustic word embeddings.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, we use the efficient FAISS library.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/facebookresearch/faiss</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nBut unlike&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we switch the default random initialisation to </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means++ initialisation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis selects the first centroid at random, with each subsequent centroid chosen from the remaining data points with probability proportional to the squared distance from the closest existing centroid.\nThis alternative initialisation gave large improvements over&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on development data.\nBIRCH and agglomerative clustering are implemented using scikit-learn.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://scikit-learn.org</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply BIRCH with a distance threshold of 0.25.\nAgglomerative clustering uses Ward linkage&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which merges clusters by attempting to minimise the total within-cluster variance.</span>\n</p>\n\n",
                "matched_terms": [
                    "lexicon",
                    "birch",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Graph clustering can be used with either averaged acoustic embeddings or feature sequences.\nWe use the efficient igraph library </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDistance-based thresholds are applied globally: nodes with no edges remain isolated. Thresholds are tuned based on memory constraints and development performance, with values of 0.65 for edit distance graphs, 0.4 for cosine distance graphs, and 0.35 for DTW graphs.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "edit",
                    "graph",
                    "dtw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Before comparing the various combinations of representations and clustering methods, we start by selecting the best-performing SSL features.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports English development scores using two representative systems: continuous averaged features which are </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustered (top), and discrete unit sequences which are graph clustered using edit distance (bottom).</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "edit",
                    "discrete",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Systems using WavLM Large features perform well, especially in its continuous form (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-top) where it outperforms all other models.\nIn the discrete graph clustering system, HuBERT Large also performs well.\nIn contrast, mHuBERT generally results in worse lexicons, indicating that diluting English pre-training data with other languages worsens performance compared to using English-only models.\nAlthough HuBERT Soft is competitive in the continuous setting, the discretised version (bottom) performs much worse.\nBased on these results, we use WavLM Large features for the subsequent lexicon learning experiments on English data.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "system",
                    "lexicon",
                    "discrete",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In general, continuous representations outperform discrete units on purity and NED, with the discrete graph system only being competitive in V-measure (88.4%).\nOf the different clustering methods, graph-based clustering consistently yields higher V-measure scores than simpler methods like </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means or agglomerative clustering.\nBitrate indicates that graph clustering produces a more compact encoding output, especially when used with the averaged embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "system",
                    "bitrate",
                    "vmeasure",
                    "different",
                    "ned",
                    "discrete",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although all systems deliver competitive lexicons and improve on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, none of them achieve a perfect lexicon, even when provided with a ground-truth word segmentation: NED shows that the clustered sequences still have phonetic differences, and none of the systems achieve perfect purity or completeness.\nBefore we investigate what limits performance, we see if this is also true on another language, Mandarin.\nApart from being in a different language family, the SSL representations available in Mandarin are also different from those available in English.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "lexicon",
                    "different",
                    "ned",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although we show that quality lexicons can be obtained on a different language, the results here corroborate those on English where even the strongest systems produce lexicons with a NED of around 5% and word-level purity of between 80% and 90%.\nBelow we examine the source of this limitation.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "quality",
                    "different",
                    "ned",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the second row of each section in the table, the two clustering algorithms are initialised so that all instances of the same word type are assigned to the same cluster.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means (top section), this means that centroids are initialised at the mean embeddings of segments corresponding to the same word type.\nFor graph clustering (bottom), segments are initially assigned to clusters based on their true word types.\nAt initialisation, both systems have a purity and V-measure of 100%.\nHowever, after the methods are run from their perfect starting points, the resulting lexicons are far from ideal: purity and V-measure drop to baseline levels.\nFrom the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system we conclude that embeddings of the same word type are not close to all the other instances of that word.\nSimilarly, from the discrete system we see that unit sequences are variable across instances of the same word type.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "system",
                    "vmeasure",
                    "discrete",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next we consider the case where representations are ideal and clustering is performed as usual.\nFor continuous features (top section), each word type is represented by its mean acoustic word embedding with a small amount of random noise added.\nFor the discrete unit sequences (bottom), a single sequence is randomly selected to represent all instances of that word type.\nResults are given in the third row of each section in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe see that, in both cases, a perfect lexicon is produced with word-level purities and V-measures of 100%.\nIn both cases, NED is worse than the baselines, dropping from roughly 8% to 12%.\nThis is because, although all of the same words are clustered together, some of their phonetic transcriptions (used to calculate NED) differ due to differences in how they are produced.\nThis highlights the danger of using NED as the only metric for lexicon evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "lexicon",
                    "ned",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Pronunciation and co-articulation not only affects NED but also lead to differences in the representations, which in part explains the poor performance when initialising the clustering methods perfectly (the second rows in the two sections).\nBut in the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means systems we see that perfect initialisation actually leads to worse performance than the non-ideal baseline, e.g. purity dropping from 88.4% to 81.5%.\nTogether these findings suggest that clustering methods, particularly graph clustering, perform robustly, but inconsistency in the representation of spoken words hinder effective lexicon learning.\nIf consistent representations aligned with word types were available, perfect word-level purity and V-measure would be achievable.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "vmeasure",
                    "lexicon",
                    "ned",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper compared six systems for lexicon learning that combine different feature representations and clustering approaches, achieving clear improvements over previous methods.\nThese results summarise the current landscape of lexicon learning in terms of accuracy and computational cost.\nIn controlled experiments, we showed that if representations are idealised, existing\nclustering methods can learn perfect lexicons.\nThe current bottleneck is therefore not in clustering, but in the consistency of feature representations.\nFuture work will explore how these insights can be applied to the training of self-supervised speech models to improve the resulting representations.\nWe will also consider how our best systems can be extended for true word discovery, where boundaries are&#160;unknown.</span>\n</p>\n\n",
                "matched_terms": [
                    "terms",
                    "six",
                    "lexicon",
                    "different",
                    "systems"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering",
        "caption": "Table 3: \nLexicon quality (%) on Mandarin data using different SSL features in a subset of relevant systems.",
        "body": "Features\n\n\nNED\n\n\n\n\nPurity\n\n\nV-measure\n\n\n\nContinuous + average + kk-means\n\n\n\nWavLM Large\n\n\n52.7\n\n\n\n\n63.3\n\n\n87.9\n\n\nmHuBERT\n\n\n46.9\n\n\n\n\n66.6\n\n\n88.6\n\n\nMandarin HuBERT Large\n\n\n16.8\n\n\n\n\n80.5\n\n\n92.1\n\n\nContinuous + average + cosine distance + graph clustering\n\n\nWavLM Large\n\n\n43.4\n\n\n\n\n64.3\n\n\n88.5\n\n\nmHuBERT\n\n\n39.1\n\n\n\n\n67.5\n\n\n89.3\n\n\nMandarin HuBERT Large\n\n\n4.9\n\n\n\n\n82.8\n\n\n94.2\n\n\nContinuous + DTW distance + graph clustering\n\n\nWavLM Large\n\n\n33.2\n\n\n\n\n68.3\n\n\n89.6\n\n\nmHuBERT\n\n\n30.3\n\n\n\n\n71.5\n\n\n90.5\n\n\nMandarin HuBERT Large\n\n\n5.8\n\n\n\n\n82.0\n\n\n93.6",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Features</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">NED</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">Purity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">V-measure</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Continuous + average + </span><math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">-means</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">mHuBERT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mandarin HuBERT Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">16.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">92.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Continuous + average + cosine distance + graph clustering</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">mHuBERT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mandarin HuBERT Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">82.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">94.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Continuous + DTW distance + graph clustering</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">mHuBERT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mandarin HuBERT Large</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.8</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">82.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "quality",
            "subset",
            "features",
            "hubert",
            "relevant",
            "dtw",
            "ned",
            "large",
            "purity",
            "graph",
            "clustering",
            "distance",
            "mandarin",
            "wavlm",
            "cosine",
            "ssl",
            "continuous",
            "lexicon",
            "vmeasure",
            "average",
            "systems",
            "mhubert",
            "kkmeans",
            "different",
            "data"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows how well our systems developed on English data generalise to Mandarin data.\nWe also evaluate the effect of an SSL model&#8217;s pre-training language by testing with models trained on different amounts of Mandarin data.\nConcretely, we compare the English-only WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the multilingual mHuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> including limited Mandarin data, and the Mandarin HuBERT Large trained entirely on Mandarin (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for details).\nWe show results for the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system representative of previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with the two systems that worked best on English (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels.\nDespite progress, the induced lexicons are still far from perfect.\nIn an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types.\nWe combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means, hierarchical, graph-based) on English and Mandarin data.\nThe best system uses graph clustering with dynamic time warping on continuous features.\nFaster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences.\nThrough controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type&#8212;rather than clustering&#8212;is the primary factor limiting performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "systems",
                    "distance",
                    "features",
                    "continuous",
                    "mandarin",
                    "different",
                    "data",
                    "cosine",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unsupervised word discovery aims to discover word-like units from unlabelled speech by locating word boundaries and clustering the resulting segments into hypothesised lexical categories&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis is difficult because, unlike text, speech is continuous and lacks explicit word delimiters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nMoreover, different instances of the same word exhibit substantial acoustic variability, even when produced by the same speaker.\nDespite these challenges, human infants can discriminate between words in their native language within their first year </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDeveloping computational models that mimic this ability could therefore both inform theories of language acquisition </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and support the development of low-resource speech technology </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "clustering",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Full-coverage systems can typically be broken into three components: an unsupervised word boundary detection component, a feature extraction method for representing word-like segments, and a clustering method used to group the hypothesised segments into a lexicon of word-like types.\nTo assess the influence of boundary detection, Malan et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> performed word discovery using ground-truth word boundaries.\nEven in this idealised setting, the resulting lexicon is still far from perfect.\nSince true boundaries were used, the errors stem either from the representation or clustering method.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "clustering",
                    "lexicon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper investigates these limitations by revisiting the task of unsupervised lexicon learning, where unlabelled word segments with true boundaries need to be clustered into hypothesised word types.\nWe combine a range of representations from self-supervised learned (SSL) speech models with different clustering approaches.\nWe consider both discrete and continuous frame-level features.\nThese are either kept in sequence form to represent word segments, with distances between sequences used for clustering, or averaged to obtain fixed-dimensional acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor clustering, we consider </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, agglomerative, BIRCH, and graph clustering.\nIn experiments on English and Mandarin, our best system uses graph clustering with DTW over continuous sequences.\nWhile producing a better lexicon than previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the result remains imperfect, with word-level purity below 90%.\nThis system is also much slower than those using averaged embeddings or discrete sequences.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "ssl",
                    "features",
                    "continuous",
                    "mandarin",
                    "lexicon",
                    "different",
                    "dtw",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To definitively answer whether it is the representations or clustering that limits performance, we conduct two controlled experiments.\nFirst, we reduce variability in clustering by forcing all instances of a word into the same cluster at initialisation.\nSecond, we eliminate representational inconsistency by artificially fixing all instances of the same word type to near-identical representations.\nThis allows the influence of speech representations and clustering methods to be assessed individually.\nThe results show that, despite substantial improvements, current speech representations, not clustering methods, constrain lexicon quality.\nThus, further research on SSL features is needed to make progress on unsupervised word discovery.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "ssl",
                    "quality",
                    "features",
                    "lexicon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We investigate several combinations of speech representation and clustering methods to understand the current landscape of unsupervised lexicon learning.\nThe systems are shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nApart from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who implicitly looked at lexicon learning, work on this task is limited. One exception is&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied different clustering approaches on acoustic word embeddings. But this was done using conventional features in a time before neural representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "features",
                    "lexicon",
                    "different",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given ground truth word segments (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-a), we start by extracting continuous features from intermediate layers of SSL speech models (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i).\nWe consider a range of SSL models, all trained using a masked prediction pretext task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe different SSL models are described in detail in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAs an alternative to continuous features, discretised representations are obtained by quantising continuous features, producing a lower-bitrate encoding which reduces downstream computational load (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii).\nPrevious work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showed that quantisation can remove speaker-specific information from continuous features, potentially improving consistency in representations used for lexicon learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "features",
                    "continuous",
                    "lexicon",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While some clustering approaches can be applied directly to feature sequences, others require word segments to be represented as single fixed-dimensional vectors.\nA common approach is to use an acoustic word embedding obtained\nby averaging the features of a word segment across the temporal dimension&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhile simple, several studies have shown that averaging appropriate SSL features results in\nrobust acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib30\" title=\"\">30</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "ssl",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We couple the different representations with a range of clustering methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c), resulting in six lexicon learning systems.\nThe first four cluster averaged acoustic word embeddings using one of the following methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c): </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering; BIRCH clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an efficient tree-based algorithm; agglomerative hierarchical clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; or graph clustering, where word segments are represented as nodes and edges are weighted by the cosine similarity between their embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "systems",
                    "lexicon",
                    "different",
                    "cosine",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The two remaining systems also use graph clustering, but operate on feature sequences, retaining temporal information.\nThe distance metric used for graph construction corresponds to the type of representations.\nFor continuous features, pairwise distances are computed using DTW, which aligns sequences of varying lengths and calculates the cost of the optimal alignment (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i to Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor discrete unit sequences, edit distance is used, which measures the minimum number of operations required to convert one sequence into another (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii to Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor all graph clustering systems, a graph is incrementally constructed with edges inserted between nodes when the similarity exceeds a predefined threshold.\nThe graph is then partitioned using the efficient Leiden algorithm </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with the constant Potts model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as clustering objective.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "distance",
                    "features",
                    "continuous",
                    "dtw",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use LibriSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dev-clean for development and test-clean for final evaluations.\nEach set contains 5.4 hours of English speech from 40 speakers.\nWord boundaries are obtained from forced alignments&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the Mandarin evaluation, we use data from Track&#160;2 of the ZeroSpeech challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, consisting of 2.5 hours of speech from 12 speakers with word alignments.\nFor the Mandarin experiments, the optimal hyperparameters from the English development experiments are retained as is.</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "mandarin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate lexicon quality using a range of standard metrics.\nNormalised edit distance&#160;(NED)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures the phonetic purity of clusters by calculating the length-normalised edit distance between phonetic transcriptions (from forced alignments) of all word segment pairs within a cluster.\nA single average over all clusters gives the final NED; lower is better.\nPurity measures how homogeneous each cluster is with respect to the true word labels.\nFor every cluster, the most frequent word type is identified, and the number of segments belonging to that type is counted.\nThese are summed over all clusters and divided by the total number of segments in&#160;the&#160;dataset.\nV-measure&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the harmonic mean of purity (also called homogeneity) and completeness, where the latter measures the degree to&#160;which segments of the same type are assigned to the same cluster.\nHigher purity and V-measure are better.\nBitrate is the average data rate of&#160;the encoded output, measured in bits per second (bits/s); lower is better.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "distance",
                    "quality",
                    "lexicon",
                    "vmeasure",
                    "ned",
                    "average",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Purity and NED can be artificially improved by increasing the number of clusters.\nTo enable a fair comparison between methods, and since we are operating in an idealised setting, we fix the number of clusters to the true number of word types in each dataset.\nThe number of clusters for LibriSpeech dev-clean and test-clean is 8,216 and 8,006, respectively, while for Mandarin it is 8,871.\nGraph clustering does not take the number of clusters as an explicit input but instead infers it from the data; we tune the hyperparameters to get approximately the desired number of clusters.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "mandarin",
                    "ned",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare speech representations from a range of SSL models, all producing 20 ms frames.\nTo test state-of-the-art features&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nA HuBERT Base model fine-tuned for voice-conversion, HuBERT Soft&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, is also considered for its ability to preserve phonetic content while suppressing speaker-specific information: its continuous features are extracted from a linear projection layer that predicts a distribution over discrete units, thereby providing a middle-ground between continuous and discrete features.\nTo assess the effects of pre-training language, we use the multilingual HuBERT (mHuBERT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pre-trained on 90k hours across 147 languages (including English and Mandarin).\nFor the Mandarin tests, we include a HuBERT Large model pre-trained on 10k hours of Mandarin.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/TencentGameMate/chinese-hubert-large</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor all large SSL models, 1,024-dimensional features are extracted from the 21st layer based on its ability to capture word information&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFollowing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 768-dimensional mHuBERT features are extracted from the eighth&#160;layer.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "features",
                    "continuous",
                    "hubert",
                    "mandarin",
                    "mhubert",
                    "wavlm",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All these features are subsequently mean and variance normalised, and then projected to 350 dimensions using principal component analysis (PCA).\nThis gave the best performance on dev-clean, and improved on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where 250 dimensions were used.\nFor HuBERT Soft, features from the 12th layer are pushed through the projection layer, resulting in 256-dimensional soft features, which are mean and variance normalised.\nFor each SSL model, acoustic word embeddings are obtained by averaging the different low-dimensional features and then normalising to the unit sphere.</span>\n</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "different",
                    "ssl",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Discrete features are obtained by applying </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering with 500 clusters on the different SSL representations (without normalisation or PCA).\nThese unit extraction models are trained on 50 hours of audio from LibriSpeech train-clean for English data, or on the complete dataset for the Mandarin data.\nFollowing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib41\" title=\"\">41</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use duration-penalised dynamic programming to smooth the resulting sequences by discouraging rapid unit changes.\nThe unit sequences are not deduplicated, which worked better in development experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "ssl",
                    "features",
                    "mandarin",
                    "different",
                    "data",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first describe the clustering methods that learn a lexicon on averaged acoustic word embeddings.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, we use the efficient FAISS library.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/facebookresearch/faiss</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nBut unlike&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we switch the default random initialisation to </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means++ initialisation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis selects the first centroid at random, with each subsequent centroid chosen from the remaining data points with probability proportional to the squared distance from the closest existing centroid.\nThis alternative initialisation gave large improvements over&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on development data.\nBIRCH and agglomerative clustering are implemented using scikit-learn.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://scikit-learn.org</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply BIRCH with a distance threshold of 0.25.\nAgglomerative clustering uses Ward linkage&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which merges clusters by attempting to minimise the total within-cluster variance.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "distance",
                    "lexicon",
                    "data",
                    "large",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Graph clustering can be used with either averaged acoustic embeddings or feature sequences.\nWe use the efficient igraph library </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDistance-based thresholds are applied globally: nodes with no edges remain isolated. Thresholds are tuned based on memory constraints and development performance, with values of 0.65 for edit distance graphs, 0.4 for cosine distance graphs, and 0.35 for DTW graphs.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "distance",
                    "dtw",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Before comparing the various combinations of representations and clustering methods, we start by selecting the best-performing SSL features.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports English development scores using two representative systems: continuous averaged features which are </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustered (top), and discrete unit sequences which are graph clustered using edit distance (bottom).</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "ssl",
                    "distance",
                    "features",
                    "continuous",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Systems using WavLM Large features perform well, especially in its continuous form (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-top) where it outperforms all other models.\nIn the discrete graph clustering system, HuBERT Large also performs well.\nIn contrast, mHuBERT generally results in worse lexicons, indicating that diluting English pre-training data with other languages worsens performance compared to using English-only models.\nAlthough HuBERT Soft is competitive in the continuous setting, the discretised version (bottom) performs much worse.\nBased on these results, we use WavLM Large features for the subsequent lexicon learning experiments on English data.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "features",
                    "continuous",
                    "hubert",
                    "lexicon",
                    "mhubert",
                    "data",
                    "wavlm",
                    "systems",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the lexicon learning performance on English test data of the different representation&#8211;clustering systems using WavLM Large features (continuous or discrete).\nContinuous averaged features with graph clustering achieves the best purity (89.6%), V-measure (90.3%), and bitrate (35.6).\nThe continuous features with DTW and graph clustering yield the lowest NED (5.2%), but at a substantial computational cost, being around 250 times slower than alternative systems.\nThese results all improve on the continuous averaged </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means approach from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieves a NED of 17.3%, purity of 78.2%, V-measure of 80.0%, and a bitrate of 41.4 (after altering the experimental setup to exactly match the protocol here).</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "graph",
                    "clustering",
                    "features",
                    "continuous",
                    "lexicon",
                    "vmeasure",
                    "different",
                    "data",
                    "dtw",
                    "ned",
                    "wavlm",
                    "systems",
                    "large",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In general, continuous representations outperform discrete units on purity and NED, with the discrete graph system only being competitive in V-measure (88.4%).\nOf the different clustering methods, graph-based clustering consistently yields higher V-measure scores than simpler methods like </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means or agglomerative clustering.\nBitrate indicates that graph clustering produces a more compact encoding output, especially when used with the averaged embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "continuous",
                    "vmeasure",
                    "different",
                    "ned",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although all systems deliver competitive lexicons and improve on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, none of them achieve a perfect lexicon, even when provided with a ground-truth word segmentation: NED shows that the clustered sequences still have phonetic differences, and none of the systems achieve perfect purity or completeness.\nBefore we investigate what limits performance, we see if this is also true on another language, Mandarin.\nApart from being in a different language family, the SSL representations available in Mandarin are also different from those available in English.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "ssl",
                    "mandarin",
                    "lexicon",
                    "different",
                    "ned",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Across the different systems, the English-only WavLM performs worst, the mHuBERT being exposed to some Mandarin gives intermediate results, and the Mandarin HuBERT performs the best by a substantial margin.\nIn particular, Mandarin HuBERT features achieve a NED of 4.9% when averaged and graph clustered.\nThis results in a lexicon of comparable quality to the best English lexicon (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and shows that including more of the target language in pre-training improves lexicon quality.\nThis underscores the importance of language-specific SSL representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "ssl",
                    "quality",
                    "features",
                    "hubert",
                    "mandarin",
                    "lexicon",
                    "different",
                    "ned",
                    "wavlm",
                    "systems",
                    "mhubert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although we show that quality lexicons can be obtained on a different language, the results here corroborate those on English where even the strongest systems produce lexicons with a NED of around 5% and word-level purity of between 80% and 90%.\nBelow we examine the source of this limitation.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "quality",
                    "different",
                    "ned",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given that we are using true word boundaries, lexicon imperfections stem\nfrom clustering methods that fail to group instances of the same word together and/or representations that fail to capture similarity across different instances of the same word.\nIn Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we present controlled experiments on LibriSpeech test-clean where we idealise either the clustering method or representations in two representative systems.\nThe baseline results are from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "clustering",
                    "lexicon",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the second row of each section in the table, the two clustering algorithms are initialised so that all instances of the same word type are assigned to the same cluster.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means (top section), this means that centroids are initialised at the mean embeddings of segments corresponding to the same word type.\nFor graph clustering (bottom), segments are initially assigned to clusters based on their true word types.\nAt initialisation, both systems have a purity and V-measure of 100%.\nHowever, after the methods are run from their perfect starting points, the resulting lexicons are far from ideal: purity and V-measure drop to baseline levels.\nFrom the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system we conclude that embeddings of the same word type are not close to all the other instances of that word.\nSimilarly, from the discrete system we see that unit sequences are variable across instances of the same word type.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "vmeasure",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next we consider the case where representations are ideal and clustering is performed as usual.\nFor continuous features (top section), each word type is represented by its mean acoustic word embedding with a small amount of random noise added.\nFor the discrete unit sequences (bottom), a single sequence is randomly selected to represent all instances of that word type.\nResults are given in the third row of each section in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe see that, in both cases, a perfect lexicon is produced with word-level purities and V-measures of 100%.\nIn both cases, NED is worse than the baselines, dropping from roughly 8% to 12%.\nThis is because, although all of the same words are clustered together, some of their phonetic transcriptions (used to calculate NED) differ due to differences in how they are produced.\nThis highlights the danger of using NED as the only metric for lexicon evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "features",
                    "continuous",
                    "lexicon",
                    "ned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Pronunciation and co-articulation not only affects NED but also lead to differences in the representations, which in part explains the poor performance when initialising the clustering methods perfectly (the second rows in the two sections).\nBut in the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means systems we see that perfect initialisation actually leads to worse performance than the non-ideal baseline, e.g. purity dropping from 88.4% to 81.5%.\nTogether these findings suggest that clustering methods, particularly graph clustering, perform robustly, but inconsistency in the representation of spoken words hinder effective lexicon learning.\nIf consistent representations aligned with word types were available, perfect word-level purity and V-measure would be achievable.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "lexicon",
                    "vmeasure",
                    "ned",
                    "systems",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper compared six systems for lexicon learning that combine different feature representations and clustering approaches, achieving clear improvements over previous methods.\nThese results summarise the current landscape of lexicon learning in terms of accuracy and computational cost.\nIn controlled experiments, we showed that if representations are idealised, existing\nclustering methods can learn perfect lexicons.\nThe current bottleneck is therefore not in clustering, but in the consistency of feature representations.\nFuture work will explore how these insights can be applied to the training of self-supervised speech models to improve the resulting representations.\nWe will also consider how our best systems can be extended for true word discovery, where boundaries are&#160;unknown.</span>\n</p>\n\n",
                "matched_terms": [
                    "systems",
                    "clustering",
                    "lexicon",
                    "different"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering",
        "caption": "Table 4: \nLexicon quality (%) on LibriSpeech test-clean when the effect of representations or clustering methods is isolated.",
        "body": "System\n\n\nNED\n\n\n\n\nPurity\n\n\nV-measure\n\n\n\nContinuous + average + kk-means\n\n\n\nBaseline\n\n\n8.6\n\n\n\n\n88.4\n\n\n83.6\n\n\nPerfect cluster initialisation\n\n\n17.0\n\n\n\n\n81.5\n\n\n81.3\n\n\nPerfect word embeddings\n\n\n12.1\n\n\n\n\n100.0\n\n\n100.0\n\n\nDiscrete + edit + graph clustering\n\n\nBaseline\n\n\n7.9\n\n\n\n\n83.0\n\n\n88.4\n\n\nPerfect cluster initialisation\n\n\n7.4\n\n\n\n\n83.6\n\n\n88.7\n\n\nPerfect word representations\n\n\n12.1\n\n\n\n\n100.0\n\n\n100.0",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">NED</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">Purity</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">V-measure</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Continuous + average + </span><math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">-means</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Perfect cluster initialisation</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Perfect word embeddings</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Discrete + edit + graph clustering</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Perfect cluster initialisation</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Perfect word representations</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_right ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">100.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "quality",
            "isolated",
            "librispeech",
            "testclean",
            "ned",
            "representations",
            "purity",
            "clustering",
            "graph",
            "cluster",
            "baseline",
            "methods",
            "system",
            "continuous",
            "lexicon",
            "vmeasure",
            "word",
            "discrete",
            "average",
            "perfect",
            "effect",
            "embeddings",
            "kkmeans",
            "edit",
            "initialisation",
            "when"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given that we are using true word boundaries, lexicon imperfections stem\nfrom clustering methods that fail to group instances of the same word together and/or representations that fail to capture similarity across different instances of the same word.\nIn Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> we present controlled experiments on LibriSpeech test-clean where we idealise either the clustering method or representations in two representative systems.\nThe baseline results are from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next we consider the case where representations are ideal and clustering is performed as usual.\nFor continuous features (top section), each word type is represented by its mean acoustic word embedding with a small amount of random noise added.\nFor the discrete unit sequences (bottom), a single sequence is randomly selected to represent all instances of that word type.\nResults are given in the third row of each section in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5 Representations vs Clustering &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe see that, in both cases, a perfect lexicon is produced with word-level purities and V-measures of 100%.\nIn both cases, NED is worse than the baselines, dropping from roughly 8% to 12%.\nThis is because, although all of the same words are clustered together, some of their phonetic transcriptions (used to calculate NED) differ due to differences in how they are produced.\nThis highlights the danger of using NED as the only metric for lexicon evaluation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels.\nDespite progress, the induced lexicons are still far from perfect.\nIn an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types.\nWe combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means, hierarchical, graph-based) on English and Mandarin data.\nThe best system uses graph clustering with dynamic time warping on continuous features.\nFaster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences.\nThrough controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type&#8212;rather than clustering&#8212;is the primary factor limiting performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "system",
                    "continuous",
                    "edit",
                    "word",
                    "methods",
                    "representations",
                    "discrete",
                    "perfect",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nword segmentation, word discovery, lexicon learning, zero-resource speech processing, unsupervised learning</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "lexicon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unsupervised word discovery aims to discover word-like units from unlabelled speech by locating word boundaries and clustering the resulting segments into hypothesised lexical categories&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis is difficult because, unlike text, speech is continuous and lacks explicit word delimiters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nMoreover, different instances of the same word exhibit substantial acoustic variability, even when produced by the same speaker.\nDespite these challenges, human infants can discriminate between words in their native language within their first year </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDeveloping computational models that mimic this ability could therefore both inform theories of language acquisition </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and support the development of low-resource speech technology </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "word",
                    "clustering",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Early approaches to unsupervised word discovery sought to identify recurring patterns across speech utterances, often relying on dynamic time warping (DTW)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDespite advances&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, these techniques still struggle to segment the entire input into meaningful units.\nTo address this, full-coverage methods have been proposed which aim to tokenise all of the input speech into word-like units&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Full-coverage systems can typically be broken into three components: an unsupervised word boundary detection component, a feature extraction method for representing word-like segments, and a clustering method used to group the hypothesised segments into a lexicon of word-like types.\nTo assess the influence of boundary detection, Malan et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> performed word discovery using ground-truth word boundaries.\nEven in this idealised setting, the resulting lexicon is still far from perfect.\nSince true boundaries were used, the errors stem either from the representation or clustering method.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "clustering",
                    "lexicon",
                    "perfect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper investigates these limitations by revisiting the task of unsupervised lexicon learning, where unlabelled word segments with true boundaries need to be clustered into hypothesised word types.\nWe combine a range of representations from self-supervised learned (SSL) speech models with different clustering approaches.\nWe consider both discrete and continuous frame-level features.\nThese are either kept in sequence form to represent word segments, with distances between sequences used for clustering, or averaged to obtain fixed-dimensional acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor clustering, we consider </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, agglomerative, BIRCH, and graph clustering.\nIn experiments on English and Mandarin, our best system uses graph clustering with DTW over continuous sequences.\nWhile producing a better lexicon than previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the result remains imperfect, with word-level purity below 90%.\nThis system is also much slower than those using averaged embeddings or discrete sequences.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "system",
                    "continuous",
                    "lexicon",
                    "word",
                    "representations",
                    "discrete",
                    "embeddings",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To definitively answer whether it is the representations or clustering that limits performance, we conduct two controlled experiments.\nFirst, we reduce variability in clustering by forcing all instances of a word into the same cluster at initialisation.\nSecond, we eliminate representational inconsistency by artificially fixing all instances of the same word type to near-identical representations.\nThis allows the influence of speech representations and clustering methods to be assessed individually.\nThe results show that, despite substantial improvements, current speech representations, not clustering methods, constrain lexicon quality.\nThus, further research on SSL features is needed to make progress on unsupervised word discovery.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "quality",
                    "cluster",
                    "lexicon",
                    "word",
                    "methods",
                    "representations",
                    "initialisation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We investigate several combinations of speech representation and clustering methods to understand the current landscape of unsupervised lexicon learning.\nThe systems are shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nApart from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who implicitly looked at lexicon learning, work on this task is limited. One exception is&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which applied different clustering approaches on acoustic word embeddings. But this was done using conventional features in a time before neural representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "lexicon",
                    "word",
                    "methods",
                    "representations",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given ground truth word segments (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-a), we start by extracting continuous features from intermediate layers of SSL speech models (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i).\nWe consider a range of SSL models, all trained using a masked prediction pretext task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe different SSL models are described in detail in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAs an alternative to continuous features, discretised representations are obtained by quantising continuous features, producing a lower-bitrate encoding which reduces downstream computational load (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii).\nPrevious work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showed that quantisation can remove speaker-specific information from continuous features, potentially improving consistency in representations used for lexicon learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "word",
                    "lexicon",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While some clustering approaches can be applied directly to feature sequences, others require word segments to be represented as single fixed-dimensional vectors.\nA common approach is to use an acoustic word embedding obtained\nby averaging the features of a word segment across the temporal dimension&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhile simple, several studies have shown that averaging appropriate SSL features results in\nrobust acoustic word embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib30\" title=\"\">30</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "clustering",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We couple the different representations with a range of clustering methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c), resulting in six lexicon learning systems.\nThe first four cluster averaged acoustic word embeddings using one of the following methods (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c): </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering; BIRCH clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an efficient tree-based algorithm; agglomerative hierarchical clustering&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; or graph clustering, where word segments are represented as nodes and edges are weighted by the cosine similarity between their embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "cluster",
                    "lexicon",
                    "word",
                    "methods",
                    "representations",
                    "embeddings",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The two remaining systems also use graph clustering, but operate on feature sequences, retaining temporal information.\nThe distance metric used for graph construction corresponds to the type of representations.\nFor continuous features, pairwise distances are computed using DTW, which aligns sequences of varying lengths and calculates the cost of the optimal alignment (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-i to Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor discrete unit sequences, edit distance is used, which measures the minimum number of operations required to convert one sequence into another (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-b-ii to Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-c-iii).\nFor all graph clustering systems, a graph is incrementally constructed with edges inserted between nodes when the similarity exceeds a predefined threshold.\nThe graph is then partitioned using the efficient Leiden algorithm </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with the constant Potts model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as clustering objective.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "continuous",
                    "edit",
                    "discrete",
                    "representations",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use LibriSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dev-clean for development and test-clean for final evaluations.\nEach set contains 5.4 hours of English speech from 40 speakers.\nWord boundaries are obtained from forced alignments&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the Mandarin evaluation, we use data from Track&#160;2 of the ZeroSpeech challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, consisting of 2.5 hours of speech from 12 speakers with word alignments.\nFor the Mandarin experiments, the optimal hyperparameters from the English development experiments are retained as is.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "testclean",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate lexicon quality using a range of standard metrics.\nNormalised edit distance&#160;(NED)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures the phonetic purity of clusters by calculating the length-normalised edit distance between phonetic transcriptions (from forced alignments) of all word segment pairs within a cluster.\nA single average over all clusters gives the final NED; lower is better.\nPurity measures how homogeneous each cluster is with respect to the true word labels.\nFor every cluster, the most frequent word type is identified, and the number of segments belonging to that type is counted.\nThese are summed over all clusters and divided by the total number of segments in&#160;the&#160;dataset.\nV-measure&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the harmonic mean of purity (also called homogeneity) and completeness, where the latter measures the degree to&#160;which segments of the same type are assigned to the same cluster.\nHigher purity and V-measure are better.\nBitrate is the average data rate of&#160;the encoded output, measured in bits per second (bits/s); lower is better.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "quality",
                    "edit",
                    "cluster",
                    "lexicon",
                    "vmeasure",
                    "word",
                    "ned",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Purity and NED can be artificially improved by increasing the number of clusters.\nTo enable a fair comparison between methods, and since we are operating in an idealised setting, we fix the number of clusters to the true number of word types in each dataset.\nThe number of clusters for LibriSpeech dev-clean and test-clean is 8,216 and 8,006, respectively, while for Mandarin it is 8,871.\nGraph clustering does not take the number of clusters as an explicit input but instead infers it from the data; we tune the hyperparameters to get approximately the desired number of clusters.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "librispeech",
                    "word",
                    "testclean",
                    "ned",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare speech representations from a range of SSL models, all producing 20 ms frames.\nTo test state-of-the-art features&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use HuBERT Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nA HuBERT Base model fine-tuned for voice-conversion, HuBERT Soft&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, is also considered for its ability to preserve phonetic content while suppressing speaker-specific information: its continuous features are extracted from a linear projection layer that predicts a distribution over discrete units, thereby providing a middle-ground between continuous and discrete features.\nTo assess the effects of pre-training language, we use the multilingual HuBERT (mHuBERT)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pre-trained on 90k hours across 147 languages (including English and Mandarin).\nFor the Mandarin tests, we include a HuBERT Large model pre-trained on 10k hours of Mandarin.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/TencentGameMate/chinese-hubert-large</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFor all large SSL models, 1,024-dimensional features are extracted from the 21st layer based on its ability to capture word information&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFollowing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 768-dimensional mHuBERT features are extracted from the eighth&#160;layer.</span>\n</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "word",
                    "discrete",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All these features are subsequently mean and variance normalised, and then projected to 350 dimensions using principal component analysis (PCA).\nThis gave the best performance on dev-clean, and improved on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where 250 dimensions were used.\nFor HuBERT Soft, features from the 12th layer are pushed through the projection layer, resulting in 256-dimensional soft features, which are mean and variance normalised.\nFor each SSL model, acoustic word embeddings are obtained by averaging the different low-dimensional features and then normalising to the unit sphere.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Discrete features are obtained by applying </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustering with 500 clusters on the different SSL representations (without normalisation or PCA).\nThese unit extraction models are trained on 50 hours of audio from LibriSpeech train-clean for English data, or on the complete dataset for the Mandarin data.\nFollowing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib41\" title=\"\">41</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use duration-penalised dynamic programming to smooth the resulting sequences by discouraging rapid unit changes.\nThe unit sequences are not deduplicated, which worked better in development experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "librispeech",
                    "discrete",
                    "representations",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first describe the clustering methods that learn a lexicon on averaged acoustic word embeddings.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means, we use the efficient FAISS library.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/facebookresearch/faiss</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nBut unlike&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we switch the default random initialisation to </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means++ initialisation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis selects the first centroid at random, with each subsequent centroid chosen from the remaining data points with probability proportional to the squared distance from the closest existing centroid.\nThis alternative initialisation gave large improvements over&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on development data.\nBIRCH and agglomerative clustering are implemented using scikit-learn.</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://scikit-learn.org</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe apply BIRCH with a distance threshold of 0.25.\nAgglomerative clustering uses Ward linkage&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which merges clusters by attempting to minimise the total within-cluster variance.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "lexicon",
                    "word",
                    "methods",
                    "initialisation",
                    "embeddings",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Graph clustering can be used with either averaged acoustic embeddings or feature sequences.\nWe use the efficient igraph library </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDistance-based thresholds are applied globally: nodes with no edges remain isolated. Thresholds are tuned based on memory constraints and development performance, with values of 0.65 for edit distance graphs, 0.4 for cosine distance graphs, and 0.35 for DTW graphs.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "isolated",
                    "edit",
                    "embeddings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Before comparing the various combinations of representations and clustering methods, we start by selecting the best-performing SSL features.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports English development scores using two representative systems: continuous averaged features which are </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means clustered (top), and discrete unit sequences which are graph clustered using edit distance (bottom).</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "continuous",
                    "edit",
                    "methods",
                    "representations",
                    "discrete",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Systems using WavLM Large features perform well, especially in its continuous form (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 A First Evaluation: Representations &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-top) where it outperforms all other models.\nIn the discrete graph clustering system, HuBERT Large also performs well.\nIn contrast, mHuBERT generally results in worse lexicons, indicating that diluting English pre-training data with other languages worsens performance compared to using English-only models.\nAlthough HuBERT Soft is competitive in the continuous setting, the discretised version (bottom) performs much worse.\nBased on these results, we use WavLM Large features for the subsequent lexicon learning experiments on English data.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "clustering",
                    "system",
                    "continuous",
                    "lexicon",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the lexicon learning performance on English test data of the different representation&#8211;clustering systems using WavLM Large features (continuous or discrete).\nContinuous averaged features with graph clustering achieves the best purity (89.6%), V-measure (90.3%), and bitrate (35.6).\nThe continuous features with DTW and graph clustering yield the lowest NED (5.2%), but at a substantial computational cost, being around 250 times slower than alternative systems.\nThese results all improve on the continuous averaged </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means approach from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which achieves a NED of 17.3%, purity of 78.2%, V-measure of 80.0%, and a bitrate of 41.4 (after altering the experimental setup to exactly match the protocol here).</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "continuous",
                    "lexicon",
                    "vmeasure",
                    "ned",
                    "discrete",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In general, continuous representations outperform discrete units on purity and NED, with the discrete graph system only being competitive in V-measure (88.4%).\nOf the different clustering methods, graph-based clustering consistently yields higher V-measure scores than simpler methods like </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means or agglomerative clustering.\nBitrate indicates that graph clustering produces a more compact encoding output, especially when used with the averaged embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "system",
                    "continuous",
                    "vmeasure",
                    "ned",
                    "representations",
                    "methods",
                    "discrete",
                    "when",
                    "embeddings",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although all systems deliver competitive lexicons and improve on the results from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, none of them achieve a perfect lexicon, even when provided with a ground-truth word segmentation: NED shows that the clustered sequences still have phonetic differences, and none of the systems achieve perfect purity or completeness.\nBefore we investigate what limits performance, we see if this is also true on another language, Mandarin.\nApart from being in a different language family, the SSL representations available in Mandarin are also different from those available in English.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "lexicon",
                    "word",
                    "ned",
                    "representations",
                    "when",
                    "perfect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows how well our systems developed on English data generalise to Mandarin data.\nWe also evaluate the effect of an SSL model&#8217;s pre-training language by testing with models trained on different amounts of Mandarin data.\nConcretely, we compare the English-only WavLM Large&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the multilingual mHuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> including limited Mandarin data, and the Mandarin HuBERT Large trained entirely on Mandarin (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S3\" style=\"font-size:90%;\" title=\"3 Experimental Setup &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for details).\nWe show results for the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system representative of previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with the two systems that worked best on English (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "effect",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Across the different systems, the English-only WavLM performs worst, the mHuBERT being exposed to some Mandarin gives intermediate results, and the Mandarin HuBERT performs the best by a substantial margin.\nIn particular, Mandarin HuBERT features achieve a NED of 4.9% when averaged and graph clustered.\nThis results in a lexicon of comparable quality to the best English lexicon (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Evaluation on English: Representations with Clustering &#8227; 4 Experimental Results &#8227; Unsupervised Lexicon Learning from Speech is Limited by Representations Rather Than Clustering\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and shows that including more of the target language in pre-training improves lexicon quality.\nThis underscores the importance of language-specific SSL representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09225v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "graph",
                    "quality",
                    "lexicon",
                    "ned",
                    "representations",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although we show that quality lexicons can be obtained on a different language, the results here corroborate those on English where even the strongest systems produce lexicons with a NED of around 5% and word-level purity of between 80% and 90%.\nBelow we examine the source of this limitation.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "ned",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the second row of each section in the table, the two clustering algorithms are initialised so that all instances of the same word type are assigned to the same cluster.\nFor </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means (top section), this means that centroids are initialised at the mean embeddings of segments corresponding to the same word type.\nFor graph clustering (bottom), segments are initially assigned to clusters based on their true word types.\nAt initialisation, both systems have a purity and V-measure of 100%.\nHowever, after the methods are run from their perfect starting points, the resulting lexicons are far from ideal: purity and V-measure drop to baseline levels.\nFrom the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means system we conclude that embeddings of the same word type are not close to all the other instances of that word.\nSimilarly, from the discrete system we see that unit sequences are variable across instances of the same word type.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "system",
                    "cluster",
                    "vmeasure",
                    "word",
                    "embeddings",
                    "baseline",
                    "methods",
                    "discrete",
                    "initialisation",
                    "perfect",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Pronunciation and co-articulation not only affects NED but also lead to differences in the representations, which in part explains the poor performance when initialising the clustering methods perfectly (the second rows in the two sections).\nBut in the </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-means systems we see that perfect initialisation actually leads to worse performance than the non-ideal baseline, e.g. purity dropping from 88.4% to 81.5%.\nTogether these findings suggest that clustering methods, particularly graph clustering, perform robustly, but inconsistency in the representation of spoken words hinder effective lexicon learning.\nIf consistent representations aligned with word types were available, perfect word-level purity and V-measure would be achievable.</span>\n</p>\n\n",
                "matched_terms": [
                    "purity",
                    "clustering",
                    "graph",
                    "lexicon",
                    "vmeasure",
                    "word",
                    "initialisation",
                    "baseline",
                    "ned",
                    "representations",
                    "methods",
                    "when",
                    "perfect",
                    "kkmeans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper compared six systems for lexicon learning that combine different feature representations and clustering approaches, achieving clear improvements over previous methods.\nThese results summarise the current landscape of lexicon learning in terms of accuracy and computational cost.\nIn controlled experiments, we showed that if representations are idealised, existing\nclustering methods can learn perfect lexicons.\nThe current bottleneck is therefore not in clustering, but in the consistency of feature representations.\nFuture work will explore how these insights can be applied to the training of self-supervised speech models to improve the resulting representations.\nWe will also consider how our best systems can be extended for true word discovery, where boundaries are&#160;unknown.</span>\n</p>\n\n",
                "matched_terms": [
                    "clustering",
                    "lexicon",
                    "word",
                    "methods",
                    "representations",
                    "perfect"
                ]
            }
        ]
    }
}