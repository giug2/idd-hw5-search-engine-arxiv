{
    "S4.T1": {
        "caption": "Table 1: Primary Outcome Measures - Descriptive Statistics (N=40). All results are significant (p<0.001p<0.001) via one-sample t-tests against a neutral midpoint of 3.0.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Measure</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Mean (M)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Std. Dev. (SD)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RQ1: Healing Effects</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Positive mood impact</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.89</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RQ2: System Responsiveness</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Perceived emotion accuracy</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RQ3: User Experience</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Overall atmosphere</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RQ4: Content Quality</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multimodal coherence</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.76</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "positive",
            "significant",
            "content",
            "atmosphere",
            "rq2",
            "perceived",
            "rq3",
            "experience",
            "overall",
            "healing",
            "mood",
            "measures",
            "against",
            "user",
            "statistics",
            "primary",
            "n40",
            "measure",
            "system",
            "outcome",
            "accuracy",
            "onesample",
            "ttests",
            "rq1",
            "rq4",
            "dev",
            "mean",
            "results",
            "midpoint",
            "responsiveness",
            "via",
            "impact",
            "emotion",
            "multimodal",
            "coherence",
            "p0001p0001",
            "neutral",
            "all",
            "std",
            "quality",
            "effects",
            "descriptive"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To provide a qualitative illustration of the system&#8217;s personalization capabilities, Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S3.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 3.2 Emotion-Music Knowledge Graph &#8227; 3 THE PROPOSED EMOHEAL SYSTEM &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showcases how two distinct emotional inputs result in markedly different audiovisual outputs. The primary quantitative results, summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Results &#8227; 4 EVALUATION &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Results &#8227; 4 EVALUATION &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, confirm the system&#8217;s effectiveness. It demonstrated a significant positive impact on user mood (M=4.12, </span>\n  <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.001</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and was perceived as highly responsive to their described emotions (M=4.05, </span>\n  <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.001</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The high ratings for overall atmosphere and multimodal coherence (both M=4.18) further suggest a high-quality user experience. These findings are further supported by the rating distributions, where 85.0% of participants rated the emotion accuracy and 87.5% rated their mood improvement as &#8220;Agree&#8221; (Score 4) or &#8220;Strongly Agree&#8221; (Score 5). A notable 78% also explicitly mentioned feeling &#8220;understood&#8221; by the system&#8217;s personalized response.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, pre-sleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and &#8220;one-size-fits-all&#8221;, failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, maping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLaMP3 model to guide users from their current state toward a calmer one (&#8220;match-guide-target&#8221;). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) and high perceived emotion recognition accuracy (M=4.05, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). A strong correlation between perceived accuracy and therapeutic outcome (<math alttext=\"r=0.72\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.72</mn></mrow><annotation encoding=\"application/x-tex\">r=0.72</annotation></semantics></math>, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) validates our fine-grained approach. These findings establishes the viability of theory-driven, emotion-aware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.</span>\n</p>\n\n",
                "matched_terms": [
                    "mood",
                    "significant",
                    "content",
                    "user",
                    "p0001p0001",
                    "n40",
                    "perceived",
                    "accuracy",
                    "emotion",
                    "system",
                    "outcome",
                    "effects",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nDigital Wellness, Emotion Recognition, Music Healing, Multimodal Content Retrieval, Personalized Intervention</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "content",
                    "healing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these limitations, we propose EmoHeal, a novel system that creates personalized supportive journeys by guiding users through a three-stage &#8220;match-guide-target&#8221; narrative that operationalizes the iso-principle. It integrates three core components: a fine-tuned XLM-RoBERTa model for fine-grained emotion recognition, a knowledge graph to map emotions to musical parameters, and a CLaMP3 model for real-time content retrieval. Our contributions are threefold: (1) a novel end-to-end system integrating these state-of-the-art AI technologies; (2) a computational blueprint for operationalizing music therapy principles; and (3) an empirical validation through a 40-participant user study demonstrating the efficacy of our theory-driven, personalized approach.</span>\n</p>\n\n",
                "matched_terms": [
                    "content",
                    "user",
                    "emotion",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The second pillar is </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">computational music therapy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Our work is grounded in psychological frameworks like GEMS, which defines music-specific emotional dimensions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and long-standing therapeutic theories such as the iso-principle </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Neuroscience research, showing that music engages unique emotional processing pathways in the brain </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, further motivates music&#8217;s use as a therapeutic modality. The primary challenge here is computationally operationalizing these complex, dynamic principles into an automated system.</span>\n</p>\n\n",
                "matched_terms": [
                    "primary",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For content delivery, we build on advances in multimodal retrieval. Early audio&#8211;language models such as MuLan and CLAP established strong text&#8211;audio alignment&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while some recent systems like CLaMP3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further improved scalability and retrieval accuracy. More recently, large language models (LLMs) have been extended to audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, showing promise as general-purpose learners, but their heavy computational demands restrict deployment in resource-constrained settings. In parallel, generative models such as MusicLM can synthesize novel music&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, though their potential in wellness applications remains largely untapped. LLM-driven audio generators&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adapt to task-specific needs by integrating specialist models, yet they face the same efficiency challenges as other LLM-based systems. In contrast, our retrieval-based strategy ensures high-fidelity, professionally produced audiovisual content while remaining computationally efficient&#8212;an essential requirement for wellness applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "content",
                    "accuracy",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Finally, the user experience is informed by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Human-Computer Interaction (HCI)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We apply concepts of &#8220;Calm Technology&#8221; </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and established guidelines for digital wellness tools </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, creating a sense of &#8220;presence&#8221; and immersion is known to enhance the efficacy of virtual environments </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a principle we aimed to incorporate. A key research gap remains in designing for users in vulnerable emotional states, where traditional usability heuristics may not apply.</span>\n</p>\n\n",
                "matched_terms": [
                    "user",
                    "experience"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our system, EmoHeal, is an end-to-end pipeline that generates personalized supportive music experiences by processing user text input into a final audio-visual presentation.</span>\n</p>\n\n",
                "matched_terms": [
                    "user",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To capture nuanced emotional states, we selected XLM-RoBERTa-base as our backbone network. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2 Related Work &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a linear classification head (</span>\n  <math alttext=\"768\\times 27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">768</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">27</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">768\\times 27</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) is added on top of the model&#8217;s [CLS] token representation to predict a 27-dimensional probability vector, </span>\n  <math alttext=\"e\\in\\mathbb{R}^{27}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">27</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">e\\in\\mathbb{R}^{27}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, corresponding to the 27 fine-grained emotion classes defined by the GoEmotions dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For this task, the pre-trained XLM-ROBERTa-base model was fine-tuned on a custom multi-corpus dataset. This dataset combines two sources: the foundational English GoEmotions dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which defines our 27 fine-grained emotion labels, and the NLPCC-2014 Chinese Emotion Analysis Dataset. To align the label spaces, as the NLPCC-2014 dataset uses a different set of coarser emotion labels, we developed a programmatic mapping strategy. For instance, the coarse label &#8216;joy&#8217; from the Chinese dataset was heuristically mapped to a multi-label vector where the corresponding fine-grained GoEmotions labels for &#8216;joy&#8217;, &#8216;amusement&#8217;, and &#8216;excitement&#8217; were all activated (set to 1), while all other labels were set to 0. The training utilized the Focal Loss function, formulated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This module translates the emotion vector </span>\n  <math alttext=\"e\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">e</mi>\n      <annotation encoding=\"application/x-tex\">e</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into a set of six musical parameters </span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, including tempo (continuous, e.g., 60-120 BPM), mode (e.g., major/minor), timbre (e.g., bright/dark), harmony (e.g., consonant/dissonant), register (e.g., high/low), and density (e.g., sparse/dense). The logic, grounded in music psychology </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, is operationalized via a two-tier inference system shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.1 Fine-Grained Emotion Computation &#8227; 3 THE PROPOSED EMOHEAL SYSTEM &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The first tier handles unambiguous emotions, defined as cases where a single primary emotion score exceeds a high-intensity threshold, which was set empirically to </span>\n  <math alttext=\"\\tau=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.7</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tau=0.7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> based on pilot testing. For these cases, an expert-curated rule is triggered to provide a rapid, consistent response, following the form:</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "primary",
                    "via",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The real-time retrieval process is enabled by the CLaMP3 model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, whose jointly trained text and audio encoders map semantically similar content to nearby vectors in a shared multimodal embedding space, thus allowing for direct text-to-audio similarity comparison. During a user session, as detailed in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3 THE PROPOSED EMOHEAL SYSTEM &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the musical parameters </span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the knowledge graph are first converted into a descriptive natural language prompt using a template-based function. This prompt is then fed into the CLaMP3 text encoder to generate a query text embedding. This query is used to perform a cosine similarity search against the pre-indexed audio embeddings, and the top-3 most similar videos are retrieved for the user.</span>\n</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "content",
                    "against",
                    "user",
                    "descriptive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The system was implemented as a web application</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>A project showcase, including video demonstrations, is available at: [https://jeweled-scarer-812.notion.site/EmoHeal-Project-Showcase-27278a073579807b8e82e5fac88b821f]</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The front-end, designed for a responsive and dynamic user experience, was built with HTML5, CSS3, and JavaScript (ES6+), utilizing the Bootstrap framework for its UI components. The back-end consists of a Python-based REST API, developed with the Flask framework, which serves the core machine learning models. The interface design follows &#8221;Calm Technology&#8221; principles </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, employing a dark theme and a &#8221;progressive disclosure&#8221; model to minimize cognitive load and protect user privacy by using all data ephemerally.</span>\n</p>\n\n",
                "matched_terms": [
                    "user",
                    "all",
                    "experience",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A total of 40 participants were recruited for the within-subjects study. The cohort consisted primarily of students from Queen Mary University of London, recruited using a snowball sampling method through university-affiliated WhatsApp and WeChat student groups. The demographics of the participants are as follows: the mean age was 26.2 years (SD=4.8), with a gender distribution of 22 female and 18 male. Regarding language proficiency, 60.0% reported English as their primary language, while 40.0% reported Chinese. As this was a non-clinical study, inclusion criteria required participants to have no severe mental health conditions requiring clinical intervention.</span>\n</p>\n\n",
                "matched_terms": [
                    "mean",
                    "primary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The experimental procedure was straightforward. Each participant provided a text description of their current mood, watched the 3-minute therapeutic video generated in real-time by the system, and subsequently completed a post-session questionnaire. Primary outcomes were assessed using a 5-point Likert scale (1=Strongly Disagree, 5=Strongly Agree), where participants rated their agreement with statements such as, &#8220;The generated video accurately reflected the emotions I described in my text&#8221; (System Responsiveness) and &#8220;Watching the video had a positive impact on my mood&#8221; (Supportive Effect). We also collected qualitative feedback through open-ended questions.</span>\n</p>\n\n",
                "matched_terms": [
                    "impact",
                    "mood",
                    "positive",
                    "primary",
                    "system",
                    "responsiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our key finding, detailed in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Results &#8227; 4 EVALUATION &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, is the strong, positive correlation between the perceived accuracy of emotion recognition and the subsequent mood improvement (</span>\n  <math alttext=\"r=0.72,p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.72</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mo mathsize=\"0.900em\">&lt;</mo>\n          <mn mathsize=\"0.900em\">0.001</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=0.72,p&lt;0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). This statistically validates our entire fine-grained approach, providing the empirical backbone for the following discussion.</span>\n</p>\n\n",
                "matched_terms": [
                    "mood",
                    "positive",
                    "perceived",
                    "accuracy",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work introduced </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EmoHeal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an AI system that translates fine-grained emotional states from text into personalized, theory-driven musical experiences. Results show that moving beyond coarse emotion models is essential: recognition accuracy strongly correlated with therapeutic efficacy (</span>\n  <math alttext=\"r=0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.72</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=0.72</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), underscoring the role of nuanced recognition in fostering a sense of being &#8220;understood.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "emotion",
                    "results",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Overall, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EmoHeal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrates that fine-grained emotion recognition coupled with hybrid AI can deliver measurable therapeutic benefits and provide a scalable blueprint for digital wellness tools.</span>\n</p>\n\n",
                "matched_terms": [
                    "overall",
                    "emotion"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Correlation Analysis.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:156.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variable Pair</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">r</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">p-value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:156.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emotion accuracy </span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> Mood improvement</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&lt;</mo><mn mathsize=\"0.900em\">0.001</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.001</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:156.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text length </span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> Emotion accuracy</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><math alttext=\"&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&lt;</mo><mn mathsize=\"0.900em\">0.05</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.05</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pvalue",
            "correlation",
            "mood",
            "Ã—times",
            "improvement",
            "pair",
            "analysis",
            "length",
            "variable",
            "accuracy",
            "text",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To provide a qualitative illustration of the system&#8217;s personalization capabilities, Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S3.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 3.2 Emotion-Music Knowledge Graph &#8227; 3 THE PROPOSED EMOHEAL SYSTEM &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> showcases how two distinct emotional inputs result in markedly different audiovisual outputs. The primary quantitative results, summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Results &#8227; 4 EVALUATION &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Results &#8227; 4 EVALUATION &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, confirm the system&#8217;s effectiveness. It demonstrated a significant positive impact on user mood (M=4.12, </span>\n  <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.001</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and was perceived as highly responsive to their described emotions (M=4.05, </span>\n  <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.001</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The high ratings for overall atmosphere and multimodal coherence (both M=4.18) further suggest a high-quality user experience. These findings are further supported by the rating distributions, where 85.0% of participants rated the emotion accuracy and 87.5% rated their mood improvement as &#8220;Agree&#8221; (Score 4) or &#8220;Strongly Agree&#8221; (Score 5). A notable 78% also explicitly mentioned feeling &#8220;understood&#8221; by the system&#8217;s personalized response.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our key finding, detailed in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Results &#8227; 4 EVALUATION &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, is the strong, positive correlation between the perceived accuracy of emotion recognition and the subsequent mood improvement (</span>\n  <math alttext=\"r=0.72,p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.72</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mo mathsize=\"0.900em\">&lt;</mo>\n          <mn mathsize=\"0.900em\">0.001</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=0.72,p&lt;0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). This statistically validates our entire fine-grained approach, providing the empirical backbone for the following discussion.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, pre-sleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and &#8220;one-size-fits-all&#8221;, failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, maping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLaMP3 model to guide users from their current state toward a calmer one (&#8220;match-guide-target&#8221;). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) and high perceived emotion recognition accuracy (M=4.05, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). A strong correlation between perceived accuracy and therapeutic outcome (<math alttext=\"r=0.72\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.72</mn></mrow><annotation encoding=\"application/x-tex\">r=0.72</annotation></semantics></math>, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) validates our fine-grained approach. These findings establishes the viability of theory-driven, emotion-aware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.</span>\n</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "mood",
                    "improvement",
                    "accuracy",
                    "text",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our work integrates advancements from four key domains. First, in fine-grained emotion recognition, some advanced approaches such as XLM-RoBERTa </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have provided powerful language representations capable of capturing subtle affective cues. Together with large-scale benchmarks like GoEmotions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, these developments have enabled a shift beyond coarse sentiment analysis toward more nuanced emotion understanding. These models have proven effective in complex domains, including health-related online contexts </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, a key challenge remains in applying these models, often trained on public social media data, to the more private and nuanced characteristic of wellness setting </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To capture nuanced emotional states, we selected XLM-RoBERTa-base as our backbone network. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2 Related Work &#8227; EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-Grained Emotions\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a linear classification head (</span>\n  <math alttext=\"768\\times 27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">768</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">27</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">768\\times 27</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) is added on top of the model&#8217;s [CLS] token representation to predict a 27-dimensional probability vector, </span>\n  <math alttext=\"e\\in\\mathbb{R}^{27}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mn mathsize=\"0.900em\">27</mn>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">e\\in\\mathbb{R}^{27}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, corresponding to the 27 fine-grained emotion classes defined by the GoEmotions dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For this task, the pre-trained XLM-ROBERTa-base model was fine-tuned on a custom multi-corpus dataset. This dataset combines two sources: the foundational English GoEmotions dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15986v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which defines our 27 fine-grained emotion labels, and the NLPCC-2014 Chinese Emotion Analysis Dataset. To align the label spaces, as the NLPCC-2014 dataset uses a different set of coarser emotion labels, we developed a programmatic mapping strategy. For instance, the coarse label &#8216;joy&#8217; from the Chinese dataset was heuristically mapped to a multi-label vector where the corresponding fine-grained GoEmotions labels for &#8216;joy&#8217;, &#8216;amusement&#8217;, and &#8216;excitement&#8217; were all activated (set to 1), while all other labels were set to 0. The training utilized the Focal Loss function, formulated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The experimental procedure was straightforward. Each participant provided a text description of their current mood, watched the 3-minute therapeutic video generated in real-time by the system, and subsequently completed a post-session questionnaire. Primary outcomes were assessed using a 5-point Likert scale (1=Strongly Disagree, 5=Strongly Agree), where participants rated their agreement with statements such as, &#8220;The generated video accurately reflected the emotions I described in my text&#8221; (System Responsiveness) and &#8220;Watching the video had a positive impact on my mood&#8221; (Supportive Effect). We also collected qualitative feedback through open-ended questions.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "mood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work introduced </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EmoHeal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an AI system that translates fine-grained emotional states from text into personalized, theory-driven musical experiences. Results show that moving beyond coarse emotion models is essential: recognition accuracy strongly correlated with therapeutic efficacy (</span>\n  <math alttext=\"r=0.72\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.72</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=0.72</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), underscoring the role of nuanced recognition in fostering a sense of being &#8220;understood.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "text",
                    "emotion"
                ]
            }
        ]
    }
}