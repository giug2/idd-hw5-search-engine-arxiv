{
    "S6.T1": {
        "caption": "Table 1: Comparison of MOS and SMOS for different systems. BELLE-stream is fixed to a single speaker timbre and does not use an audio prompt,\nhence SMOS is not reported.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Ground Truth</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MaskGCT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">F5-TTS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MELLE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BELLE</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BELLE-stream</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MOS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.21</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">4.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">SMOS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.13</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_border_bb\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "does",
            "hence",
            "smos",
            "single",
            "not",
            "comparison",
            "mos",
            "systems",
            "audio",
            "speaker",
            "reported",
            "system",
            "belle",
            "truth",
            "melle",
            "timbre",
            "maskgct",
            "f5tts",
            "fixed",
            "use",
            "prompt",
            "bellestream",
            "different",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, BELLE, F5-TTS, and Ground Truth achieve MOS scores of around 4.2, indicating that the audio produced by both F5-TTS and BELLE is close to that of natural human speech. F5-TTS attains a slightly higher MOS, which can be attributed to its lower background noise &#8212; human raters tend to prefer cleaner audio. In contrast, the real speech in Ground Truth inherently contains a certain level of noise, and BELLE replicates this natural background characteristic, leading to MOS scores that are very close to the Ground Truth.\nRegarding SMOS, it is worth noting that in the LibriSpeech dataset, different utterances from the same speaker sometimes exhibit perceptible timbre variation, which results in a relatively lower SMOS score for Ground Truth. BELLE achieves the highest speaker similarity among all tested systems. Notably, while both F5&#8209;TTS and MaskGCT were trained on datasets comprising 50,000 hours of English speech, BELLE was trained on less than 5,000 hours of data, yet it attained competitive, and in some instances superior performance. These results underscore the efficacy of the proposed Bayesian sampling strategy and the multi&#8209;teacher learning framework.</p>\n\n",
            "<p class=\"ltx_p\">Since BELLE-stream is finetuned to a fixed speaker timbre, it does not accept an audio prompt; therefore, it is excluded from SMOS scoring in the subjective evaluation and SIM computation in the objective evaluation, and its results are reported only under the Cross-sentence setting. BELLE-stream only takes the target text to be synthesized as input. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, streaming generation does not cause significant degradation in speech naturalness, achieving higher MOS scores than non&#8209;streaming MELLE. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE-stream achieves the lowest WER among streaming TTS systems, with no appreciable increase in WER compared to BELLE. In our tests, BELLE-stream achieved a real-time factor (RTF)&#8212;defined as the ratio of audio generation time to the length of the generated audio&#8212;of 0.55 when producing a 10-second utterance. Given that BELLE-stream operates with 0.8-second chunks, this implies a first-packet latency (FPL) of only 440ms, demonstrating its effective balance between performance and latency.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper-distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://belletts.github.io/Belle/\" title=\"\">https://belletts.github.io/Belle/</a>.\nThe code, checkpoints, and synthetic data will be released after the paper is accepted.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "audio",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, autoregressive (AR) next-token prediction models based on discrete audio codecs have gained significant attention in the audio generation and text-to-speech (TTS) communities, driven by the success of models such as AudioLM <cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib4\" title=\"\">2023</a>)</cite> and VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite>, among others. However, continuous-valued prediction has emerged as a promising alternative due to its ability to eliminate quantization errors introduced by token-based codecs <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>; Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib8\" title=\"\">2024b</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>; Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib25\" title=\"\">2025</a>)</cite>.\nGenerative models typically rely on sampling mechanisms to introduce stochasticity and improve synthesis quality. In audio-codec-based AR TTS, this is often achieved using mature top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling techniques, similar to those employed in token-based language models (LMs). Diffusion-based models, by contrast, introduce randomness by injecting noise into the input, thereby embedding variability directly into the generation process. However, effective sampling strategies for continuous-valued AR models remain underexplored.\nRecent studies have begun to address this gap. In image generation, MAR proposed a hybrid approach where an AR model predicts conditioning signals, followed by diffusion-based sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib27\" title=\"\">2024a</a>)</cite>. In zero-shot TTS, MELLE introduced Gaussian sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, while FELLE adopted flow-matching techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib30\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite> to enhance sampling efficiency and output diversity.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE: <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS Synthesis&#8212;a continuous-valued AR model that directly predicts mel-spectrograms from text inputs based on the evidential deep learning (EDL) framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Amini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib2\" title=\"\">2020</a>)</cite>. This represents the first application of Bayesian deep learning to TTS, enabling controllable and theoretically grounded sampling at test time.\nSpecifically, BELLE models each predicted mel-spectrogram frame as a sample from a Gaussian distribution, whose parameters themselves are drawn from hyper-distributions. Metaphorically, the model first predicts the parameters of this hyper-distribution, then samples a specific Gaussian distribution from it, and finally generates mel-spectrogram frames from the sampled Gaussian.\nHowever, learning accurate Gaussian posterior estimates from limited data poses a challenge, as typical TTS datasets contain only a single audio recording per text prompt. To overcome this, the training corpus is augmented with synthetic audio samples generated by several publicly available pre-trained zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "audio",
                    "single",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE achieves competitive performance compared with leading open-source TTS systems trained on approximately 50k hours of speech data, while BELLE uses only around 5k hours of data that largely consist of synthetic samples. Additionally, its streaming variant, BELLE-stream, attains an effective balance between audio generation quality and latency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "bellestream",
                    "belle",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Continuous-valued TTS models can be broadly categorized into AR and non-autoregressive (NAR) approaches. AR methods include MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, which generates mel spectrogram frames using decoder-only Transformer-based AR modelling combined with Gaussian sampling. FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite>, ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>)</cite> and DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>)</cite> first generate continuous-valued hidden states autoregressively, then employ flow-matching or diffusion-based modules, such as DiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib39\" title=\"\">2023</a>)</cite>, to sample and produce mel spectrograms or a continuous latent space learned by a VAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Pinheiro&#160;Cinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib40\" title=\"\">2021</a>)</cite>. Moreover, VAE-based AR methods, including GMM-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>)</cite> and KALLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>)</cite>, initially leverage a VAE to estimate a latent distribution (mean and variance), and subsequently autoregressively predict latent representations sampled from this distribution. NAR models such as E2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>)</cite> and F5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, on the other hand, predict mel-spectrograms directly via iterative mask-based refinement. Recently, several streaming TTS systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib54\" title=\"\">2024</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib46\" title=\"\">2025</a>; Sheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib49\" title=\"\">2025b</a>)</cite>. Most of these approaches model discrete features using a language model, followed by a flow-matching module or other similar components to generate speech features. Such architectures are relatively complex and may introduce potentially higher latency.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The regression loss <math alttext=\"\\mathcal{L}_{\\text{reg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>reg</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{reg}}</annotation></semantics></math> ensures predicted mel-spectrograms closely match the ground truth <math alttext=\"\\bm{y}^{\\text{gt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mtext>gt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{\\text{gt}}</annotation></semantics></math>. It includes L1 and L2 terms for both coarse predictions <math alttext=\"\\bm{y}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{(1)}</annotation></semantics></math> and refined predictions <math alttext=\"\\bm{y}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{(2)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram flux loss <math alttext=\"\\mathcal{L}_{\\text{flux}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>flux</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{flux}}</annotation></semantics></math> encourages temporal dynamics, promoting variability between the current predicted distribution location <math alttext=\"\\bm{\\gamma}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#120632;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{\\gamma}_{t}</annotation></semantics></math> and previous-frame ground truth <math alttext=\"\\bm{y}_{t-1}^{\\text{gt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mtext>gt</mtext></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t-1}^{\\text{gt}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset is derived from the Librispeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib38\" title=\"\">2015</a>)</cite> training set and contains audio samples whose duration is from 0.5 second to 14 seconds, creating a final training dataset containing approximately 706 hours of speech. To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, our training data are augmented by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite>, IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite>, F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite>, and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>. Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. Details of the data processing methodology can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS1\" title=\"C.1 Training Data &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "f5tts",
                    "maskgct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE, BELLE-stream and MELLE are trained on the training dataset, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For BELLE-stream, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. It&#8217;s observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre. Details about model configuration and training could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS2\" title=\"C.2 Model Configurations &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS3\" title=\"C.3 Training Details of Main Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.3</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "belle",
                    "audio",
                    "prompt",
                    "timbre",
                    "speaker",
                    "bellestream",
                    "single"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot TTS capabilities of our model is evaluated using the LibriSpeech test-clean subset by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, two inference conditions are considered: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where a reference utterance and its corresponding transcription from a given speaker is used as a prompt, and then the model generates speech for a different sentence.</p>\n\n",
                "matched_terms": [
                    "different",
                    "speaker",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, word error rate (WER) is reported to measure intelligibility and robustness. WER-C and WER-H are evaluated using Conformer<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and HuBERT<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite> based ASR models respectively. Speaker similarity is measured via cosine similarity of extracted speaker embeddings, with SIM-o referencing the original speech prompt and SIM-r referencing the vocoder-reconstructed prompt.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "prompt",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, MOS and SMOS scores are obtained via a crowd-sourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. MOS and SMOS is evaluated following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "audio",
                    "prompt",
                    "speaker",
                    "smos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE attains the best WER and SIM scores under the Continuation setting. Under the Cross-sentence setting, MaskGCT achieves slightly higher scores; however, in our reproduction, MaskGCT&#8217;s WER-C is only 3.86% under our evaluation protocol. This suggests that BELLE remains highly competitive in the Cross-sentence scenario. Additionally, BELLE outperforms MELLE across both subjective and objective metrics, including MOS, SMOS, WER, and SIM. These results demonstrate that Bayesian sampling delivers superior performance compared to Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "mos",
                    "maskgct",
                    "smos",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity evaluation is conducted using the evaluation method showed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS2\" title=\"5.2 Evaluation Settings &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nIn the NIG prior, the parameter <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> denotes the scale of the inverse-gamma distribution over the variance <math alttext=\"\\sigma_{t}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#963;</mi><mi>t</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">\\sigma_{t}^{2}</annotation></semantics></math>. Increasing <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> raises the expected variance, thereby promoting greater diversity in the sampled outcomes (See Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E7\" title=\"In 4.2 From Gaussian Sampling to Bayesian Evidential Sampling &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nFrom Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T3\" title=\"Table 3 &#8227; 6.2 Diversity Analysis &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it&#8217;s observed that BELLE (<math alttext=\"\\beta*2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\beta*2</annotation></semantics></math>) exhibits the largest distances across all three metrics, confirming that increasing the sampling parameter <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m5\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> effectively enhances generation diversity. Default BELLE achieves moderate diversity, slightly higher than MELLE, suggesting that Bayesian sampling yields superior diversity compared to Gaussian sampling. In contrast, F5-TTS produces consistently small distances, indicating that its outputs are more deterministic and less diverse across repeated sampling. These findings highlight that, beyond intelligibility and speaker similarity, BELLE offers a controllable trade-off between stability and diversity at inference time, with the <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> parameter serving as a practical knob to adjust sample variability, analogous to the role of the temperature parameter in the sampling process of token-based language models.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "speaker",
                    "belle",
                    "f5tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T4\" title=\"Table 4 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of BELLE consistently improves as the number of teachers increases. Furthermore, the possibility that the performance gain is solely due to the increased amount of training data is excluded. In the <span class=\"ltx_text ltx_font_italic\">data-aug</span> setting, the total training data is larger than that of the <span class=\"ltx_text ltx_font_italic\">3-teacher</span> configuration; however, its performance is inferior to <span class=\"ltx_text ltx_font_italic\">3-teacher</span> on multiple metrics. This degradation may be attributed to the conventional data augmentation strategy, in which the same text can correspond to acoustically different speeches, potentially causing confusion for the model. In contrast, under our proposed multi-teacher training scheme, increasing the number of teachers leads to consistent performance gains, thereby demonstrating the effectiveness of the multi teacher training strategy.</p>\n\n",
                "matched_terms": [
                    "different",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE, a novel continuous-valued AR TTS model integrating Bayesian EDL for sampling mel-spectrogram frames. Our approach addresses the underexplored area of effective sampling methods for continuous-valued AR TTS. Additionally, a multi-teacher knowledge distillation framework is introduced, substantially improving TTS synthesis quality using synthesized data from publicly available models. Experimental results highlight the efficacy of Bayesian evidential sampling and multi-teacher distillation in achieving competitive speech naturalness, speaker similarity, and diversity, rivaling models trained on much larger real-data corpora. Remarkably, our approach attains these results using only one-tenth of the data, much of it synthetic, while enabling high-quality, low-latency TTS suitable for both offline and streaming applications.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE introduces a novel Bayesian evidential sampling approach within continuous-valued autoregressive text-to-speech, significantly enhancing the zero-shot TTS synthesis quality. By effectively modeling and generating natural, expressive, and intelligible speech with limited reference data, BELLE advances the flexibility and realism of synthesized audio, making it valuable for various beneficial applications in society. Notably, BELLE can facilitate natural human-machine conversational systems, assistive communication technologies for individuals with speech impairments, and personalized education platforms, thereby positively impacting accessibility, education, and user experiences in interactive dialogue systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "belle",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, alongside the benefits, zero-shot text-to-speech technologies like BELLE also pose potential risks. They could be misused in unethical or harmful scenarios, such as impersonation, identity fraud, and targeted social engineering attacks. In principle, BELLE could mimic any person&#8217;s voice from minimal audio samples, leading to malicious applications aimed at deceiving or misleading individuals. Therefore, responsible use and appropriate safeguards, such as speaker verification, synthetic audio detection, and strong regulatory oversight, are critical directions for addressing and mitigating these societal risks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "use",
                    "speaker",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although BELLE demonstrates strong performance, there are two main limitations. First, the current study focuses solely on training and evaluation with English speech data, without validating multilingual generalization; extending BELLE to diverse languages remains an important direction for future work. Second, as with most AR TTS systems, BELLE&#8217;s RTF is still higher than that of recent flow-matching-based models. Further research on predicting multiple mel-spectrogram frames per step could substantially reduce RTF and enable lower-latency streaming TTS.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, we augment our training data by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite> (LLM + flow matching-based streaming TTS), IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite> (AR discrete acoustic token TTS), SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite> (LLM-based single-stage TTS), F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite> (flow matching-based efficient TTS), MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite> (masked generative NAR TTS), and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite> (cross-lingual expressive TTS). Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. For consistency, all synthesized audio samples generated by these models are also resampled to 16 kHz.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "f5tts",
                    "maskgct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train BELLE, BELLE-stream and reproduce MELLE on the data in Sec.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>), with predefined training weights that the original Librispeech audio is weighted by 0.22 and each synthesized audio source (six TTS models) is weighted by 0.13, which ensures the original Librispeech audio receives approximately twice the weighting of synthesized audio, reflecting relative considerations of synthetic audio quality. Models are trained by AdamW optimiser with a total batch size of about 160K frames distributed across 16 NVIDIA A800 GPUs. BELLE and MELLE&#8217;s training proceeds for 450K updates, where the learning rate is first linearly warmed up to a peak value of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> over the initial 10% of training steps and thereafter linearly decayed to zero. Regarding the hyperparameters for BELLE, we set <math alttext=\"\\lambda=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.5</annotation></semantics></math> in Eqn.&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E10\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <math alttext=\"\\lambda_{\\text{samp}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.2</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math> in Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E8\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As for MELLE, our hyperparameter settings strictly follow the original MELLE paper, where <math alttext=\"\\lambda_{\\text{samp}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "audio",
                    "bellestream",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE-stream is initialized from the trained BELLE model, trained with a batch size of 80K frames across 8 NVIDIA A800 GPUs for 150K updates, using <math alttext=\"\\lambda_{\\text{flux}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.1</annotation></semantics></math> while keeping all other hyperparameters identical to BELLE. In our implementation, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m4\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. Here, <math alttext=\"S_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}</annotation></semantics></math> is computed from the number of phonemes obtained after G2P conversion, while <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math> is determined from the number of mel-spectrogram frames. Let <math alttext=\"L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{text}}</annotation></semantics></math> and <math alttext=\"L_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}</annotation></semantics></math> denote the total phoneme count and the total mel-frame count of an utterance, respectively. During training, we filter out audio samples whose ratio <math alttext=\"L_{\\mathrm{audio}}:L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>audio</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>L</mi><mi>text</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}:L_{\\mathrm{text}}</annotation></semantics></math> is less than <math alttext=\"2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m10\" intent=\":literal\"><semantics><mn>2.5</mn><annotation encoding=\"application/x-tex\">2.5</annotation></semantics></math>, ensuring that each text chunk retains sufficient corresponding audio frames for effective streaming generation. We observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "audio",
                    "prompt",
                    "timbre",
                    "speaker",
                    "bellestream",
                    "single"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the zero-shot TTS capabilities of our model using the LibriSpeech test-clean subset following VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite> by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, we consider two inference conditions: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where we use a reference utterance and its corresponding transcription from a given speaker as a prompt, and then the model generates speech for a different sentence while preserving speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "different",
                    "speaker",
                    "use",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, we report word error rate (WER) to measure intelligibility and robustness, using two automatic speech recognition models: a Conformer-Transducer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and a fine-tuned HuBERT-Large model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite>. We denote results obtained from these systems as WER-C and WER-H, respectively. To quantify speaker similarity, we calculate the cosine similarity between extracted speaker embeddings using a WavLM-TDCNN model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib6\" title=\"\">2022</a>)</cite>. We provide two similarity metrics: SIM-o computes the similarity against the original speech prompt, whereas SIM-r uses the vocoder-reconstructed version of the prompt.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "prompt",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, we obtain MOS and SMOS scores via a crowdsourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. We evaluate MOS and SMOS following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Note that BELLE-stream is fine-tuned to a fixed speaker timbre and does not use an audio prompt; therefore, it is not evaluated for SMOS or SIM metrics, and its results are reported only under the Cross-sentence setting.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "does",
                    "fixed",
                    "systems",
                    "audio",
                    "use",
                    "prompt",
                    "timbre",
                    "speaker",
                    "reported",
                    "different",
                    "bellestream",
                    "smos",
                    "not",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM in <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, we adopt the 13th (of 24) layer of WavLM-Large as the speaker representation; 1,234 audio prompts are sampled from the LibriSpeech test-clean subset, and under the Cross-sentence setting each prompt is inferred three times to obtain 1,234 groups of outputs (three per group); frame-level hidden states from layer 13 are extracted for each generation, mean-pooled and L2-normalized, and within-group pairwise cosine similarity, L1, and L2 distances are computed; corpus-level means and standard deviations across all 1,234 groups are reported to quantify the diversity of the generated speech.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speaker",
                    "prompt",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the TTS task, we focus on the Mean Opinion Score (MOS) and Speaker Similarity (SMOS). From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. The details are as follows: For speech quality evaluation, we conducted an MOS (Mean Opinion Score) test and explicitly instructed the raters to focus on assessing audio quality and naturalness, while ignoring differences in style (e.g., timbre, emotion, and prosody). The raters were presented with and scored samples, and each rater was asked to evaluate the subjective naturalness on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "systems",
                    "audio",
                    "timbre",
                    "speaker",
                    "smos",
                    "different",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speaker similarity evaluation, we asked the raters to focus on the similarity of the speaker&#8217;s identity (timbre) to the reference, while ignoring differences in content, grammar, or audio quality. We paired each synthetic utterance with a reference utterance to assess the degree of matching between the synthesized speech and the target speaker. Each rater was asked to evaluate the speaker similarity on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speaker",
                    "timbre"
                ]
            }
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Comparison of WER (%) and speaker similarity metrics for BELLE and baselines. Entries marked with  are trained using the dataset described in Sec.5.1, marked with * correspond to the streaming TTS setting. BELLE-stream only takes text as input and does not use audio prompt, different from Cross-Sentence setting, so SIM metrics are omitted.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Continuation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Cross-Sentence</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER-C</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER-H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-r</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER-C</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER-H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-r</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">SIM-o</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Ground Truth</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.672</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">VALL-E</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.508</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.580</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MaskGCT</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.63</span></td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.687</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CLAM-TTS<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib21\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">2.36</td>\n<td class=\"ltx_td ltx_align_center\">0.513</td>\n<td class=\"ltx_td ltx_align_center\">0.477</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">5.11</td>\n<td class=\"ltx_td ltx_align_center\">0.538</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.495</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MELLE &#8224;</th>\n<td class=\"ltx_td ltx_align_center\">2.04</td>\n<td class=\"ltx_td ltx_align_center\">2.59</td>\n<td class=\"ltx_td ltx_align_center\">0.526</td>\n<td class=\"ltx_td ltx_align_center\">0.488</td>\n<td class=\"ltx_td ltx_align_center\">3.30</td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n<td class=\"ltx_td ltx_align_center\">0.652</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.606</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SMLLE<cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib46\" title=\"\">2025</a>)</cite> *</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.516</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.489</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">IST-LM<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib54\" title=\"\">2024</a>)</cite> *</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">3.60</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">4.53</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.653</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BELLE &#8224;</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.549</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.519</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.679</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.641</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">BELLE-stream*</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">-</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "simo",
            "does",
            "marked",
            "wer",
            "valle",
            "setting",
            "baselines",
            "omitted",
            "not",
            "comparison",
            "crosssentence",
            "similarity",
            "tts",
            "clamttskim",
            "istlmyang",
            "takes",
            "audio",
            "entries",
            "speaker",
            "streaming",
            "from",
            "werc",
            "system",
            "input",
            "described",
            "trained",
            "belle",
            "text",
            "ground",
            "truth",
            "melle",
            "simr",
            "metrics",
            "smllesun",
            "maskgct",
            "only",
            "use",
            "prompt",
            "continuation",
            "sim",
            "correspond",
            "sec",
            "dataset",
            "bellestream",
            "different",
            "werh"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the objective metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE attains the best WER and SIM scores under the Continuation setting. Under the Cross-sentence setting, MaskGCT achieves slightly higher scores; however, in our reproduction, MaskGCT&#8217;s WER-C is only 3.86% under our evaluation protocol. This suggests that BELLE remains highly competitive in the Cross-sentence scenario. Additionally, BELLE outperforms MELLE across both subjective and objective metrics, including MOS, SMOS, WER, and SIM. These results demonstrate that Bayesian sampling delivers superior performance compared to Gaussian sampling.</p>\n\n",
            "<p class=\"ltx_p\">Since BELLE-stream is finetuned to a fixed speaker timbre, it does not accept an audio prompt; therefore, it is excluded from SMOS scoring in the subjective evaluation and SIM computation in the objective evaluation, and its results are reported only under the Cross-sentence setting. BELLE-stream only takes the target text to be synthesized as input. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, streaming generation does not cause significant degradation in speech naturalness, achieving higher MOS scores than non&#8209;streaming MELLE. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE-stream achieves the lowest WER among streaming TTS systems, with no appreciable increase in WER compared to BELLE. In our tests, BELLE-stream achieved a real-time factor (RTF)&#8212;defined as the ratio of audio generation time to the length of the generated audio&#8212;of 0.55 when producing a 10-second utterance. Given that BELLE-stream operates with 0.8-second chunks, this implies a first-packet latency (FPL) of only 440ms, demonstrating its effective balance between performance and latency.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper-distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://belletts.github.io/Belle/\" title=\"\">https://belletts.github.io/Belle/</a>.\nThe code, checkpoints, and synthetic data will be released after the paper is accepted.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "belle",
                    "audio",
                    "prompt",
                    "from",
                    "input",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, autoregressive (AR) next-token prediction models based on discrete audio codecs have gained significant attention in the audio generation and text-to-speech (TTS) communities, driven by the success of models such as AudioLM <cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib4\" title=\"\">2023</a>)</cite> and VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite>, among others. However, continuous-valued prediction has emerged as a promising alternative due to its ability to eliminate quantization errors introduced by token-based codecs <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>; Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib8\" title=\"\">2024b</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>; Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib25\" title=\"\">2025</a>)</cite>.\nGenerative models typically rely on sampling mechanisms to introduce stochasticity and improve synthesis quality. In audio-codec-based AR TTS, this is often achieved using mature top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling techniques, similar to those employed in token-based language models (LMs). Diffusion-based models, by contrast, introduce randomness by injecting noise into the input, thereby embedding variability directly into the generation process. However, effective sampling strategies for continuous-valued AR models remain underexplored.\nRecent studies have begun to address this gap. In image generation, MAR proposed a hybrid approach where an AR model predicts conditioning signals, followed by diffusion-based sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib27\" title=\"\">2024a</a>)</cite>. In zero-shot TTS, MELLE introduced Gaussian sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, while FELLE adopted flow-matching techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib30\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite> to enhance sampling efficiency and output diversity.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "tts",
                    "audio",
                    "valle",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE: <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS Synthesis&#8212;a continuous-valued AR model that directly predicts mel-spectrograms from text inputs based on the evidential deep learning (EDL) framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Amini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib2\" title=\"\">2020</a>)</cite>. This represents the first application of Bayesian deep learning to TTS, enabling controllable and theoretically grounded sampling at test time.\nSpecifically, BELLE models each predicted mel-spectrogram frame as a sample from a Gaussian distribution, whose parameters themselves are drawn from hyper-distributions. Metaphorically, the model first predicts the parameters of this hyper-distribution, then samples a specific Gaussian distribution from it, and finally generates mel-spectrogram frames from the sampled Gaussian.\nHowever, learning accurate Gaussian posterior estimates from limited data poses a challenge, as typical TTS datasets contain only a single audio recording per text prompt. To overcome this, the training corpus is augmented with synthetic audio samples generated by several publicly available pre-trained zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "belle",
                    "audio",
                    "prompt",
                    "from",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present BELLE, the first AR TTS model using evidential deep learning to train and sample from continuous-valued Mel-Spectrogram space. The results show that Bayesian evidential sampling outperforms conventional Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "from",
                    "tts",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE achieves competitive performance compared with leading open-source TTS systems trained on approximately 50k hours of speech data, while BELLE uses only around 5k hours of data that largely consist of synthetic samples. Additionally, its streaming variant, BELLE-stream, attains an effective balance between audio generation quality and latency.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "belle",
                    "audio",
                    "streaming",
                    "bellestream",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Continuous-valued TTS models can be broadly categorized into AR and non-autoregressive (NAR) approaches. AR methods include MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, which generates mel spectrogram frames using decoder-only Transformer-based AR modelling combined with Gaussian sampling. FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite>, ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>)</cite> and DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>)</cite> first generate continuous-valued hidden states autoregressively, then employ flow-matching or diffusion-based modules, such as DiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib39\" title=\"\">2023</a>)</cite>, to sample and produce mel spectrograms or a continuous latent space learned by a VAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Pinheiro&#160;Cinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib40\" title=\"\">2021</a>)</cite>. Moreover, VAE-based AR methods, including GMM-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>)</cite> and KALLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>)</cite>, initially leverage a VAE to estimate a latent distribution (mean and variance), and subsequently autoregressively predict latent representations sampled from this distribution. NAR models such as E2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>)</cite> and F5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, on the other hand, predict mel-spectrograms directly via iterative mask-based refinement. Recently, several streaming TTS systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib54\" title=\"\">2024</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib46\" title=\"\">2025</a>; Sheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib49\" title=\"\">2025b</a>)</cite>. Most of these approaches model discrete features using a language model, followed by a flow-matching module or other similar components to generate speech features. Such architectures are relatively complex and may introduce potentially higher latency.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "streaming",
                    "from",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Knowledge distillation (KD) transfers knowledge from larger teacher models to smaller, faster student models <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib37\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib53\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib28\" title=\"\">2024b</a>)</cite>. Originally, KD is used to reduce exposure bias in AR models like Tacotron <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib52\" title=\"\">2017</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib31\" title=\"\">2020</a>)</cite> and to guide NAR models such as FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib41\" title=\"\">2019</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib42\" title=\"\">2020</a>)</cite>. It has been further applied to improve pronunciation <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib37\" title=\"\">2025</a>)</cite>, style transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib53\" title=\"\">2022</a>)</cite>, perceptual quality in diffusion-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib28\" title=\"\">2024b</a>)</cite>, and to distill semantic knowledge from HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib14\" title=\"\">2025</a>)</cite>. To our knowledge, multi-teacher KD has not yet been explored in TTS.</p>\n\n",
                "matched_terms": [
                    "not",
                    "tts",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AR TTS models based on mel-spectrogram synthesis typically formulate the generation process as a sequential next-frame prediction task. Formally, given an input text sequence <math alttext=\"\\bm{x}=[x_{1},x_{2},...,x_{N}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119961;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{x}=[x_{1},x_{2},...,x_{N}]</annotation></semantics></math>, it&#8217;s aimed at generating an acoustic mel-spectrogram sequence <math alttext=\"\\bm{y}=[\\bm{y}_{1},\\bm{y}_{2},...,\\bm{y}_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119962;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119962;</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}=[\\bm{y}_{1},\\bm{y}_{2},...,\\bm{y}_{T}]</annotation></semantics></math>, where each frame <math alttext=\"\\bm{y}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math> denotes the spectral representation at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the number of mel-frequency bands.\nIn AR modeling, each mel-spectrogram frame <math alttext=\"\\bm{y}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#119962;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{y}_{t}</annotation></semantics></math> is generated conditionally depending on the textual content <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>&#119961;</mi><annotation encoding=\"application/x-tex\">\\bm{x}</annotation></semantics></math> and previous frames <math alttext=\"\\bm{y}_{&lt;t}=[\\bm{y}_{1},...,\\bm{y}_{t-1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119962;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{&lt;t}=[\\bm{y}_{1},...,\\bm{y}_{t-1}]</annotation></semantics></math>. Thus, the generative process can be described by the following conditional probability decomposition:</p>\n\n",
                "matched_terms": [
                    "input",
                    "described",
                    "text",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">streaming generation task</span> is defined that both the input text and target mel-spectrogram are segmented into <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> consecutive chunks\n<math alttext=\"\\bm{X}=[\\bm{x}^{(1)},\\ldots,\\bm{x}^{(M)}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119935;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msup><mi>&#119961;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119961;</mi><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{X}=[\\bm{x}^{(1)},\\ldots,\\bm{x}^{(M)}]</annotation></semantics></math> and\n<math alttext=\"\\bm{Y}=[\\bm{y}^{(1)},\\ldots,\\bm{y}^{(M)}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119936;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{Y}=[\\bm{y}^{(1)},\\ldots,\\bm{y}^{(M)}]</annotation></semantics></math>.\nAt step <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, the current audio chunk <math alttext=\"\\bm{y}^{(m)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{(m)}</annotation></semantics></math> is predicted conditioned on all previously generated audio chunks <math alttext=\"\\bm{y}^{(&lt;m)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi/><mo>&lt;</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{(&lt;m)}</annotation></semantics></math> and all available text chunks <math alttext=\"\\bm{x}^{(\\leq m)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119961;</mi><mrow><mo stretchy=\"false\">(</mo><mrow><mi/><mo>&#8804;</mo><mi>m</mi></mrow><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{x}^{(\\leq m)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "input",
                    "audio",
                    "streaming",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The continuous-valued AR generation model closely resembles standard LM architectures, specifically decoder-only Transformer models that are frequently employed in contemporary large language models (LLMs), which is shown in the left part of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 General Architecture of Mel-based Autoregressive TTS Model &#8227; 3 Mel-based Autoregressive TTS &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To delineate the input text clearly, a special <math alttext=\"\\langle\\text{BOS}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>BOS</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{BOS}\\rangle</annotation></semantics></math> token is added at the beginning of textual sequences, and an <math alttext=\"\\langle\\text{EOS}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mtext>EOS</mtext><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle\\text{EOS}\\rangle</annotation></semantics></math> token is appended at the end. The Prenet then maps discrete textual tokens <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>&#119961;</mi><annotation encoding=\"application/x-tex\">\\bm{x}</annotation></semantics></math> into continuous embeddings as well as maps mel-spectrogram frames <math alttext=\"\\bm{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>&#119962;</mi><annotation encoding=\"application/x-tex\">\\bm{y}</annotation></semantics></math> to the hidden dimension of the AR LM. Next, the AR LM accepts a concatenation of textual embeddings and mel-spectrogram embeddings as inputs, producing corresponding hidden representations <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Sampling Module introduces critical stochasticity into the generative process. Specifically, the hidden states <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math> from the AR LM are first projected down to the mel-spectrogram dimension. The projected representation is assumed to follow a specified parametric distribution (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, a Gaussian distribution). Subsequently, the Sampling Module samples from this distribution, obtaining an intermediate sampled representation denoted as <math alttext=\"\\bm{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119963;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{z}_{t}</annotation></semantics></math>. Afterwards, a multilayer perception (MLP)-based denoising network refines and further processes the sampled representation, and the resulting denoised output is denoted as <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> as shown in the right part of Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 General Architecture of Mel-based Autoregressive TTS Model &#8227; 3 Mel-based Autoregressive TTS &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The detailed description of our proposed evidential Bayesian sampling method is provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4\" title=\"4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "sec",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 General Architecture of Mel-based Autoregressive TTS Model &#8227; 3 Mel-based Autoregressive TTS &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, a streaming generation strategy by segmenting both the input text and target acoustic sequence into fixed-size chunks is adopted. Let the predefined text chunk size be denoted as <math alttext=\"S_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}</annotation></semantics></math> and the audio chunk size as <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math>. The text sequence is first partitioned into chunks <math alttext=\"\\bm{x}^{(1)},\\ldots,\\bm{x}^{(M)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119961;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119961;</mi><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{x}^{(1)},\\ldots,\\bm{x}^{(M)}</annotation></semantics></math>, where the last chunk may contain fewer than <math alttext=\"S_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}</annotation></semantics></math> tokens. Similarly, the mel-spectrogram is partitioned into <math alttext=\"\\bm{y}^{(1)},\\ldots,\\bm{y}^{(M)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mi>M</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{y}^{(1)},\\ldots,\\bm{y}^{(M)}</annotation></semantics></math>, each containing up to <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math> frames, with the last chunk possibly exceeding <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math> frames.</p>\n\n",
                "matched_terms": [
                    "input",
                    "audio",
                    "streaming",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The generation process interleaves text and audio chunks in the input to the AR LM, maintaining a causal mask over the sequence. By setting a relatively smaller <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math>, each audio chunk is generated with sufficient preceding textual context to ensure coherent synthesis. At inference time, chunk-by-chunk audio generation is performed until the last audio chunk, where the Stop-Prediction Module is invoked to decide the termination point of synthesis.</p>\n\n",
                "matched_terms": [
                    "input",
                    "audio",
                    "text",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EDL is a Bayesian approach for uncertainty quantification in regression tasks by explicitly modelling the posterior distribution of predictions.\nIn our TTS setting, the observed data <math alttext=\"\\bm{y}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119962;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{y}_{t}</annotation></semantics></math> denotes the mel-spectrogram frame at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, where <math alttext=\"\\bm{y}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite> directly predicts the mean and variance of a Gaussian distribution, then samples embeddings accordingly; implementation details are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A5.SS1\" title=\"E.1 Gaussian Sampling in MELLE &#8227; Appendix E MELLE &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>. We propose to replace the Gaussian sampling mechanism with a Bayesian evidential approach. Specifically, NIG distribution parameters are predicted from the AR hidden representation <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math> via a linear layer followed by suitable activation functions to constrain each parameter within appropriate numerical ranges:</p>\n\n",
                "matched_terms": [
                    "melle",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE is trained end-to-end using a composite loss:</p>\n\n",
                "matched_terms": [
                    "trained",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The regression loss <math alttext=\"\\mathcal{L}_{\\text{reg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>reg</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{reg}}</annotation></semantics></math> ensures predicted mel-spectrograms closely match the ground truth <math alttext=\"\\bm{y}^{\\text{gt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mtext>gt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{\\text{gt}}</annotation></semantics></math>. It includes L1 and L2 terms for both coarse predictions <math alttext=\"\\bm{y}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{(1)}</annotation></semantics></math> and refined predictions <math alttext=\"\\bm{y}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{(2)}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram flux loss <math alttext=\"\\mathcal{L}_{\\text{flux}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>flux</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{flux}}</annotation></semantics></math> encourages temporal dynamics, promoting variability between the current predicted distribution location <math alttext=\"\\bm{\\gamma}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#120632;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{\\gamma}_{t}</annotation></semantics></math> and previous-frame ground truth <math alttext=\"\\bm{y}_{t-1}^{\\text{gt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mtext>gt</mtext></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t-1}^{\\text{gt}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A couple of open-sourced TTS models are used to generate multiple audio samples given the texts provided by the dataset. Given a textual input <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#119961;</mi><annotation encoding=\"application/x-tex\">\\bm{x}</annotation></semantics></math>, audio samples are synthesized using a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> external TTS teacher models, resulting in <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> synthesized mel-spectrograms. Together with the original human-recorded mel-spectrogram from the dataset, a total of <math alttext=\"N+1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">N+1</annotation></semantics></math> corresponding mel-spectrograms are gotten. Denote these ground-truth mel-spectrograms as <math alttext=\"\\bm{y}^{\\text{gt}}_{i},~i=1,2,\\dots,N+1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>&#119962;</mi><mi>i</mi><mtext>gt</mtext></msubsup><mo rspace=\"0.497em\">,</mo><mi>i</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}^{\\text{gt}}_{i},~i=1,2,\\dots,N+1</annotation></semantics></math>. Each mel-spectrogram is treated as a ground-truth example and calculated an individual loss corresponding to each one. To balance the contributions from multiple teachers and the original dataset recording, the predefined weights <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> are assigned to each ground-truth mel-spectrogram.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "audio",
                    "from",
                    "input",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset is derived from the Librispeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib38\" title=\"\">2015</a>)</cite> training set and contains audio samples whose duration is from 0.5 second to 14 seconds, creating a final training dataset containing approximately 706 hours of speech. To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, our training data are augmented by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite>, IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite>, F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite>, and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>. Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. Details of the data processing methodology can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS1\" title=\"C.1 Training Data &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "audio",
                    "maskgct",
                    "from",
                    "input",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE, BELLE-stream and MELLE are trained on the training dataset, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For BELLE-stream, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. It&#8217;s observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre. Details about model configuration and training could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS2\" title=\"C.2 Model Configurations &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS3\" title=\"C.3 Training Details of Main Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.3</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "belle",
                    "audio",
                    "prompt",
                    "streaming",
                    "speaker",
                    "from",
                    "sec",
                    "dataset",
                    "bellestream",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot TTS capabilities of our model is evaluated using the LibriSpeech test-clean subset by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, two inference conditions are considered: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where a reference utterance and its corresponding transcription from a given speaker is used as a prompt, and then the model generates speech for a different sentence.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "prompt",
                    "continuation",
                    "speaker",
                    "from",
                    "different",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, word error rate (WER) is reported to measure intelligibility and robustness. WER-C and WER-H are evaluated using Conformer<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and HuBERT<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite> based ASR models respectively. Speaker similarity is measured via cosine similarity of extracted speaker embeddings, with SIM-o referencing the original speech prompt and SIM-r referencing the vocoder-reconstructed prompt.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "simo",
                    "wer",
                    "simr",
                    "prompt",
                    "speaker",
                    "werc",
                    "werh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, MOS and SMOS scores are obtained via a crowd-sourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. MOS and SMOS is evaluated following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "audio",
                    "prompt",
                    "speaker",
                    "described"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, it&#8217;s found that the middle-layer features contain rich paralinguistic information, which can be leveraged to assess the acoustic characteristics of speech. Frame-level hidden states from layer-13 are extracted for each sample, mean-pooled and L2-normalized, followed by computation of within-group pairwise cosine similarity and L1/L2 distances.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, BELLE, F5-TTS, and Ground Truth achieve MOS scores of around 4.2, indicating that the audio produced by both F5-TTS and BELLE is close to that of natural human speech. F5-TTS attains a slightly higher MOS, which can be attributed to its lower background noise &#8212; human raters tend to prefer cleaner audio. In contrast, the real speech in Ground Truth inherently contains a certain level of noise, and BELLE replicates this natural background characteristic, leading to MOS scores that are very close to the Ground Truth.\nRegarding SMOS, it is worth noting that in the LibriSpeech dataset, different utterances from the same speaker sometimes exhibit perceptible timbre variation, which results in a relatively lower SMOS score for Ground Truth. BELLE achieves the highest speaker similarity among all tested systems. Notably, while both F5&#8209;TTS and MaskGCT were trained on datasets comprising 50,000 hours of English speech, BELLE was trained on less than 5,000 hours of data, yet it attained competitive, and in some instances superior performance. These results underscore the efficacy of the proposed Bayesian sampling strategy and the multi&#8209;teacher learning framework.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "belle",
                    "audio",
                    "maskgct",
                    "speaker",
                    "truth",
                    "from",
                    "different",
                    "dataset",
                    "trained",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity evaluation is conducted using the evaluation method showed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS2\" title=\"5.2 Evaluation Settings &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nIn the NIG prior, the parameter <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> denotes the scale of the inverse-gamma distribution over the variance <math alttext=\"\\sigma_{t}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#963;</mi><mi>t</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">\\sigma_{t}^{2}</annotation></semantics></math>. Increasing <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> raises the expected variance, thereby promoting greater diversity in the sampled outcomes (See Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E7\" title=\"In 4.2 From Gaussian Sampling to Bayesian Evidential Sampling &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nFrom Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T3\" title=\"Table 3 &#8227; 6.2 Diversity Analysis &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it&#8217;s observed that BELLE (<math alttext=\"\\beta*2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\beta*2</annotation></semantics></math>) exhibits the largest distances across all three metrics, confirming that increasing the sampling parameter <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m5\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> effectively enhances generation diversity. Default BELLE achieves moderate diversity, slightly higher than MELLE, suggesting that Bayesian sampling yields superior diversity compared to Gaussian sampling. In contrast, F5-TTS produces consistently small distances, indicating that its outputs are more deterministic and less diverse across repeated sampling. These findings highlight that, beyond intelligibility and speaker similarity, BELLE offers a controllable trade-off between stability and diversity at inference time, with the <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> parameter serving as a practical knob to adjust sample variability, analogous to the role of the temperature parameter in the sampling process of token-based language models.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "similarity",
                    "metrics",
                    "speaker",
                    "from",
                    "sec",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computational resources, the analysis experiments are conducted on a smaller-scale dataset. The <span class=\"ltx_text ltx_font_italic\">1-teacher</span> setting trains solely on the original Librispeech data without synthesized augmentation. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting incorporates additional synthesized speech from two TTS models, while the <span class=\"ltx_text ltx_font_italic\">7-teacher</span> setting augments the training data with speech from all six TTS models described earlier. The <span class=\"ltx_text ltx_font_italic\">data-aug</span> configuration serves as a baseline using conventional data augmentation, randomly sampling from all available data sources during training. Details about the small dataset and model training configuration could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS4\" title=\"C.4 Training Details of Analysis Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "setting",
                    "dataset",
                    "from",
                    "described"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T4\" title=\"Table 4 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of BELLE consistently improves as the number of teachers increases. Furthermore, the possibility that the performance gain is solely due to the increased amount of training data is excluded. In the <span class=\"ltx_text ltx_font_italic\">data-aug</span> setting, the total training data is larger than that of the <span class=\"ltx_text ltx_font_italic\">3-teacher</span> configuration; however, its performance is inferior to <span class=\"ltx_text ltx_font_italic\">3-teacher</span> on multiple metrics. This degradation may be attributed to the conventional data augmentation strategy, in which the same text can correspond to acoustically different speeches, potentially causing confusion for the model. In contrast, under our proposed multi-teacher training scheme, increasing the number of teachers leads to consistent performance gains, thereby demonstrating the effectiveness of the multi teacher training strategy.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "metrics",
                    "setting",
                    "correspond",
                    "different",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study is conducted to investigate how the sampling module and flux loss affect the performance of the BELLE model. Due to limited computational resources, the analysis experiments are also conducted on the smaller-scale dataset. Our experiments are based on the BELLE <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting. As demonstrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T5\" title=\"Table 5 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the sampling module plays a more crucial role than flux loss. Removing the flux loss results in only a slight performance degradation, whereas removing the sampling module leads to a severe performance drop. This indicates that Bayesian sampling plays a crucial role in the effectiveness of BELLE.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "only",
                    "belle",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE, a novel continuous-valued AR TTS model integrating Bayesian EDL for sampling mel-spectrogram frames. Our approach addresses the underexplored area of effective sampling methods for continuous-valued AR TTS. Additionally, a multi-teacher knowledge distillation framework is introduced, substantially improving TTS synthesis quality using synthesized data from publicly available models. Experimental results highlight the efficacy of Bayesian evidential sampling and multi-teacher distillation in achieving competitive speech naturalness, speaker similarity, and diversity, rivaling models trained on much larger real-data corpora. Remarkably, our approach attains these results using only one-tenth of the data, much of it synthetic, while enabling high-quality, low-latency TTS suitable for both offline and streaming applications.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "tts",
                    "belle",
                    "speaker",
                    "streaming",
                    "from",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE introduces a novel Bayesian evidential sampling approach within continuous-valued autoregressive text-to-speech, significantly enhancing the zero-shot TTS synthesis quality. By effectively modeling and generating natural, expressive, and intelligible speech with limited reference data, BELLE advances the flexibility and realism of synthesized audio, making it valuable for various beneficial applications in society. Notably, BELLE can facilitate natural human-machine conversational systems, assistive communication technologies for individuals with speech impairments, and personalized education platforms, thereby positively impacting accessibility, education, and user experiences in interactive dialogue systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "belle",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, alongside the benefits, zero-shot text-to-speech technologies like BELLE also pose potential risks. They could be misused in unethical or harmful scenarios, such as impersonation, identity fraud, and targeted social engineering attacks. In principle, BELLE could mimic any person&#8217;s voice from minimal audio samples, leading to malicious applications aimed at deceiving or misleading individuals. Therefore, responsible use and appropriate safeguards, such as speaker verification, synthetic audio detection, and strong regulatory oversight, are critical directions for addressing and mitigating these societal risks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "use",
                    "speaker",
                    "from",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although BELLE demonstrates strong performance, there are two main limitations. First, the current study focuses solely on training and evaluation with English speech data, without validating multilingual generalization; extending BELLE to diverse languages remains an important direction for future work. Second, as with most AR TTS systems, BELLE&#8217;s RTF is still higher than that of recent flow-matching-based models. Further research on predicting multiple mel-spectrogram frames per step could substantially reduce RTF and enable lower-latency streaming TTS.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "belle",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset is derived from the Librispeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib38\" title=\"\">2015</a>)</cite> training set. We preprocess this dataset using a voice activity detection <cite class=\"ltx_cite ltx_citemacro_citep\">(SileroTeam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib45\" title=\"\">2024</a>)</cite> algorithm to remove prolonged silent intervals. Subsequently, to ensure quality and manageability, we pick out audio samples whose duration is from 0.5 second to 14 seconds, creating a final training dataset containing approximately 706 hours of speech. All audio samples are resampled to 16 kHz and converted into 80-dimensional mel-frequency spectrograms. Additionally, we apply grapheme-to-phoneme (G2P) <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/espeak-ng/espeak-ng\" title=\"\">https://github.com/espeak-ng/espeak-ng</a></span></span></span> conversion to preprocess textual transcriptions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, we augment our training data by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite> (LLM + flow matching-based streaming TTS), IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite> (AR discrete acoustic token TTS), SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite> (LLM-based single-stage TTS), F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite> (flow matching-based efficient TTS), MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite> (masked generative NAR TTS), and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite> (cross-lingual expressive TTS). Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. For consistency, all synthesized audio samples generated by these models are also resampled to 16 kHz.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "audio",
                    "maskgct",
                    "streaming",
                    "input",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input mel-spectrograms first pass through a 3-layer prenet with dropout of 0.5 in both training and inference stages following Tacotron <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib52\" title=\"\">2017</a>)</cite>. The AR LM is a decoder-only Transformer consisting of 12 blocks, each with 16 attention heads, a hidden size of 1024, a feed-forward dimension of 4096, and a dropout rate of 0.1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is modified from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lifeiteng/vall-e\" title=\"\">https://github.com/lifeiteng/vall-e</a>.</span></span></span>. The sampling module includes a linear projection to derive the sampling parameters and a 3-layer residual MLP for denoising. A post-processing module comprising five convolutional blocks (a kernel size of 5 and 256 channels) is applied for mel-spectrogram refinement. The final waveform is synthesized using the pretrained HiFi-GAN vocoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib22\" title=\"\">2020</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The pretrained vocoder can be found in <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mechanicalsea/speecht5-tts\" title=\"\">https://huggingface.co/mechanicalsea/speecht5-tts</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train BELLE, BELLE-stream and reproduce MELLE on the data in Sec.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>), with predefined training weights that the original Librispeech audio is weighted by 0.22 and each synthesized audio source (six TTS models) is weighted by 0.13, which ensures the original Librispeech audio receives approximately twice the weighting of synthesized audio, reflecting relative considerations of synthetic audio quality. Models are trained by AdamW optimiser with a total batch size of about 160K frames distributed across 16 NVIDIA A800 GPUs. BELLE and MELLE&#8217;s training proceeds for 450K updates, where the learning rate is first linearly warmed up to a peak value of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> over the initial 10% of training steps and thereafter linearly decayed to zero. Regarding the hyperparameters for BELLE, we set <math alttext=\"\\lambda=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.5</annotation></semantics></math> in Eqn.&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E10\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <math alttext=\"\\lambda_{\\text{samp}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.2</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math> in Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E8\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As for MELLE, our hyperparameter settings strictly follow the original MELLE paper, where <math alttext=\"\\lambda_{\\text{samp}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "tts",
                    "belle",
                    "audio",
                    "sec",
                    "bellestream",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE-stream is initialized from the trained BELLE model, trained with a batch size of 80K frames across 8 NVIDIA A800 GPUs for 150K updates, using <math alttext=\"\\lambda_{\\text{flux}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.1</annotation></semantics></math> while keeping all other hyperparameters identical to BELLE. In our implementation, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m4\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. Here, <math alttext=\"S_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}</annotation></semantics></math> is computed from the number of phonemes obtained after G2P conversion, while <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math> is determined from the number of mel-spectrogram frames. Let <math alttext=\"L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{text}}</annotation></semantics></math> and <math alttext=\"L_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}</annotation></semantics></math> denote the total phoneme count and the total mel-frame count of an utterance, respectively. During training, we filter out audio samples whose ratio <math alttext=\"L_{\\mathrm{audio}}:L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>audio</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>L</mi><mi>text</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}:L_{\\mathrm{text}}</annotation></semantics></math> is less than <math alttext=\"2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m10\" intent=\":literal\"><semantics><mn>2.5</mn><annotation encoding=\"application/x-tex\">2.5</annotation></semantics></math>, ensuring that each text chunk retains sufficient corresponding audio frames for effective streaming generation. We observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "audio",
                    "prompt",
                    "streaming",
                    "speaker",
                    "text",
                    "from",
                    "bellestream",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computational resources, the analysis experiments are conducted on a smaller-scale dataset. The training dataset is derived from the Librispeech train-clean 100-hour subset, following the same processing procedure in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, which contains approximately 72 hours of speech. Training solely on the original Librispeech data without synthesized augmentation is denoted as <span class=\"ltx_text ltx_font_italic\">1-teacher</span>. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> condition introduces additional synthesized speech from XTTS-v2 and MaskGCT, about 218h. The <span class=\"ltx_text ltx_font_italic\">7-teacher</span> condition includes augmented speech from all six TTS models mentioned in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, about 455h. The <span class=\"ltx_text ltx_font_italic\">data-aug</span> configuration serves as a strong baseline using a conventional data augmentation strategy, where data from all sources totaling 455 hours is randomly sampled during training. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> and <span class=\"ltx_text ltx_font_italic\">7-teacher</span> follow the training strategy in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>, with a total batch size of about 80K frames and 47K training steps.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "maskgct",
                    "from",
                    "sec",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the zero-shot TTS capabilities of our model using the LibriSpeech test-clean subset following VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite> by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, we consider two inference conditions: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where we use a reference utterance and its corresponding transcription from a given speaker as a prompt, and then the model generates speech for a different sentence while preserving speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "use",
                    "valle",
                    "prompt",
                    "continuation",
                    "speaker",
                    "from",
                    "different",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, we report word error rate (WER) to measure intelligibility and robustness, using two automatic speech recognition models: a Conformer-Transducer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and a fine-tuned HuBERT-Large model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite>. We denote results obtained from these systems as WER-C and WER-H, respectively. To quantify speaker similarity, we calculate the cosine similarity between extracted speaker embeddings using a WavLM-TDCNN model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib6\" title=\"\">2022</a>)</cite>. We provide two similarity metrics: SIM-o computes the similarity against the original speech prompt, whereas SIM-r uses the vocoder-reconstructed version of the prompt.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "simo",
                    "wer",
                    "simr",
                    "metrics",
                    "prompt",
                    "speaker",
                    "from",
                    "werc",
                    "werh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, we obtain MOS and SMOS scores via a crowdsourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. We evaluate MOS and SMOS following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Note that BELLE-stream is fine-tuned to a fixed speaker timbre and does not use an audio prompt; therefore, it is not evaluated for SMOS or SIM metrics, and its results are reported only under the Cross-sentence setting.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "does",
                    "audio",
                    "use",
                    "metrics",
                    "setting",
                    "prompt",
                    "speaker",
                    "sim",
                    "from",
                    "different",
                    "described",
                    "bellestream",
                    "only",
                    "not",
                    "crosssentence",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM in <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, we adopt the 13th (of 24) layer of WavLM-Large as the speaker representation; 1,234 audio prompts are sampled from the LibriSpeech test-clean subset, and under the Cross-sentence setting each prompt is inferred three times to obtain 1,234 groups of outputs (three per group); frame-level hidden states from layer 13 are extracted for each generation, mean-pooled and L2-normalized, and within-group pairwise cosine similarity, L1, and L2 distances are computed; corpus-level means and standard deviations across all 1,234 groups are reported to quantify the diversity of the generated speech.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "audio",
                    "prompt",
                    "setting",
                    "speaker",
                    "from",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain fairness and consistency across all evaluations, we filter the test utterances to only those between 4 and 10 seconds in duration, and report all evaluation metrics using a fixed evaluation set shared among all compared models.</p>\n\n",
                "matched_terms": [
                    "only",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, the latent sampling module assumes that the embedding at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, denoted as <math alttext=\"\\bm{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119963;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{z}_{t}</annotation></semantics></math>, is drawn from a multivariate Gaussian distribution:</p>\n\n",
                "matched_terms": [
                    "melle",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the TTS task, we focus on the Mean Opinion Score (MOS) and Speaker Similarity (SMOS). From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. The details are as follows: For speech quality evaluation, we conducted an MOS (Mean Opinion Score) test and explicitly instructed the raters to focus on assessing audio quality and naturalness, while ignoring differences in style (e.g., timbre, emotion, and prosody). The raters were presented with and scored samples, and each rater was asked to evaluate the subjective naturalness on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "tts",
                    "audio",
                    "setting",
                    "speaker",
                    "from",
                    "different",
                    "crosssentence",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speaker similarity evaluation, we asked the raters to focus on the similarity of the speaker&#8217;s identity (timbre) to the reference, while ignoring differences in content, grammar, or audio quality. We paired each synthetic utterance with a reference utterance to assess the degree of matching between the synthesized speech and the target speaker. Each rater was asked to evaluate the speaker similarity on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "similarity",
                    "speaker"
                ]
            }
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Comparison of diversity metrics (cosine distance, L1, and L2) under the Cross-sentence setting. Each of 1,234 LibriSpeech test-clean prompts is generated three times; metrics are computed as within-prompt pairwise distances using WavLM-Large (layer 13) embeddings. BELLE (2\\beta*2) doubles the sampling \\beta relative to the default to encourage diversity.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Cosine distance</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">L1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">L2</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BELLE (<math alttext=\"\\beta*2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T3.m5\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\beta*2</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.0053</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.095</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BELLE</th>\n<td class=\"ltx_td ltx_align_center\">0.0037</td>\n<td class=\"ltx_td ltx_align_center\">2.62</td>\n<td class=\"ltx_td ltx_align_center\">0.080</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MELLE</th>\n<td class=\"ltx_td ltx_align_center\">0.0033</td>\n<td class=\"ltx_td ltx_align_center\">2.55</td>\n<td class=\"ltx_td ltx_align_center\">0.078</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">F5-TTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0005</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.012</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "testclean",
            "beta",
            "prompts",
            "embeddings",
            "librispeech",
            "setting",
            "each",
            "crosssentence",
            "withinprompt",
            "comparison",
            "2beta2",
            "sampling",
            "pairwise",
            "distance",
            "encourage",
            "diversity",
            "computed",
            "default",
            "system",
            "doubles",
            "belle",
            "distances",
            "melle",
            "metrics",
            "under",
            "f5tts",
            "cosine",
            "relative",
            "three",
            "generated",
            "layer",
            "times",
            "wavlmlarge"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The diversity evaluation is conducted using the evaluation method showed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS2\" title=\"5.2 Evaluation Settings &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nIn the NIG prior, the parameter <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> denotes the scale of the inverse-gamma distribution over the variance <math alttext=\"\\sigma_{t}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#963;</mi><mi>t</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">\\sigma_{t}^{2}</annotation></semantics></math>. Increasing <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> raises the expected variance, thereby promoting greater diversity in the sampled outcomes (See Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E7\" title=\"In 4.2 From Gaussian Sampling to Bayesian Evidential Sampling &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nFrom Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T3\" title=\"Table 3 &#8227; 6.2 Diversity Analysis &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it&#8217;s observed that BELLE (<math alttext=\"\\beta*2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\beta*2</annotation></semantics></math>) exhibits the largest distances across all three metrics, confirming that increasing the sampling parameter <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m5\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> effectively enhances generation diversity. Default BELLE achieves moderate diversity, slightly higher than MELLE, suggesting that Bayesian sampling yields superior diversity compared to Gaussian sampling. In contrast, F5-TTS produces consistently small distances, indicating that its outputs are more deterministic and less diverse across repeated sampling. These findings highlight that, beyond intelligibility and speaker similarity, BELLE offers a controllable trade-off between stability and diversity at inference time, with the <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> parameter serving as a practical knob to adjust sample variability, analogous to the role of the temperature parameter in the sampling process of token-based language models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper-distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://belletts.github.io/Belle/\" title=\"\">https://belletts.github.io/Belle/</a>.\nThe code, checkpoints, and synthetic data will be released after the paper is accepted.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "prompts",
                    "generated",
                    "each",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, autoregressive (AR) next-token prediction models based on discrete audio codecs have gained significant attention in the audio generation and text-to-speech (TTS) communities, driven by the success of models such as AudioLM <cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib4\" title=\"\">2023</a>)</cite> and VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite>, among others. However, continuous-valued prediction has emerged as a promising alternative due to its ability to eliminate quantization errors introduced by token-based codecs <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>; Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib8\" title=\"\">2024b</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>; Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib25\" title=\"\">2025</a>)</cite>.\nGenerative models typically rely on sampling mechanisms to introduce stochasticity and improve synthesis quality. In audio-codec-based AR TTS, this is often achieved using mature top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling techniques, similar to those employed in token-based language models (LMs). Diffusion-based models, by contrast, introduce randomness by injecting noise into the input, thereby embedding variability directly into the generation process. However, effective sampling strategies for continuous-valued AR models remain underexplored.\nRecent studies have begun to address this gap. In image generation, MAR proposed a hybrid approach where an AR model predicts conditioning signals, followed by diffusion-based sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib27\" title=\"\">2024a</a>)</cite>. In zero-shot TTS, MELLE introduced Gaussian sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, while FELLE adopted flow-matching techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib30\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite> to enhance sampling efficiency and output diversity.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "melle",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE: <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS Synthesis&#8212;a continuous-valued AR model that directly predicts mel-spectrograms from text inputs based on the evidential deep learning (EDL) framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Amini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib2\" title=\"\">2020</a>)</cite>. This represents the first application of Bayesian deep learning to TTS, enabling controllable and theoretically grounded sampling at test time.\nSpecifically, BELLE models each predicted mel-spectrogram frame as a sample from a Gaussian distribution, whose parameters themselves are drawn from hyper-distributions. Metaphorically, the model first predicts the parameters of this hyper-distribution, then samples a specific Gaussian distribution from it, and finally generates mel-spectrogram frames from the sampled Gaussian.\nHowever, learning accurate Gaussian posterior estimates from limited data poses a challenge, as typical TTS datasets contain only a single audio recording per text prompt. To overcome this, the training corpus is augmented with synthetic audio samples generated by several publicly available pre-trained zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "each",
                    "belle",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present BELLE, the first AR TTS model using evidential deep learning to train and sample from continuous-valued Mel-Spectrogram space. The results show that Bayesian evidential sampling outperforms conventional Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Continuous-valued TTS models can be broadly categorized into AR and non-autoregressive (NAR) approaches. AR methods include MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, which generates mel spectrogram frames using decoder-only Transformer-based AR modelling combined with Gaussian sampling. FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite>, ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>)</cite> and DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>)</cite> first generate continuous-valued hidden states autoregressively, then employ flow-matching or diffusion-based modules, such as DiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib39\" title=\"\">2023</a>)</cite>, to sample and produce mel spectrograms or a continuous latent space learned by a VAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Pinheiro&#160;Cinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib40\" title=\"\">2021</a>)</cite>. Moreover, VAE-based AR methods, including GMM-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>)</cite> and KALLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>)</cite>, initially leverage a VAE to estimate a latent distribution (mean and variance), and subsequently autoregressively predict latent representations sampled from this distribution. NAR models such as E2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>)</cite> and F5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, on the other hand, predict mel-spectrograms directly via iterative mask-based refinement. Recently, several streaming TTS systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib54\" title=\"\">2024</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib46\" title=\"\">2025</a>; Sheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib49\" title=\"\">2025b</a>)</cite>. Most of these approaches model discrete features using a language model, followed by a flow-matching module or other similar components to generate speech features. Such architectures are relatively complex and may introduce potentially higher latency.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "melle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AR TTS models based on mel-spectrogram synthesis typically formulate the generation process as a sequential next-frame prediction task. Formally, given an input text sequence <math alttext=\"\\bm{x}=[x_{1},x_{2},...,x_{N}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119961;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{x}=[x_{1},x_{2},...,x_{N}]</annotation></semantics></math>, it&#8217;s aimed at generating an acoustic mel-spectrogram sequence <math alttext=\"\\bm{y}=[\\bm{y}_{1},\\bm{y}_{2},...,\\bm{y}_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119962;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119962;</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}=[\\bm{y}_{1},\\bm{y}_{2},...,\\bm{y}_{T}]</annotation></semantics></math>, where each frame <math alttext=\"\\bm{y}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math> denotes the spectral representation at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the number of mel-frequency bands.\nIn AR modeling, each mel-spectrogram frame <math alttext=\"\\bm{y}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#119962;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{y}_{t}</annotation></semantics></math> is generated conditionally depending on the textual content <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>&#119961;</mi><annotation encoding=\"application/x-tex\">\\bm{x}</annotation></semantics></math> and previous frames <math alttext=\"\\bm{y}_{&lt;t}=[\\bm{y}_{1},...,\\bm{y}_{t-1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119962;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{&lt;t}=[\\bm{y}_{1},...,\\bm{y}_{t-1}]</annotation></semantics></math>. Thus, the generative process can be described by the following conditional probability decomposition:</p>\n\n",
                "matched_terms": [
                    "each",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Postnet consists of multiple convolutional layers to refine generated acoustic features like methods in <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib43\" title=\"\">2018</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib26\" title=\"\">2019</a>)</cite>. Given the output of the Sampling Module <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math>, the Postnet predicts a residual term, which is subsequently added back to <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> and produces the refined output <math alttext=\"\\bm{y}_{t}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(2)}</annotation></semantics></math>. During training, the model employs the teacher-forcing paradigm; at inference, all intermediate <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> frames are first collected and then passed through the Postnet to obtain the final output <math alttext=\"\\bm{y}_{t}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(2)}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The generation process interleaves text and audio chunks in the input to the AR LM, maintaining a causal mask over the sequence. By setting a relatively smaller <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p2.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math>, each audio chunk is generated with sufficient preceding textual context to ensure coherent synthesis. At inference time, chunk-by-chunk audio generation is performed until the last audio chunk, where the Stop-Prediction Module is invoked to decide the termination point of synthesis.</p>\n\n",
                "matched_terms": [
                    "each",
                    "setting",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite> directly predicts the mean and variance of a Gaussian distribution, then samples embeddings accordingly; implementation details are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A5.SS1\" title=\"E.1 Gaussian Sampling in MELLE &#8227; Appendix E MELLE &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>. We propose to replace the Gaussian sampling mechanism with a Bayesian evidential approach. Specifically, NIG distribution parameters are predicted from the AR hidden representation <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math> via a linear layer followed by suitable activation functions to constrain each parameter within appropriate numerical ranges:</p>\n\n",
                "matched_terms": [
                    "melle",
                    "sampling",
                    "embeddings",
                    "layer",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">whereas MELLE employs a KL loss for sampling.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "melle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset is derived from the Librispeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib38\" title=\"\">2015</a>)</cite> training set and contains audio samples whose duration is from 0.5 second to 14 seconds, creating a final training dataset containing approximately 706 hours of speech. To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, our training data are augmented by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite>, IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite>, F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite>, and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>. Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. Details of the data processing methodology can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS1\" title=\"C.1 Training Data &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "f5tts",
                    "diversity",
                    "librispeech",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE, BELLE-stream and MELLE are trained on the training dataset, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For BELLE-stream, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. It&#8217;s observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre. Details about model configuration and training could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS2\" title=\"C.2 Model Configurations &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS3\" title=\"C.3 Training Details of Main Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.3</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "librispeech",
                    "each",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot TTS capabilities of our model is evaluated using the LibriSpeech test-clean subset by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, two inference conditions are considered: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where a reference utterance and its corresponding transcription from a given speaker is used as a prompt, and then the model generates speech for a different sentence.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, word error rate (WER) is reported to measure intelligibility and robustness. WER-C and WER-H are evaluated using Conformer<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and HuBERT<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite> based ASR models respectively. Speaker similarity is measured via cosine similarity of extracted speaker embeddings, with SIM-o referencing the original speech prompt and SIM-r referencing the vocoder-reconstructed prompt.</p>\n\n",
                "matched_terms": [
                    "embeddings",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, it&#8217;s found that the middle-layer features contain rich paralinguistic information, which can be leveraged to assess the acoustic characteristics of speech. Frame-level hidden states from layer-13 are extracted for each sample, mean-pooled and L2-normalized, followed by computation of within-group pairwise cosine similarity and L1/L2 distances.</p>\n\n",
                "matched_terms": [
                    "distances",
                    "pairwise",
                    "diversity",
                    "each",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, BELLE, F5-TTS, and Ground Truth achieve MOS scores of around 4.2, indicating that the audio produced by both F5-TTS and BELLE is close to that of natural human speech. F5-TTS attains a slightly higher MOS, which can be attributed to its lower background noise &#8212; human raters tend to prefer cleaner audio. In contrast, the real speech in Ground Truth inherently contains a certain level of noise, and BELLE replicates this natural background characteristic, leading to MOS scores that are very close to the Ground Truth.\nRegarding SMOS, it is worth noting that in the LibriSpeech dataset, different utterances from the same speaker sometimes exhibit perceptible timbre variation, which results in a relatively lower SMOS score for Ground Truth. BELLE achieves the highest speaker similarity among all tested systems. Notably, while both F5&#8209;TTS and MaskGCT were trained on datasets comprising 50,000 hours of English speech, BELLE was trained on less than 5,000 hours of data, yet it attained competitive, and in some instances superior performance. These results underscore the efficacy of the proposed Bayesian sampling strategy and the multi&#8209;teacher learning framework.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "librispeech",
                    "f5tts",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE attains the best WER and SIM scores under the Continuation setting. Under the Cross-sentence setting, MaskGCT achieves slightly higher scores; however, in our reproduction, MaskGCT&#8217;s WER-C is only 3.86% under our evaluation protocol. This suggests that BELLE remains highly competitive in the Cross-sentence scenario. Additionally, BELLE outperforms MELLE across both subjective and objective metrics, including MOS, SMOS, WER, and SIM. These results demonstrate that Bayesian sampling delivers superior performance compared to Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "sampling",
                    "metrics",
                    "setting",
                    "under",
                    "belle",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since BELLE-stream is finetuned to a fixed speaker timbre, it does not accept an audio prompt; therefore, it is excluded from SMOS scoring in the subjective evaluation and SIM computation in the objective evaluation, and its results are reported only under the Cross-sentence setting. BELLE-stream only takes the target text to be synthesized as input. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, streaming generation does not cause significant degradation in speech naturalness, achieving higher MOS scores than non&#8209;streaming MELLE. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE-stream achieves the lowest WER among streaming TTS systems, with no appreciable increase in WER compared to BELLE. In our tests, BELLE-stream achieved a real-time factor (RTF)&#8212;defined as the ratio of audio generation time to the length of the generated audio&#8212;of 0.55 when producing a 10-second utterance. Given that BELLE-stream operates with 0.8-second chunks, this implies a first-packet latency (FPL) of only 440ms, demonstrating its effective balance between performance and latency.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "setting",
                    "generated",
                    "under",
                    "belle",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computational resources, the analysis experiments are conducted on a smaller-scale dataset. The <span class=\"ltx_text ltx_font_italic\">1-teacher</span> setting trains solely on the original Librispeech data without synthesized augmentation. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting incorporates additional synthesized speech from two TTS models, while the <span class=\"ltx_text ltx_font_italic\">7-teacher</span> setting augments the training data with speech from all six TTS models described earlier. The <span class=\"ltx_text ltx_font_italic\">data-aug</span> configuration serves as a baseline using conventional data augmentation, randomly sampling from all available data sources during training. Details about the small dataset and model training configuration could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS4\" title=\"C.4 Training Details of Analysis Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "librispeech",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T4\" title=\"Table 4 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of BELLE consistently improves as the number of teachers increases. Furthermore, the possibility that the performance gain is solely due to the increased amount of training data is excluded. In the <span class=\"ltx_text ltx_font_italic\">data-aug</span> setting, the total training data is larger than that of the <span class=\"ltx_text ltx_font_italic\">3-teacher</span> configuration; however, its performance is inferior to <span class=\"ltx_text ltx_font_italic\">3-teacher</span> on multiple metrics. This degradation may be attributed to the conventional data augmentation strategy, in which the same text can correspond to acoustically different speeches, potentially causing confusion for the model. In contrast, under our proposed multi-teacher training scheme, increasing the number of teachers leads to consistent performance gains, thereby demonstrating the effectiveness of the multi teacher training strategy.</p>\n\n",
                "matched_terms": [
                    "under",
                    "metrics",
                    "belle",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study is conducted to investigate how the sampling module and flux loss affect the performance of the BELLE model. Due to limited computational resources, the analysis experiments are also conducted on the smaller-scale dataset. Our experiments are based on the BELLE <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting. As demonstrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T5\" title=\"Table 5 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the sampling module plays a more crucial role than flux loss. Removing the flux loss results in only a slight performance degradation, whereas removing the sampling module leads to a severe performance drop. This indicates that Bayesian sampling plays a crucial role in the effectiveness of BELLE.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE, a novel continuous-valued AR TTS model integrating Bayesian EDL for sampling mel-spectrogram frames. Our approach addresses the underexplored area of effective sampling methods for continuous-valued AR TTS. Additionally, a multi-teacher knowledge distillation framework is introduced, substantially improving TTS synthesis quality using synthesized data from publicly available models. Experimental results highlight the efficacy of Bayesian evidential sampling and multi-teacher distillation in achieving competitive speech naturalness, speaker similarity, and diversity, rivaling models trained on much larger real-data corpora. Remarkably, our approach attains these results using only one-tenth of the data, much of it synthetic, while enabling high-quality, low-latency TTS suitable for both offline and streaming applications.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "diversity",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE introduces a novel Bayesian evidential sampling approach within continuous-valued autoregressive text-to-speech, significantly enhancing the zero-shot TTS synthesis quality. By effectively modeling and generating natural, expressive, and intelligible speech with limited reference data, BELLE advances the flexibility and realism of synthesized audio, making it valuable for various beneficial applications in society. Notably, BELLE can facilitate natural human-machine conversational systems, assistive communication technologies for individuals with speech impairments, and personalized education platforms, thereby positively impacting accessibility, education, and user experiences in interactive dialogue systems.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, we augment our training data by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite> (LLM + flow matching-based streaming TTS), IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite> (AR discrete acoustic token TTS), SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite> (LLM-based single-stage TTS), F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite> (flow matching-based efficient TTS), MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite> (masked generative NAR TTS), and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite> (cross-lingual expressive TTS). Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. For consistency, all synthesized audio samples generated by these models are also resampled to 16 kHz.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "each",
                    "f5tts",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input mel-spectrograms first pass through a 3-layer prenet with dropout of 0.5 in both training and inference stages following Tacotron <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib52\" title=\"\">2017</a>)</cite>. The AR LM is a decoder-only Transformer consisting of 12 blocks, each with 16 attention heads, a hidden size of 1024, a feed-forward dimension of 4096, and a dropout rate of 0.1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is modified from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lifeiteng/vall-e\" title=\"\">https://github.com/lifeiteng/vall-e</a>.</span></span></span>. The sampling module includes a linear projection to derive the sampling parameters and a 3-layer residual MLP for denoising. A post-processing module comprising five convolutional blocks (a kernel size of 5 and 256 channels) is applied for mel-spectrogram refinement. The final waveform is synthesized using the pretrained HiFi-GAN vocoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib22\" title=\"\">2020</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The pretrained vocoder can be found in <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mechanicalsea/speecht5-tts\" title=\"\">https://huggingface.co/mechanicalsea/speecht5-tts</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train BELLE, BELLE-stream and reproduce MELLE on the data in Sec.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>), with predefined training weights that the original Librispeech audio is weighted by 0.22 and each synthesized audio source (six TTS models) is weighted by 0.13, which ensures the original Librispeech audio receives approximately twice the weighting of synthesized audio, reflecting relative considerations of synthetic audio quality. Models are trained by AdamW optimiser with a total batch size of about 160K frames distributed across 16 NVIDIA A800 GPUs. BELLE and MELLE&#8217;s training proceeds for 450K updates, where the learning rate is first linearly warmed up to a peak value of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> over the initial 10% of training steps and thereafter linearly decayed to zero. Regarding the hyperparameters for BELLE, we set <math alttext=\"\\lambda=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.5</annotation></semantics></math> in Eqn.&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E10\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <math alttext=\"\\lambda_{\\text{samp}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.2</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math> in Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E8\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As for MELLE, our hyperparameter settings strictly follow the original MELLE paper, where <math alttext=\"\\lambda_{\\text{samp}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "melle",
                    "relative",
                    "librispeech",
                    "each",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE-stream is initialized from the trained BELLE model, trained with a batch size of 80K frames across 8 NVIDIA A800 GPUs for 150K updates, using <math alttext=\"\\lambda_{\\text{flux}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.1</annotation></semantics></math> while keeping all other hyperparameters identical to BELLE. In our implementation, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m4\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. Here, <math alttext=\"S_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}</annotation></semantics></math> is computed from the number of phonemes obtained after G2P conversion, while <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math> is determined from the number of mel-spectrogram frames. Let <math alttext=\"L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{text}}</annotation></semantics></math> and <math alttext=\"L_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}</annotation></semantics></math> denote the total phoneme count and the total mel-frame count of an utterance, respectively. During training, we filter out audio samples whose ratio <math alttext=\"L_{\\mathrm{audio}}:L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>audio</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>L</mi><mi>text</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}:L_{\\mathrm{text}}</annotation></semantics></math> is less than <math alttext=\"2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m10\" intent=\":literal\"><semantics><mn>2.5</mn><annotation encoding=\"application/x-tex\">2.5</annotation></semantics></math>, ensuring that each text chunk retains sufficient corresponding audio frames for effective streaming generation. We observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "librispeech",
                    "each",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the zero-shot TTS capabilities of our model using the LibriSpeech test-clean subset following VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite> by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, we consider two inference conditions: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where we use a reference utterance and its corresponding transcription from a given speaker as a prompt, and then the model generates speech for a different sentence while preserving speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, we report word error rate (WER) to measure intelligibility and robustness, using two automatic speech recognition models: a Conformer-Transducer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and a fine-tuned HuBERT-Large model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite>. We denote results obtained from these systems as WER-C and WER-H, respectively. To quantify speaker similarity, we calculate the cosine similarity between extracted speaker embeddings using a WavLM-TDCNN model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib6\" title=\"\">2022</a>)</cite>. We provide two similarity metrics: SIM-o computes the similarity against the original speech prompt, whereas SIM-r uses the vocoder-reconstructed version of the prompt.</p>\n\n",
                "matched_terms": [
                    "embeddings",
                    "metrics",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, we obtain MOS and SMOS scores via a crowdsourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. We evaluate MOS and SMOS following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Note that BELLE-stream is fine-tuned to a fixed speaker timbre and does not use an audio prompt; therefore, it is not evaluated for SMOS or SIM metrics, and its results are reported only under the Cross-sentence setting.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "setting",
                    "generated",
                    "under",
                    "crosssentence",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM in <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, we adopt the 13th (of 24) layer of WavLM-Large as the speaker representation; 1,234 audio prompts are sampled from the LibriSpeech test-clean subset, and under the Cross-sentence setting each prompt is inferred three times to obtain 1,234 groups of outputs (three per group); frame-level hidden states from layer 13 are extracted for each generation, mean-pooled and L2-normalized, and within-group pairwise cosine similarity, L1, and L2 distances are computed; corpus-level means and standard deviations across all 1,234 groups are reported to quantify the diversity of the generated speech.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "prompts",
                    "pairwise",
                    "three",
                    "librispeech",
                    "setting",
                    "wavlmlarge",
                    "generated",
                    "cosine",
                    "layer",
                    "diversity",
                    "each",
                    "computed",
                    "under",
                    "times",
                    "crosssentence",
                    "distances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, the latent sampling module assumes that the embedding at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, denoted as <math alttext=\"\\bm{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119963;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{z}_{t}</annotation></semantics></math>, is drawn from a multivariate Gaussian distribution:</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "melle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the TTS task, we focus on the Mean Opinion Score (MOS) and Speaker Similarity (SMOS). From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. The details are as follows: For speech quality evaluation, we conducted an MOS (Mean Opinion Score) test and explicitly instructed the raters to focus on assessing audio quality and naturalness, while ignoring differences in style (e.g., timbre, emotion, and prosody). The raters were presented with and scored samples, and each rater was asked to evaluate the subjective naturalness on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "generated",
                    "under",
                    "each",
                    "crosssentence",
                    "comparison"
                ]
            }
        ]
    },
    "S6.T4": {
        "caption": "Table 4: Comparison of WER (%) and speaker similarity metrics for BELLE under different conditions trained on a smaller-scale dataset. 1-teacher indicates training with only the original Librispeech dataset; 3-teacher additionally includes XTTS-v2 and MaskGCT synthesized data; 7-teacher further includes synthesized data from six TTS models. data-aug randomly samples from combined all data sources of 7 teachers.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Continuation</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Cross-Sentence</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-H</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-r</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-o</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-H</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-r</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-o</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BELLE <span class=\"ltx_text ltx_font_italic\">1-teacher</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.344</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.303</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.374</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.330</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BELLE <span class=\"ltx_text ltx_font_italic\">3-teacher</span>\n</th>\n<td class=\"ltx_td ltx_align_center\">2.04</td>\n<td class=\"ltx_td ltx_align_center\">2.66</td>\n<td class=\"ltx_td ltx_align_center\">0.398</td>\n<td class=\"ltx_td ltx_align_center\">0.362</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.12</span></td>\n<td class=\"ltx_td ltx_align_center\">4.84</td>\n<td class=\"ltx_td ltx_align_center\">0.433</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.395</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BELLE <span class=\"ltx_text ltx_font_italic\">7-teacher</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.444</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.409</span></td>\n<td class=\"ltx_td ltx_align_center\">4.17</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.485</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.449</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">BELLE <span class=\"ltx_text ltx_font_italic\">data-aug</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.439</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.404</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.480</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">0.443</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "combined",
            "models",
            "simo",
            "six",
            "original",
            "wer",
            "synthesized",
            "librispeech",
            "comparison",
            "sources",
            "crosssentence",
            "similarity",
            "tts",
            "randomly",
            "speaker",
            "from",
            "training",
            "werc",
            "system",
            "conditions",
            "trained",
            "belle",
            "3teacher",
            "includes",
            "simr",
            "samples",
            "metrics",
            "smallerscale",
            "maskgct",
            "under",
            "further",
            "indicates",
            "dataaug",
            "only",
            "xttsv2",
            "1teacher",
            "additionally",
            "7teacher",
            "teachers",
            "continuation",
            "all",
            "data",
            "dataset",
            "different",
            "werh"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T4\" title=\"Table 4 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of BELLE consistently improves as the number of teachers increases. Furthermore, the possibility that the performance gain is solely due to the increased amount of training data is excluded. In the <span class=\"ltx_text ltx_font_italic\">data-aug</span> setting, the total training data is larger than that of the <span class=\"ltx_text ltx_font_italic\">3-teacher</span> configuration; however, its performance is inferior to <span class=\"ltx_text ltx_font_italic\">3-teacher</span> on multiple metrics. This degradation may be attributed to the conventional data augmentation strategy, in which the same text can correspond to acoustically different speeches, potentially causing confusion for the model. In contrast, under our proposed multi-teacher training scheme, increasing the number of teachers leads to consistent performance gains, thereby demonstrating the effectiveness of the multi teacher training strategy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper-distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://belletts.github.io/Belle/\" title=\"\">https://belletts.github.io/Belle/</a>.\nThe code, checkpoints, and synthetic data will be released after the paper is accepted.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "belle",
                    "synthesized",
                    "samples",
                    "data",
                    "from",
                    "training",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, autoregressive (AR) next-token prediction models based on discrete audio codecs have gained significant attention in the audio generation and text-to-speech (TTS) communities, driven by the success of models such as AudioLM <cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib4\" title=\"\">2023</a>)</cite> and VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite>, among others. However, continuous-valued prediction has emerged as a promising alternative due to its ability to eliminate quantization errors introduced by token-based codecs <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>; Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib8\" title=\"\">2024b</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>; Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib25\" title=\"\">2025</a>)</cite>.\nGenerative models typically rely on sampling mechanisms to introduce stochasticity and improve synthesis quality. In audio-codec-based AR TTS, this is often achieved using mature top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> sampling techniques, similar to those employed in token-based language models (LMs). Diffusion-based models, by contrast, introduce randomness by injecting noise into the input, thereby embedding variability directly into the generation process. However, effective sampling strategies for continuous-valued AR models remain underexplored.\nRecent studies have begun to address this gap. In image generation, MAR proposed a hybrid approach where an AR model predicts conditioning signals, followed by diffusion-based sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib27\" title=\"\">2024a</a>)</cite>. In zero-shot TTS, MELLE introduced Gaussian sampling <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, while FELLE adopted flow-matching techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib30\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite> to enhance sampling efficiency and output diversity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE: <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS Synthesis&#8212;a continuous-valued AR model that directly predicts mel-spectrograms from text inputs based on the evidential deep learning (EDL) framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Amini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib2\" title=\"\">2020</a>)</cite>. This represents the first application of Bayesian deep learning to TTS, enabling controllable and theoretically grounded sampling at test time.\nSpecifically, BELLE models each predicted mel-spectrogram frame as a sample from a Gaussian distribution, whose parameters themselves are drawn from hyper-distributions. Metaphorically, the model first predicts the parameters of this hyper-distribution, then samples a specific Gaussian distribution from it, and finally generates mel-spectrogram frames from the sampled Gaussian.\nHowever, learning accurate Gaussian posterior estimates from limited data poses a challenge, as typical TTS datasets contain only a single audio recording per text prompt. To overcome this, the training corpus is augmented with synthetic audio samples generated by several publicly available pre-trained zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "samples",
                    "data",
                    "from",
                    "training",
                    "only",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present BELLE, the first AR TTS model using evidential deep learning to train and sample from continuous-valued Mel-Spectrogram space. The results show that Bayesian evidential sampling outperforms conventional Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "from",
                    "tts",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose the first multi-teacher knowledge distillation approach for TTS model training, integrating it with evidential deep learning.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE achieves competitive performance compared with leading open-source TTS systems trained on approximately 50k hours of speech data, while BELLE uses only around 5k hours of data that largely consist of synthetic samples. Additionally, its streaming variant, BELLE-stream, attains an effective balance between audio generation quality and latency.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "belle",
                    "samples",
                    "additionally",
                    "data",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Continuous-valued TTS models can be broadly categorized into AR and non-autoregressive (NAR) approaches. AR methods include MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, which generates mel spectrogram frames using decoder-only Transformer-based AR modelling combined with Gaussian sampling. FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite>, ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>)</cite> and DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>)</cite> first generate continuous-valued hidden states autoregressively, then employ flow-matching or diffusion-based modules, such as DiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib39\" title=\"\">2023</a>)</cite>, to sample and produce mel spectrograms or a continuous latent space learned by a VAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Pinheiro&#160;Cinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib40\" title=\"\">2021</a>)</cite>. Moreover, VAE-based AR methods, including GMM-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>)</cite> and KALLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>)</cite>, initially leverage a VAE to estimate a latent distribution (mean and variance), and subsequently autoregressively predict latent representations sampled from this distribution. NAR models such as E2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>)</cite> and F5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, on the other hand, predict mel-spectrograms directly via iterative mask-based refinement. Recently, several streaming TTS systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib54\" title=\"\">2024</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib46\" title=\"\">2025</a>; Sheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib49\" title=\"\">2025b</a>)</cite>. Most of these approaches model discrete features using a language model, followed by a flow-matching module or other similar components to generate speech features. Such architectures are relatively complex and may introduce potentially higher latency.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "models",
                    "from",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evidential Deep Learning (EDL) <cite class=\"ltx_cite ltx_citemacro_citep\">(Amini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib2\" title=\"\">2020</a>)</cite> is commonly used in recurrent neural networks (RNNs) and Transformers to provide per-time-step uncertainty <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib10\" title=\"\">2024</a>; Khot et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib20\" title=\"\">2024</a>; Marvi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib34\" title=\"\">2025</a>)</cite>, benefiting tasks like time-series forecasting <cite class=\"ltx_cite ltx_citemacro_citep\">(Khot et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib20\" title=\"\">2024</a>; Marvi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib34\" title=\"\">2025</a>)</cite>. Other Bayesian approaches, such as Bayesian neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Kononenko, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib23\" title=\"\">1989</a>)</cite> in Long Short-Term Memory (LSTMs) and Transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(Mu&#241;oz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib36\" title=\"\">2024</a>)</cite>, and frameworks like SYMHnet <cite class=\"ltx_cite ltx_citemacro_citep\">(Abduallah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib1\" title=\"\">2024</a>)</cite>, also address sequential uncertainty. Previous Bayesian TTS work focused on HMM-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Hashimoto et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib17\" title=\"\">2009b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib16\" title=\"\">a</a>; Lu &amp; King, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib33\" title=\"\">2012</a>)</cite>; to our knowledge, we are the first to apply Bayesian methods to neural network-based TTS.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Knowledge distillation (KD) transfers knowledge from larger teacher models to smaller, faster student models <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib37\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib53\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib28\" title=\"\">2024b</a>)</cite>. Originally, KD is used to reduce exposure bias in AR models like Tacotron <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib52\" title=\"\">2017</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib31\" title=\"\">2020</a>)</cite> and to guide NAR models such as FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib41\" title=\"\">2019</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib42\" title=\"\">2020</a>)</cite>. It has been further applied to improve pronunciation <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib37\" title=\"\">2025</a>)</cite>, style transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib53\" title=\"\">2022</a>)</cite>, perceptual quality in diffusion-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib28\" title=\"\">2024b</a>)</cite>, and to distill semantic knowledge from HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(G&#225;llego et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib14\" title=\"\">2025</a>)</cite>. To our knowledge, multi-teacher KD has not yet been explored in TTS.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "tts",
                    "further"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AR TTS models based on mel-spectrogram synthesis typically formulate the generation process as a sequential next-frame prediction task. Formally, given an input text sequence <math alttext=\"\\bm{x}=[x_{1},x_{2},...,x_{N}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119961;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{x}=[x_{1},x_{2},...,x_{N}]</annotation></semantics></math>, it&#8217;s aimed at generating an acoustic mel-spectrogram sequence <math alttext=\"\\bm{y}=[\\bm{y}_{1},\\bm{y}_{2},...,\\bm{y}_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119962;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119962;</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}=[\\bm{y}_{1},\\bm{y}_{2},...,\\bm{y}_{T}]</annotation></semantics></math>, where each frame <math alttext=\"\\bm{y}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math> denotes the spectral representation at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> represents the number of mel-frequency bands.\nIn AR modeling, each mel-spectrogram frame <math alttext=\"\\bm{y}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#119962;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{y}_{t}</annotation></semantics></math> is generated conditionally depending on the textual content <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>&#119961;</mi><annotation encoding=\"application/x-tex\">\\bm{x}</annotation></semantics></math> and previous frames <math alttext=\"\\bm{y}_{&lt;t}=[\\bm{y}_{1},...,\\bm{y}_{t-1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119962;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{&lt;t}=[\\bm{y}_{1},...,\\bm{y}_{t-1}]</annotation></semantics></math>. Thus, the generative process can be described by the following conditional probability decomposition:</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Sampling Module introduces critical stochasticity into the generative process. Specifically, the hidden states <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math> from the AR LM are first projected down to the mel-spectrogram dimension. The projected representation is assumed to follow a specified parametric distribution (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, a Gaussian distribution). Subsequently, the Sampling Module samples from this distribution, obtaining an intermediate sampled representation denoted as <math alttext=\"\\bm{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119963;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{z}_{t}</annotation></semantics></math>. Afterwards, a multilayer perception (MLP)-based denoising network refines and further processes the sampled representation, and the resulting denoised output is denoted as <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> as shown in the right part of Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 General Architecture of Mel-based Autoregressive TTS Model &#8227; 3 Mel-based Autoregressive TTS &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The detailed description of our proposed evidential Bayesian sampling method is provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4\" title=\"4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "from",
                    "further"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Postnet consists of multiple convolutional layers to refine generated acoustic features like methods in <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib43\" title=\"\">2018</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib26\" title=\"\">2019</a>)</cite>. Given the output of the Sampling Module <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math>, the Postnet predicts a residual term, which is subsequently added back to <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> and produces the refined output <math alttext=\"\\bm{y}_{t}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(2)}</annotation></semantics></math>. During training, the model employs the teacher-forcing paradigm; at inference, all intermediate <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> frames are first collected and then passed through the Postnet to obtain the final output <math alttext=\"\\bm{y}_{t}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(2)}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EDL is a Bayesian approach for uncertainty quantification in regression tasks by explicitly modelling the posterior distribution of predictions.\nIn our TTS setting, the observed data <math alttext=\"\\bm{y}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119962;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{y}_{t}</annotation></semantics></math> denotes the mel-spectrogram frame at time step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, where <math alttext=\"\\bm{y}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119962;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{y}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite> directly predicts the mean and variance of a Gaussian distribution, then samples embeddings accordingly; implementation details are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A5.SS1\" title=\"E.1 Gaussian Sampling in MELLE &#8227; Appendix E MELLE &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>. We propose to replace the Gaussian sampling mechanism with a Bayesian evidential approach. Specifically, NIG distribution parameters are predicted from the AR hidden representation <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math> via a linear layer followed by suitable activation functions to constrain each parameter within appropriate numerical ranges:</p>\n\n",
                "matched_terms": [
                    "samples",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE is trained end-to-end using a composite loss:</p>\n\n",
                "matched_terms": [
                    "trained",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A couple of open-sourced TTS models are used to generate multiple audio samples given the texts provided by the dataset. Given a textual input <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#119961;</mi><annotation encoding=\"application/x-tex\">\\bm{x}</annotation></semantics></math>, audio samples are synthesized using a set of <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> external TTS teacher models, resulting in <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> synthesized mel-spectrograms. Together with the original human-recorded mel-spectrogram from the dataset, a total of <math alttext=\"N+1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">N+1</annotation></semantics></math> corresponding mel-spectrograms are gotten. Denote these ground-truth mel-spectrograms as <math alttext=\"\\bm{y}^{\\text{gt}}_{i},~i=1,2,\\dots,N+1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><msubsup><mi>&#119962;</mi><mi>i</mi><mtext>gt</mtext></msubsup><mo rspace=\"0.497em\">,</mo><mi>i</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mn>2</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bm{y}^{\\text{gt}}_{i},~i=1,2,\\dots,N+1</annotation></semantics></math>. Each mel-spectrogram is treated as a ground-truth example and calculated an individual loss corresponding to each one. To balance the contributions from multiple teachers and the original dataset recording, the predefined weights <math alttext=\"w_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math> are assigned to each ground-truth mel-spectrogram.</p>\n\n",
                "matched_terms": [
                    "models",
                    "original",
                    "tts",
                    "synthesized",
                    "samples",
                    "teachers",
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset is derived from the Librispeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib38\" title=\"\">2015</a>)</cite> training set and contains audio samples whose duration is from 0.5 second to 14 seconds, creating a final training dataset containing approximately 706 hours of speech. To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, our training data are augmented by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite>, IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite>, F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite>, and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>. Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. Details of the data processing methodology can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS1\" title=\"C.1 Training Data &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "xttsv2",
                    "six",
                    "tts",
                    "samples",
                    "librispeech",
                    "maskgct",
                    "data",
                    "from",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE, BELLE-stream and MELLE are trained on the training dataset, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>). For BELLE-stream, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. It&#8217;s observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre. Details about model configuration and training could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS2\" title=\"C.2 Model Configurations &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS3\" title=\"C.3 Training Details of Main Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.3</span></a> respectively.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "librispeech",
                    "speaker",
                    "data",
                    "from",
                    "training",
                    "dataset",
                    "trained",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot TTS capabilities of our model is evaluated using the LibriSpeech test-clean subset by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, two inference conditions are considered: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where a reference utterance and its corresponding transcription from a given speaker is used as a prompt, and then the model generates speech for a different sentence.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "librispeech",
                    "continuation",
                    "speaker",
                    "from",
                    "conditions",
                    "different",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, word error rate (WER) is reported to measure intelligibility and robustness. WER-C and WER-H are evaluated using Conformer<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and HuBERT<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite> based ASR models respectively. Speaker similarity is measured via cosine similarity of extracted speaker embeddings, with SIM-o referencing the original speech prompt and SIM-r referencing the vocoder-reconstructed prompt.</p>\n\n",
                "matched_terms": [
                    "models",
                    "similarity",
                    "simo",
                    "original",
                    "wer",
                    "simr",
                    "speaker",
                    "werc",
                    "werh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, MOS and SMOS scores are obtained via a crowd-sourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. MOS and SMOS is evaluated following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "similarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, it&#8217;s found that the middle-layer features contain rich paralinguistic information, which can be leveraged to assess the acoustic characteristics of speech. Frame-level hidden states from layer-13 are extracted for each sample, mean-pooled and L2-normalized, followed by computation of within-group pairwise cosine similarity and L1/L2 distances.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, BELLE, F5-TTS, and Ground Truth achieve MOS scores of around 4.2, indicating that the audio produced by both F5-TTS and BELLE is close to that of natural human speech. F5-TTS attains a slightly higher MOS, which can be attributed to its lower background noise &#8212; human raters tend to prefer cleaner audio. In contrast, the real speech in Ground Truth inherently contains a certain level of noise, and BELLE replicates this natural background characteristic, leading to MOS scores that are very close to the Ground Truth.\nRegarding SMOS, it is worth noting that in the LibriSpeech dataset, different utterances from the same speaker sometimes exhibit perceptible timbre variation, which results in a relatively lower SMOS score for Ground Truth. BELLE achieves the highest speaker similarity among all tested systems. Notably, while both F5&#8209;TTS and MaskGCT were trained on datasets comprising 50,000 hours of English speech, BELLE was trained on less than 5,000 hours of data, yet it attained competitive, and in some instances superior performance. These results underscore the efficacy of the proposed Bayesian sampling strategy and the multi&#8209;teacher learning framework.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "belle",
                    "librispeech",
                    "maskgct",
                    "speaker",
                    "all",
                    "data",
                    "from",
                    "different",
                    "dataset",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE attains the best WER and SIM scores under the Continuation setting. Under the Cross-sentence setting, MaskGCT achieves slightly higher scores; however, in our reproduction, MaskGCT&#8217;s WER-C is only 3.86% under our evaluation protocol. This suggests that BELLE remains highly competitive in the Cross-sentence scenario. Additionally, BELLE outperforms MELLE across both subjective and objective metrics, including MOS, SMOS, WER, and SIM. These results demonstrate that Bayesian sampling delivers superior performance compared to Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "additionally",
                    "metrics",
                    "maskgct",
                    "continuation",
                    "under",
                    "werc",
                    "only",
                    "belle",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since BELLE-stream is finetuned to a fixed speaker timbre, it does not accept an audio prompt; therefore, it is excluded from SMOS scoring in the subjective evaluation and SIM computation in the objective evaluation, and its results are reported only under the Cross-sentence setting. BELLE-stream only takes the target text to be synthesized as input. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, streaming generation does not cause significant degradation in speech naturalness, achieving higher MOS scores than non&#8209;streaming MELLE. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE-stream achieves the lowest WER among streaming TTS systems, with no appreciable increase in WER compared to BELLE. In our tests, BELLE-stream achieved a real-time factor (RTF)&#8212;defined as the ratio of audio generation time to the length of the generated audio&#8212;of 0.55 when producing a 10-second utterance. Given that BELLE-stream operates with 0.8-second chunks, this implies a first-packet latency (FPL) of only 440ms, demonstrating its effective balance between performance and latency.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "wer",
                    "synthesized",
                    "speaker",
                    "under",
                    "from",
                    "only",
                    "belle",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity evaluation is conducted using the evaluation method showed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS2\" title=\"5.2 Evaluation Settings &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nIn the NIG prior, the parameter <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> denotes the scale of the inverse-gamma distribution over the variance <math alttext=\"\\sigma_{t}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#963;</mi><mi>t</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">\\sigma_{t}^{2}</annotation></semantics></math>. Increasing <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> raises the expected variance, thereby promoting greater diversity in the sampled outcomes (See Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E7\" title=\"In 4.2 From Gaussian Sampling to Bayesian Evidential Sampling &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nFrom Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T3\" title=\"Table 3 &#8227; 6.2 Diversity Analysis &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it&#8217;s observed that BELLE (<math alttext=\"\\beta*2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\beta*2</annotation></semantics></math>) exhibits the largest distances across all three metrics, confirming that increasing the sampling parameter <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m5\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> effectively enhances generation diversity. Default BELLE achieves moderate diversity, slightly higher than MELLE, suggesting that Bayesian sampling yields superior diversity compared to Gaussian sampling. In contrast, F5-TTS produces consistently small distances, indicating that its outputs are more deterministic and less diverse across repeated sampling. These findings highlight that, beyond intelligibility and speaker similarity, BELLE offers a controllable trade-off between stability and diversity at inference time, with the <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> parameter serving as a practical knob to adjust sample variability, analogous to the role of the temperature parameter in the sampling process of token-based language models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "similarity",
                    "metrics",
                    "speaker",
                    "all",
                    "from",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computational resources, the analysis experiments are conducted on a smaller-scale dataset. The <span class=\"ltx_text ltx_font_italic\">1-teacher</span> setting trains solely on the original Librispeech data without synthesized augmentation. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting incorporates additional synthesized speech from two TTS models, while the <span class=\"ltx_text ltx_font_italic\">7-teacher</span> setting augments the training data with speech from all six TTS models described earlier. The <span class=\"ltx_text ltx_font_italic\">data-aug</span> configuration serves as a baseline using conventional data augmentation, randomly sampling from all available data sources during training. Details about the small dataset and model training configuration could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS4\" title=\"C.4 Training Details of Analysis Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "six",
                    "original",
                    "tts",
                    "3teacher",
                    "randomly",
                    "1teacher",
                    "synthesized",
                    "librispeech",
                    "smallerscale",
                    "7teacher",
                    "all",
                    "data",
                    "from",
                    "training",
                    "dataset",
                    "dataaug",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An ablation study is conducted to investigate how the sampling module and flux loss affect the performance of the BELLE model. Due to limited computational resources, the analysis experiments are also conducted on the smaller-scale dataset. Our experiments are based on the BELLE <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting. As demonstrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T5\" title=\"Table 5 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the sampling module plays a more crucial role than flux loss. Removing the flux loss results in only a slight performance degradation, whereas removing the sampling module leads to a severe performance drop. This indicates that Bayesian sampling plays a crucial role in the effectiveness of BELLE.</p>\n\n",
                "matched_terms": [
                    "3teacher",
                    "smallerscale",
                    "indicates",
                    "dataset",
                    "only",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE, a novel continuous-valued AR TTS model integrating Bayesian EDL for sampling mel-spectrogram frames. Our approach addresses the underexplored area of effective sampling methods for continuous-valued AR TTS. Additionally, a multi-teacher knowledge distillation framework is introduced, substantially improving TTS synthesis quality using synthesized data from publicly available models. Experimental results highlight the efficacy of Bayesian evidential sampling and multi-teacher distillation in achieving competitive speech naturalness, speaker similarity, and diversity, rivaling models trained on much larger real-data corpora. Remarkably, our approach attains these results using only one-tenth of the data, much of it synthetic, while enabling high-quality, low-latency TTS suitable for both offline and streaming applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "similarity",
                    "tts",
                    "belle",
                    "synthesized",
                    "additionally",
                    "speaker",
                    "data",
                    "from",
                    "only",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure reproducibility, we provide detailed model configurations, training parameters, and training data specifications in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5\" title=\"5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3\" title=\"Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. Upon acceptance, we will release all code, checkpoints, and synthesized speech data, enabling the community to reliably reproduce our work and further explore its potential.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "all",
                    "data",
                    "training",
                    "further"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE introduces a novel Bayesian evidential sampling approach within continuous-valued autoregressive text-to-speech, significantly enhancing the zero-shot TTS synthesis quality. By effectively modeling and generating natural, expressive, and intelligible speech with limited reference data, BELLE advances the flexibility and realism of synthesized audio, making it valuable for various beneficial applications in society. Notably, BELLE can facilitate natural human-machine conversational systems, assistive communication technologies for individuals with speech impairments, and personalized education platforms, thereby positively impacting accessibility, education, and user experiences in interactive dialogue systems.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthesized",
                    "belle",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, alongside the benefits, zero-shot text-to-speech technologies like BELLE also pose potential risks. They could be misused in unethical or harmful scenarios, such as impersonation, identity fraud, and targeted social engineering attacks. In principle, BELLE could mimic any person&#8217;s voice from minimal audio samples, leading to malicious applications aimed at deceiving or misleading individuals. Therefore, responsible use and appropriate safeguards, such as speaker verification, synthetic audio detection, and strong regulatory oversight, are critical directions for addressing and mitigating these societal risks.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "samples",
                    "from",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although BELLE demonstrates strong performance, there are two main limitations. First, the current study focuses solely on training and evaluation with English speech data, without validating multilingual generalization; extending BELLE to diverse languages remains an important direction for future work. Second, as with most AR TTS systems, BELLE&#8217;s RTF is still higher than that of recent flow-matching-based models. Further research on predicting multiple mel-spectrogram frames per step could substantially reduce RTF and enable lower-latency streaming TTS.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "data",
                    "training",
                    "further",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the preparation of this work, large language models (LLMs) were employed to refine the writing for clarity and conciseness. During the experimental phase, LLMs also provided code modification suggestions that assisted in implementing and debugging parts of the system. All conceptual designs, experimental settings, and core contributions remain the authors&#8217; original work.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all",
                    "original",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset is derived from the Librispeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib38\" title=\"\">2015</a>)</cite> training set. We preprocess this dataset using a voice activity detection <cite class=\"ltx_cite ltx_citemacro_citep\">(SileroTeam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib45\" title=\"\">2024</a>)</cite> algorithm to remove prolonged silent intervals. Subsequently, to ensure quality and manageability, we pick out audio samples whose duration is from 0.5 second to 14 seconds, creating a final training dataset containing approximately 706 hours of speech. All audio samples are resampled to 16 kHz and converted into 80-dimensional mel-frequency spectrograms. Additionally, we apply grapheme-to-phoneme (G2P) <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/espeak-ng/espeak-ng\" title=\"\">https://github.com/espeak-ng/espeak-ng</a></span></span></span> conversion to preprocess textual transcriptions.</p>\n\n",
                "matched_terms": [
                    "additionally",
                    "samples",
                    "librispeech",
                    "all",
                    "from",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide richer acoustic diversity necessary for robust Bayesian distribution estimation, we augment our training data by synthesizing multiple audio samples for each textual input using six publicly available pretrained TTS models, namely CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>)</cite> (LLM + flow matching-based streaming TTS), IndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>)</cite> (AR discrete acoustic token TTS), SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>)</cite> (LLM-based single-stage TTS), F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite> (flow matching-based efficient TTS), MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>)</cite> (masked generative NAR TTS), and XTTS-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite> (cross-lingual expressive TTS). Including the TTS-synthesized data, the total dataset amounts to 4,817 hours of speech. For consistency, all synthesized audio samples generated by these models are also resampled to 16 kHz.</p>\n\n",
                "matched_terms": [
                    "models",
                    "xttsv2",
                    "six",
                    "tts",
                    "synthesized",
                    "samples",
                    "maskgct",
                    "all",
                    "data",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input mel-spectrograms first pass through a 3-layer prenet with dropout of 0.5 in both training and inference stages following Tacotron <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib52\" title=\"\">2017</a>)</cite>. The AR LM is a decoder-only Transformer consisting of 12 blocks, each with 16 attention heads, a hidden size of 1024, a feed-forward dimension of 4096, and a dropout rate of 0.1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is modified from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lifeiteng/vall-e\" title=\"\">https://github.com/lifeiteng/vall-e</a>.</span></span></span>. The sampling module includes a linear projection to derive the sampling parameters and a 3-layer residual MLP for denoising. A post-processing module comprising five convolutional blocks (a kernel size of 5 and 256 channels) is applied for mel-spectrogram refinement. The final waveform is synthesized using the pretrained HiFi-GAN vocoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib22\" title=\"\">2020</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The pretrained vocoder can be found in <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mechanicalsea/speecht5-tts\" title=\"\">https://huggingface.co/mechanicalsea/speecht5-tts</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "includes",
                    "synthesized",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train BELLE, BELLE-stream and reproduce MELLE on the data in Sec.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, following the training strategy of combining various data sources within each batch (see Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>), with predefined training weights that the original Librispeech audio is weighted by 0.22 and each synthesized audio source (six TTS models) is weighted by 0.13, which ensures the original Librispeech audio receives approximately twice the weighting of synthesized audio, reflecting relative considerations of synthetic audio quality. Models are trained by AdamW optimiser with a total batch size of about 160K frames distributed across 16 NVIDIA A800 GPUs. BELLE and MELLE&#8217;s training proceeds for 450K updates, where the learning rate is first linearly warmed up to a peak value of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> over the initial 10% of training steps and thereafter linearly decayed to zero. Regarding the hyperparameters for BELLE, we set <math alttext=\"\\lambda=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=0.5</annotation></semantics></math> in Eqn.&#160;equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E10\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <math alttext=\"\\lambda_{\\text{samp}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.2</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math> in Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E8\" title=\"In 4.3 Training Objectives &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As for MELLE, our hyperparameter settings strictly follow the original MELLE paper, where <math alttext=\"\\lambda_{\\text{samp}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>samp</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{flux}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "six",
                    "original",
                    "tts",
                    "belle",
                    "synthesized",
                    "librispeech",
                    "data",
                    "training",
                    "trained",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE-stream is initialized from the trained BELLE model, trained with a batch size of 80K frames across 8 NVIDIA A800 GPUs for 150K updates, using <math alttext=\"\\lambda_{\\text{flux}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>flux</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}=0.1</annotation></semantics></math> while keeping all other hyperparameters identical to BELLE. In our implementation, we set <math alttext=\"S_{\\mathrm{text}}=20\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>text</mi></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}=20</annotation></semantics></math> and <math alttext=\"S_{\\mathrm{audio}}=50\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>audio</mi></msub><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}=50</annotation></semantics></math>, corresponding to an audio chunk duration of approximately <math alttext=\"0.8\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m4\" intent=\":literal\"><semantics><mn>0.8</mn><annotation encoding=\"application/x-tex\">0.8</annotation></semantics></math> seconds. Here, <math alttext=\"S_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{text}}</annotation></semantics></math> is computed from the number of phonemes obtained after G2P conversion, while <math alttext=\"S_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">S_{\\mathrm{audio}}</annotation></semantics></math> is determined from the number of mel-spectrogram frames. Let <math alttext=\"L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>text</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{text}}</annotation></semantics></math> and <math alttext=\"L_{\\mathrm{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mi>audio</mi></msub><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}</annotation></semantics></math> denote the total phoneme count and the total mel-frame count of an utterance, respectively. During training, we filter out audio samples whose ratio <math alttext=\"L_{\\mathrm{audio}}:L_{\\mathrm{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mi>audio</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>L</mi><mi>text</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_{\\mathrm{audio}}:L_{\\mathrm{text}}</annotation></semantics></math> is less than <math alttext=\"2.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p2.m10\" intent=\":literal\"><semantics><mn>2.5</mn><annotation encoding=\"application/x-tex\">2.5</annotation></semantics></math>, ensuring that each text chunk retains sufficient corresponding audio frames for effective streaming generation. We observed that the synthesis quality deteriorates with speech prompt. To address this, we first investigate the potential of BELLE for streaming synthesis in a single-speaker scenario, finetuning BELLE-stream using recordings from a single speaker of Librispeech to fix its voice timbre.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "samples",
                    "librispeech",
                    "speaker",
                    "all",
                    "from",
                    "training",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computational resources, the analysis experiments are conducted on a smaller-scale dataset. The training dataset is derived from the Librispeech train-clean 100-hour subset, following the same processing procedure in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, which contains approximately 72 hours of speech. Training solely on the original Librispeech data without synthesized augmentation is denoted as <span class=\"ltx_text ltx_font_italic\">1-teacher</span>. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> condition introduces additional synthesized speech from XTTS-v2 and MaskGCT, about 218h. The <span class=\"ltx_text ltx_font_italic\">7-teacher</span> condition includes augmented speech from all six TTS models mentioned in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS1\" title=\"5.1 Training Data and Details &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, about 455h. The <span class=\"ltx_text ltx_font_italic\">data-aug</span> configuration serves as a strong baseline using a conventional data augmentation strategy, where data from all sources totaling 455 hours is randomly sampled during training. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> and <span class=\"ltx_text ltx_font_italic\">7-teacher</span> follow the training strategy in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.SS4\" title=\"4.4 Multi Teacher Learning &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>, with a total batch size of about 80K frames and 47K training steps.</p>\n\n",
                "matched_terms": [
                    "models",
                    "six",
                    "original",
                    "synthesized",
                    "librispeech",
                    "sources",
                    "tts",
                    "randomly",
                    "from",
                    "training",
                    "3teacher",
                    "includes",
                    "smallerscale",
                    "maskgct",
                    "dataaug",
                    "xttsv2",
                    "1teacher",
                    "7teacher",
                    "all",
                    "data",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the zero-shot TTS capabilities of our model using the LibriSpeech test-clean subset following VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite> by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, we consider two inference conditions: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where we use a reference utterance and its corresponding transcription from a given speaker as a prompt, and then the model generates speech for a different sentence while preserving speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "librispeech",
                    "continuation",
                    "speaker",
                    "from",
                    "conditions",
                    "different",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, we report word error rate (WER) to measure intelligibility and robustness, using two automatic speech recognition models: a Conformer-Transducer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and a fine-tuned HuBERT-Large model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite>. We denote results obtained from these systems as WER-C and WER-H, respectively. To quantify speaker similarity, we calculate the cosine similarity between extracted speaker embeddings using a WavLM-TDCNN model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib6\" title=\"\">2022</a>)</cite>. We provide two similarity metrics: SIM-o computes the similarity against the original speech prompt, whereas SIM-r uses the vocoder-reconstructed version of the prompt.</p>\n\n",
                "matched_terms": [
                    "models",
                    "similarity",
                    "simo",
                    "original",
                    "wer",
                    "simr",
                    "metrics",
                    "speaker",
                    "from",
                    "werc",
                    "werh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, we obtain MOS and SMOS scores via a crowdsourcing platform. MOS for evaluating overall speech quality, and SMOS for measuring speaker similarity between the generated audio and the prompt. From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. We evaluate MOS and SMOS following the detailed procedure described in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A6\" title=\"Appendix F Subjective Evaluation &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. Note that BELLE-stream is fine-tuned to a fixed speaker timbre and does not use an audio prompt; therefore, it is not evaluated for SMOS or SIM metrics, and its results are reported only under the Cross-sentence setting.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "samples",
                    "metrics",
                    "speaker",
                    "all",
                    "under",
                    "from",
                    "only",
                    "different",
                    "crosssentence",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For diversity evaluation, building on the layer-wise analysis of WavLM in <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib9\" title=\"\">2025</a>)</cite>, we adopt the 13th (of 24) layer of WavLM-Large as the speaker representation; 1,234 audio prompts are sampled from the LibriSpeech test-clean subset, and under the Cross-sentence setting each prompt is inferred three times to obtain 1,234 groups of outputs (three per group); frame-level hidden states from layer 13 are extracted for each generation, mean-pooled and L2-normalized, and within-group pairwise cosine similarity, L1, and L2 distances are computed; corpus-level means and standard deviations across all 1,234 groups are reported to quantify the diversity of the generated speech.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "librispeech",
                    "speaker",
                    "all",
                    "under",
                    "from",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain fairness and consistency across all evaluations, we filter the test utterances to only those between 4 and 10 seconds in duration, and report all evaluation metrics using a fixed evaluation set shared among all compared models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "only",
                    "metrics",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Bayesian Evidential Learning (EDL) provides a principled framework for uncertainty estimation in regression tasks by explicitly modeling the posterior distribution of predictions. Observed data <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> are assumed to be drawn independently and identically distributed (i.i.d.) from a Gaussian distribution with unknown mean and variance. According to Maximum Likelihood Estimation (MLE), we aim to find parameters <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi>&#956;</mi><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math> and <math alttext=\"\\sigma^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#963;</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">\\sigma^{2}</annotation></semantics></math> that maximize the likelihood of observing the given data <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math>, or equivalently, minimize the Negative Log Likelihood (NLL) loss. We model this problem by placing prior distributions on both the mean and variance:</p>\n\n",
                "matched_terms": [
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the TTS task, we focus on the Mean Opinion Score (MOS) and Speaker Similarity (SMOS). From the audio samples generated under the Cross-sentence setting, we select one sample per speaker, resulting in a total of 40 audio samples. All samples from different systems are resampled to 16kHz to ensure a fair comparison. The details are as follows: For speech quality evaluation, we conducted an MOS (Mean Opinion Score) test and explicitly instructed the raters to focus on assessing audio quality and naturalness, while ignoring differences in style (e.g., timbre, emotion, and prosody). The raters were presented with and scored samples, and each rater was asked to evaluate the subjective naturalness on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "tts",
                    "randomly",
                    "samples",
                    "speaker",
                    "all",
                    "under",
                    "from",
                    "different",
                    "crosssentence",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speaker similarity evaluation, we asked the raters to focus on the similarity of the speaker&#8217;s identity (timbre) to the reference, while ignoring differences in content, grammar, or audio quality. We paired each synthetic utterance with a reference utterance to assess the degree of matching between the synthesized speech and the target speaker. Each rater was asked to evaluate the speaker similarity on a 1-5 Likert scale. When scoring, the order of audio samples is randomly shuffled.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "randomly",
                    "synthesized",
                    "samples",
                    "speaker"
                ]
            }
        ]
    },
    "S6.T5": {
        "caption": "Table 5: Ablation study examining the effects of the sampling module and flux loss on BELLE 3-teacher.  indicates enabled,  indicates disabled.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Sampling</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Flux Loss</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Continuation</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Cross-Sentence</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-H</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-r</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-o</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-C</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER-H</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-r</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-o</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.382</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.344</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.392</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.351</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">2.50</td>\n<td class=\"ltx_td ltx_align_center\">3.25</td>\n<td class=\"ltx_td ltx_align_center\">0.388</td>\n<td class=\"ltx_td ltx_align_center\">0.347</td>\n<td class=\"ltx_td ltx_align_center\">5.61</td>\n<td class=\"ltx_td ltx_align_center\">6.37</td>\n<td class=\"ltx_td ltx_align_center\">0.417</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.372</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.398</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.362</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.433</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.395</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "simo",
            "crosssentence",
            "study",
            "sampling",
            "ablation",
            "flux",
            "loss",
            "werc",
            "belle",
            "disabled",
            "3teacher",
            "simr",
            "indicates",
            "examining",
            "module",
            "continuation",
            "enabled",
            "effects",
            "werh"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">An ablation study is conducted to investigate how the sampling module and flux loss affect the performance of the BELLE model. Due to limited computational resources, the analysis experiments are also conducted on the smaller-scale dataset. Our experiments are based on the BELLE <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting. As demonstrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T5\" title=\"Table 5 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the sampling module plays a more crucial role than flux loss. Removing the flux loss results in only a slight performance degradation, whereas removing the sampling module leads to a severe performance drop. This indicates that Bayesian sampling plays a crucial role in the effectiveness of BELLE.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper-distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://belletts.github.io/Belle/\" title=\"\">https://belletts.github.io/Belle/</a>.\nThe code, checkpoints, and synthetic data will be released after the paper is accepted.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE: <span class=\"ltx_text ltx_font_bold\">B</span>ayesian <span class=\"ltx_text ltx_font_bold\">e</span>vidential <span class=\"ltx_text ltx_font_bold\">l</span>earning with <span class=\"ltx_text ltx_font_bold\">l</span>anguag<span class=\"ltx_text ltx_font_bold\">e</span> modelling for TTS Synthesis&#8212;a continuous-valued AR model that directly predicts mel-spectrograms from text inputs based on the evidential deep learning (EDL) framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Amini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib2\" title=\"\">2020</a>)</cite>. This represents the first application of Bayesian deep learning to TTS, enabling controllable and theoretically grounded sampling at test time.\nSpecifically, BELLE models each predicted mel-spectrogram frame as a sample from a Gaussian distribution, whose parameters themselves are drawn from hyper-distributions. Metaphorically, the model first predicts the parameters of this hyper-distribution, then samples a specific Gaussian distribution from it, and finally generates mel-spectrogram frames from the sampled Gaussian.\nHowever, learning accurate Gaussian posterior estimates from limited data poses a challenge, as typical TTS datasets contain only a single audio recording per text prompt. To overcome this, the training corpus is augmented with synthetic audio samples generated by several publicly available pre-trained zero-shot TTS models <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib11\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib50\" title=\"\">2025c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib51\" title=\"\">2024</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib5\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present BELLE, the first AR TTS model using evidential deep learning to train and sample from continuous-valued Mel-Spectrogram space. The results show that Bayesian evidential sampling outperforms conventional Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Continuous-valued TTS models can be broadly categorized into AR and non-autoregressive (NAR) approaches. AR methods include MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, which generates mel spectrogram frames using decoder-only Transformer-based AR modelling combined with Gaussian sampling. FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib48\" title=\"\">2025a</a>)</cite>, ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib32\" title=\"\">2024</a>)</cite> and DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib19\" title=\"\">2025</a>)</cite> first generate continuous-valued hidden states autoregressively, then employ flow-matching or diffusion-based modules, such as DiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib39\" title=\"\">2023</a>)</cite>, to sample and produce mel spectrograms or a continuous latent space learned by a VAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Pinheiro&#160;Cinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib40\" title=\"\">2021</a>)</cite>. Moreover, VAE-based AR methods, including GMM-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin &amp; He, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib29\" title=\"\">2025</a>)</cite> and KALLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib55\" title=\"\">2024</a>)</cite>, initially leverage a VAE to estimate a latent distribution (mean and variance), and subsequently autoregressively predict latent representations sampled from this distribution. NAR models such as E2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib13\" title=\"\">2024</a>)</cite> and F5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib7\" title=\"\">2024a</a>)</cite>, on the other hand, predict mel-spectrograms directly via iterative mask-based refinement. Recently, several streaming TTS systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib12\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib54\" title=\"\">2024</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib46\" title=\"\">2025</a>; Sheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib44\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib49\" title=\"\">2025b</a>)</cite>. Most of these approaches model discrete features using a language model, followed by a flow-matching module or other similar components to generate speech features. Such architectures are relatively complex and may introduce potentially higher latency.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General mel-based AR TTS model consists of several interconnected modules: a <span class=\"ltx_text ltx_font_bold\">Prenet</span>, an <span class=\"ltx_text ltx_font_bold\">AR LM</span>, a <span class=\"ltx_text ltx_font_bold\">Sampling Module</span>, a <span class=\"ltx_text ltx_font_bold\">Postnet</span>, a <span class=\"ltx_text ltx_font_bold\">Stop-Prediction Module</span>, and a <span class=\"ltx_text ltx_font_bold\">Vocoder</span>, which are shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 General Architecture of Mel-based Autoregressive TTS Model &#8227; 3 Mel-based Autoregressive TTS &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. These modules will be introduced in the following sections.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Sampling Module introduces critical stochasticity into the generative process. Specifically, the hidden states <math alttext=\"\\bm{e}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{e}_{t}</annotation></semantics></math> from the AR LM are first projected down to the mel-spectrogram dimension. The projected representation is assumed to follow a specified parametric distribution (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, a Gaussian distribution). Subsequently, the Sampling Module samples from this distribution, obtaining an intermediate sampled representation denoted as <math alttext=\"\\bm{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119963;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{z}_{t}</annotation></semantics></math>. Afterwards, a multilayer perception (MLP)-based denoising network refines and further processes the sampled representation, and the resulting denoised output is denoted as <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> as shown in the right part of Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 General Architecture of Mel-based Autoregressive TTS Model &#8227; 3 Mel-based Autoregressive TTS &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The detailed description of our proposed evidential Bayesian sampling method is provided in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4\" title=\"4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Postnet consists of multiple convolutional layers to refine generated acoustic features like methods in <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib43\" title=\"\">2018</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib26\" title=\"\">2019</a>)</cite>. Given the output of the Sampling Module <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math>, the Postnet predicts a residual term, which is subsequently added back to <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> and produces the refined output <math alttext=\"\\bm{y}_{t}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(2)}</annotation></semantics></math>. During training, the model employs the teacher-forcing paradigm; at inference, all intermediate <math alttext=\"\\bm{y}_{t}^{(1)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(1)}</annotation></semantics></math> frames are first collected and then passed through the Postnet to obtain the final output <math alttext=\"\\bm{y}_{t}^{(2)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t}^{(2)}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, the hyperparameters <math alttext=\"(\\bm{\\gamma}_{t},\\bm{\\nu}_{t},\\bm{\\alpha}_{t},\\bm{\\beta}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#120632;</mi><mi>t</mi></msub><mo>,</mo><msub><mi>&#120642;</mi><mi>t</mi></msub><mo>,</mo><msub><mi>&#120630;</mi><mi>t</mi></msub><mo>,</mo><msub><mi>&#120631;</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\bm{\\gamma}_{t},\\bm{\\nu}_{t},\\bm{\\alpha}_{t},\\bm{\\beta}_{t})</annotation></semantics></math> are predicted by the sampling module and optimized via the evidential loss:</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "loss",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE is trained end-to-end using a composite loss:</p>\n\n",
                "matched_terms": [
                    "loss",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda_{\\text{samp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mtext>samp</mtext></msub><annotation encoding=\"application/x-tex\">\\lambda_{\\text{samp}}</annotation></semantics></math> and <math alttext=\"\\lambda_{\\text{flux}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mtext>flux</mtext></msub><annotation encoding=\"application/x-tex\">\\lambda_{\\text{flux}}</annotation></semantics></math> balance the contribution of sampling and flux losses.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "flux"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sampling loss <math alttext=\"\\mathcal{L}_{\\text{samp}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>samp</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{samp}}</annotation></semantics></math> is used as the EDL loss:</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">whereas MELLE employs a KL loss for sampling.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spectrogram flux loss <math alttext=\"\\mathcal{L}_{\\text{flux}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>flux</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{flux}}</annotation></semantics></math> encourages temporal dynamics, promoting variability between the current predicted distribution location <math alttext=\"\\bm{\\gamma}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#120632;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{\\gamma}_{t}</annotation></semantics></math> and previous-frame ground truth <math alttext=\"\\bm{y}_{t-1}^{\\text{gt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119962;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mtext>gt</mtext></msubsup><annotation encoding=\"application/x-tex\">\\bm{y}_{t-1}^{\\text{gt}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "loss",
                    "flux"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot TTS capabilities of our model is evaluated using the LibriSpeech test-clean subset by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, two inference conditions are considered: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where a reference utterance and its corresponding transcription from a given speaker is used as a prompt, and then the model generates speech for a different sentence.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, word error rate (WER) is reported to measure intelligibility and robustness. WER-C and WER-H are evaluated using Conformer<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and HuBERT<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite> based ASR models respectively. Speaker similarity is measured via cosine similarity of extracted speaker embeddings, with SIM-o referencing the original speech prompt and SIM-r referencing the vocoder-reconstructed prompt.</p>\n\n",
                "matched_terms": [
                    "simr",
                    "simo",
                    "werc",
                    "werh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, BELLE, F5-TTS, and Ground Truth achieve MOS scores of around 4.2, indicating that the audio produced by both F5-TTS and BELLE is close to that of natural human speech. F5-TTS attains a slightly higher MOS, which can be attributed to its lower background noise &#8212; human raters tend to prefer cleaner audio. In contrast, the real speech in Ground Truth inherently contains a certain level of noise, and BELLE replicates this natural background characteristic, leading to MOS scores that are very close to the Ground Truth.\nRegarding SMOS, it is worth noting that in the LibriSpeech dataset, different utterances from the same speaker sometimes exhibit perceptible timbre variation, which results in a relatively lower SMOS score for Ground Truth. BELLE achieves the highest speaker similarity among all tested systems. Notably, while both F5&#8209;TTS and MaskGCT were trained on datasets comprising 50,000 hours of English speech, BELLE was trained on less than 5,000 hours of data, yet it attained competitive, and in some instances superior performance. These results underscore the efficacy of the proposed Bayesian sampling strategy and the multi&#8209;teacher learning framework.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective metrics in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE attains the best WER and SIM scores under the Continuation setting. Under the Cross-sentence setting, MaskGCT achieves slightly higher scores; however, in our reproduction, MaskGCT&#8217;s WER-C is only 3.86% under our evaluation protocol. This suggests that BELLE remains highly competitive in the Cross-sentence scenario. Additionally, BELLE outperforms MELLE across both subjective and objective metrics, including MOS, SMOS, WER, and SIM. These results demonstrate that Bayesian sampling delivers superior performance compared to Gaussian sampling.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "continuation",
                    "werc",
                    "belle",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since BELLE-stream is finetuned to a fixed speaker timbre, it does not accept an audio prompt; therefore, it is excluded from SMOS scoring in the subjective evaluation and SIM computation in the objective evaluation, and its results are reported only under the Cross-sentence setting. BELLE-stream only takes the target text to be synthesized as input. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T1\" title=\"Table 1 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, streaming generation does not cause significant degradation in speech naturalness, achieving higher MOS scores than non&#8209;streaming MELLE. From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T2\" title=\"Table 2 &#8227; 6.1 Main Results &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BELLE-stream achieves the lowest WER among streaming TTS systems, with no appreciable increase in WER compared to BELLE. In our tests, BELLE-stream achieved a real-time factor (RTF)&#8212;defined as the ratio of audio generation time to the length of the generated audio&#8212;of 0.55 when producing a 10-second utterance. Given that BELLE-stream operates with 0.8-second chunks, this implies a first-packet latency (FPL) of only 440ms, demonstrating its effective balance between performance and latency.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The diversity evaluation is conducted using the evaluation method showed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S5.SS2\" title=\"5.2 Evaluation Settings &#8227; 5 Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nIn the NIG prior, the parameter <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> denotes the scale of the inverse-gamma distribution over the variance <math alttext=\"\\sigma_{t}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#963;</mi><mi>t</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">\\sigma_{t}^{2}</annotation></semantics></math>. Increasing <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> raises the expected variance, thereby promoting greater diversity in the sampled outcomes (See Eqn. equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S4.E7\" title=\"In 4.2 From Gaussian Sampling to Bayesian Evidential Sampling &#8227; 4 Bayesian Evidential Learning &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).\nFrom Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T3\" title=\"Table 3 &#8227; 6.2 Diversity Analysis &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it&#8217;s observed that BELLE (<math alttext=\"\\beta*2\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8727;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\beta*2</annotation></semantics></math>) exhibits the largest distances across all three metrics, confirming that increasing the sampling parameter <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m5\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> effectively enhances generation diversity. Default BELLE achieves moderate diversity, slightly higher than MELLE, suggesting that Bayesian sampling yields superior diversity compared to Gaussian sampling. In contrast, F5-TTS produces consistently small distances, indicating that its outputs are more deterministic and less diverse across repeated sampling. These findings highlight that, beyond intelligibility and speaker similarity, BELLE offers a controllable trade-off between stability and diversity at inference time, with the <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> parameter serving as a practical knob to adjust sample variability, analogous to the role of the temperature parameter in the sampling process of token-based language models.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computational resources, the analysis experiments are conducted on a smaller-scale dataset. The <span class=\"ltx_text ltx_font_italic\">1-teacher</span> setting trains solely on the original Librispeech data without synthesized augmentation. The <span class=\"ltx_text ltx_font_italic\">3-teacher</span> setting incorporates additional synthesized speech from two TTS models, while the <span class=\"ltx_text ltx_font_italic\">7-teacher</span> setting augments the training data with speech from all six TTS models described earlier. The <span class=\"ltx_text ltx_font_italic\">data-aug</span> configuration serves as a baseline using conventional data augmentation, randomly sampling from all available data sources during training. Details about the small dataset and model training configuration could be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#A3.SS4\" title=\"C.4 Training Details of Analysis Results &#8227; Appendix C Detailed Experimental Setup &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">C.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "3teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#S6.T4\" title=\"Table 4 &#8227; 6.3 The effect of multi teachers &#8227; 6 Results and Discussions &#8227; Bayesian Speech synthesizers Can Learn from Multiple Teachers\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the performance of BELLE consistently improves as the number of teachers increases. Furthermore, the possibility that the performance gain is solely due to the increased amount of training data is excluded. In the <span class=\"ltx_text ltx_font_italic\">data-aug</span> setting, the total training data is larger than that of the <span class=\"ltx_text ltx_font_italic\">3-teacher</span> configuration; however, its performance is inferior to <span class=\"ltx_text ltx_font_italic\">3-teacher</span> on multiple metrics. This degradation may be attributed to the conventional data augmentation strategy, in which the same text can correspond to acoustically different speeches, potentially causing confusion for the model. In contrast, under our proposed multi-teacher training scheme, increasing the number of teachers leads to consistent performance gains, thereby demonstrating the effectiveness of the multi teacher training strategy.</p>\n\n",
                "matched_terms": [
                    "belle",
                    "3teacher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose BELLE, a novel continuous-valued AR TTS model integrating Bayesian EDL for sampling mel-spectrogram frames. Our approach addresses the underexplored area of effective sampling methods for continuous-valued AR TTS. Additionally, a multi-teacher knowledge distillation framework is introduced, substantially improving TTS synthesis quality using synthesized data from publicly available models. Experimental results highlight the efficacy of Bayesian evidential sampling and multi-teacher distillation in achieving competitive speech naturalness, speaker similarity, and diversity, rivaling models trained on much larger real-data corpora. Remarkably, our approach attains these results using only one-tenth of the data, much of it synthetic, while enabling high-quality, low-latency TTS suitable for both offline and streaming applications.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">BELLE introduces a novel Bayesian evidential sampling approach within continuous-valued autoregressive text-to-speech, significantly enhancing the zero-shot TTS synthesis quality. By effectively modeling and generating natural, expressive, and intelligible speech with limited reference data, BELLE advances the flexibility and realism of synthesized audio, making it valuable for various beneficial applications in society. Notably, BELLE can facilitate natural human-machine conversational systems, assistive communication technologies for individuals with speech impairments, and personalized education platforms, thereby positively impacting accessibility, education, and user experiences in interactive dialogue systems.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although BELLE demonstrates strong performance, there are two main limitations. First, the current study focuses solely on training and evaluation with English speech data, without validating multilingual generalization; extending BELLE to diverse languages remains an important direction for future work. Second, as with most AR TTS systems, BELLE&#8217;s RTF is still higher than that of recent flow-matching-based models. Further research on predicting multiple mel-spectrogram frames per step could substantially reduce RTF and enable lower-latency streaming TTS.</p>\n\n",
                "matched_terms": [
                    "study",
                    "belle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input mel-spectrograms first pass through a 3-layer prenet with dropout of 0.5 in both training and inference stages following Tacotron <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib52\" title=\"\">2017</a>)</cite>. The AR LM is a decoder-only Transformer consisting of 12 blocks, each with 16 attention heads, a hidden size of 1024, a feed-forward dimension of 4096, and a dropout rate of 0.1 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is modified from <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lifeiteng/vall-e\" title=\"\">https://github.com/lifeiteng/vall-e</a>.</span></span></span>. The sampling module includes a linear projection to derive the sampling parameters and a 3-layer residual MLP for denoising. A post-processing module comprising five convolutional blocks (a kernel size of 5 and 256 channels) is applied for mel-spectrogram refinement. The final waveform is synthesized using the pretrained HiFi-GAN vocoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib22\" title=\"\">2020</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The pretrained vocoder can be found in <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mechanicalsea/speecht5-tts\" title=\"\">https://huggingface.co/mechanicalsea/speecht5-tts</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the zero-shot TTS capabilities of our model using the LibriSpeech test-clean subset following VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib47\" title=\"\">2023</a>)</cite> by an open-source evaluation protocol <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib24\" title=\"\">2024</a>)</cite>. Specifically, we consider two inference conditions: <span class=\"ltx_text ltx_font_bold\">Continuation</span>, in which the first 3 seconds of an utterance and its corresponding transcription serve as the prompt, and the model synthesizes the continuation of speech thereafter; and <span class=\"ltx_text ltx_font_bold\">Cross-sentence</span>, where we use a reference utterance and its corresponding transcription from a given speaker as a prompt, and then the model generates speech for a different sentence while preserving speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "crosssentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For objective evaluation, we report word error rate (WER) to measure intelligibility and robustness, using two automatic speech recognition models: a Conformer-Transducer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge\" title=\"\">https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib15\" title=\"\">2020</a>)</cite> and a fine-tuned HuBERT-Large model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib18\" title=\"\">2021</a>)</cite>. We denote results obtained from these systems as WER-C and WER-H, respectively. To quantify speaker similarity, we calculate the cosine similarity between extracted speaker embeddings using a WavLM-TDCNN model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib6\" title=\"\">2022</a>)</cite>. We provide two similarity metrics: SIM-o computes the similarity against the original speech prompt, whereas SIM-r uses the vocoder-reconstructed version of the prompt.</p>\n\n",
                "matched_terms": [
                    "simr",
                    "simo",
                    "werc",
                    "werh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.24372v1#bib.bib35\" title=\"\">2024</a>)</cite>, the latent sampling module assumes that the embedding at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, denoted as <math alttext=\"\\bm{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119963;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bm{z}_{t}</annotation></semantics></math>, is drawn from a multivariate Gaussian distribution:</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When Gaussian sampling is used, the sampling loss is typically implemented as a Kullback&#8211;Leibler (KL) divergence loss between the predicted Gaussian distribution and a predefined Gaussian prior distribution with mean <math alttext=\"\\bm{y}^{\\text{gt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119962;</mi><mtext>gt</mtext></msup><annotation encoding=\"application/x-tex\">\\bm{y}^{\\text{gt}}</annotation></semantics></math> and variance <math alttext=\"\\bm{I}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS2.p1.m2\" intent=\":literal\"><semantics><mi>&#119920;</mi><annotation encoding=\"application/x-tex\">\\bm{I}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "loss"
                ]
            }
        ]
    }
}