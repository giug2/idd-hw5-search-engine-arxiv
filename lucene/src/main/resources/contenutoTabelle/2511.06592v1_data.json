{
    "S3.T1": {
        "source_file": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making",
        "caption": "Table 1: Statistics of our created MedVoiceBias data. NN represents the number of unique voice profile.",
        "body": "Category\nCohort\nWER (%)\nLength (s)\nN\n\n\nAge\nYoung\n6.1\n34.0\n10\n\n\nOld\n8.9\n42.4\n6\n\n\nGender\nFemale\n6.6\n35.7\n10\n\n\nMale\n7.3\n37.3\n10\n\n\nExpression\nHappy\n5.1\n31.0\n4\n\n\nLaughing\n5.3\n33.7\n4\n\n\nSad\n5.6\n33.7\n4\n\n\nConfused\n5.6\n36.5\n4\n\n\nEnunciated\n6.3\n38.9\n4\n\n\nWhisper\n7.8\n37.8\n4",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Category</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Cohort</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Length (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">N</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Age</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Young</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Old</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Gender</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Female</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Male</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Expression</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Happy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Laughing</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sad</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Confused</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Enunciated</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "laughing",
            "cohort",
            "length",
            "male",
            "young",
            "unique",
            "statistics",
            "our",
            "happy",
            "voice",
            "medvoicebias",
            "represents",
            "wer",
            "gender",
            "whisper",
            "old",
            "age",
            "female",
            "enunciated",
            "number",
            "expression",
            "sad",
            "confused",
            "category",
            "created",
            "data",
            "profile"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In total, we constructed 36 unique voice profiles. For each selected speaker profile (demographic or emotional), we used Sesame-1B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sesame2024csm1b</span>]</cite> to synthesize patient speech from DDXPlus <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fansi2022ddxplus</span>]</cite> clinical contexts, adapted to match the target voice characteristics.\nBecause input length constraints degraded synthesis quality, we segmented the original patient profiles at the sentence level. For each sentence, we generated three candidate synthesis samples per voice profile and applied Whisper-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>]</cite> to perform ASR on all outputs. The sample with the lowest word error rate (WER) was selected as the final audio input for downstream experiments, yielding an average WER of 6.4% across all profiles. The statistics of our MedVoiceBias data is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S3.T1\" title=\"Table 1 &#8227; 3.2.3 Voice Cloning &#8227; 3.2 MedVoiceBias Dataset Construction &#8227; 3 Methodology &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations.\nFurther analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance.\nThese results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient&#8217;s voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "age",
                    "young",
                    "gender",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a comprehensive framework for assessing demographic bias in audio LLMs&#8217; medical decision-making. Our approach focuses on surgical recommendation tasks, leveraging their documented susceptibility to demographic substitution effects. Through controlled experiments using synthesized speech, we isolate the effects of voice characteristics while holding clinical content constant. This methodology allows us to determine whether audio LLMs perpetuate the problematic pattern of using demographic proxies instead of clinical indicators.\nOur contributions are 3-fold:</p>\n\n",
                "matched_terms": [
                    "voice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We create <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span> with 170 clinical cases from DDXPlus with 36 synthesized speaker profiles (age, gender, emotional expressions) to benchmark controlled bias assessment. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Dataset available at <a class=\"ltx_ref ltx_href\" href=\"https://hf.co/datasets/theblackcat102/MedVoiceBias\" title=\"\">https://hf.co/datasets/theblackcat102/MedVoiceBias</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "medvoicebias",
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate severe modality-dependent biases (up to 34.9pp deviation between text/audio), with age disparities persisting under chain-of-thought (CoT) while gender bias is eliminated.\n</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR and Audio LLM Bias.</span> Speech recognition systems exhibit systematic performance disparities across demographic groups. Koenecke et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koenecke2020racial</span>]</cite> first documented significantly higher word error rates for Black speakers compared to White speakers. Harris et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">harris2024modeling</span>]</cite> demonstrated that intersectional bias patterns compound these disparities, with the Fair-Speech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">veliche2024towards</span>]</cite> revealing performance gaps exceeding 40% across demographic groups.\nBeyond ASR performance, Spoken StereoSet <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024spoken</span>]</cite> evaluated behavioral biases in speech language models, finding minimal bias scores close to 50% for general stereotype tasks involving gender and age demographics. However, domain-specific applications, particularly high-stakes medical contexts, may reveal different bias patterns as clinical decision-making involves complex reasoning beyond simple stereotype association.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Research Gap and Contribution.</span>\nTo the best of our knowledge, no prior work has systematically investigated how voice characteristics influence medical reasoning in audio LLMs.\nSurgical recommendations are an especially suitable testbed for bias evaluation, as they involve binary, high-stakes decisions with documented demographic disparities <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">al2022demographic</span>]</cite>, are governed by clinical guidelines that should be demographic-agnostic, and contain inherent uncertainty that allows bias to surface <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dill2022role</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work makes the first systematic contribution in this area by leveraging controlled speech synthesis to isolate the impact of voice characteristics on surgical decision-making. Through this design, we reveal how the speech modality can introduce previously overlooked bias vectors in medical AI systems.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sourced utterances from the Common Voice repository <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2020common</span>]</cite> (release <span class=\"ltx_text ltx_font_typewriter\">cv-corpus-22.0-2025-06-20</span>). Using the provided metadata, we focused on two age strata: (i) speakers self-reporting ages 20&#8211;29 (<span class=\"ltx_text ltx_font_italic\">young</span>) and (ii) speakers self-reporting ages 60 (<span class=\"ltx_text ltx_font_italic\">old</span>).\nWithin each cohort, we retained only those speakers whose perceived age and gender are unambiguous to human raters. Manual validation was essential, as a systematic comparison of 200 randomly sampled profiles revealed discrepancies between self-reported and acoustically perceived demographics in 23% of cases (age: 18%, gender: 5%). From this process, we selected 12 speakers, balanced across both age and gender (6M/6F; 6 young/6 old), with demographic classifications confirmed by consensus among three annotators.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "age",
                    "cohort",
                    "young",
                    "gender",
                    "old"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine whether paralinguistic cues influence downstream bias, we augmented the corpus with controlled emotional renderings. Specifically, we used the Expresso dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2023expresso</span>]</cite>, selecting six affective conditions: <span class=\"ltx_text ltx_font_italic\">happy</span>, <span class=\"ltx_text ltx_font_italic\">laughing</span>, <span class=\"ltx_text ltx_font_italic\">sad</span>, <span class=\"ltx_text ltx_font_italic\">confused</span>, <span class=\"ltx_text ltx_font_italic\">enunciated</span>, and <span class=\"ltx_text ltx_font_italic\">whisper</span>.\nExpresso includes two male and two female speakers self-identified as young, and we incorporated all four speakers expressing each of the six emotions available in the dataset.</p>\n\n",
                "matched_terms": [
                    "happy",
                    "laughing",
                    "female",
                    "enunciated",
                    "male",
                    "young",
                    "whisper",
                    "sad",
                    "confused"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary metric is the <span class=\"ltx_text ltx_font_italic\">surgery recommendation rate</span>, defined as the proportion of cases in which a model recommends a surgical intervention.\nFor each model and prompting strategy, we first establish this rate using the text-only baseline. We then calculate the recommendation rate for each distinct demographic cohort in our dataset (age, gender, and emotion), enabling a direct comparison of the model&#8217;s behavior with and without audio cues.</p>\n\n",
                "matched_terms": [
                    "age",
                    "cohort",
                    "gender",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the main experiments, we performed two preliminary evaluations. First, we conducted a basic sanity check on the text-only patient profiles to ensure all models could correctly perform the fundamental task of providing a surgical recommendation.\nSecond, we fed audio from a Common Voice profile to each model to verify its ability to distinguish age and gender. This evaluation confirmed that the models could accurately recognize gender from voice, a key prerequisite for our bias analysis. This entire process was repeated for every model and prompting strategy to ensure a robust and comprehensive analysis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "voice",
                    "age",
                    "gender",
                    "profile"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated six state-of-the-art audio LLMs: DeSTA2.5-Audio 8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span>]</cite>, Qwen2.5-Omni 3B and 7B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>]</cite>, Gemini Flash (2.0, 2.5) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>]</cite>, and GPT-4o-mini-audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hurst2024gpt</span>]</cite>. Before examining potential bias, we first confirmed that these models could reliably distinguish age, gender, and emotions from audio, as well as achieve performance above random chance when making surgical recommendations from the original text.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show varying levels of demographic detection across models. Gender prediction accuracy ranged from 96.1% to 99.9% for most models (with the exception of GPT-4o-mini at 0%), while age prediction exhibited wider variance (32.6% to 85.7%). This demographic detection ability is a prerequisite for analyzing bias, as models are capable of perceiving demographic cues and then exhibit differential behavior.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Demographic-Aware Effects &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the surgical recommendation rates stratified by patient age. Contrary to our hypothesis that explicit reasoning would compel models to focus solely on the disease, CoT prompting revealed persistent and sometimes amplified age-related disparities.</p>\n\n",
                "matched_terms": [
                    "age",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the DA condition, 4 of 6 models showed recommendations that significantly varied by age. Qwen2.5-3B showed the largest difference, recommending surgery for 85.3% of young vs. 73.5% of elderly patients.\nThis 11.8% gap is a clinically meaningful finding that could lead to the systematic under-treatment of elderly patients.</p>\n\n",
                "matched_terms": [
                    "age",
                    "young"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Gender bias patterns differed markedly from age-related disparities, as shown in Table 1. In the Direct Answer (DA) condition, only half of the models exhibited significant gender bias. The absolute differences in recommendation rates (ranging from 1.9% to 8.0%) were substantially smaller than those observed for age-related disparities.\nDeSTA2.5 showed the largest gender gap, recommending surgery for 92.5% of male vs. 84.5% of female patients.</p>\n\n",
                "matched_terms": [
                    "male",
                    "gender",
                    "female"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoT prompting eliminated gender bias across all models, with no significant differences observed. This complete mitigation contrasts sharply with the age-related effects under CoT. This finding suggests that models may encode and process age and gender information through fundamentally different mechanisms, and that they are better equipped to handle gender cues than age cues.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study provides the first systematic evaluation of voice-based bias in audio LLMs for medical decision-making. We created <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span>, which uses 170 clinical cases and 36 synthesized voice profiles to enable a controlled bias assessment.\nOur findings reveal that audio LLMs exhibit significant instability, with surgery recommendation rates deviating a lot from text-only baselines. We demonstrate that these biases manifest across all demographic groups, with emotional expressions further amplifying the effect. While explicit reasoning from the models mitigates these biases, it does not eliminate them. This highlights a fundamental architectural challenge: the inability to reliably disentangle a patient&#8217;s medical information from the paralinguistic features of their voice.\nThese results have critical implications for the deployment of speech-enabled AI in healthcare. Audio LLMs currently pose unacceptable risks of creating disparities based on how patients sound rather than on their medical needs. We conclude that bias-aware training and architectural innovations are imperative before clinical deployment to ensure that decisions are driven by medical evidence, not by a patient&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "medvoicebias",
                    "voice",
                    "created",
                    "our"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making",
        "caption": "Table 2: Audio model performance in text-mode surgery accuracy, voice-based age/gender/emotion identification accuracy (%).",
        "body": "Model\nSurgery\nAge\nGender\nEmotion\n\n\ngpt-4o-mini\n76.2\n0.0\n0.0\n0.0\n\n\ngemini-2.0-flash\n68.3\n66.0\n99.5\n0.2\n\n\ngemini-2.5-flash\n55.5\n57.4\n99.9\n17.0\n\n\nQwen2.5-Omni-3B\n63.9\n66.1\n96.1\n12.2\n\n\nQwen2.5-Omni-7B\n60.3\n66.1\n97.5\n16.9\n\n\nDeSTA2.5\n57.8\n65.4\n99.5\n40.5",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Surgery</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Age</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Gender</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emotion</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gpt-4o-mini</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gemini-2.0-flash</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gemini-2.5-flash</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qwen2.5-Omni-3B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">96.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qwen2.5-Omni-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DeSTA2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.5</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "voicebased",
            "gender",
            "gemini20flash",
            "qwen25omni7b",
            "model",
            "surgery",
            "qwen25omni3b",
            "age",
            "emotion",
            "desta25",
            "agegenderemotion",
            "identification",
            "gemini25flash",
            "gpt4omini",
            "performance",
            "accuracy",
            "audio",
            "textmode"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show varying levels of demographic detection across models. Gender prediction accuracy ranged from 96.1% to 99.9% for most models (with the exception of GPT-4o-mini at 0%), while age prediction exhibited wider variance (32.6% to 85.7%). This demographic detection ability is a prerequisite for analyzing bias, as models are capable of perceiving demographic cues and then exhibit differential behavior.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations.\nFurther analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance.\nThese results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient&#8217;s voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "age",
                    "emotion",
                    "performance",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid deployment of large language models (LLMs) in healthcare presents both a promising frontier and a critical concern. Among medical decisions, surgical recommendations are particularly challenging for bias assessment. While these decisions should be driven by clinical factors, research shows that implicit biases often lead to the substitution of demographic proxies, such as using a patient&#8217;s age instead of their frailty as a decisive factor <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">montroni2021surgical</span>]</cite>. Disentangling such biases from legitimate clinical variations, like the higher surgery rates for females due to conditions like cholecystitis <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bicket2024prevalence</span>]</cite>, requires controlled experimental designs that isolate demographic factors.</p>\n\n",
                "matched_terms": [
                    "age",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the first systematic evaluation of voice-based bias in audio LLMs through binary surgery decisions, revealing how paralinguistic features influence high-stakes medical recommendations.</p>\n\n",
                "matched_terms": [
                    "voicebased",
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We create <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span> with 170 clinical cases from DDXPlus with 36 synthesized speaker profiles (age, gender, emotional expressions) to benchmark controlled bias assessment. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Dataset available at <a class=\"ltx_ref ltx_href\" href=\"https://hf.co/datasets/theblackcat102/MedVoiceBias\" title=\"\">https://hf.co/datasets/theblackcat102/MedVoiceBias</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate severe modality-dependent biases (up to 34.9pp deviation between text/audio), with age disparities persisting under chain-of-thought (CoT) while gender bias is eliminated.\n</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR and Audio LLM Bias.</span> Speech recognition systems exhibit systematic performance disparities across demographic groups. Koenecke et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koenecke2020racial</span>]</cite> first documented significantly higher word error rates for Black speakers compared to White speakers. Harris et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">harris2024modeling</span>]</cite> demonstrated that intersectional bias patterns compound these disparities, with the Fair-Speech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">veliche2024towards</span>]</cite> revealing performance gaps exceeding 40% across demographic groups.\nBeyond ASR performance, Spoken StereoSet <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024spoken</span>]</cite> evaluated behavioral biases in speech language models, finding minimal bias scores close to 50% for general stereotype tasks involving gender and age demographics. However, domain-specific applications, particularly high-stakes medical contexts, may reveal different bias patterns as clinical decision-making involves complex reasoning beyond simple stereotype association.</p>\n\n",
                "matched_terms": [
                    "age",
                    "performance",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For structured diagnostic reasoning, we employ the DDXPlus dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fansi2022ddxplus</span>]</cite>, which provides differential diagnoses: a list of 49 plausible conditions aligned with a patient&#8217;s symptoms and widely regarded as a cornerstone of physician reasoning. By prompting audio LLMs to generate differential diagnoses, we move beyond surface-level accuracy metrics and evaluate whether vocal characteristics systematically influence the quality and clinical soundness of reasoning.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For treatment recommendations, we employ yes/no surgery decisions as clear, high-stakes binary classification tasks.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our implementation allowed models to output &#8220;yes,&#8221; &#8220;maybe,&#8221; or &#8220;no,&#8221; but for analysis we treated only &#8220;yes&#8221; as a positive decision and grouped the others together.</span></span></span>\nBinary classification provides a well-established framework for evaluating algorithmic bias, yielding unambiguous outcomes that facilitate straightforward fairness measurement <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">estiri2022objective</span>]</cite>.\nSurgical decisions are particularly consequential, as historical evidence shows that human clinical judgment has been influenced by patient demographics, resulting in disparities in access to major surgery <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">binkley2022should</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kiyasseh2023human</span>]</cite>.\nBy requiring a definitive yes/no recommendation, we create a clear and interpretable signal for bias. This design enables precise quantification of how vocal profiles may skew audio LLMs&#8217; recommendations, offering a critical mechanism to detect and mitigate potential harms before such systems are deployed in real-world clinical practice.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sourced utterances from the Common Voice repository <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2020common</span>]</cite> (release <span class=\"ltx_text ltx_font_typewriter\">cv-corpus-22.0-2025-06-20</span>). Using the provided metadata, we focused on two age strata: (i) speakers self-reporting ages 20&#8211;29 (<span class=\"ltx_text ltx_font_italic\">young</span>) and (ii) speakers self-reporting ages 60 (<span class=\"ltx_text ltx_font_italic\">old</span>).\nWithin each cohort, we retained only those speakers whose perceived age and gender are unambiguous to human raters. Manual validation was essential, as a systematic comparison of 200 randomly sampled profiles revealed discrepancies between self-reported and acoustically perceived demographics in 23% of cases (age: 18%, gender: 5%). From this process, we selected 12 speakers, balanced across both age and gender (6M/6F; 6 young/6 old), with demographic classifications confirmed by consensus among three annotators.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate synthesis quality, we applied the MOSA-Net+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zezario2024study</span>]</cite> automatic speech quality assessment model. The generated samples achieved average PESQ and intelligibility scores of 3.6/5.0 and 0.97, respectively, indicating consistently high-quality audio for subsequent evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate bias by systematically comparing a model&#8217;s clinical recommendations from <span class=\"ltx_text ltx_font_bold\">voice-based</span> inputs against those generated from a <span class=\"ltx_text ltx_font_bold\">text-only</span> control baseline. This approach allows us to isolate and quantify the influence of paralinguistic and demographic cues present in the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "voicebased",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary metric is the <span class=\"ltx_text ltx_font_italic\">surgery recommendation rate</span>, defined as the proportion of cases in which a model recommends a surgical intervention.\nFor each model and prompting strategy, we first establish this rate using the text-only baseline. We then calculate the recommendation rate for each distinct demographic cohort in our dataset (age, gender, and emotion), enabling a direct comparison of the model&#8217;s behavior with and without audio cues.</p>\n\n",
                "matched_terms": [
                    "model",
                    "surgery",
                    "age",
                    "emotion",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the main experiments, we performed two preliminary evaluations. First, we conducted a basic sanity check on the text-only patient profiles to ensure all models could correctly perform the fundamental task of providing a surgical recommendation.\nSecond, we fed audio from a Common Voice profile to each model to verify its ability to distinguish age and gender. This evaluation confirmed that the models could accurately recognize gender from voice, a key prerequisite for our bias analysis. This entire process was repeated for every model and prompting strategy to ensure a robust and comprehensive analysis.</p>\n\n",
                "matched_terms": [
                    "age",
                    "model",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated six state-of-the-art audio LLMs: DeSTA2.5-Audio 8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span>]</cite>, Qwen2.5-Omni 3B and 7B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>]</cite>, Gemini Flash (2.0, 2.5) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>]</cite>, and GPT-4o-mini-audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hurst2024gpt</span>]</cite>. Before examining potential bias, we first confirmed that these models could reliably distinguish age, gender, and emotions from audio, as well as achieve performance above random chance when making surgical recommendations from the original text.</p>\n\n",
                "matched_terms": [
                    "age",
                    "performance",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each model, we conducted experiments under two conditions: <span class=\"ltx_text ltx_font_italic\">direct answer (DA)</span> and <span class=\"ltx_text ltx_font_italic\">diagnose-then-decide chain-of-thought (CoT)</span>. In the DA setting, the model received the full patient profile (either audio or ASR transcripts) and was asked to directly output a binary decision regarding surgical necessity.\nIn the CoT setting, the model was first prompted to infer the possible disease and then determine whether surgery is required.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveals substantial modality-dependent bias in surgical recommendations. Under the DA condition, 66.7% (4 of 6) evaluated models exhibited statistically significant differences between text and audio inputs, with recommendation rate shifts ranging from -22.2% for Qwen2.5-3B to +34.9% for DeSTA2.5.\nA striking example is GPT-4o-mini, whose recommendation rate dropped from 26.5% with text to just 5.3% with audio, a relative reduction of 80%, highlighting strong susceptibility to paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "gpt4omini",
                    "audio",
                    "desta25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR condition, which uses transcripts without paralinguistic features, showed intermediate levels of bias. For example, Qwen2.5-3B produced recommendation rates of 14.8% (ASR) versus 97.6% (text) and 75.3% (audio), suggesting that both transcription errors and vocal characteristics contribute to disparities. Despite relatively strong ASR accuracy (average WER = 6.4%), the ASR condition still produced significant deviations in 3 of 6 models under the DA setting, highlighting how even minor transcription errors can cascade into clinically meaningful differences in decision-making.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the DA condition, 4 of 6 models showed recommendations that significantly varied by age. Qwen2.5-3B showed the largest difference, recommending surgery for 85.3% of young vs. 73.5% of elderly patients.\nThis 11.8% gap is a clinically meaningful finding that could lead to the systematic under-treatment of elderly patients.</p>\n\n",
                "matched_terms": [
                    "age",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unexpectedly, CoT prompting increased the <span class=\"ltx_text ltx_font_italic\">prevalence</span> of age-related differences, with five of six models showing significant variations. While the magnitude of these differences slightly decreased on average (mean absolute difference: DA = 4.9%, CoT = 3.7%), their increased consistency across models suggests that explicit reasoning may activate shared yet problematic clinical heuristics about age and surgical risk. Interestingly, models like DeSTA2.5 and Qwen2.5-3B reversed their recommendation patterns between the DA and CoT conditions, indicating that the reasoning pathways fundamentally alter how age cues influence a model&#8217;s decisions. The findings imply that current audio LLMs are not yet equipped to effectively process and appropriately handle paralinguistic signals.</p>\n\n",
                "matched_terms": [
                    "age",
                    "audio",
                    "desta25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Gender bias patterns differed markedly from age-related disparities, as shown in Table 1. In the Direct Answer (DA) condition, only half of the models exhibited significant gender bias. The absolute differences in recommendation rates (ranging from 1.9% to 8.0%) were substantially smaller than those observed for age-related disparities.\nDeSTA2.5 showed the largest gender gap, recommending surgery for 92.5% of male vs. 84.5% of female patients.</p>\n\n",
                "matched_terms": [
                    "gender",
                    "desta25",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoT prompting eliminated gender bias across all models, with no significant differences observed. This complete mitigation contrasts sharply with the age-related effects under CoT. This finding suggests that models may encode and process age and gender information through fundamentally different mechanisms, and that they are better equipped to handle gender cues than age cues.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated whether emotional expression in speech affected surgery recommendations across six emotion categories.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T5\" title=\"Table 5 &#8227; 4.4 Emotional Expression Effect &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the percentage of &#8220;Yes&#8221; recommendations for each emotion in the DA setting.\nMost models exhibited relatively consistent behavior across emotions, with only two models showing significant differences: gemini-2.0 and DeSTA2.5. For gemini-2.0, the &#8220;happy&#8221; emotion (1.8%) showed a notably higher recommendation rate compared to other emotions (0.3% to 0.8%).</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "desta25",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the minimal variation in surgery recommendations across emotional expressions should be interpreted with caution. Given that most models demonstrated extremely low emotion detection accuracy (below 17%), the observed consistency likely reflects their inability to perceive emotional cues rather than deliberate emotional robustness.\nTherefore, only results from models with demonstrated emotion detection capabilities, such as DeSTA 2.5, can provide meaningful insights into genuine emotion-based bias.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "accuracy",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study provides the first systematic evaluation of voice-based bias in audio LLMs for medical decision-making. We created <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span>, which uses 170 clinical cases and 36 synthesized voice profiles to enable a controlled bias assessment.\nOur findings reveal that audio LLMs exhibit significant instability, with surgery recommendation rates deviating a lot from text-only baselines. We demonstrate that these biases manifest across all demographic groups, with emotional expressions further amplifying the effect. While explicit reasoning from the models mitigates these biases, it does not eliminate them. This highlights a fundamental architectural challenge: the inability to reliably disentangle a patient&#8217;s medical information from the paralinguistic features of their voice.\nThese results have critical implications for the deployment of speech-enabled AI in healthcare. Audio LLMs currently pose unacceptable risks of creating disparities based on how patients sound rather than on their medical needs. We conclude that bias-aware training and architectural innovations are imperative before clinical deployment to ensure that decisions are driven by medical evidence, not by a patient&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "voicebased",
                    "audio",
                    "surgery"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making",
        "caption": "Table 3: Surgery recommendation rates without emotional expressions (%).\nBold fonts indicate statistically significant differences (p <<0.05) compared to Text baseline.",
        "body": "Direct Answer (DA)\nChain-of-Thought (CoT)\n\n\nModel\nText\nText+Profile\nASR\nAudio\nText\nText+Profile\nASR\nAudio\n\n\ngpt-4o-mini\n26.5\n26.5\n19.4\n\n5.3\n\n14.7\n14.7\n11.2\n12.4\n\n\ngemini-2.0-flash\n0.0\n0.0\n14.1\n0.6\n7.6\n7.6\n6.5\n6.5\n\n\ngemini-2.5-flash\n27.6\n27.6\n21.2\n31.8\n6.7\n6.7\n23.5\n18.2\n\n\nQwen2.5-Omni-3B\n97.6\n97.6\n14.8\n75.3\n31.8\n31.8\n15.4\n35.9\n\n\nQwen2.5-Omni-7B\n11.2\n11.2\n5.3\n20.6\n22.7\n22.7\n26.5\n27.6\n\n\nDeSTA2.5\n53.9\n53.9\n26.5\n88.8\n26.8\n26.8\n28.3\n28.5",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Direct Answer (DA)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chain-of-Thought (CoT)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text+Profile</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ASR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text+Profile</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ASR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gpt-4o-mini</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.3</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gemini-2.0-flash</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gemini-2.5-flash</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qwen2.5-Omni-3B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">75.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">15.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qwen2.5-Omni-7B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DeSTA2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">88.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.5</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recommendation",
            "emotional",
            "answer",
            "qwen25omni3b",
            "text",
            "gemini20flash",
            "chainofthought",
            "baseline",
            "textprofile",
            "gemini25flash",
            "indicate",
            "cot",
            "model",
            "fonts",
            "desta25",
            "without",
            "differences",
            "significant",
            "bold",
            "asr",
            "rates",
            "qwen25omni7b",
            "surgery",
            "compared",
            "statistically",
            "gpt4omini",
            "expressions",
            "direct",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveals substantial modality-dependent bias in surgical recommendations. Under the DA condition, 66.7% (4 of 6) evaluated models exhibited statistically significant differences between text and audio inputs, with recommendation rate shifts ranging from -22.2% for Qwen2.5-3B to +34.9% for DeSTA2.5.\nA striking example is GPT-4o-mini, whose recommendation rate dropped from 26.5% with text to just 5.3% with audio, a relative reduction of 80%, highlighting strong susceptibility to paralinguistic cues.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations.\nFurther analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance.\nThese results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient&#8217;s voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "model",
                    "audio",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid deployment of large language models (LLMs) in healthcare presents both a promising frontier and a critical concern. Among medical decisions, surgical recommendations are particularly challenging for bias assessment. While these decisions should be driven by clinical factors, research shows that implicit biases often lead to the substitution of demographic proxies, such as using a patient&#8217;s age instead of their frailty as a decisive factor <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">montroni2021surgical</span>]</cite>. Disentangling such biases from legitimate clinical variations, like the higher surgery rates for females due to conditions like cholecystitis <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bicket2024prevalence</span>]</cite>, requires controlled experimental designs that isolate demographic factors.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a comprehensive framework for assessing demographic bias in audio LLMs&#8217; medical decision-making. Our approach focuses on surgical recommendation tasks, leveraging their documented susceptibility to demographic substitution effects. Through controlled experiments using synthesized speech, we isolate the effects of voice characteristics while holding clinical content constant. This methodology allows us to determine whether audio LLMs perpetuate the problematic pattern of using demographic proxies instead of clinical indicators.\nOur contributions are 3-fold:</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the first systematic evaluation of voice-based bias in audio LLMs through binary surgery decisions, revealing how paralinguistic features influence high-stakes medical recommendations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We create <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span> with 170 clinical cases from DDXPlus with 36 synthesized speaker profiles (age, gender, emotional expressions) to benchmark controlled bias assessment. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Dataset available at <a class=\"ltx_ref ltx_href\" href=\"https://hf.co/datasets/theblackcat102/MedVoiceBias\" title=\"\">https://hf.co/datasets/theblackcat102/MedVoiceBias</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "expressions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate severe modality-dependent biases (up to 34.9pp deviation between text/audio), with age disparities persisting under chain-of-thought (CoT) while gender bias is eliminated.\n</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "cot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The intersection of speech AI and bias in healthcare is understudied but critical field. We categorize existing research into three key areas: automatic speech recognition (ASR) bias, behavioral bias in audio LLMs, and bias in medical AI systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR and Audio LLM Bias.</span> Speech recognition systems exhibit systematic performance disparities across demographic groups. Koenecke et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koenecke2020racial</span>]</cite> first documented significantly higher word error rates for Black speakers compared to White speakers. Harris et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">harris2024modeling</span>]</cite> demonstrated that intersectional bias patterns compound these disparities, with the Fair-Speech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">veliche2024towards</span>]</cite> revealing performance gaps exceeding 40% across demographic groups.\nBeyond ASR performance, Spoken StereoSet <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024spoken</span>]</cite> evaluated behavioral biases in speech language models, finding minimal bias scores close to 50% for general stereotype tasks involving gender and age demographics. However, domain-specific applications, particularly high-stakes medical contexts, may reveal different bias patterns as clinical decision-making involves complex reasoning beyond simple stereotype association.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "audio",
                    "compared",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For treatment recommendations, we employ yes/no surgery decisions as clear, high-stakes binary classification tasks.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our implementation allowed models to output &#8220;yes,&#8221; &#8220;maybe,&#8221; or &#8220;no,&#8221; but for analysis we treated only &#8220;yes&#8221; as a positive decision and grouped the others together.</span></span></span>\nBinary classification provides a well-established framework for evaluating algorithmic bias, yielding unambiguous outcomes that facilitate straightforward fairness measurement <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">estiri2022objective</span>]</cite>.\nSurgical decisions are particularly consequential, as historical evidence shows that human clinical judgment has been influenced by patient demographics, resulting in disparities in access to major surgery <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">binkley2022should</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kiyasseh2023human</span>]</cite>.\nBy requiring a definitive yes/no recommendation, we create a clear and interpretable signal for bias. This design enables precise quantification of how vocal profiles may skew audio LLMs&#8217; recommendations, offering a critical mechanism to detect and mitigate potential harms before such systems are deployed in real-world clinical practice.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, we constructed 36 unique voice profiles. For each selected speaker profile (demographic or emotional), we used Sesame-1B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sesame2024csm1b</span>]</cite> to synthesize patient speech from DDXPlus <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fansi2022ddxplus</span>]</cite> clinical contexts, adapted to match the target voice characteristics.\nBecause input length constraints degraded synthesis quality, we segmented the original patient profiles at the sentence level. For each sentence, we generated three candidate synthesis samples per voice profile and applied Whisper-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>]</cite> to perform ASR on all outputs. The sample with the lowest word error rate (WER) was selected as the final audio input for downstream experiments, yielding an average WER of 6.4% across all profiles. The statistics of our MedVoiceBias data is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S3.T1\" title=\"Table 1 &#8227; 3.2.3 Voice Cloning &#8227; 3.2 MedVoiceBias Dataset Construction &#8227; 3 Methodology &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "audio",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate synthesis quality, we applied the MOSA-Net+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zezario2024study</span>]</cite> automatic speech quality assessment model. The generated samples achieved average PESQ and intelligibility scores of 3.6/5.0 and 0.97, respectively, indicating consistently high-quality audio for subsequent evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate bias by systematically comparing a model&#8217;s clinical recommendations from <span class=\"ltx_text ltx_font_bold\">voice-based</span> inputs against those generated from a <span class=\"ltx_text ltx_font_bold\">text-only</span> control baseline. This approach allows us to isolate and quantify the influence of paralinguistic and demographic cues present in the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary metric is the <span class=\"ltx_text ltx_font_italic\">surgery recommendation rate</span>, defined as the proportion of cases in which a model recommends a surgical intervention.\nFor each model and prompting strategy, we first establish this rate using the text-only baseline. We then calculate the recommendation rate for each distinct demographic cohort in our dataset (age, gender, and emotion), enabling a direct comparison of the model&#8217;s behavior with and without audio cues.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "model",
                    "surgery",
                    "without",
                    "baseline",
                    "direct",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All statistical comparisons utilize Fisher&#8217;s exact test <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">upton1992fisher</span>]</cite> to compare surgery recommendation rates between audio-based cohorts and the text-only baseline. We chose this test over Chi-squared to avoid issues with low expected cell counts in our demographic subgroups, ensuring precise statistical inference and exact p-values. A difference is considered statistically significant if the p-value is less than 0.05. To focus on clinically meaningful effects, we additionally require an absolute difference of 2% or greater in the surgical recommendation rate to highlight a finding.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "recommendation",
                    "surgery",
                    "statistically",
                    "baseline",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the main experiments, we performed two preliminary evaluations. First, we conducted a basic sanity check on the text-only patient profiles to ensure all models could correctly perform the fundamental task of providing a surgical recommendation.\nSecond, we fed audio from a Common Voice profile to each model to verify its ability to distinguish age and gender. This evaluation confirmed that the models could accurately recognize gender from voice, a key prerequisite for our bias analysis. This entire process was repeated for every model and prompting strategy to ensure a robust and comprehensive analysis.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate implicit bias in audio LLMs for medical decision-making, we designed a two-phase experimental protocol. In the first phase, we validated each model&#8217;s ability to (1) identify demographic characteristics from voice and (2) generate accurate surgical recommendations from text. In the second phase, we compared surgical recommendation rates across different input modalities (text vs. audio) and demographic groups.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "recommendation",
                    "text",
                    "compared",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated six state-of-the-art audio LLMs: DeSTA2.5-Audio 8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span>]</cite>, Qwen2.5-Omni 3B and 7B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>]</cite>, Gemini Flash (2.0, 2.5) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>]</cite>, and GPT-4o-mini-audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hurst2024gpt</span>]</cite>. Before examining potential bias, we first confirmed that these models could reliably distinguish age, gender, and emotions from audio, as well as achieve performance above random chance when making surgical recommendations from the original text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each model, we conducted experiments under two conditions: <span class=\"ltx_text ltx_font_italic\">direct answer (DA)</span> and <span class=\"ltx_text ltx_font_italic\">diagnose-then-decide chain-of-thought (CoT)</span>. In the DA setting, the model received the full patient profile (either audio or ASR transcripts) and was asked to directly output a binary decision regarding surgical necessity.\nIn the CoT setting, the model was first prompted to infer the possible disease and then determine whether surgery is required.</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "cot",
                    "model",
                    "answer",
                    "surgery",
                    "direct",
                    "audio",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR condition, which uses transcripts without paralinguistic features, showed intermediate levels of bias. For example, Qwen2.5-3B produced recommendation rates of 14.8% (ASR) versus 97.6% (text) and 75.3% (audio), suggesting that both transcription errors and vocal characteristics contribute to disparities. Despite relatively strong ASR accuracy (average WER = 6.4%), the ASR condition still produced significant deviations in 3 of 6 models under the DA setting, highlighting how even minor transcription errors can cascade into clinically meaningful differences in decision-making.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "recommendation",
                    "text",
                    "without",
                    "differences",
                    "significant",
                    "audio",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Demographic-Aware Effects &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the surgical recommendation rates stratified by patient age. Contrary to our hypothesis that explicit reasoning would compel models to focus solely on the disease, CoT prompting revealed persistent and sometimes amplified age-related disparities.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "recommendation",
                    "cot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unexpectedly, CoT prompting increased the <span class=\"ltx_text ltx_font_italic\">prevalence</span> of age-related differences, with five of six models showing significant variations. While the magnitude of these differences slightly decreased on average (mean absolute difference: DA = 4.9%, CoT = 3.7%), their increased consistency across models suggests that explicit reasoning may activate shared yet problematic clinical heuristics about age and surgical risk. Interestingly, models like DeSTA2.5 and Qwen2.5-3B reversed their recommendation patterns between the DA and CoT conditions, indicating that the reasoning pathways fundamentally alter how age cues influence a model&#8217;s decisions. The findings imply that current audio LLMs are not yet equipped to effectively process and appropriately handle paralinguistic signals.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "cot",
                    "desta25",
                    "differences",
                    "significant",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Gender bias patterns differed markedly from age-related disparities, as shown in Table 1. In the Direct Answer (DA) condition, only half of the models exhibited significant gender bias. The absolute differences in recommendation rates (ranging from 1.9% to 8.0%) were substantially smaller than those observed for age-related disparities.\nDeSTA2.5 showed the largest gender gap, recommending surgery for 92.5% of male vs. 84.5% of female patients.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "recommendation",
                    "answer",
                    "desta25",
                    "surgery",
                    "differences",
                    "significant",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoT prompting eliminated gender bias across all models, with no significant differences observed. This complete mitigation contrasts sharply with the age-related effects under CoT. This finding suggests that models may encode and process age and gender information through fundamentally different mechanisms, and that they are better equipped to handle gender cues than age cues.</p>\n\n",
                "matched_terms": [
                    "cot",
                    "significant",
                    "differences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated whether emotional expression in speech affected surgery recommendations across six emotion categories.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T5\" title=\"Table 5 &#8227; 4.4 Emotional Expression Effect &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the percentage of &#8220;Yes&#8221; recommendations for each emotion in the DA setting.\nMost models exhibited relatively consistent behavior across emotions, with only two models showing significant differences: gemini-2.0 and DeSTA2.5. For gemini-2.0, the &#8220;happy&#8221; emotion (1.8%) showed a notably higher recommendation rate compared to other emotions (0.3% to 0.8%).</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "emotional",
                    "desta25",
                    "surgery",
                    "compared",
                    "differences",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the minimal variation in surgery recommendations across emotional expressions should be interpreted with caution. Given that most models demonstrated extremely low emotion detection accuracy (below 17%), the observed consistency likely reflects their inability to perceive emotional cues rather than deliberate emotional robustness.\nTherefore, only results from models with demonstrated emotion detection capabilities, such as DeSTA 2.5, can provide meaningful insights into genuine emotion-based bias.</p>\n\n",
                "matched_terms": [
                    "expressions",
                    "emotional",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study provides the first systematic evaluation of voice-based bias in audio LLMs for medical decision-making. We created <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span>, which uses 170 clinical cases and 36 synthesized voice profiles to enable a controlled bias assessment.\nOur findings reveal that audio LLMs exhibit significant instability, with surgery recommendation rates deviating a lot from text-only baselines. We demonstrate that these biases manifest across all demographic groups, with emotional expressions further amplifying the effect. While explicit reasoning from the models mitigates these biases, it does not eliminate them. This highlights a fundamental architectural challenge: the inability to reliably disentangle a patient&#8217;s medical information from the paralinguistic features of their voice.\nThese results have critical implications for the deployment of speech-enabled AI in healthcare. Audio LLMs currently pose unacceptable risks of creating disparities based on how patients sound rather than on their medical needs. We conclude that bias-aware training and architectural innovations are imperative before clinical deployment to ensure that decisions are driven by medical evidence, not by a patient&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "rates",
                    "recommendation",
                    "emotional",
                    "surgery",
                    "significant",
                    "expressions",
                    "audio"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making",
        "caption": "Table 4: Age and gender effects in audio surgery recommendation rate without emotional expressions (%).\nBold fonts indicate statistically significant differences (p < 0.05) between groups.",
        "body": "Direct Answer (DA)\nChain-of-Thought (CoT)\n\n\nModel\nYoung\nOld\nMale\nFemale\nYoung\nOld\nMale\nFemale\n\n\ngpt-4o-mini\n3.6\n3.6\n3.9\n2.6\n8.4\n5.4\n5.0\n5.0\n\n\ngemini-2.0-flash\n0.7\n0.6\n0.6\n0.5\n6.0\n3.7\n3.7\n3.5\n\n\ngemini-2.5-flash\n25.3\n17.9\n19.7\n18.8\n16.1\n8.5\n10.1\n9.4\n\n\nQwen2.5-Omni-3B\n85.3\n73.5\n76.7\n73.2\n23.7\n28.2\n30.0\n28.1\n\n\nQwen2.5-Omni-7B\n16.8\n14.9\n14.3\n15.7\n25.8\n22.6\n22.8\n22.4\n\n\nDeSTA2.5\n87.6\n90.1\n93.5\n83.7\n22.6\n20.9\n20.9\n18.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Direct Answer (DA)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chain-of-Thought (CoT)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Young</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Old</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Male</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Female</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Young</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Old</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Male</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Female</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gpt-4o-mini</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gemini-2.0-flash</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">gemini-2.5-flash</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">25.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">16.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qwen2.5-Omni-3B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">85.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">76.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">23.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qwen2.5-Omni-7B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">25.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">22.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DeSTA2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">87.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">83.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.9</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "recommendation",
            "emotional",
            "answer",
            "qwen25omni3b",
            "rate",
            "male",
            "young",
            "gemini20flash",
            "chainofthought",
            "between",
            "gemini25flash",
            "indicate",
            "gender",
            "old",
            "cot",
            "model",
            "fonts",
            "desta25",
            "age",
            "without",
            "differences",
            "female",
            "significant",
            "groups",
            "bold",
            "gpt4omini",
            "qwen25omni7b",
            "surgery",
            "statistically",
            "effects",
            "expressions",
            "direct",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Demographic-Aware Effects &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the surgical recommendation rates stratified by patient age. Contrary to our hypothesis that explicit reasoning would compel models to focus solely on the disease, CoT prompting revealed persistent and sometimes amplified age-related disparities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations.\nFurther analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance.\nThese results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient&#8217;s voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "model",
                    "age",
                    "between",
                    "young",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid deployment of large language models (LLMs) in healthcare presents both a promising frontier and a critical concern. Among medical decisions, surgical recommendations are particularly challenging for bias assessment. While these decisions should be driven by clinical factors, research shows that implicit biases often lead to the substitution of demographic proxies, such as using a patient&#8217;s age instead of their frailty as a decisive factor <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">montroni2021surgical</span>]</cite>. Disentangling such biases from legitimate clinical variations, like the higher surgery rates for females due to conditions like cholecystitis <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bicket2024prevalence</span>]</cite>, requires controlled experimental designs that isolate demographic factors.</p>\n\n",
                "matched_terms": [
                    "age",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a comprehensive framework for assessing demographic bias in audio LLMs&#8217; medical decision-making. Our approach focuses on surgical recommendation tasks, leveraging their documented susceptibility to demographic substitution effects. Through controlled experiments using synthesized speech, we isolate the effects of voice characteristics while holding clinical content constant. This methodology allows us to determine whether audio LLMs perpetuate the problematic pattern of using demographic proxies instead of clinical indicators.\nOur contributions are 3-fold:</p>\n\n",
                "matched_terms": [
                    "effects",
                    "recommendation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the first systematic evaluation of voice-based bias in audio LLMs through binary surgery decisions, revealing how paralinguistic features influence high-stakes medical recommendations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We create <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span> with 170 clinical cases from DDXPlus with 36 synthesized speaker profiles (age, gender, emotional expressions) to benchmark controlled bias assessment. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Dataset available at <a class=\"ltx_ref ltx_href\" href=\"https://hf.co/datasets/theblackcat102/MedVoiceBias\" title=\"\">https://hf.co/datasets/theblackcat102/MedVoiceBias</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "age",
                    "expressions",
                    "emotional",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate severe modality-dependent biases (up to 34.9pp deviation between text/audio), with age disparities persisting under chain-of-thought (CoT) while gender bias is eliminated.\n</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "cot",
                    "age",
                    "between",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR and Audio LLM Bias.</span> Speech recognition systems exhibit systematic performance disparities across demographic groups. Koenecke et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koenecke2020racial</span>]</cite> first documented significantly higher word error rates for Black speakers compared to White speakers. Harris et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">harris2024modeling</span>]</cite> demonstrated that intersectional bias patterns compound these disparities, with the Fair-Speech dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">veliche2024towards</span>]</cite> revealing performance gaps exceeding 40% across demographic groups.\nBeyond ASR performance, Spoken StereoSet <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024spoken</span>]</cite> evaluated behavioral biases in speech language models, finding minimal bias scores close to 50% for general stereotype tasks involving gender and age demographics. However, domain-specific applications, particularly high-stakes medical contexts, may reveal different bias patterns as clinical decision-making involves complex reasoning beyond simple stereotype association.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "audio",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For treatment recommendations, we employ yes/no surgery decisions as clear, high-stakes binary classification tasks.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our implementation allowed models to output &#8220;yes,&#8221; &#8220;maybe,&#8221; or &#8220;no,&#8221; but for analysis we treated only &#8220;yes&#8221; as a positive decision and grouped the others together.</span></span></span>\nBinary classification provides a well-established framework for evaluating algorithmic bias, yielding unambiguous outcomes that facilitate straightforward fairness measurement <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">estiri2022objective</span>]</cite>.\nSurgical decisions are particularly consequential, as historical evidence shows that human clinical judgment has been influenced by patient demographics, resulting in disparities in access to major surgery <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">binkley2022should</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kiyasseh2023human</span>]</cite>.\nBy requiring a definitive yes/no recommendation, we create a clear and interpretable signal for bias. This design enables precise quantification of how vocal profiles may skew audio LLMs&#8217; recommendations, offering a critical mechanism to detect and mitigate potential harms before such systems are deployed in real-world clinical practice.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "audio",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sourced utterances from the Common Voice repository <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2020common</span>]</cite> (release <span class=\"ltx_text ltx_font_typewriter\">cv-corpus-22.0-2025-06-20</span>). Using the provided metadata, we focused on two age strata: (i) speakers self-reporting ages 20&#8211;29 (<span class=\"ltx_text ltx_font_italic\">young</span>) and (ii) speakers self-reporting ages 60 (<span class=\"ltx_text ltx_font_italic\">old</span>).\nWithin each cohort, we retained only those speakers whose perceived age and gender are unambiguous to human raters. Manual validation was essential, as a systematic comparison of 200 randomly sampled profiles revealed discrepancies between self-reported and acoustically perceived demographics in 23% of cases (age: 18%, gender: 5%). From this process, we selected 12 speakers, balanced across both age and gender (6M/6F; 6 young/6 old), with demographic classifications confirmed by consensus among three annotators.</p>\n\n",
                "matched_terms": [
                    "age",
                    "between",
                    "young",
                    "gender",
                    "old"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine whether paralinguistic cues influence downstream bias, we augmented the corpus with controlled emotional renderings. Specifically, we used the Expresso dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2023expresso</span>]</cite>, selecting six affective conditions: <span class=\"ltx_text ltx_font_italic\">happy</span>, <span class=\"ltx_text ltx_font_italic\">laughing</span>, <span class=\"ltx_text ltx_font_italic\">sad</span>, <span class=\"ltx_text ltx_font_italic\">confused</span>, <span class=\"ltx_text ltx_font_italic\">enunciated</span>, and <span class=\"ltx_text ltx_font_italic\">whisper</span>.\nExpresso includes two male and two female speakers self-identified as young, and we incorporated all four speakers expressing each of the six emotions available in the dataset.</p>\n\n",
                "matched_terms": [
                    "male",
                    "young",
                    "emotional",
                    "female"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, we constructed 36 unique voice profiles. For each selected speaker profile (demographic or emotional), we used Sesame-1B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sesame2024csm1b</span>]</cite> to synthesize patient speech from DDXPlus <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fansi2022ddxplus</span>]</cite> clinical contexts, adapted to match the target voice characteristics.\nBecause input length constraints degraded synthesis quality, we segmented the original patient profiles at the sentence level. For each sentence, we generated three candidate synthesis samples per voice profile and applied Whisper-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>]</cite> to perform ASR on all outputs. The sample with the lowest word error rate (WER) was selected as the final audio input for downstream experiments, yielding an average WER of 6.4% across all profiles. The statistics of our MedVoiceBias data is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S3.T1\" title=\"Table 1 &#8227; 3.2.3 Voice Cloning &#8227; 3.2 MedVoiceBias Dataset Construction &#8227; 3 Methodology &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "rate",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate synthesis quality, we applied the MOSA-Net+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zezario2024study</span>]</cite> automatic speech quality assessment model. The generated samples achieved average PESQ and intelligibility scores of 3.6/5.0 and 0.97, respectively, indicating consistently high-quality audio for subsequent evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary metric is the <span class=\"ltx_text ltx_font_italic\">surgery recommendation rate</span>, defined as the proportion of cases in which a model recommends a surgical intervention.\nFor each model and prompting strategy, we first establish this rate using the text-only baseline. We then calculate the recommendation rate for each distinct demographic cohort in our dataset (age, gender, and emotion), enabling a direct comparison of the model&#8217;s behavior with and without audio cues.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "model",
                    "surgery",
                    "age",
                    "rate",
                    "without",
                    "direct",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All statistical comparisons utilize Fisher&#8217;s exact test <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">upton1992fisher</span>]</cite> to compare surgery recommendation rates between audio-based cohorts and the text-only baseline. We chose this test over Chi-squared to avoid issues with low expected cell counts in our demographic subgroups, ensuring precise statistical inference and exact p-values. A difference is considered statistically significant if the p-value is less than 0.05. To focus on clinically meaningful effects, we additionally require an absolute difference of 2% or greater in the surgical recommendation rate to highlight a finding.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "surgery",
                    "rate",
                    "statistically",
                    "between",
                    "significant",
                    "effects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the main experiments, we performed two preliminary evaluations. First, we conducted a basic sanity check on the text-only patient profiles to ensure all models could correctly perform the fundamental task of providing a surgical recommendation.\nSecond, we fed audio from a Common Voice profile to each model to verify its ability to distinguish age and gender. This evaluation confirmed that the models could accurately recognize gender from voice, a key prerequisite for our bias analysis. This entire process was repeated for every model and prompting strategy to ensure a robust and comprehensive analysis.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "model",
                    "age",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate implicit bias in audio LLMs for medical decision-making, we designed a two-phase experimental protocol. In the first phase, we validated each model&#8217;s ability to (1) identify demographic characteristics from voice and (2) generate accurate surgical recommendations from text. In the second phase, we compared surgical recommendation rates across different input modalities (text vs. audio) and demographic groups.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "audio",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated six state-of-the-art audio LLMs: DeSTA2.5-Audio 8B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025desta2</span>]</cite>, Qwen2.5-Omni 3B and 7B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2.5-Omni</span>]</cite>, Gemini Flash (2.0, 2.5) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">comanici2025gemini</span>]</cite>, and GPT-4o-mini-audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hurst2024gpt</span>]</cite>. Before examining potential bias, we first confirmed that these models could reliably distinguish age, gender, and emotions from audio, as well as achieve performance above random chance when making surgical recommendations from the original text.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show varying levels of demographic detection across models. Gender prediction accuracy ranged from 96.1% to 99.9% for most models (with the exception of GPT-4o-mini at 0%), while age prediction exhibited wider variance (32.6% to 85.7%). This demographic detection ability is a prerequisite for analyzing bias, as models are capable of perceiving demographic cues and then exhibit differential behavior.</p>\n\n",
                "matched_terms": [
                    "age",
                    "gender",
                    "gpt4omini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each model, we conducted experiments under two conditions: <span class=\"ltx_text ltx_font_italic\">direct answer (DA)</span> and <span class=\"ltx_text ltx_font_italic\">diagnose-then-decide chain-of-thought (CoT)</span>. In the DA setting, the model received the full patient profile (either audio or ASR transcripts) and was asked to directly output a binary decision regarding surgical necessity.\nIn the CoT setting, the model was first prompted to infer the possible disease and then determine whether surgery is required.</p>\n\n",
                "matched_terms": [
                    "chainofthought",
                    "cot",
                    "model",
                    "answer",
                    "surgery",
                    "direct",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveals substantial modality-dependent bias in surgical recommendations. Under the DA condition, 66.7% (4 of 6) evaluated models exhibited statistically significant differences between text and audio inputs, with recommendation rate shifts ranging from -22.2% for Qwen2.5-3B to +34.9% for DeSTA2.5.\nA striking example is GPT-4o-mini, whose recommendation rate dropped from 26.5% with text to just 5.3% with audio, a relative reduction of 80%, highlighting strong susceptibility to paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "desta25",
                    "rate",
                    "differences",
                    "statistically",
                    "between",
                    "significant",
                    "gpt4omini",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR condition, which uses transcripts without paralinguistic features, showed intermediate levels of bias. For example, Qwen2.5-3B produced recommendation rates of 14.8% (ASR) versus 97.6% (text) and 75.3% (audio), suggesting that both transcription errors and vocal characteristics contribute to disparities. Despite relatively strong ASR accuracy (average WER = 6.4%), the ASR condition still produced significant deviations in 3 of 6 models under the DA setting, highlighting how even minor transcription errors can cascade into clinically meaningful differences in decision-making.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "without",
                    "differences",
                    "significant",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the DA condition, 4 of 6 models showed recommendations that significantly varied by age. Qwen2.5-3B showed the largest difference, recommending surgery for 85.3% of young vs. 73.5% of elderly patients.\nThis 11.8% gap is a clinically meaningful finding that could lead to the systematic under-treatment of elderly patients.</p>\n\n",
                "matched_terms": [
                    "age",
                    "young",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unexpectedly, CoT prompting increased the <span class=\"ltx_text ltx_font_italic\">prevalence</span> of age-related differences, with five of six models showing significant variations. While the magnitude of these differences slightly decreased on average (mean absolute difference: DA = 4.9%, CoT = 3.7%), their increased consistency across models suggests that explicit reasoning may activate shared yet problematic clinical heuristics about age and surgical risk. Interestingly, models like DeSTA2.5 and Qwen2.5-3B reversed their recommendation patterns between the DA and CoT conditions, indicating that the reasoning pathways fundamentally alter how age cues influence a model&#8217;s decisions. The findings imply that current audio LLMs are not yet equipped to effectively process and appropriately handle paralinguistic signals.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "cot",
                    "desta25",
                    "age",
                    "differences",
                    "between",
                    "significant",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Gender bias patterns differed markedly from age-related disparities, as shown in Table 1. In the Direct Answer (DA) condition, only half of the models exhibited significant gender bias. The absolute differences in recommendation rates (ranging from 1.9% to 8.0%) were substantially smaller than those observed for age-related disparities.\nDeSTA2.5 showed the largest gender gap, recommending surgery for 92.5% of male vs. 84.5% of female patients.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "answer",
                    "desta25",
                    "surgery",
                    "female",
                    "differences",
                    "significant",
                    "male",
                    "direct",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoT prompting eliminated gender bias across all models, with no significant differences observed. This complete mitigation contrasts sharply with the age-related effects under CoT. This finding suggests that models may encode and process age and gender information through fundamentally different mechanisms, and that they are better equipped to handle gender cues than age cues.</p>\n\n",
                "matched_terms": [
                    "cot",
                    "age",
                    "differences",
                    "significant",
                    "effects",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated whether emotional expression in speech affected surgery recommendations across six emotion categories.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T5\" title=\"Table 5 &#8227; 4.4 Emotional Expression Effect &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the percentage of &#8220;Yes&#8221; recommendations for each emotion in the DA setting.\nMost models exhibited relatively consistent behavior across emotions, with only two models showing significant differences: gemini-2.0 and DeSTA2.5. For gemini-2.0, the &#8220;happy&#8221; emotion (1.8%) showed a notably higher recommendation rate compared to other emotions (0.3% to 0.8%).</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "emotional",
                    "desta25",
                    "surgery",
                    "rate",
                    "differences",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the minimal variation in surgery recommendations across emotional expressions should be interpreted with caution. Given that most models demonstrated extremely low emotion detection accuracy (below 17%), the observed consistency likely reflects their inability to perceive emotional cues rather than deliberate emotional robustness.\nTherefore, only results from models with demonstrated emotion detection capabilities, such as DeSTA 2.5, can provide meaningful insights into genuine emotion-based bias.</p>\n\n",
                "matched_terms": [
                    "expressions",
                    "emotional",
                    "surgery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study provides the first systematic evaluation of voice-based bias in audio LLMs for medical decision-making. We created <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span>, which uses 170 clinical cases and 36 synthesized voice profiles to enable a controlled bias assessment.\nOur findings reveal that audio LLMs exhibit significant instability, with surgery recommendation rates deviating a lot from text-only baselines. We demonstrate that these biases manifest across all demographic groups, with emotional expressions further amplifying the effect. While explicit reasoning from the models mitigates these biases, it does not eliminate them. This highlights a fundamental architectural challenge: the inability to reliably disentangle a patient&#8217;s medical information from the paralinguistic features of their voice.\nThese results have critical implications for the deployment of speech-enabled AI in healthcare. Audio LLMs currently pose unacceptable risks of creating disparities based on how patients sound rather than on their medical needs. We conclude that bias-aware training and architectural innovations are imperative before clinical deployment to ensure that decisions are driven by medical evidence, not by a patient&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "emotional",
                    "surgery",
                    "significant",
                    "groups",
                    "expressions",
                    "audio"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making",
        "caption": "Table 5: Positive rate across all expressions in the DA setting (%).",
        "body": "Model\nConf\nEnun\nHap\nLau\nSad\nWhi\nRef\n\n\ngpt-4o-mini\n3.8\n4.6\n4.2\n4.8\n3.6\n3.8\n26.5\n\n\ngemini-2.0\n0.8\n0.8\n1.8\n0.5\n0.5\n0.3\n0.0\n\n\ngemini-2.5\n29.2\n27.8\n27.0\n29.5\n29.7\n27.8\n27.6\n\n\nQwen2.5-3B\n92.0\n91.2\n92.3\n91.3\n91.8\n89.8\n97.6\n\n\nQwen2.5-7B\n17.3\n16.8\n20.3\n17.5\n16.8\n18.2\n11.2\n\n\nDeSTA2.5\n90.3\n87.4\n84.7\n87.8\n92.5\n87.9\n53.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Conf</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Enun</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Hap</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Lau</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sad</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Whi</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ref</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">gpt-4o-mini</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">gemini-2.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">gemini-2.5</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.5</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-3B</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-7B</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.5</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DeSTA2.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.8</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.9</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "conf",
            "enun",
            "rate",
            "qwen257b",
            "across",
            "qwen253b",
            "all",
            "model",
            "desta25",
            "gemini20",
            "ref",
            "sad",
            "lau",
            "hap",
            "setting",
            "positive",
            "gpt4omini",
            "expressions",
            "gemini25",
            "whi"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluated whether emotional expression in speech affected surgery recommendations across six emotion categories.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T5\" title=\"Table 5 &#8227; 4.4 Emotional Expression Effect &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the percentage of &#8220;Yes&#8221; recommendations for each emotion in the DA setting.\nMost models exhibited relatively consistent behavior across emotions, with only two models showing significant differences: gemini-2.0 and DeSTA2.5. For gemini-2.0, the &#8220;happy&#8221; emotion (1.8%) showed a notably higher recommendation rate compared to other emotions (0.3% to 0.8%).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To examine whether paralinguistic cues influence downstream bias, we augmented the corpus with controlled emotional renderings. Specifically, we used the Expresso dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nguyen2023expresso</span>]</cite>, selecting six affective conditions: <span class=\"ltx_text ltx_font_italic\">happy</span>, <span class=\"ltx_text ltx_font_italic\">laughing</span>, <span class=\"ltx_text ltx_font_italic\">sad</span>, <span class=\"ltx_text ltx_font_italic\">confused</span>, <span class=\"ltx_text ltx_font_italic\">enunciated</span>, and <span class=\"ltx_text ltx_font_italic\">whisper</span>.\nExpresso includes two male and two female speakers self-identified as young, and we incorporated all four speakers expressing each of the six emotions available in the dataset.</p>\n\n",
                "matched_terms": [
                    "all",
                    "sad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, we constructed 36 unique voice profiles. For each selected speaker profile (demographic or emotional), we used Sesame-1B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sesame2024csm1b</span>]</cite> to synthesize patient speech from DDXPlus <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fansi2022ddxplus</span>]</cite> clinical contexts, adapted to match the target voice characteristics.\nBecause input length constraints degraded synthesis quality, we segmented the original patient profiles at the sentence level. For each sentence, we generated three candidate synthesis samples per voice profile and applied Whisper-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>]</cite> to perform ASR on all outputs. The sample with the lowest word error rate (WER) was selected as the final audio input for downstream experiments, yielding an average WER of 6.4% across all profiles. The statistics of our MedVoiceBias data is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S3.T1\" title=\"Table 1 &#8227; 3.2.3 Voice Cloning &#8227; 3.2 MedVoiceBias Dataset Construction &#8227; 3 Methodology &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "rate",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our primary metric is the <span class=\"ltx_text ltx_font_italic\">surgery recommendation rate</span>, defined as the proportion of cases in which a model recommends a surgical intervention.\nFor each model and prompting strategy, we first establish this rate using the text-only baseline. We then calculate the recommendation rate for each distinct demographic cohort in our dataset (age, gender, and emotion), enabling a direct comparison of the model&#8217;s behavior with and without audio cues.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All statistical comparisons utilize Fisher&#8217;s exact test <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">upton1992fisher</span>]</cite> to compare surgery recommendation rates between audio-based cohorts and the text-only baseline. We chose this test over Chi-squared to avoid issues with low expected cell counts in our demographic subgroups, ensuring precise statistical inference and exact p-values. A difference is considered statistically significant if the p-value is less than 0.05. To focus on clinically meaningful effects, we additionally require an absolute difference of 2% or greater in the surgical recommendation rate to highlight a finding.</p>\n\n",
                "matched_terms": [
                    "all",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the main experiments, we performed two preliminary evaluations. First, we conducted a basic sanity check on the text-only patient profiles to ensure all models could correctly perform the fundamental task of providing a surgical recommendation.\nSecond, we fed audio from a Common Voice profile to each model to verify its ability to distinguish age and gender. This evaluation confirmed that the models could accurately recognize gender from voice, a key prerequisite for our bias analysis. This entire process was repeated for every model and prompting strategy to ensure a robust and comprehensive analysis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show varying levels of demographic detection across models. Gender prediction accuracy ranged from 96.1% to 99.9% for most models (with the exception of GPT-4o-mini at 0%), while age prediction exhibited wider variance (32.6% to 85.7%). This demographic detection ability is a prerequisite for analyzing bias, as models are capable of perceiving demographic cues and then exhibit differential behavior.</p>\n\n",
                "matched_terms": [
                    "gpt4omini",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each model, we conducted experiments under two conditions: <span class=\"ltx_text ltx_font_italic\">direct answer (DA)</span> and <span class=\"ltx_text ltx_font_italic\">diagnose-then-decide chain-of-thought (CoT)</span>. In the DA setting, the model received the full patient profile (either audio or ASR transcripts) and was asked to directly output a binary decision regarding surgical necessity.\nIn the CoT setting, the model was first prompted to infer the possible disease and then determine whether surgery is required.</p>\n\n",
                "matched_terms": [
                    "model",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.06592v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Setting &#8227; 4 Experiments &#8227; MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reveals substantial modality-dependent bias in surgical recommendations. Under the DA condition, 66.7% (4 of 6) evaluated models exhibited statistically significant differences between text and audio inputs, with recommendation rate shifts ranging from -22.2% for Qwen2.5-3B to +34.9% for DeSTA2.5.\nA striking example is GPT-4o-mini, whose recommendation rate dropped from 26.5% with text to just 5.3% with audio, a relative reduction of 80%, highlighting strong susceptibility to paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "gpt4omini",
                    "qwen253b",
                    "rate",
                    "desta25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR condition, which uses transcripts without paralinguistic features, showed intermediate levels of bias. For example, Qwen2.5-3B produced recommendation rates of 14.8% (ASR) versus 97.6% (text) and 75.3% (audio), suggesting that both transcription errors and vocal characteristics contribute to disparities. Despite relatively strong ASR accuracy (average WER = 6.4%), the ASR condition still produced significant deviations in 3 of 6 models under the DA setting, highlighting how even minor transcription errors can cascade into clinically meaningful differences in decision-making.</p>\n\n",
                "matched_terms": [
                    "qwen253b",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unexpectedly, CoT prompting increased the <span class=\"ltx_text ltx_font_italic\">prevalence</span> of age-related differences, with five of six models showing significant variations. While the magnitude of these differences slightly decreased on average (mean absolute difference: DA = 4.9%, CoT = 3.7%), their increased consistency across models suggests that explicit reasoning may activate shared yet problematic clinical heuristics about age and surgical risk. Interestingly, models like DeSTA2.5 and Qwen2.5-3B reversed their recommendation patterns between the DA and CoT conditions, indicating that the reasoning pathways fundamentally alter how age cues influence a model&#8217;s decisions. The findings imply that current audio LLMs are not yet equipped to effectively process and appropriately handle paralinguistic signals.</p>\n\n",
                "matched_terms": [
                    "qwen253b",
                    "across",
                    "desta25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoT prompting eliminated gender bias across all models, with no significant differences observed. This complete mitigation contrasts sharply with the age-related effects under CoT. This finding suggests that models may encode and process age and gender information through fundamentally different mechanisms, and that they are better equipped to handle gender cues than age cues.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the minimal variation in surgery recommendations across emotional expressions should be interpreted with caution. Given that most models demonstrated extremely low emotion detection accuracy (below 17%), the observed consistency likely reflects their inability to perceive emotional cues rather than deliberate emotional robustness.\nTherefore, only results from models with demonstrated emotion detection capabilities, such as DeSTA 2.5, can provide meaningful insights into genuine emotion-based bias.</p>\n\n",
                "matched_terms": [
                    "expressions",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study provides the first systematic evaluation of voice-based bias in audio LLMs for medical decision-making. We created <span class=\"ltx_text ltx_font_bold\">MedVoiceBias</span>, which uses 170 clinical cases and 36 synthesized voice profiles to enable a controlled bias assessment.\nOur findings reveal that audio LLMs exhibit significant instability, with surgery recommendation rates deviating a lot from text-only baselines. We demonstrate that these biases manifest across all demographic groups, with emotional expressions further amplifying the effect. While explicit reasoning from the models mitigates these biases, it does not eliminate them. This highlights a fundamental architectural challenge: the inability to reliably disentangle a patient&#8217;s medical information from the paralinguistic features of their voice.\nThese results have critical implications for the deployment of speech-enabled AI in healthcare. Audio LLMs currently pose unacceptable risks of creating disparities based on how patients sound rather than on their medical needs. We conclude that bias-aware training and architectural innovations are imperative before clinical deployment to ensure that decisions are driven by medical evidence, not by a patient&#8217;s voice.</p>\n\n",
                "matched_terms": [
                    "all",
                    "expressions",
                    "across"
                ]
            }
        ]
    }
}