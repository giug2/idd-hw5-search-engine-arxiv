{
    "S2.T1": {
        "source_file": "Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and Context-Aware Instruct-TTS",
        "caption": "Table 1: Examples of Text-to-Timbre Prompt, Context-Aware Instruct-TTS Prompt.",
        "body": "Text-to-Timbre Prompt\n\n\n\n\n\n\n\n1. 该角色是一个中年男性，身份是王朝将军，性格铁血威严、霸气侧漏，气质不怒自威\n\n\n\n\n\n\n\n2. 该角色是一个幼儿女性，身份是世家千金，性格活泼机敏、爱撒娇，气质天真灵动\n\n\n\n\n\n\n\nTemplate: 该角色是一个[幼年、青年、中年、老年][男性、女性]，身份是[xxx]，性格[xxx]\n\n\n\n\n\n\nContext-Aware Instruct-TTS Prompt\n\n\n\n\n\n\n\n1. 坚定 |阵前发布指令时的呐喊 |”三军听令！擂鼓，进军！后退者，斩！”\n\n\n\n\n\n\n\n2. 撒娇 |向长辈讨要东西时的快语 |”娘亲娘亲！你看那个糖人，翅膀亮晶晶的！给我买一个嘛～”\n\n\n\n\n\n\n\nTemplate: [单句情感] |[上下文场景] |[待合成文本]",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-to-Timbre Prompt</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n1. &#35813;&#35282;&#33394;&#26159;&#19968;&#20010;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#20013;&#24180;&#30007;&#24615;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#36523;&#20221;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#26159;&#29579;&#26397;&#23558;&#20891;&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#24615;&#26684;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#38081;&#34880;&#23041;&#20005;&#12289;&#38712;&#27668;&#20391;&#28431;&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#27668;&#36136;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#19981;&#24594;&#33258;&#23041;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n2. &#35813;&#35282;&#33394;&#26159;&#19968;&#20010;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#24188;&#20799;&#22899;&#24615;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#36523;&#20221;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#26159;&#19990;&#23478;&#21315;&#37329;&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#24615;&#26684;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#27963;&#27900;&#26426;&#25935;&#12289;&#29233;&#25746;&#23047;&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#27668;&#36136;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#22825;&#30495;&#28789;&#21160;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\nTemplate: &#35813;&#35282;&#33394;&#26159;&#19968;&#20010;[&#24188;&#24180;&#12289;&#38738;&#24180;&#12289;&#20013;&#24180;&#12289;&#32769;&#24180;][&#30007;&#24615;&#12289;&#22899;&#24615;]&#65292;&#36523;&#20221;&#26159;[xxx]&#65292;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#24615;&#26684;</span><span class=\"ltx_text\" style=\"font-size:90%;\">[xxx]</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context-Aware Instruct-TTS Prompt</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n1. </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#22362;&#23450;</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#38453;&#21069;&#21457;&#24067;&#25351;&#20196;&#26102;&#30340;&#21584;&#21898;</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |&#8221;&#19977;&#20891;&#21548;&#20196;&#65281;&#25794;&#40723;&#65292;&#36827;&#20891;&#65281;&#21518;&#36864;&#32773;&#65292;&#26025;&#65281;&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n2. </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#25746;&#23047;</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#21521;&#38271;&#36744;&#35752;&#35201;&#19996;&#35199;&#26102;&#30340;&#24555;&#35821;</span><span class=\"ltx_text\" style=\"font-size:90%;\"> |&#8221;&#23064;&#20146;&#23064;&#20146;&#65281;&#20320;&#30475;&#37027;&#20010;&#31958;&#20154;&#65292;&#32709;&#33152;&#20142;&#26230;&#26230;&#30340;&#65281;&#32473;&#25105;&#20080;&#19968;&#20010;&#22043;&#65374;&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\nTemplate: [&#21333;&#21477;&#24773;&#24863;] |[&#19978;&#19979;&#25991;&#22330;&#26223;] |[&#24453;&#21512;&#25104;&#25991;&#26412;]</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "上下文场景",
            "单句情感",
            "该角色是一个幼年、青年、中年、老年男性、女性，身份是xxx，性格xxx",
            "prompt",
            "该角色是一个中年男性，身份是王朝将军，性格铁血威严、霸气侧漏，气质不怒自威",
            "”娘亲娘亲！你看那个糖人，翅膀亮晶晶的！给我买一个嘛～”",
            "template",
            "examples",
            "待合成文本",
            "texttotimbre",
            "instructtts",
            "contextaware",
            "阵前发布指令时的呐喊",
            "”三军听令！擂鼓，进军！后退者，斩！”",
            "向长辈讨要东西时的快语",
            "该角色是一个幼儿女性，身份是世家千金，性格活泼机敏、爱撒娇，气质天真灵动"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To advance research in text-guided voice generation and expressive audiobook synthesis, we release BookVoice-50h, a novel synthetic dataset generated by our proposed models to support two tasks: TTT and CA-Instruct-TTS. The templates for text descriptions of the TTT model and instructions for CA-Instruct-TTS are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15845v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 Automated Audiobook Synthesis Pipeline &#8227; 2 Methed &#8227; Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and Context-Aware Instruct-TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The BookVoice-50h synthetic dataset will be released on Hugging Face upon publication. For audio samples and generation capabilities, visit our demo page: https://tme-lyra-lab.github.io/DeepDubbing.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The pipeline for multi-participant audiobook production primarily consists of three stages: script analysis, character voice timbre selection, and speech synthesis. Among these, script analysis can be automated with high accuracy using NLP models, whereas character voice timbre selection still relies on manual effort. Speech synthesis uses either manual dubbing or text-to-speech (TTS). While TTS boosts efficiency, it struggles with emotional expression, intonation control, and contextual scene adaptation. To address these challenges, we propose DeepDubbing, an end-to-end automated system for multi-participant audiobook production. The system comprises two main components: a Text-to-Timbre (TTT) model and a Context-Aware Instruct-TTS (CA-Instruct-TTS) model. The TTT model generates role-specific timbre embeddings conditioned on text descriptions. The CA-Instruct-TTS model synthesizes expressive speech by analyzing contextual dialogue and incorporating fine-grained emotional instructions. This system enables the automated generation of multi-participant audiobooks with both timbre-matched character voices and emotionally expressive narration, offering a novel solution for audiobook production.</span>\n</p>\n\n",
                "matched_terms": [
                    "texttotimbre",
                    "instructtts",
                    "contextaware"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nAudiobook Synthesis, Text-to-Timbre, Context-Aware Instruct-TTS, Conditional Flow Matching</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "texttotimbre",
                    "instructtts",
                    "contextaware"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We develop a Context-Aware Instruct-TTS (CA-Instruct-TTS) system that leverages LLM-derived emotion-scene instructions from narrative context to synthesize expressive speech, effectively mitigating contextual fragmentation.</span>\n</p>\n\n",
                "matched_terms": [
                    "instructtts",
                    "contextaware"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The DeepDubbing system consists of two main components: a Text-to-Timbre model that generates speaker embeddings from structured textual descriptions, and a Context-Aware Instruct-TTS model that synthesizes expressive speech conditioned on the generated speaker embedding and context-derived instructions. Details of each component are described in the following subsections.</span>\n</p>\n\n",
                "matched_terms": [
                    "texttotimbre",
                    "instructtts",
                    "contextaware"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ a large-scale internal multi-participant audiobook dataset comprising over 4,000 hours of high-quality speech. An automated LLM-based annotation pipeline generates structured labels for two tasks. For the Text-to-Timbre task, it constructs over 300K text-based timbre descriptions following a Gender|Age|Personality |Identity template; for the Context-Aware Instruct-TTS task, more than 2 million instructions are generated under an Emotion|Contextual Scenario template, covering over 44 fine-grained emotion categories. To support both tasks, the Cam++ model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15845v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is employed to extract speaker embeddings for each speech segment during training. The test set contains only unseen speaker identities to facilitate evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "texttotimbre",
                    "instructtts",
                    "contextaware",
                    "template"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and Context-Aware Instruct-TTS",
        "caption": "Table 2: Comparison of subjective and objective scores by CA-Instruct-TTS: Word Error Rate (WER), and Mean Opinion Scores (MOS): N-Naturalness, E-Emotion.",
        "body": "Method\n\nWER ↓\\downarrow\n\n\nMOS-N ↑\\uparrow\n\n\nMOS-E ↑\\uparrow\n\n\n\n\n\nCA-TTS\n2.39%\n3.10\n3.67\n\n\nCA-Instruct-TTS\n2.54%\n3.33\n4.15",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.7pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#8201;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.7pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS-N</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#8201;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.7pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS-E</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#8201;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CA-TTS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.39%</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.10</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CA-Instruct-TTS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.54%</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.33</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t\" style=\"padding:0.7pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.15</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "cainstructtts",
            "subjective",
            "↑uparrow",
            "opinion",
            "error",
            "nnaturalness",
            "rate",
            "mose",
            "objective",
            "scores",
            "method",
            "catts",
            "↓downarrow",
            "comparison",
            "mos",
            "eemotion",
            "word",
            "mosn",
            "mean"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For speech synthesis quality evaluation, we compared CA-Instruct-TTS with a baseline approach that directly inputs text and speech to the LLM without instruction guidance, as presented in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15845v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and Context-Aware Instruct-TTS\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The experimental results demonstrate that CA-Instruct-TTS achieves the best performance or is highly competitive across all subjective and objective metrics. While maintaining comparable word error rates, our proposed method demonstrates significant improvements in both naturalness (MOS-N) and emotional expressiveness (MOS-E). The enhanced emotional expressiveness particularly highlights the effectiveness of our context-aware instruction mechanism in generating more expressive and contextually appropriate speech synthesis. The improved naturalness scores indicate that the instruction-based approach better captures the intended emotional and contextual nuances, thereby producing more human-like speech generation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The core of OT-CFM is to train a neural network </span>\n  <math alttext=\"v_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">v_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to approximate this &#8221;direction field&#8221; of the forward process, i.e., to estimate the direction from any intermediate point </span>\n  <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"x_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The vector field estimator </span>\n  <math alttext=\"v_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">v_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the TTT model is based on Diffusion Transformer and adopts a multi-level fusion mechanism to integrate conditional information. At the input level, the noised speaker embedding </span>\n  <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is concatenated with text embedding </span>\n  <math alttext=\"c_{\\text{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mtext mathsize=\"0.900em\">t</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{\\text{t}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and gender embedding </span>\n  <math alttext=\"c_{\\text{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mtext mathsize=\"0.900em\">s</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{\\text{s}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to form a joint input </span>\n  <math alttext=\"[x_{t};c_{\\text{t}};c_{\\text{s}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">;</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mtext mathsize=\"0.900em\">t</mtext>\n        </msub>\n        <mo mathsize=\"0.900em\">;</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mtext mathsize=\"0.900em\">s</mtext>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[x_{t};c_{\\text{t}};c_{\\text{s}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For deep feature-wise conditioning, text conditions are injected into each DiT block via SALN, enabling fine-grained control, while timestep information is incorporated using FiLM to guide the denoising trajectory. Our model is trained to regress the true velocity vector </span>\n  <math alttext=\"u_{t}=x_{1}-(1-\\sigma_{min})x_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">u</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mrow>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mrow>\n                <mn mathsize=\"0.900em\">1</mn>\n                <mo mathsize=\"0.900em\">&#8722;</mo>\n                <msub>\n                  <mi mathsize=\"0.900em\">&#963;</mi>\n                  <mrow>\n                    <mi mathsize=\"0.900em\">m</mi>\n                    <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                    <mi mathsize=\"0.900em\">i</mi>\n                    <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                    <mi mathsize=\"0.900em\">n</mi>\n                  </mrow>\n                </msub>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">x</mi>\n              <mn mathsize=\"0.900em\">0</mn>\n            </msub>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">u_{t}=x_{1}-(1-\\sigma_{min})x_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using a mean squared error (MSE) loss:</span>\n</p>\n\n",
                "matched_terms": [
                    "mean",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To comprehensively evaluate the performance of the proposed system, we employ both subjective and objective metrics assessing intelligibility, naturalness, and attribute consistency for both TTT and CA-Instruct-TTS modules. All subjective tests were carried out by rigorously screened and trained expert listeners.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "cainstructtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The CA-Instruct-TTS module was evaluated for expressiveness and speech quality. The model synthesized 195 utterances covering over 44 emotion categories with balanced distribution. The evaluation comprises Mean Opinion Score for emotion (MOS-E) and naturalness (MOS-N), along with Word Error Rate (WER) computed using the Whisper-large-v3 model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15845v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "opinion",
                    "error",
                    "wer",
                    "rate",
                    "mose",
                    "cainstructtts",
                    "mosn",
                    "mean"
                ]
            }
        ]
    }
}