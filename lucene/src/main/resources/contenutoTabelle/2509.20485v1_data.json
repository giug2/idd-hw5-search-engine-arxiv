{
    "S4.T1": {
        "source_file": "Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens",
        "caption": "Table 1: Correlations of objective metrics with WER and CER in SOMOS and VoiceMOS datasets.",
        "body": "Model\nDiscrete Speech token\nSOMOS Dataset\nVoiceMOS22 Dataset\n\n\nWER\nCER\nWER\nCER\n\n\nModel\nLayer\nk\nUtt-level\nSys-level\nUtt-level\nSys-level\nUtt-level\nSys-level\nUtt-level\nSys-level\n\n\nSpeechLMScore\nHuBERT\nL3\n50\n0.08\n0.22\n0.11\n0.21\n0.32\n0.42\n0.31\n0.40\n\n\nuLM (ours)\nHuBERT\nL3\n50\n0.08\n0.21\n0.07\n0.16\n0.30\n0.39\n0.30\n0.38\n\n\nL3\n500\n0.16\n0.39\n0.19\n0.39\n0.33\n0.45\n0.32\n0.44\n\n\nL9\n50\n0.24\n0.43\n0.21\n0.37\n0.33\n0.44\n0.33\n0.43\n\n\nL9\n500\n0.42\n0.69\n0.43\n0.68\n0.44\n0.55\n0.45\n0.57\n\n\nL12\n50\n0.20\n0.37\n0.19\n0.33\n0.32\n0.41\n0.32\n0.40\n\n\nL12\n500\n0.32\n0.54\n0.34\n0.55\n0.41\n0.51\n0.37\n0.51\n\n\nTTScore-int (proposed)\nHuBERT\nL3\n50\n0.38\n0.67\n0.38\n0.64\n0.77\n0.94\n0.76\n0.94\n\n\nL3\n500\n0.37\n0.70\n0.43\n0.71\n0.75\n0.93\n0.74\n0.96\n\n\nL9\n50\n0.48\n0.77\n0.50\n0.73\n0.78\n0.94\n0.78\n0.95\n\n\nL9\n500\n0.53\n0.78\n0.53\n0.77\n0.74\n0.95\n0.74\n0.96\n\n\nL12\n50\n0.47\n0.75\n0.50\n0.73\n0.77\n0.94\n0.77\n0.95\n\n\nL12\n500\n0.48\n0.76\n0.53\n0.77\n0.77\n0.96\n0.77\n0.97",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" rowspan=\"2\">Discrete Speech token</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">SOMOS Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">VoiceMOS22 Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">CER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">CER</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Layer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Utt-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Sys-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Utt-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Sys-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Utt-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Sys-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Utt-level</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Sys-level</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SpeechLMScore</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">L3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"6\">uLM (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">L3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">500</td>\n<td class=\"ltx_td ltx_align_center\">0.16</td>\n<td class=\"ltx_td ltx_align_center\">0.39</td>\n<td class=\"ltx_td ltx_align_center\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.39</td>\n<td class=\"ltx_td ltx_align_center\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">50</td>\n<td class=\"ltx_td ltx_align_center\">0.24</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">0.44</td>\n<td class=\"ltx_td ltx_align_center\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">500</td>\n<td class=\"ltx_td ltx_align_center\">0.42</td>\n<td class=\"ltx_td ltx_align_center\">0.69</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">0.44</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">50</td>\n<td class=\"ltx_td ltx_align_center\">0.20</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">500</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">0.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.55</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">0.51</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text ltx_font_bold\">TTScore-int (proposed)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"6\">HuBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">L3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">500</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.71</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">0.74</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">50</td>\n<td class=\"ltx_td ltx_align_center\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.73</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">500</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center\">0.74</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n<td class=\"ltx_td ltx_align_center\">0.74</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">L12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">50</td>\n<td class=\"ltx_td ltx_align_center\">0.47</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.73</td>\n<td class=\"ltx_td ltx_align_center\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\">L12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">500</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.97</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "voicemos22",
            "hubert",
            "ttscoreint",
            "ours",
            "proposed",
            "datasets",
            "layer",
            "voicemos",
            "objective",
            "speech",
            "metrics",
            "correlations",
            "somos",
            "wer",
            "syslevel",
            "model",
            "dataset",
            "uttlevel",
            "l12",
            "token",
            "discrete",
            "cer",
            "speechlmscore",
            "ulm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S4.T1\" title=\"Table 1 &#8227; 4.D.3 CORRELATION WITH MOS AND TTSArena ELO &#8227; 4.D EVALUATION SETUP FOR PROSODY &#8227; 4 EXPERIMENTS &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that the proposed conditional likelihood metric achieves very high correlations with WER and CER in both datasets, especially VoiceMOS, confirming its effectiveness for measuring intelligibility. Assessing speech tokens as a conditional generation task (TTScore-int) yields much stronger correlations than language modeling approaches (uLM, SpeechLMScore), since conditioning on text guides the predictor to capture more linguistic content more directly.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Objective evaluation of synthesized speech is critical for advancing speech generation systems, yet existing metrics for intelligibility and prosody remain limited in scope and weakly correlated with human perception. Word Error Rate (WER) provides only a coarse text-based measure of intelligibility, while F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent view of prosody. To address these limitations, we propose <span class=\"ltx_text ltx_font_italic\">TTScore</span>, a targeted and reference-free evaluation framework based on conditional prediction of discrete speech tokens. TTScore employs two sequence-to-sequence predictors conditioned on input text: <span class=\"ltx_text ltx_font_italic\">TTScore-int</span>, which measures intelligibility through content tokens, and <span class=\"ltx_text ltx_font_italic\">TTScore-pro</span>, which evaluates prosody through prosody tokens. For each synthesized utterance, the predictors compute the likelihood of the corresponding token sequences, yielding interpretable scores that capture alignment with intended linguistic content and prosodic structure. Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and achieve stronger correlations with human judgments of overall quality than existing intelligibility and prosody-focused metrics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttscoreint",
                    "metrics",
                    "correlations",
                    "wer",
                    "token",
                    "discrete",
                    "somos",
                    "voicemos",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">speech generation, objective evaluation, intelligibility, speech tokens, prosody evaluation</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, evaluation has relied on subjective and objective methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>]</cite>. In subjective tests, listeners rate the naturalness or overall quality of speech. While these tests directly capture human perception, they are costly, time-consuming, and difficult to scale. Objective evaluation offers automatic and inexpensive alternatives, but typically focuses on narrower aspects such as intelligibility<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib4\" title=\"\">4</a>]</cite> or prosody <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib5\" title=\"\">5</a>]</cite>. This creates a gap: subjective evaluation reflects overall quality, whereas objective metrics capture isolated properties. For evaluation to be reliable, aspect-specific metrics must not only measure the targeted dimension but also align with broader human judgments <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib6\" title=\"\">6</a>]</cite>. In this work, we address this need by proposing objective metrics for intelligibility and prosody that remain targeted yet show stronger alignment with overall naturalness as perceived by human listeners.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two core aspects strongly influence human judgments of naturalness and quality: intelligibility and prosody. Intelligibility reflects how well the linguistic content is conveyed in speech, directly affecting usability across domains such as assistive technologies, human-computer dialogue systems, and machine translation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib7\" title=\"\">7</a>]</cite>. Even highly natural-sounding speech becomes ineffective if it is not intelligible. Prosody, on the other hand, encodes rhythm, stress, and intonation, giving speech its expressiveness and supporting the communication of pragmatic and emotional meaning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib8\" title=\"\">8</a>]</cite>. Appropriate prosody is critical for engaging and effective human&#8211;computer interaction. Despite their importance, the objective evaluation of intelligibility and prosody in synthesized speech remains an open challenge.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For intelligibility, widely used objective metrics such as Word Error Rate (WER) and Character Error Rate (CER) measure the discrepancy between automatic speech recognition (ASR) transcriptions and ground-truth text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib4\" title=\"\">4</a>]</cite>. While simple and effective at a coarse level, these metrics operate in the text domain and fail to capture acoustic and temporal artifacts that influence human perception. Furthermore, as both speech synthesis and ASR systems improve, reported WER and CER values continue to decrease, making them less discriminative and informative for comparing modern systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib11\" title=\"\">11</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "wer",
                    "cer",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For prosody, objective evaluation has been even more limited. Metrics such as F0 root-mean-square error (F0-RMSE) &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib12\" title=\"\">12</a>]</cite> and pitch correlation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib13\" title=\"\">13</a>]</cite> remain common <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib16\" title=\"\">16</a>]</cite>. However, these metrics face several drawbacks: they typically require a reference utterance, are sensitive to alignment errors, and ignore the sequential structure of prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib2\" title=\"\">2</a>]</cite>. Importantly, they often correlate poorly with subjective judgments of naturalness&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>]</cite>. As a result, subjective listening tests remain the dominant method for prosody evaluation, despite their cost and inefficiency.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations highlight the need for more reliable, interpretable, and aspect-focused objective metrics. Recent advances in self-supervised learning and speech coding have made it possible to represent speech as discrete tokens that capture different aspects of the signal. For example, discrete units derived from HuBERT have been shown to correlate strongly with phoneme classes and encode fine-grained linguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>, while FACodec tokens can be trained to disentangle prosody from content and acoustic details&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib18\" title=\"\">18</a>]</cite>. Such representations open the door to evaluation metrics that directly target intelligibility or prosody while remaining scalable and reference-free.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hubert",
                    "metrics",
                    "discrete",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A recent study, SpeechLMScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib19\" title=\"\">19</a>]</cite>, showed that likelihood modeling over discrete speech tokens can provide useful signals for evaluating synthesized speech. While encouraging, SpeechLMScore was designed as a general-purpose quality metric: it uses a decoder-only language model without any conditioning, and it does not explicitly target intelligibility or prosody. As a result, it offers limited interpretability for analyzing in which aspects a system performs well or poorly. Instead, we propose that conditional likelihood, specifically the likelihood of aspect-specific speech tokens given the input text, is a more appropriate formulation. This measure allows more direct and correct formulations targeted to measure specific attributes of synthesized speech, resulting in more detailed and interpretable measures.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechlmscore",
                    "model",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both predictors take input text and estimate the conditional likelihood of the corresponding discrete token sequence extracted from synthesized speech. The resulting likelihood scores reflect how well the generated speech aligns with the intended linguistic content or has an appropriate prosodic structure. Unlike existing approaches, TTScore-int and TTScore-pro operate directly in the speech domain, are reference-free in terms of ground-truth speech, and provide meaningful measures that correlate with both aspect-specific performance and overall perceived quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttscoreint",
                    "token",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a new paradigm for targeted evaluation of synthesized speech, where intelligibility and prosody are modeled as conditional generation tasks over discrete tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose two novel metrics, TTScore-int and TTScore-pro, that separately evaluate intelligibility and prosody, overcoming the limitations of current objective metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "ttscoreint",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through experiments on SOMOS, VoiceMOS, and TTSArena benchmarks, we demonstrate that TTScore-int and TTScore-pro not only provide meaningful, fine-grained assessments of intelligibility and prosody but also align more closely with human judgments of overall quality.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "ttscoreint",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper is organized as follows: Section 2 reviews related work, highlighting the limitations of existing metrics and exploring sequence-to-sequence evaluation paradigms. Section 3 introduces the proposed metric, TTScore, detailing the text-to-speech token generator, different discrete speech tokens, and the calculation of the intelligibility and prosody score. Section 4 describes the experimental setup, datasets used, and implementation details. Section 5 presents experiments and discussions for intelligibility evaluation. Section 6 presents experiments for prosody evaluation. Finally, Section 7 concludes the paper and discusses future directions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "proposed",
                    "token",
                    "datasets",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the advancements in robust automatic speech recognition (ASR) methods such as wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite> and Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib20\" title=\"\">20</a>]</cite>, evaluating intelligibility via ASR on synthesized speech has become standard practice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib14\" title=\"\">14</a>]</cite>. This approach measures intelligibility in the textual domain by comparing recognized text with ground-truth transcriptions, typically reporting Word Error Rate (WER) or Character Error Rate (CER)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib4\" title=\"\">4</a>]</cite>. While widely used, these measures provide only coarse-grained signals and fail to capture local artifacts or acoustic details that influence perception. Moreover, as synthesis and ASR systems improve, reported WER and CER continue to decrease, reducing their discriminative ability when comparing modern systems. This motivates the need for fine-grained, speech-domain measures of intelligibility.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "cer",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prosody plays a central role in speech naturalness, shaping rhythm, stress, and intonation. Despite its importance, the objective evaluation of prosody in synthesized speech has received less attention than intelligibility. Existing metrics are dominated by F0-based measures as a very fundamental aspect of prosody, such as F0 root-mean-square error (F0-RMSE) and F0 correlation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib12\" title=\"\">12</a>]</cite>. However, these approaches rely on the availability of reference utterances, require frame-level alignment with reference speech, and often correlate poorly with human judgments of naturalness&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>]</cite>. Importantly, reference utterances are not always available (e.g., in cross-lingual style transfer), making such metrics impractical. In addition, synthesized speech may have multiple appropriate prosodic renditions that differ from the reference. As a result, subjective MOS ratings remain the dominant method for prosody evaluation, despite their inefficiency and high cost.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Reference-guided evaluation has been studied in natural language processing (NLP), particularly in tasks such as summarization and translation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib21\" title=\"\">21</a>]</cite>, where the consistency between a generated output and its source text is assessed. For example, BARTScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib22\" title=\"\">22</a>]</cite> computes the conditional likelihood of a reference or candidate sentence given its source text, utilizing a text-generator and its task-specific knowledge. Similar likelihood-based formulations have also been investigated in speech, such as SpeechLMScore&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib19\" title=\"\">19</a>]</cite>, which evaluates synthesized speech by the token likelihoods with a decoder-only language model. While promising to use the knowledge from generative models for evaluation, these methods are general-purpose or do not explicitly target intelligibility or prosody.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechlmscore",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intelligibility metrics (WER, CER) are coarse and text-based. They fail to capture acoustic or temporal artifacts that affect perception and are becoming less discriminative as systems improve.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "metrics",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central question in evaluating synthesized speech is whether to rely on broad measures of overall quality or on targeted measures of specific aspects such as intelligibility and prosody. The ultimate goal of speech generation is to produce high-quality, natural-sounding output, and evaluations often rely on mean opinion scores (MOS) as a comprehensive indicator. While MOS captures overall perception, it does not reveal the specific strengths and weaknesses of a system. Intelligibility and prosody, however, are essential in their own right, as deficiencies in either directly degrade naturalness and usability. We argue that broad and targeted evaluations should be interconnected. Broad measures such as MOS provide an overall view of performance but are less interpretable for diagnosis, whereas targeted evaluations highlight specific shortcomings but may not fully represent overall quality. Ideally, targeted metrics should measure the intended aspect while also aligning with broader perceptual judgments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by this, we propose TTScore, fine-grained metrics for intelligibility, namely TTScore-int, and prosody, namely TTScore-pro, that operate directly in the speech domain. Both variants of TTScore are formulated as conditional generation tasks, where sequence-to-sequence (seq2seq) autoregressive models predict discrete speech token sequences from text. Each score is defined as the average log-likelihood of the synthesized tokens given the input text, computed with teacher forcing. TTScore-int evaluates intelligibility by measuring the alignment between content tokens and the input text, providing a finer-grained assessment than text-based metrics such as WER or CER. TTScore-pro evaluates prosody by measuring the alignment between prosody tokens and the intended content. Unlike F0-based metrics, it does not require a reference utterance; thus, it is robust to alignment errors and captures the sequential structure of prosody. Crucially, TTScore-pro can accommodate multiple valid prosodic renditions without relying on a single reference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "ttscoreint",
                    "token",
                    "discrete",
                    "wer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Central to our method is the selection of discrete speech tokens that serve as linguistic content representations for measuring intelligibility. The most common approach is to obtain tokens via <span class=\"ltx_text ltx_font_italic\">k</span>-means quantization of intermediate representations from HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>. Prior studies have shown that the dominant information captured by these discrete SSL tokens depends on both the choice of the intermediate layer and the number of clusters (<span class=\"ltx_text ltx_font_italic\">k</span>) used in the quantization process. Earlier HuBERT layers tend to encode acoustic and paralinguistic information, while later layers increasingly capture linguistic content&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib24\" title=\"\">24</a>]</cite>. In this work, we explore different HuBERT layers and cluster sizes to identify effective configurations. The resulting discrete SSL tokens are used as <span class=\"ltx_text ltx_font_italic\">content speech tokens</span>, which primarily encode the linguistic content of a given utterance and form the basis for our intelligibility metric.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "hubert",
                    "discrete",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For prosodic speech tokens, we adopt discrete prosody representations from FACodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib18\" title=\"\">18</a>]</cite>, which has demonstrated strong prosody modeling and controllability in speech synthesis experiments with NaturalSpeech&#160;3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib18\" title=\"\">18</a>]</cite>. FACodec prosody tokens primarily encode F0 information, one of the most crucial indicators of prosody, making them well-suited for prosody evaluation. FACodec employs residual vector quantization (RVQ) to factorize and disentangle speech into discrete representations corresponding to different attributes: prosody, content, and acoustic detail. To ensure effective disentanglement, explicit training objectives are imposed on each token type in addition to the speech reconstruction objective. For prosody tokens, these objectives include normalized F0 contour prediction and adversarial phone classification.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "discrete",
                    "token",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">FACodec prosody tokens, denoted as <math alttext=\"\\mathbf{f}=[f_{1},f_{2},...,f_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119839;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}=[f_{1},f_{2},...,f_{T}]</annotation></semantics></math>, are generated at the frame level, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of frames. Predicting such fine-grained prosody at the frame level is a complex task. Therefore, in this work, we refactor frame-level prosody tokens to phoneme-level representations to simplify the prediction task and better model prosody token distributions. For phoneme-level refactoring, first, we extract frame-level continuous prosody representations, <math alttext=\"\\mathbf{r}=[r_{1},r_{2},...,r_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119851;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>r</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{r}=[r_{1},r_{2},...,r_{T}]</annotation></semantics></math>, before applying RVQ. Next, we align these frame-level features with phonemes, <math alttext=\"\\mathbf{x}=[x_{1},x_{2},...,x_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}=[x_{1},x_{2},...,x_{L}]</annotation></semantics></math>, using timestamps obtained from a forced aligner, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the number of phonemes. For each phoneme, we pool the corresponding frame-level continuous features as\n<math alttext=\"r_{\\text{ph}}=[\\text{pool}(r_{x1[\\text{begin}]},..,r_{x1[\\text{end}]}),\\text{pool}(r_{x2[\\text{begin}]},..,r_{x2[\\text{end}]}),...]\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>r</mi><mtext>ph</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mtext>pool</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mtext>begin</mtext><mo stretchy=\"false\">]</mo></mrow></mrow></msub><mo>,</mo><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mo>,</mo><msub><mi>r</mi><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mtext>end</mtext><mo stretchy=\"false\">]</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mtext>pool</mtext><mrow><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mtext>begin</mtext><mo stretchy=\"false\">]</mo></mrow></mrow></msub><mo>,</mo><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mo>,</mo><msub><mi>r</mi><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mtext>end</mtext><mo stretchy=\"false\">]</mo></mrow></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r_{\\text{ph}}=[\\text{pool}(r_{x1[\\text{begin}]},..,r_{x1[\\text{end}]}),\\text{pool}(r_{x2[\\text{begin}]},..,r_{x2[\\text{end}]}),...]</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To estimate the distribution of discrete speech tokens in a synthesized utterance given its textual content, we train a sequence-to-sequence text-to-token generator that autoregressively predicts discrete speech tokens while incorporating content information from the input text. We adopt a transformer-based encoder&#8211;decoder architecture, specifically BART&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib25\" title=\"\">25</a>]</cite>, which has been widely used for various sequence-to-sequence tasks. We train two text-to-token generators for TTScore-int and TTScore-pro. Both generators takes a phoneme sequence, <math alttext=\"\\mathbf{x}=[x_{1},x_{2},...,x_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119857;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}=[x_{1},x_{2},...,x_{L}]</annotation></semantics></math>, derived from the input text using a grapheme-to-phoneme converter, as the input. The difference between the two generator is the output tokens that they generate making them specialized in modeling different aspects of synthesized speech. The generator for TTScore-int predicts the content token sequence <math alttext=\"\\mathbf{c}=[c_{1},c_{2},...,c_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119836;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>c</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{c}=[c_{1},c_{2},...,c_{T}]</annotation></semantics></math>, for the given utterance, obtained using a pre-trained SSL model and k-means clustering as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S3.SS1\" title=\"3.A CONTENT SPEECH TOKENS &#8227; 3 THE PROPOSED SCORE: TTScore &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3.A</span></a>. The generator for TTScore-pro predicts phoneme-level prosody token sequence <math alttext=\"\\mathbf{f_{ph}}=[f_{1},f_{2},...,f_{L}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119839;</mi><mi>&#119849;&#119841;</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>f</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f_{ph}}=[f_{1},f_{2},...,f_{L}]</annotation></semantics></math> described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S3.SS2\" title=\"3.B PROSODIC SPEECH TOKENS &#8227; 3 THE PROPOSED SCORE: TTScore &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3.B</span></a>. <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> and <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> denote lengths of phoneme sequence and discrete speech token sequence, respectively. Both models are trained on a large-scale dataset of paired text and speech token sequences, enabling them to learn the distributions of aspect-specific tokens in natural speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "ttscoreint",
                    "dataset",
                    "token",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This joint probability represents the expectation of the discrete speech token sequence that should occur for the corresponding content. For a given synthesized speech, if the content token sequence is likely for the given input text according to content token generator, we assume better intelligibility as the sequence contains the expected linguistic information. Similarly if the prosody token sequence is likely for the input text according to the prosody token generator, we assume a more appropriate prosody for the given linguistic information.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to measure intelligibility and prosody for a synthesized utterance, we first extract the corresponding discrete speech token sequences, content tokens <math alttext=\"\\mathbf{c}=[c_{1},c_{2},\\ldots,c_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p6.m1\" intent=\":literal\"><semantics><mrow><mi>&#119836;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><msub><mi>c</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>c</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{c}=[c_{1},c_{2},\\ldots,c_{T}]</annotation></semantics></math> and prosody tokens <math alttext=\"\\mathbf{f}_{\\text{ph}}=[f_{1},f_{2},\\ldots,f_{T}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p6.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#119839;</mi><mtext>ph</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><msub><mi>f</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}_{\\text{ph}}=[f_{1},f_{2},\\ldots,f_{T}]</annotation></semantics></math>, using pre-trained speech tokenizers. The text-to-speech token generators, together with the conditional log-probability formulation, then evaluate these sequences by computing their probabilities as the average log-likelihood of each token, conditioned on the preceding tokens and the input phoneme sequence <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p6.m3\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "token",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For training both the text-to-content token generator and the text-to-prosody token generator, we use LibriSpeech-960&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib26\" title=\"\">26</a>]</cite>, a large and diverse dataset containing 960 hours of speech from 2,311 speakers. This dataset is also used for training the HuBERT-base model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>. To train the <span class=\"ltx_text ltx_font_italic\">k</span>-means quantization models on top of the HuBERT features, we utilize the LibriSpeech-100 subset. A reliable evaluation of both the proposed and baseline metrics requires a large number of synthesized samples from diverse systems, as well as extensive subjective ratings from human listeners. To meet these requirements, we evaluate our metric using the SOMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib27\" title=\"\">27</a>]</cite> and VoiceMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib29\" title=\"\">29</a>]</cite> datasets, two widely used benchmarks that provide system diversity and large-scale MOS ratings. SOMOS contains approximately 20,000 synthesized samples generated by 200 different neural network-based speech synthesis systems, each annotated with multiple MOS ratings of overall quality. For our experiments, we use the test-clean subset, consisting of about 3,000 utterances. VoiceMOS is the largest benchmark for automatic evaluation of speech generation. It includes samples from systems that participated in past Voice Conversion Challenges (VCC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib30\" title=\"\">30</a>]</cite>, Blizzard Challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib31\" title=\"\">31</a>]</cite>, and methods from the ESPNet toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib32\" title=\"\">32</a>]</cite>. We use the combined training and test subsets of the VoiceMOS22 main challenge, totaling 6,000 utterances.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "voicemos22",
                    "hubert",
                    "metrics",
                    "dataset",
                    "proposed",
                    "token",
                    "datasets",
                    "somos",
                    "voicemos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both text-to-speech token generators, we use a smaller version of the BART architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib25\" title=\"\">25</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1a\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/neulab/BARTScore\" title=\"\">https://github.com/neulab/BARTScore</a></span></span></span>. Our implementation consists of 6 transformer encoder layers and 6 transformer decoder layers. Each layer has dimension 512, and input/output embedding dimensions are 256. The model uses 8-head multi-head attention for both encoder and decoder. Training is performed with a dropout rate of 0.1, the AdamW optimizer, and a batch size of 8. The maximum sequence length is set to 1024 for efficiency.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For speech content tokens, we use the pre-trained HuBERT-base model with 12 transformer encoder layers. We experiment with discrete tokens extracted from different HuBERT layers (3, 9, 12) and with different cluster sizes (<span class=\"ltx_text ltx_font_italic\">k</span>=50, 500) for <span class=\"ltx_text ltx_font_italic\">k</span>-means. For prosody tokens, we use the pre-trained FACodec tokenizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lifeiteng/naturalspeech3_facodec\" title=\"\">https://github.com/lifeiteng/naturalspeech3_facodec</a></span></span></span>, which operates at a 12.5 ms frame rate with a codebook size of 1024. Phoneme boundaries are obtained using the Montreal Forced Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib33\" title=\"\">33</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner\" title=\"\">https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner</a></span></span></span> and official transcripts. Phoneme-level prosody tokens are then computed as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S3.SS2\" title=\"3.B PROSODIC SPEECH TOKENS &#8227; 3 THE PROPOSED SCORE: TTScore &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3.B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hubert",
                    "model",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare our proposed TTScore-int against the following baselines:</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "ttscoreint"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER and CER</span>: The standard intelligibility metrics. Synthesized speech is transcribed with a state-of-the-art ASR system (wav2vec 2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self</a></span></span></span>), and WER/CER are computed against ground-truth transcripts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "cer",
                    "metrics",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SpeechLMScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib19\" title=\"\">19</a>]</cite>: A likelihood-based method that models autoregressive generation of discrete speech tokens without text input<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/soumimaiti/speechlmscore_tool\" title=\"\">https://github.com/soumimaiti/speechlmscore_tool</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechlmscore",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">uLM:</span> For a fair comparison, we also implement a discrete speech token language model (uLM) under the same settings as our model. uLM is a decoder-only BART variant (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S4.SS2\" title=\"4.B MODEL ARCHITECTURE &amp; IMPLEMENTATION DETAILS &#8227; 4 EXPERIMENTS &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">4.B</span></a>) that autoregressively predicts tokens without text input.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "ulm",
                    "token",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well the proposed metric reflects speech intelligibility, we measure correlations between the proposed metric and standard WER/CER for synthesized speech samples. Since WER and CER are reliable and widely accepted objective metrics for intelligibility, we assume that a high correlation with them is a fundamental indicator of measuring objective intelligibility and a requirement for an objective intelligibility metric. We use Pearson linear correlation coefficient (LCC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib34\" title=\"\">34</a>]</cite> to assess the correlations on both utterance level and system level where the correlation calculated between utterance scores and averaged sytem scores, respectively. Most importantly, we evaluate how well the proposed metric reflects the perceived quality of synthesized speech by measuring correlations with human MOS ratings. In this evaluation, we compare the proposed metric with WER and CER to investigate whether measuring intelligibility in the speech domain better reflects human perception of speech. We report LCC and Spearman rank correlation coefficient (SRCC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib35\" title=\"\">35</a>]</cite> on both utterance level and system level.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "correlations",
                    "proposed",
                    "wer",
                    "cer",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly, we perform a score distribution analysis of proposed prosody metric, TTScore-pro, by comparing real and synthesized speech to validate the sanity of prosody evaluation. We assume that real speech exhibits more natural prosody compared to synthesized speech, which has limited prosody modeling capabilities. We hypothesize that a reliable prosody consistency metric should produce higher scores for real speech as a fundamental requirement. For real speech, we calculated scores for utterances from the LibriSpeech-dev&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib26\" title=\"\">26</a>]</cite> and VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib36\" title=\"\">36</a>]</cite>, which contains diverse content and speaking styles. For synthesized speech, we used scores from the VoiceMOS and SOMOS datasets, as both cover a wide range of synthesis methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "proposed",
                    "datasets",
                    "somos",
                    "voicemos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to see if the proposed score, TTScore-pro, is sensitive to prosody perturbations and accounts for prosody appropriateness, we perform pitch-controllable resynthesis experiments. We performed resynthesis experiments on two different datasets, HifiTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib37\" title=\"\">37</a>]</cite> and librispeech-dev. We utilize a SOTA speech analysis-synthesis method to resynthesize speech with the original pitch sequence of the utterance and a perturbed pitch contour. We hypothesize that a reliable prosody metric should be sensitive and account for unnatural conditions of the resynthesized speech with a perturbed pitch. We utilize NANSY++&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib38\" title=\"\">38</a>]</cite> as the resynthesis method and apply the following pitch perturbations:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "proposed",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since prosody is a core component of perceived naturalness, we further evaluate TTScore-pro by measuring correlations with human MOS ratings from SOMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib27\" title=\"\">27</a>]</cite> and VoiceMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib28\" title=\"\">28</a>]</cite>. Both LCC and SRCC are reported at utterance and system levels.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "correlations",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For HuBERT-based tokens, correlations with WER and CER increase with higher layers, consistent with the expectation that later layers contain more refined content information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>. This layer effect is most visible in language modeling, where performance depends heavily on token representation. In contrast, TTScore-int benefits from textual conditioning, allowing it to capture semantic content reliably even with earlier-layer tokens, where para-linguistic information is also prominent.</p>\n\n",
                "matched_terms": [
                    "ttscoreint",
                    "correlations",
                    "token",
                    "wer",
                    "layer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The number of tokens has little effect on TTScore-int, though larger vocabularies generally correlate slightly better with WER and CER. In contrast, token language models are far more sensitive to vocabulary size, sometimes showing very low correlations. This again highlights the importance of text conditioning in producing a robust, semantically grounded intelligibility metric.</p>\n\n",
                "matched_terms": [
                    "ttscoreint",
                    "correlations",
                    "token",
                    "wer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S5.T2\" title=\"Table 2 &#8227; 5.C DISCUSSION ON INTELLIGIBILITY &amp; QUALITY &#8227; 5 RESULTS FOR INTELLIGIBILITY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports correlations with MOS. WER and CER show moderate alignment with human ratings, confirming intelligibility&#8217;s importance for the perceived quality. TTScore-int achieves consistently stronger correlations, particularly in SOMOS, indicating that evaluating intelligibility in the speech domain with a fine-grained generative formulation better reflects human perception.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttscoreint",
                    "correlations",
                    "wer",
                    "cer",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are also differences between benchmarks. In VoiceMOS, where older systems suffer from issues such as robotic prosody and noise, factors beyond intelligibility may play a larger role, making general-purpose models like uLM slightly more competitive. In contrast, SOMOS contains more recent, higher-quality systems where intelligibility differences are more salient, and TTScore-int clearly outperforms baselines in both scenarios. Moreover, consistently high correlations in different extensive benchmarks indicate the generalization ability and robustness of the proposed metric.</p>\n\n",
                "matched_terms": [
                    "ttscoreint",
                    "correlations",
                    "ulm",
                    "proposed",
                    "somos",
                    "voicemos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, TTScore-int correlates more strongly with MOS than WER and CER across all conditions, demonstrating that text-conditioned, speech-domain evaluation captures both intelligibility and its contribution to overall quality more effectively than traditional text-domain metrics.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "metrics",
                    "ttscoreint",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that intelligibility is closely correlated with MOS, particularly for more recent speech synthesis systems such as those in SOMOS. While intelligibility is a core component of speech perception and metrics like WER and CER are widely used to ensure it, their popularity is also due to the assumption that they partially reflect overall quality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib40\" title=\"\">40</a>]</cite>. These measures remain appealing because they provide a low-cost proxy for subjective evaluation, especially when large-scale human ratings are impractical.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "somos",
                    "wer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their widespread adoption, WER and CER have shortcomings as indicators of overall quality. A more accurate reflection of quality would provide deeper insight into synthesized speech and better guide system development. Our proposed method addresses these limitations, showing consistently stronger correlations with human ratings and offering a more informative alternative to conventional intelligibility metrics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "correlations",
                    "proposed",
                    "wer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The score distributions of the proposed prosody metric for real and synthesized speech samples are shown in Figure 2(a). As observed in the figure, real speech samples with natural prosody achieve higher scores than synthesized speech samples with limited prosody modeling, supporting the validity and reliability of our metric. Furthermore, the similar scores across different datasets for both real and synthesized speech indicate the generalizability of our metric across different domains.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "proposed",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since lower F0-RMSE indicates better prosody, a negative correlation with MOS is expected, while positive correlations are expected otherwise. As both baselines require reference speech, results are reported only on SOMOS, while VoiceMOS lacks aligned references. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a), the baselines show weak or no correlation with MOS, confirming their limited reliability for reflecting perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicemos",
                    "correlations",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, TTScore-pro achieves consistently stronger correlations. While moderate in absolute terms, this is expected since naturalness depends on multiple factors beyond prosody. Results from VoiceMOS (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b)) further confirm its robustness, likely due to the dataset&#8217;s greater diversity in synthesis methods and prosody modeling quality. Overall, TTScore-pro provides a more reliable and informative objective prosody metric than traditional measures.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "correlations",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) shows correlations with TTSArena ELO scores, where higher ELO indicates better perceived quality. While F0-RMSE should correlate negatively with ELO, it unexpectedly shows a positive trend, confirming its unreliability as a prosody metric. F0-corr performs slightly better but remains inconsistent across datasets. In contrast, TTScore-pro consistently aligns with human preferences in this benchmark, showing that it captures prosodic naturalness more reliably. These results highlight TTScore-pro as a more robust and informative objective metric, compared to traditional pitch-based measures which fail to reflect overall perception.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "objective",
                    "correlations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced a targeted evaluation framework for synthesized speech that measures intelligibility and prosody as distinct aspects of speech quality. We proposed two conditional likelihood&#8211;based metrics: TTScore-int, which evaluates how well the linguistic content of speech is preserved, and TTScore-pro, which assesses the appropriateness of prosody relative to the intended content. By leveraging specialized discrete speech tokens and text-conditioned sequence-to-sequence predictors, both metrics provide reference-free, speech-domain evaluations that overcome many of the shortcomings of conventional approaches such as WER, CER, and F0-based measures.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "ttscoreint",
                    "proposed",
                    "discrete",
                    "wer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive experiments across SOMOS, VoiceMOS, and TTSArena benchmarks demonstrated that TTScore-int and TTScore-pro not only correlate strongly with established baselines but also align more closely with human judgments of naturalness. These results confirm the value of targeted evaluation: aspect-specific metrics provide finer insights while remaining predictive of overall perceived quality. This work opens new opportunities for extending targeted evaluation beyond intelligibility and prosody. The proposed paradign of conditional generation for evaluation is highly flexible for new formulations for different aspects of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ttscoreint",
                    "metrics",
                    "proposed",
                    "somos",
                    "voicemos"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens",
        "caption": "Table 2: Correlations of objective metrics with MOS in SOMOS and VoiceMOS datasets.",
        "body": "HuBERT\n\n\n(k=500)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">HuBERT</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">(k=500)</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "mos",
            "metrics",
            "hubert",
            "correlations",
            "datasets",
            "somos",
            "voicemos",
            "k500",
            "objective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S5.T2\" title=\"Table 2 &#8227; 5.C DISCUSSION ON INTELLIGIBILITY &amp; QUALITY &#8227; 5 RESULTS FOR INTELLIGIBILITY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports correlations with MOS. WER and CER show moderate alignment with human ratings, confirming intelligibility&#8217;s importance for the perceived quality. TTScore-int achieves consistently stronger correlations, particularly in SOMOS, indicating that evaluating intelligibility in the speech domain with a fine-grained generative formulation better reflects human perception.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Objective evaluation of synthesized speech is critical for advancing speech generation systems, yet existing metrics for intelligibility and prosody remain limited in scope and weakly correlated with human perception. Word Error Rate (WER) provides only a coarse text-based measure of intelligibility, while F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent view of prosody. To address these limitations, we propose <span class=\"ltx_text ltx_font_italic\">TTScore</span>, a targeted and reference-free evaluation framework based on conditional prediction of discrete speech tokens. TTScore employs two sequence-to-sequence predictors conditioned on input text: <span class=\"ltx_text ltx_font_italic\">TTScore-int</span>, which measures intelligibility through content tokens, and <span class=\"ltx_text ltx_font_italic\">TTScore-pro</span>, which evaluates prosody through prosody tokens. For each synthesized utterance, the predictors compute the likelihood of the corresponding token sequences, yielding interpretable scores that capture alignment with intended linguistic content and prosodic structure. Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and achieve stronger correlations with human judgments of overall quality than existing intelligibility and prosody-focused metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "correlations",
                    "somos",
                    "voicemos",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditionally, evaluation has relied on subjective and objective methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>]</cite>. In subjective tests, listeners rate the naturalness or overall quality of speech. While these tests directly capture human perception, they are costly, time-consuming, and difficult to scale. Objective evaluation offers automatic and inexpensive alternatives, but typically focuses on narrower aspects such as intelligibility<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib4\" title=\"\">4</a>]</cite> or prosody <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib5\" title=\"\">5</a>]</cite>. This creates a gap: subjective evaluation reflects overall quality, whereas objective metrics capture isolated properties. For evaluation to be reliable, aspect-specific metrics must not only measure the targeted dimension but also align with broader human judgments <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib6\" title=\"\">6</a>]</cite>. In this work, we address this need by proposing objective metrics for intelligibility and prosody that remain targeted yet show stronger alignment with overall naturalness as perceived by human listeners.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For intelligibility, widely used objective metrics such as Word Error Rate (WER) and Character Error Rate (CER) measure the discrepancy between automatic speech recognition (ASR) transcriptions and ground-truth text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib4\" title=\"\">4</a>]</cite>. While simple and effective at a coarse level, these metrics operate in the text domain and fail to capture acoustic and temporal artifacts that influence human perception. Furthermore, as both speech synthesis and ASR systems improve, reported WER and CER values continue to decrease, making them less discriminative and informative for comparing modern systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib11\" title=\"\">11</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For prosody, objective evaluation has been even more limited. Metrics such as F0 root-mean-square error (F0-RMSE) &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib12\" title=\"\">12</a>]</cite> and pitch correlation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib13\" title=\"\">13</a>]</cite> remain common <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib16\" title=\"\">16</a>]</cite>. However, these metrics face several drawbacks: they typically require a reference utterance, are sensitive to alignment errors, and ignore the sequential structure of prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib2\" title=\"\">2</a>]</cite>. Importantly, they often correlate poorly with subjective judgments of naturalness&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>]</cite>. As a result, subjective listening tests remain the dominant method for prosody evaluation, despite their cost and inefficiency.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations highlight the need for more reliable, interpretable, and aspect-focused objective metrics. Recent advances in self-supervised learning and speech coding have made it possible to represent speech as discrete tokens that capture different aspects of the signal. For example, discrete units derived from HuBERT have been shown to correlate strongly with phoneme classes and encode fine-grained linguistic information&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>, while FACodec tokens can be trained to disentangle prosody from content and acoustic details&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib18\" title=\"\">18</a>]</cite>. Such representations open the door to evaluation metrics that directly target intelligibility or prosody while remaining scalable and reference-free.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose two novel metrics, TTScore-int and TTScore-pro, that separately evaluate intelligibility and prosody, overcoming the limitations of current objective metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through experiments on SOMOS, VoiceMOS, and TTSArena benchmarks, we demonstrate that TTScore-int and TTScore-pro not only provide meaningful, fine-grained assessments of intelligibility and prosody but also align more closely with human judgments of overall quality.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper is organized as follows: Section 2 reviews related work, highlighting the limitations of existing metrics and exploring sequence-to-sequence evaluation paradigms. Section 3 introduces the proposed metric, TTScore, detailing the text-to-speech token generator, different discrete speech tokens, and the calculation of the intelligibility and prosody score. Section 4 describes the experimental setup, datasets used, and implementation details. Section 5 presents experiments and discussions for intelligibility evaluation. Section 6 presents experiments for prosody evaluation. Finally, Section 7 concludes the paper and discusses future directions.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prosody plays a central role in speech naturalness, shaping rhythm, stress, and intonation. Despite its importance, the objective evaluation of prosody in synthesized speech has received less attention than intelligibility. Existing metrics are dominated by F0-based measures as a very fundamental aspect of prosody, such as F0 root-mean-square error (F0-RMSE) and F0 correlation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib12\" title=\"\">12</a>]</cite>. However, these approaches rely on the availability of reference utterances, require frame-level alignment with reference speech, and often correlate poorly with human judgments of naturalness&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib3\" title=\"\">3</a>]</cite>. Importantly, reference utterances are not always available (e.g., in cross-lingual style transfer), making such metrics impractical. In addition, synthesized speech may have multiple appropriate prosodic renditions that differ from the reference. As a result, subjective MOS ratings remain the dominant method for prosody evaluation, despite their inefficiency and high cost.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central question in evaluating synthesized speech is whether to rely on broad measures of overall quality or on targeted measures of specific aspects such as intelligibility and prosody. The ultimate goal of speech generation is to produce high-quality, natural-sounding output, and evaluations often rely on mean opinion scores (MOS) as a comprehensive indicator. While MOS captures overall perception, it does not reveal the specific strengths and weaknesses of a system. Intelligibility and prosody, however, are essential in their own right, as deficiencies in either directly degrade naturalness and usability. We argue that broad and targeted evaluations should be interconnected. Broad measures such as MOS provide an overall view of performance but are less interpretable for diagnosis, whereas targeted evaluations highlight specific shortcomings but may not fully represent overall quality. Ideally, targeted metrics should measure the intended aspect while also aligning with broader perceptual judgments.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For training both the text-to-content token generator and the text-to-prosody token generator, we use LibriSpeech-960&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib26\" title=\"\">26</a>]</cite>, a large and diverse dataset containing 960 hours of speech from 2,311 speakers. This dataset is also used for training the HuBERT-base model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>. To train the <span class=\"ltx_text ltx_font_italic\">k</span>-means quantization models on top of the HuBERT features, we utilize the LibriSpeech-100 subset. A reliable evaluation of both the proposed and baseline metrics requires a large number of synthesized samples from diverse systems, as well as extensive subjective ratings from human listeners. To meet these requirements, we evaluate our metric using the SOMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib27\" title=\"\">27</a>]</cite> and VoiceMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib29\" title=\"\">29</a>]</cite> datasets, two widely used benchmarks that provide system diversity and large-scale MOS ratings. SOMOS contains approximately 20,000 synthesized samples generated by 200 different neural network-based speech synthesis systems, each annotated with multiple MOS ratings of overall quality. For our experiments, we use the test-clean subset, consisting of about 3,000 utterances. VoiceMOS is the largest benchmark for automatic evaluation of speech generation. It includes samples from systems that participated in past Voice Conversion Challenges (VCC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib30\" title=\"\">30</a>]</cite>, Blizzard Challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib31\" title=\"\">31</a>]</cite>, and methods from the ESPNet toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib32\" title=\"\">32</a>]</cite>. We use the combined training and test subsets of the VoiceMOS22 main challenge, totaling 6,000 utterances.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "mos",
                    "metrics",
                    "datasets",
                    "somos",
                    "voicemos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well the proposed metric reflects speech intelligibility, we measure correlations between the proposed metric and standard WER/CER for synthesized speech samples. Since WER and CER are reliable and widely accepted objective metrics for intelligibility, we assume that a high correlation with them is a fundamental indicator of measuring objective intelligibility and a requirement for an objective intelligibility metric. We use Pearson linear correlation coefficient (LCC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib34\" title=\"\">34</a>]</cite> to assess the correlations on both utterance level and system level where the correlation calculated between utterance scores and averaged sytem scores, respectively. Most importantly, we evaluate how well the proposed metric reflects the perceived quality of synthesized speech by measuring correlations with human MOS ratings. In this evaluation, we compare the proposed metric with WER and CER to investigate whether measuring intelligibility in the speech domain better reflects human perception of speech. We report LCC and Spearman rank correlation coefficient (SRCC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib35\" title=\"\">35</a>]</cite> on both utterance level and system level.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "metrics",
                    "correlations",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly, we perform a score distribution analysis of proposed prosody metric, TTScore-pro, by comparing real and synthesized speech to validate the sanity of prosody evaluation. We assume that real speech exhibits more natural prosody compared to synthesized speech, which has limited prosody modeling capabilities. We hypothesize that a reliable prosody consistency metric should produce higher scores for real speech as a fundamental requirement. For real speech, we calculated scores for utterances from the LibriSpeech-dev&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib26\" title=\"\">26</a>]</cite> and VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib36\" title=\"\">36</a>]</cite>, which contains diverse content and speaking styles. For synthesized speech, we used scores from the VoiceMOS and SOMOS datasets, as both cover a wide range of synthesis methods.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "datasets",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since prosody is a core component of perceived naturalness, we further evaluate TTScore-pro by measuring correlations with human MOS ratings from SOMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib27\" title=\"\">27</a>]</cite> and VoiceMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib28\" title=\"\">28</a>]</cite>. Both LCC and SRCC are reported at utterance and system levels.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "mos",
                    "correlations",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S4.T1\" title=\"Table 1 &#8227; 4.D.3 CORRELATION WITH MOS AND TTSArena ELO &#8227; 4.D EVALUATION SETUP FOR PROSODY &#8227; 4 EXPERIMENTS &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that the proposed conditional likelihood metric achieves very high correlations with WER and CER in both datasets, especially VoiceMOS, confirming its effectiveness for measuring intelligibility. Assessing speech tokens as a conditional generation task (TTScore-int) yields much stronger correlations than language modeling approaches (uLM, SpeechLMScore), since conditioning on text guides the predictor to capture more linguistic content more directly.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "correlations",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are also differences between benchmarks. In VoiceMOS, where older systems suffer from issues such as robotic prosody and noise, factors beyond intelligibility may play a larger role, making general-purpose models like uLM slightly more competitive. In contrast, SOMOS contains more recent, higher-quality systems where intelligibility differences are more salient, and TTScore-int clearly outperforms baselines in both scenarios. Moreover, consistently high correlations in different extensive benchmarks indicate the generalization ability and robustness of the proposed metric.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "correlations",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, TTScore-int correlates more strongly with MOS than WER and CER across all conditions, demonstrating that text-conditioned, speech-domain evaluation captures both intelligibility and its contribution to overall quality more effectively than traditional text-domain metrics.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that intelligibility is closely correlated with MOS, particularly for more recent speech synthesis systems such as those in SOMOS. While intelligibility is a core component of speech perception and metrics like WER and CER are widely used to ensure it, their popularity is also due to the assumption that they partially reflect overall quality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib40\" title=\"\">40</a>]</cite>. These measures remain appealing because they provide a low-cost proxy for subjective evaluation, especially when large-scale human ratings are impractical.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "metrics",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their widespread adoption, WER and CER have shortcomings as indicators of overall quality. A more accurate reflection of quality would provide deeper insight into synthesized speech and better guide system development. Our proposed method addresses these limitations, showing consistently stronger correlations with human ratings and offering a more informative alternative to conventional intelligibility metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "correlations"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">(a) Correlation with MOS in SOMOS</span>\n</p>\n\n",
                "matched_terms": [
                    "mos",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">(b) Correlation with MOS in VoiceMOS</span>\n</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "mos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since lower F0-RMSE indicates better prosody, a negative correlation with MOS is expected, while positive correlations are expected otherwise. As both baselines require reference speech, results are reported only on SOMOS, while VoiceMOS lacks aligned references. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a), the baselines show weak or no correlation with MOS, confirming their limited reliability for reflecting perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "mos",
                    "correlations",
                    "somos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, TTScore-pro achieves consistently stronger correlations. While moderate in absolute terms, this is expected since naturalness depends on multiple factors beyond prosody. Results from VoiceMOS (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b)) further confirm its robustness, likely due to the dataset&#8217;s greater diversity in synthesis methods and prosody modeling quality. Overall, TTScore-pro provides a more reliable and informative objective prosody metric than traditional measures.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "correlations",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) shows correlations with TTSArena ELO scores, where higher ELO indicates better perceived quality. While F0-RMSE should correlate negatively with ELO, it unexpectedly shows a positive trend, confirming its unreliability as a prosody metric. F0-corr performs slightly better but remains inconsistent across datasets. In contrast, TTScore-pro consistently aligns with human preferences in this benchmark, showing that it captures prosodic naturalness more reliably. These results highlight TTScore-pro as a more robust and informative objective metric, compared to traditional pitch-based measures which fail to reflect overall perception.</p>\n\n",
                "matched_terms": [
                    "correlations",
                    "datasets",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive experiments across SOMOS, VoiceMOS, and TTSArena benchmarks demonstrated that TTScore-int and TTScore-pro not only correlate strongly with established baselines but also align more closely with human judgments of naturalness. These results confirm the value of targeted evaluation: aspect-specific metrics provide finer insights while remaining predictive of overall perceived quality. This work opens new opportunities for extending targeted evaluation beyond intelligibility and prosody. The proposed paradign of conditional generation for evaluation is highly flexible for new formulations for different aspects of speech.</p>\n\n",
                "matched_terms": [
                    "voicemos",
                    "metrics",
                    "somos"
                ]
            }
        ]
    },
    "S6.T3": {
        "source_file": "Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens",
        "caption": "Table 3: Prosody correlations across benchmarks",
        "body": "Utter.-level\nSystem-level\n\n\nTest Set\nLCC\nSRCC\nLCC\nSRCC\n\n\nAll Systems\n0.33\n0.33\n0.45\n0.46\n\n\nTop 25% Systems\n0.20\n0.21\n0.25\n0.32\n\n\nTop 12% Systems\n0.19\n0.19\n0.30\n0.24",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Utter.-level</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">System-level</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Test Set</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SRCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SRCC</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">All Systems</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Top 25% Systems</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Top 12% Systems</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "srcc",
            "benchmarks",
            "systemlevel",
            "across",
            "test",
            "all",
            "top",
            "utterlevel",
            "correlations",
            "lcc",
            "prosody",
            "systems",
            "set"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Since lower F0-RMSE indicates better prosody, a negative correlation with MOS is expected, while positive correlations are expected otherwise. As both baselines require reference speech, results are reported only on SOMOS, while VoiceMOS lacks aligned references. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a), the baselines show weak or no correlation with MOS, confirming their limited reliability for reflecting perceived naturalness.</p>\n\n",
            "<p class=\"ltx_p\">In contrast, TTScore-pro achieves consistently stronger correlations. While moderate in absolute terms, this is expected since naturalness depends on multiple factors beyond prosody. Results from VoiceMOS (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b)) further confirm its robustness, likely due to the dataset&#8217;s greater diversity in synthesis methods and prosody modeling quality. Overall, TTScore-pro provides a more reliable and informative objective prosody metric than traditional measures.</p>\n\n",
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#S6.T3\" title=\"Table 3 &#8227; 6.A REAL VS SYNTHESIZED SPEECH SCORES &#8227; 6 RESULTS FOR PROSODY EVALUATION &#8227; Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) shows correlations with TTSArena ELO scores, where higher ELO indicates better perceived quality. While F0-RMSE should correlate negatively with ELO, it unexpectedly shows a positive trend, confirming its unreliability as a prosody metric. F0-corr performs slightly better but remains inconsistent across datasets. In contrast, TTScore-pro consistently aligns with human preferences in this benchmark, showing that it captures prosodic naturalness more reliably. These results highlight TTScore-pro as a more robust and informative objective metric, compared to traditional pitch-based measures which fail to reflect overall perception.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Objective evaluation of synthesized speech is critical for advancing speech generation systems, yet existing metrics for intelligibility and prosody remain limited in scope and weakly correlated with human perception. Word Error Rate (WER) provides only a coarse text-based measure of intelligibility, while F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent view of prosody. To address these limitations, we propose <span class=\"ltx_text ltx_font_italic\">TTScore</span>, a targeted and reference-free evaluation framework based on conditional prediction of discrete speech tokens. TTScore employs two sequence-to-sequence predictors conditioned on input text: <span class=\"ltx_text ltx_font_italic\">TTScore-int</span>, which measures intelligibility through content tokens, and <span class=\"ltx_text ltx_font_italic\">TTScore-pro</span>, which evaluates prosody through prosody tokens. For each synthesized utterance, the predictors compute the likelihood of the corresponding token sequences, yielding interpretable scores that capture alignment with intended linguistic content and prosodic structure. Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and achieve stronger correlations with human judgments of overall quality than existing intelligibility and prosody-focused metrics.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "prosody",
                    "benchmarks",
                    "correlations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two core aspects strongly influence human judgments of naturalness and quality: intelligibility and prosody. Intelligibility reflects how well the linguistic content is conveyed in speech, directly affecting usability across domains such as assistive technologies, human-computer dialogue systems, and machine translation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib7\" title=\"\">7</a>]</cite>. Even highly natural-sounding speech becomes ineffective if it is not intelligible. Prosody, on the other hand, encodes rhythm, stress, and intonation, giving speech its expressiveness and supporting the communication of pragmatic and emotional meaning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib8\" title=\"\">8</a>]</cite>. Appropriate prosody is critical for engaging and effective human&#8211;computer interaction. Despite their importance, the objective evaluation of intelligibility and prosody in synthesized speech remains an open challenge.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "prosody",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through experiments on SOMOS, VoiceMOS, and TTSArena benchmarks, we demonstrate that TTScore-int and TTScore-pro not only provide meaningful, fine-grained assessments of intelligibility and prosody but also align more closely with human judgments of overall quality.</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For training both the text-to-content token generator and the text-to-prosody token generator, we use LibriSpeech-960&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib26\" title=\"\">26</a>]</cite>, a large and diverse dataset containing 960 hours of speech from 2,311 speakers. This dataset is also used for training the HuBERT-base model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib17\" title=\"\">17</a>]</cite>. To train the <span class=\"ltx_text ltx_font_italic\">k</span>-means quantization models on top of the HuBERT features, we utilize the LibriSpeech-100 subset. A reliable evaluation of both the proposed and baseline metrics requires a large number of synthesized samples from diverse systems, as well as extensive subjective ratings from human listeners. To meet these requirements, we evaluate our metric using the SOMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib27\" title=\"\">27</a>]</cite> and VoiceMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib29\" title=\"\">29</a>]</cite> datasets, two widely used benchmarks that provide system diversity and large-scale MOS ratings. SOMOS contains approximately 20,000 synthesized samples generated by 200 different neural network-based speech synthesis systems, each annotated with multiple MOS ratings of overall quality. For our experiments, we use the test-clean subset, consisting of about 3,000 utterances. VoiceMOS is the largest benchmark for automatic evaluation of speech generation. It includes samples from systems that participated in past Voice Conversion Challenges (VCC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib30\" title=\"\">30</a>]</cite>, Blizzard Challenges&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib31\" title=\"\">31</a>]</cite>, and methods from the ESPNet toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib32\" title=\"\">32</a>]</cite>. We use the combined training and test subsets of the VoiceMOS22 main challenge, totaling 6,000 utterances.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "top",
                    "benchmarks",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how well the proposed metric reflects speech intelligibility, we measure correlations between the proposed metric and standard WER/CER for synthesized speech samples. Since WER and CER are reliable and widely accepted objective metrics for intelligibility, we assume that a high correlation with them is a fundamental indicator of measuring objective intelligibility and a requirement for an objective intelligibility metric. We use Pearson linear correlation coefficient (LCC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib34\" title=\"\">34</a>]</cite> to assess the correlations on both utterance level and system level where the correlation calculated between utterance scores and averaged sytem scores, respectively. Most importantly, we evaluate how well the proposed metric reflects the perceived quality of synthesized speech by measuring correlations with human MOS ratings. In this evaluation, we compare the proposed metric with WER and CER to investigate whether measuring intelligibility in the speech domain better reflects human perception of speech. We report LCC and Spearman rank correlation coefficient (SRCC)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib35\" title=\"\">35</a>]</cite> on both utterance level and system level.</p>\n\n",
                "matched_terms": [
                    "lcc",
                    "correlations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since prosody is a core component of perceived naturalness, we further evaluate TTScore-pro by measuring correlations with human MOS ratings from SOMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib27\" title=\"\">27</a>]</cite> and VoiceMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib28\" title=\"\">28</a>]</cite>. Both LCC and SRCC are reported at utterance and system levels.</p>\n\n",
                "matched_terms": [
                    "srcc",
                    "lcc",
                    "prosody",
                    "correlations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we evaluate correlation with ELO scores from the TTS Arena platform&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20485v1#bib.bib39\" title=\"\">39</a>]</cite>, which are derived from large-scale pairwise preference tests. We use 535 synthesized samples from 10 state-of-the-art TTS systems. A reliable prosody metric should align with these perceptual judgments.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are also differences between benchmarks. In VoiceMOS, where older systems suffer from issues such as robotic prosody and noise, factors beyond intelligibility may play a larger role, making general-purpose models like uLM slightly more competitive. In contrast, SOMOS contains more recent, higher-quality systems where intelligibility differences are more salient, and TTScore-int clearly outperforms baselines in both scenarios. Moreover, consistently high correlations in different extensive benchmarks indicate the generalization ability and robustness of the proposed metric.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "prosody",
                    "benchmarks",
                    "correlations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, TTScore-int correlates more strongly with MOS than WER and CER across all conditions, demonstrating that text-conditioned, speech-domain evaluation captures both intelligibility and its contribution to overall quality more effectively than traditional text-domain metrics.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The score distributions of the proposed prosody metric for real and synthesized speech samples are shown in Figure 2(a). As observed in the figure, real speech samples with natural prosody achieve higher scores than synthesized speech samples with limited prosody modeling, supporting the validity and reliability of our metric. Furthermore, the similar scores across different datasets for both real and synthesized speech indicate the generalizability of our metric across different domains.</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">(c) Correlation with TTSArena ELO<span class=\"ltx_text ltx_font_upright\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:157.0pt;height:55.9pt;vertical-align:-25.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.8pt,2.4pt) scale(0.92,0.92) ;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<span class=\"ltx_td ltx_align_center ltx_colspan ltx_colspan_2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">System-level</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Metric</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">LCC</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">SRCC</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_italic\">Log f0 RMSE</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_italic\">0.18*</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_italic\">0.32*</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_italic\">f0-corr</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_italic\">0.13</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">0.32</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">TTScore-pro (large)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.25</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">0.21</span></span>\n</span>\n</span></span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "srcc",
                    "lcc",
                    "systemlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We note that, compared to real speech scores in Figure 2(a), resynthesized speech with original prosody tends to have lower values. This is expected, as resynthesis method has its own limitations that reduce naturalness relative to original recordings. Some overlap also exists between the distributions of original and perturbed prosody, which is anticipated since prosody is inherently context-dependent and variable across utterances. For this reason, analyzing the shift in general score distributions provides more meaningful insight than relying on absolute values. Although the degree of distributional shift is difficult to quantify, the key trend is clear: speech with original prosody consistently receives higher scores than speech with perturbed pitch. This confirms that our metric accounts for prosodic degradation.</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive experiments across SOMOS, VoiceMOS, and TTSArena benchmarks demonstrated that TTScore-int and TTScore-pro not only correlate strongly with established baselines but also align more closely with human judgments of naturalness. These results confirm the value of targeted evaluation: aspect-specific metrics provide finer insights while remaining predictive of overall perceived quality. This work opens new opportunities for extending targeted evaluation beyond intelligibility and prosody. The proposed paradign of conditional generation for evaluation is highly flexible for new formulations for different aspects of speech.</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "benchmarks",
                    "across"
                ]
            }
        ]
    }
}