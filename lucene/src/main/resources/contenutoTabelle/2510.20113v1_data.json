{
    "S6.T1": {
        "caption": "Table 1. Classification performance of different models across impairment classes. Each column group reports Accuracy (Acc), F1-score (F1), and ROC-AUC (AUC). The best performing model is highlighted as bold. All results are averaged over five runs with different seeds, reported as mean ±\\pm standard deviation.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Dysarthria</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Stuttering</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Aphasia</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Healthy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Overall</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">AUC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CLAP</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.00<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.19<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.08<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.44<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.00<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.16<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">97.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.41<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.36<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.12<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">.29<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ParaCLAP</th>\n<td class=\"ltx_td ltx_align_center\">1.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.4</td>\n<td class=\"ltx_td ltx_align_center\">.02<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.33<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">42.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>5.2</td>\n<td class=\"ltx_td ltx_align_center\">.20<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.15<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">12.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.9</td>\n<td class=\"ltx_td ltx_align_center\">.17<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.61<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m26\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">0.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5</td>\n<td class=\"ltx_td ltx_align_center\">.01<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m28\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.67<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m29\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">13.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m30\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.0</td>\n<td class=\"ltx_td ltx_align_center\">.10<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m31\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">.44<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m32\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ASR + LLM</th>\n<td class=\"ltx_td ltx_align_center\">48.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m33\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>5.3</td>\n<td class=\"ltx_td ltx_align_center\">.41<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m34\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.59<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m35\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">27.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m36\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>5.4</td>\n<td class=\"ltx_td ltx_align_center\">.34<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m37\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.58<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m38\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">47.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m39\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.3</td>\n<td class=\"ltx_td ltx_align_center\">.39<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m40\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.58<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m41\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center\">19.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m42\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>4.3</td>\n<td class=\"ltx_td ltx_align_center\">.25<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m43\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.54<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m44\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">35.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m45\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>4.3</td>\n<td class=\"ltx_td ltx_align_center\">.35<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m46\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">.57<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m47\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CNN + LSTM</th>\n<td class=\"ltx_td ltx_align_center\">91.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m48\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>3.4</td>\n<td class=\"ltx_td ltx_align_center\">.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m49\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.98<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m50\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center\">99.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m51\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.45</td>\n<td class=\"ltx_td ltx_align_center\">.81<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m52\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.99<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m53\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n<td class=\"ltx_td ltx_align_center\">67.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m54\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>6.4</td>\n<td class=\"ltx_td ltx_align_center\">.80<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m55\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">.98<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m56\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">86.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m57\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>9.6</td>\n<td class=\"ltx_td ltx_align_center\">.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m58\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">.99<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m59\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</span></td>\n<td class=\"ltx_td ltx_align_center\">86.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m60\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>3.6</td>\n<td class=\"ltx_td ltx_align_center\">.86<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m61\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">.98<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m62\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Transformer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">95.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m63\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">.95<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m64\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">.99<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m65\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">99.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m66\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">.88<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m67\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">1.00<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m68\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">82.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m69\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>4.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">.90<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m70\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">1.00<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m71\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">88.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m72\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">.94<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m73\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">.99<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m74\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">91.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m75\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m76\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">.99<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m77\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.01</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "59±pm02",
            "group",
            "overall",
            "33±pm02",
            "each",
            "seeds",
            "99±pm01",
            "08±pm02",
            "llm",
            "reported",
            "runs",
            "accuracy",
            "860±pm96",
            "420±pm52",
            "auc",
            "20±pm02",
            "over",
            "dysarthria",
            "29±pm02",
            "998±pm5",
            "bold",
            "averaged",
            "f1score",
            "912±pm34",
            "results",
            "888±pm23",
            "914±pm17",
            "254±pm6",
            "81±pm02",
            "acc",
            "0±pm0",
            "972±pm12",
            "healthy",
            "column",
            "01±pm01",
            "998±pm45",
            "92±pm06",
            "198±pm43",
            "15±pm02",
            "deviation",
            "120±pm19",
            "classes",
            "57±pm02",
            "impairment",
            "35±pm04",
            "cnn",
            "paraclap",
            "across",
            "39±pm01",
            "performing",
            "17±pm02",
            "80±pm04",
            "mean",
            "41±pm03",
            "100±pm00",
            "lstm",
            "278±pm54",
            "95±pm02",
            "standard",
            "02±pm01",
            "±pm",
            "92±pm04",
            "models",
            "672±pm64",
            "98±pm03",
            "820±pm45",
            "359±pm43",
            "stuttering",
            "86±pm04",
            "04±pm05",
            "16±pm02",
            "61±pm02",
            "19±pm01",
            "90±pm03",
            "486±pm53",
            "44±pm02",
            "performance",
            "67±pm04",
            "94±pm01",
            "classification",
            "asr",
            "10±pm02",
            "reports",
            "rocauc",
            "861±pm36",
            "54±pm02",
            "model",
            "five",
            "all",
            "different",
            "transformer",
            "25±pm05",
            "aphasia",
            "58±pm02",
            "clap",
            "88±pm02",
            "highlighted",
            "98±pm01",
            "34±pm05",
            "44±pm14",
            "36±pm01",
            "476±pm23",
            "950±pm20",
            "41±pm00",
            "92±pm02",
            "00±pm00",
            "12±pm4",
            "best",
            "139±pm20",
            "12±pm01",
            "58±pm01"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S6.T1\" title=\"Table 1 &#8227; 6.1. Dataset &amp; Reprocessing &#8227; 6. Benchmarking and Evaluation &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the performance of our speech recognition model. Since there is a lack of prior work directly addressing this multi-modal classification task, we benchmark our approach against two open-source baselines: CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib21\" title=\"\">2023</a>)</cite> and ParaCLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib30\" title=\"\">2024</a>)</cite>, both general-purpose language&#8211;audio alignment models. For these baselines, we follow a text&#8211;audio similarity approach to make classification. Each speech sample is compared against candidate text prompts of the form <span class=\"ltx_text ltx_font_italic\">&#8220;a speech from [dysarthria, stutter, aphasia, healthy] speaker&#8221;</span>. We then compute the similarity between the speech embedding and each candidate text embedding, and assign the label corresponding to the most similar text.\nAdditionally, we explore an alternative ASR+LLM baseline. In this approach, the impaired speech is first transcribed using an ASR model, and the resulting transcript is then passed to an LLM, which judges and classifies the impaired speech into one of the speech impairment classes.\nIn contrast, for the CNN+LSTM and Transformer approaches, we directly train the models to encode Mel-Spectrograms into speech embeddings, which are then optimized using the method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S4.SS2\" title=\"4.2. Speech Impairment Recognition &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. A linear classification layer is subsequently applied to predict the speaker condition label from these embeddings. Our trained SIR models will be released upon paper acceptance to encourage reproduction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech is essential for human communication, yet millions of people face impairments such as dysarthria, stuttering, and aphasia&#8212;conditions that often lead to social isolation and reduced participation.\nDespite recent progress in automatic speech recognition (ASR) and text-to-speech (TTS) technologies, accessible web and mobile infrastructures for users with impaired speech remain limited, hindering the practical adoption of these advances in daily communication.\nTo bridge this gap, we present <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>,\na mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> designed to facilitate people with speech impairments in everyday communication.\nThe system integrates large language model (LLM)&#8211;driven reasoning with advanced speech processing modules, providing adaptive support tailored to diverse impairment types.\nTo ensure real-world practicality, we develop a structured deployment pipeline that enables real-time speech processing on mobile and edge devices, achieving imperceptible latency while maintaining high accuracy and speech quality.\nEvaluation on real-world impaired speech datasets and edge-device latency profiling confirms that SpeechAgent delivers both effective and user-friendly performance, demonstrating its feasibility for personalized, day-to-day assistive communication.\nOur code, dataset, and speech samples are publicly available<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://anonymous.4open.science/r/SpeechAgentDemo-48EE/</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "stuttering",
                    "impairment",
                    "asr",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, not everyone can express themselves fluently through speech. Many individuals experience impairments that make their speech fragmented, unclear, or difficult to follow. Such challenges often arise from clinical speech and language disorders that directly affect articulation, fluency, or coherence <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamaguchi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib26\" title=\"\">2010</a>; Eichorn and Fabus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib20\" title=\"\">2024</a>)</cite>.\nAmong the most prevalent are <span class=\"ltx_text ltx_font_italic\">(i) dysarthria</span>, which arises from motor impairments and results in slurred or slow speech due to reduced control of the articulatory muscles&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enderby, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib23\" title=\"\">2013</a>)</cite>; <span class=\"ltx_text ltx_font_italic\">(ii) stuttering</span>, characterized by involuntary repetitions or prolongations that disrupt the natural flow of speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Prasse and Kikano, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib43\" title=\"\">2008</a>)</cite>; and <span class=\"ltx_text ltx_font_italic\">(iii) aphasia</span>, typically caused by neurological damage such as stroke, which impairs a person&#8217;s ability to produce or comprehend language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damasio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib16\" title=\"\">1992</a>; Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "results",
                    "aphasia",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These disorders extend beyond the clinical symptoms, shaping how people interact and are perceived in everyday communication.\nIndividuals with speech impairments often struggle to make themselves understood, facing frequent communication breakdowns that result in misunderstandings, interruptions, or dismissals. Such experiences can be discouraging and might,\nover time, restrict participation in social, educational, and professional contexts,\nleading to isolation and reduced quality of life&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McCormack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib37\" title=\"\">2010</a>; Bashir and Scavuzzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib7\" title=\"\">1992</a>)</cite>. Addressing such barriers is therefore essential. This motivates the design of our <span class=\"ltx_text ltx_font_bold\">mobile SpeechAgent</span>, designed to assist individuals with dysarthria, stuttering, and aphasia in achieving clearer and more effective communication.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "over",
                    "aphasia",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In clinical environments, technological interventions have typically emphasized long-term rehabilitation rather than live conversational repair. For instance, computer-mediated aphasia programs focus on vocabulary relearning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>, while stuttering management tools provide pacing strategies or fluency feedback&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez and Stoeckle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib41\" title=\"\">2016</a>)</cite>. Similarly, interactive therapy software often motivates patients with corrective prompts or gamified exercises&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Rodr&#237;guez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib47\" title=\"\">2008</a>)</cite>. Although these methods are effective in structured therapeutic environments, they are not designed to adapt flexibly to open-ended, everyday usage as well.\nRecent research has explored\ntraining automatic speech recognition (ASR) models to better transcribe impaired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vinotha et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib61\" title=\"\">2024</a>; Lea et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib32\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib28\" title=\"\">2025</a>)</cite>, or developing detection system and classifying different types of speech impairments&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sekhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib51\" title=\"\">2022</a>; Al-Banna et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib4\" title=\"\">2022</a>)</cite>. While these approaches improve the ability to recognize or analyze disordered speech, they are primarily diagnostic in nature. They do not provide direct communicative assistance to the user in real time, nor do they help transform impaired speech into a clearer, more comprehensible form during everyday interaction.\nTo the best of our knowledge, there is currently no assistive communication tool that offers real-time refinement of impaired speech for practical, everyday communication.\nTo address this gap, we introduce a mobile assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that functions as an assistive tool for spontaneous communication. The system refines impaired speech into a clearer form that preserves the speaker&#8217;s intention, allowing users to communicate easily in everyday conversations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "best",
                    "aphasia",
                    "asr",
                    "stuttering",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> advances beyond traditional speech-impairment refinement pipelines<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>; Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite> by integrating perception, reasoning, and generation into a unified agent framework.\nIt combines open-sourced and custom-trained models to not only recognize disordered speech but also understand and restore the speaker&#8217;s communicative intention. It contains a robust, general-purpose ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> transcribes speech reliably across diverse acoustic conditions, while a dedicated impairment recognition model captures speaker-specific disorder patterns, enabling adaptive, context-aware processing for downstream refinement. Building upon these perceptual foundations, the agent employs large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib3\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib58\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib65\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib59\" title=\"\">2025</a>)</cite> as cognitive refinement engines that reason about the meaning behind fragmented, ambiguous, or disfluent utterances.\nRather than performing surface-level grammatical correction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>; Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>, the LLM reconstructs semantically coherent and faithful text that preserves the speaker&#8217;s intent.\nFinally, a natural and expressive text-to-speech (TTS) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>; Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib45\" title=\"\">2020</a>)</cite> vocalizes the refined text, completing a closed perception&#8211;reasoning generation loop that transforms impaired speech into clear, intelligible, and easy to understand speech.\nThe main contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "performing",
                    "model",
                    "llm",
                    "asr",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Impairments.</span>\nSpeech impairments arise from diverse neurological, developmental, or physiological conditions and encompass a wide range of disorders affecting clarity, fluency, and intelligibility. This work focuses on three representative types: dysarthria, stuttering, and aphasia. Dysarthria is an acoustic impairment in which speech sounds are slurred, strained, or unusually slow due to weakened motor control of articulatory muscles, often resulting from stroke, cerebral palsy, traumatic brain injury, or degenerative diseases such as Parkinson&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enderby, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib23\" title=\"\">2013</a>; Pinto et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib42\" title=\"\">2004</a>)</cite>.\nStuttering, by contrast, disrupts speech rhythm through involuntary repetitions (e.g., &#8220;b-b-b-book&#8221;), prolongations (&#8220;ssssun&#8221;), or silent blocks, producing hesitation and tension perceptible even in short utterances. It has both developmental and neurological origins and is associated with atypical timing and coordination in neural circuits governing speech planning and execution&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conture and Wolk, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib14\" title=\"\">1990</a>; Bloodstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nAphasia differs from the above as it primarily involves linguistic and cognitive deficits in language production and comprehension&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ellis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib22\" title=\"\">1983</a>; Wiener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib63\" title=\"\">2004</a>; Damasio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib16\" title=\"\">1992</a>)</cite>. Although articulation and prosody may remain intact, the semantic coherence of speech is disrupted, making it difficult for listeners to grasp the intended meaning.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "aphasia",
                    "stuttering",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Assistive Tools for Speech Impairments.</span>\nResearch in speech and language technologies has long aimed to assist individuals with communication difficulties. Traditional tools largely rely on rule-based correction, including speech therapy software&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shipley and McAfee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib54\" title=\"\">2023</a>)</cite>, augmentative and alternative communication (AAC) devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beukelman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib9\" title=\"\">1998</a>; Schlosser and Wendt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib49\" title=\"\">2008</a>)</cite>, and computer-assisted language learning (CALL) platforms&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chapelle and Chapelle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib12\" title=\"\">2001</a>)</cite>. While useful within their respective domains, these systems operate on fixed rules or phrase banks, offering predictable corrections but limited adaptability. Because they depend on explicit error&#8211;correction mappings, such approaches struggle with the variability and ambiguity of spontaneous or impaired speech. In clinical contexts, technological supports have primarily targeted rehabilitation rather than real-time conversational repair. Programs for aphasia focus on relearning vocabulary and syntax&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>, while stuttering management systems provide pacing or fluency feedback&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez and Stoeckle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib41\" title=\"\">2016</a>)</cite>. Other interactive therapy tools use monitoring, feedback, or gamified exercises to motivate patients&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Rodr&#237;guez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib47\" title=\"\">2008</a>)</cite>. Although effective in structured therapeutic environments, these systems are not designed to adapt flexibly to the open-ended nature of everyday communication.</p>\n\n",
                "matched_terms": [
                    "aphasia",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For healthy speakers, the mapping between <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> and <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> is generally straightforward. The speech carries sufficient information and is clear enough for listeners to infer the intention with high accuracy, <math alttext=\"p(I\\mid S)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mi>S</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(I\\mid S)</annotation></semantics></math> is well aligned with the true <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. In contrast, for speakers with speech impairments, who produce speech with <em class=\"ltx_emph ltx_font_italic\">acoustic distortions</em> (e.g., disfluency, articulation errors) or <em class=\"ltx_emph ltx_font_italic\">logical distortions</em> degrade this mapping, making <math alttext=\"p(I\\mid S)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mi>S</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(I\\mid S)</annotation></semantics></math> less reliable and making it difficult for listeners to easily understand <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m6\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> from <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m7\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "healthy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> follows a structured workflow to refine impaired speech. Specifically, given an impaired speech input <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, the system first identifies the speech impairment class <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. The signal is then transcribed into text <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. At this stage, the raw transcript <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> may deviate significantly from the true communicative intent <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>.\nWe therefore model intent inference as a conditional distribution <math alttext=\"P(I\\mid T,C)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mrow><mi>T</mi><mo>,</mo><mi>C</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(I\\mid T,C)</annotation></semantics></math>, where the LLM leverages both the noisy transcript and the impairment class to approximate the underlying intent. Based on the inferred intent, the system generates a refined transcript <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>, modeled as <math alttext=\"P(T^{\\prime}\\mid I,C)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&#8739;</mo><mrow><mi>I</mi><mo>,</mo><mi>C</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(T^{\\prime}\\mid I,C)</annotation></semantics></math>, such that <math alttext=\"T^{\\prime}\\approx I\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&#8776;</mo><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}\\approx I</annotation></semantics></math>. Finally, the refined transcript <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted back into speech <math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>, yielding a refined speech that is easier for listeners to understand the intention. Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#alg1\" title=\"Algorithm 1 &#8227; 4.1. Framework Overview &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall workflow.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model",
                    "llm",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SIR model gives our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> the ability to understand how speech deviates from healthy patterns. Its goal is to detect impairment-related acoustic or linguistic distortions given a speech input. The input to the SIR model is a waveform <math alttext=\"S\\in\\mathbb{R}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>l</mi></msup></mrow><annotation encoding=\"application/x-tex\">S\\in\\mathbb{R}^{l}</annotation></semantics></math>, represented as a one-dimensional array with <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> time points. The waveform is segmented into overlapping frames and transformed into a log-Mel spectrogram&#160;<math alttext=\"\\mathrm{MelSpec}\\in\\mathbb{R}^{F\\times\\mathcal{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>MelSpec</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MelSpec}\\in\\mathbb{R}^{F\\times\\mathcal{T}}</annotation></semantics></math>.\n<math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> is the number of temporal frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the number of frequency bins. The Mel-spectrogram is passed through a seq2seq deep neural encoder <math alttext=\"\\mathrm{Encoder}_{\\theta}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>Encoder</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Encoder}_{\\theta}(\\cdot)</annotation></semantics></math>, parameterized by <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that processes the input as a sequence of hidden frame-level embeddings <math alttext=\"H_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">H_{C}</annotation></semantics></math>.:</p>\n\n",
                "matched_terms": [
                    "model",
                    "healthy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain a fixed-length speech-level embedding, we apply a pooling operation over the temporal dimension, <math alttext=\"h=\\mathrm{pool}(H)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mrow><mi>pool</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h=\\mathrm{pool}(H)</annotation></semantics></math>\nwhere <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> denotes the pooled embedding and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimensionality. In practice, <math alttext=\"\\mathrm{Pool}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>Pool</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Pool}(\\cdot)</annotation></semantics></math> can be implemented as mean pooling or attention-based pooling over time. This speech-level embedding is then projected into a classification logit:</p>\n\n",
                "matched_terms": [
                    "over",
                    "classification",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|C|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|C|</annotation></semantics></math> is the number of speech impairment classes. The logits <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m14\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> are converted into a probability distribution over impairment categories using the softmax function:</p>\n\n",
                "matched_terms": [
                    "over",
                    "classes",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Automatic Speech Recognition (ASR) model gives our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> the ability to &#8220;listen&#8221; and transcribe speech into text. The input to the ASR model is log-Mel spectrogram&#160;<math alttext=\"\\mathrm{MelSpec}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>MelSpec</mi><annotation encoding=\"application/x-tex\">\\mathrm{MelSpec}</annotation></semantics></math>.\nThe Mel-spectrogram is then passed through an seq2seq encoder network <math alttext=\"\\mathrm{Encoder}_{\\phi}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>Encoder</mi><mi>&#981;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Encoder}_{\\phi}(\\cdot)</annotation></semantics></math>, parameterized by <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#981;</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math>, which maps the input into a sequence of latent embeddings:</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimensionality. These latent representations capture both short-term phonetic information at the frame level and long-term semantic dependencies across the speech. On top of this encoder, a decoder network autoregressively generates a sequence of discrete text tokens. At each decoding step <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the decoder conditions on the history of previously generated tokens <math alttext=\"t_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{&lt;i}</annotation></semantics></math> and the latent representation <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m7\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> to produce a probability distribution over the next token:</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> denotes the vocabulary of text tokens. The distribution is typically obtained via a softmax over the decoder output logits. Tokens are generated iteratively until an end-of-sequence symbol is produced. The final transcription is denoted as <math alttext=\"T=(t_{1},t_{2},\\ldots,t_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><msub><mi>t</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">T=(t_{1},t_{2},\\ldots,t_{N})</annotation></semantics></math>, where each <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> corresponds to a token in the recognized text sequence.</p>\n\n",
                "matched_terms": [
                    "over",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the impaired text <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> transcribed by the ASR model and the recognized class of speech impairment <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>, the next step is to enhance its clarity and coherence through a LLM refinement model. This model directly rewrites the transcription into a more fluent and comprehensible form, conditioned on the impairment type <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. Formally, the refinement can be expressed as :</p>\n\n",
                "matched_terms": [
                    "model",
                    "llm",
                    "asr",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"X\\in\\mathbb{R}^{d\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>d</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{d\\times L}</annotation></semantics></math> denotes the phoneme embedding sequence. We incorporate a speaking style embedding <math alttext=\"Style\\in\\mathbb{R}^{d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi>s</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">Style\\in\\mathbb{R}^{d_{s}}</annotation></semantics></math>, which can be derived from descriptive text prompts, allowing flexible control over the speaking style of generated speech. The TTS model then conditions on both the phoneme embeddings <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and the speaking style embedding <math alttext=\"Style\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">Style</annotation></semantics></math> to generate the speech signal:</p>\n\n",
                "matched_terms": [
                    "over",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ ParaStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib34\" title=\"\">2025a</a>)</cite>, a neural text-to-speech architecture that jointly models prosody and paralinguistic style for expressive and natural-sounding speech generation. By directly synthesizing waveforms in an end-to-end manner, this TTS model removes the dependency on intermediate spectrogram representations and external vocoders, thereby enhancing both generation quality and computational efficiency.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> adopts a client&#8211;server architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Berson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib8\" title=\"\">1992</a>)</cite>, as illustrated in Figure 2. The mobile edge device acts as the primary user interface, while computation-intensive models are offloaded to a cloud server hosting large-scale models. All communication occurs through secure RESTful APIs over HTTPS, and the server integrates third-party services for monitoring, analytics, and authentication.</p>\n\n",
                "matched_terms": [
                    "over",
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The server executes all computation-intensive models through a modular pipeline of models and services. Incoming speech recordings from the mobile app are first processed by a speech-impairment recognition model, which identifies the type of impairment. Next, an ASR system <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> transcribes the impaired speech into text. The transcribed text is then refined by LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib3\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib58\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib59\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib65\" title=\"\">2025</a>)</cite> that interprets, clarifies, and reformulates the text while preserving semantic fidelity. The refined text is subsequently passed to a TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>, which converts it into natural, fluent speech. The final audio output is transmitted back to the edge device.\nThis server-side pipeline, exposed through RESTful APIs, allows seamless integration with the client application and external third-party APIs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "llm",
                    "all",
                    "asr",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most prior speech impairment-related research has focused on evaluating narrow, well-defined subtasks. For example, studies on speech recognition for impaired speakers typically report word error rate (WER) as the main metric to measure transcription quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mirella, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib38\" title=\"\">2024</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib56\" title=\"\">2024</a>)</cite>. Similarly, research on speech impairment detection or classification often relies on standard accuracy or F1 scores to assess model performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheikh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib52\" title=\"\">2021</a>)</cite>. While these task-specific evaluations provide valuable insights, they do not capture the broader goal of refining impaired speech into a clearer and more communicative form. To systematically benchmark our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>, we design an evaluation framework that integrates both speech and text datasets, rigorous preprocessing, and multidimensional performance analysis. Our goal is to capture not only the technical accuracy of impairment recognition and text refinement, but also the practical usability of the system in real-world applications.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "model",
                    "classification",
                    "impairment",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two categories of datasets to evaluate the performance of the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>. First is the impaired speech dataset. To capture diverse speech impairments, we combine multiple sources. For dysarthria, we select 21 hours of English speech from the TORGO dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rudzicz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib48\" title=\"\">2012</a>)</cite>, which contains eight speakers with dysarthria and seven healthy speakers. For stuttering, we select the UCLASS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Howell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib27\" title=\"\">2009</a>)</cite>, which contains recordings of speakers with stutter. For aphasia, we select speech samples from AphasiaBank&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MacWhinney et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib36\" title=\"\">2011</a>)</cite>. Additionally, we adopt a pure text dataset, on top of the impaired speech dataset, we select the SNIPS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite>, a task-oriented text dataset related to common daily-life commands such as weather, transport, reminders, and music. These texts are representative of typical user intents in conversational AI applications and thus provide a practical foundation for evaluating the robustness of the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> in refining the impaired speech.</p>\n\n",
                "matched_terms": [
                    "healthy",
                    "dysarthria",
                    "aphasia",
                    "stuttering",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although numerous studies investigate the automatic recognition of dysarthria&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aboeitta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib2\" title=\"\">2025</a>)</cite>, stuttering&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheikh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib52\" title=\"\">2021</a>)</cite>, and aphasia&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Azevedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib6\" title=\"\">2024</a>)</cite>, most approaches treat the task as a binary classification problem, detecting only a single impairment type at a time. Our goal is to equip the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> with the ability to recognize multiple impairments simultaneously, which naturally formulates the task as multi-class classification. Due to there is no suitable open-source dataset or model for this setting, we construct our own dataset and train a deep learning model that takes spectrograms as input to classify different types of speech impairment. Specifically, we randomly select <span class=\"ltx_text ltx_font_bold\">500</span> speech samples from each type of speech impairment (dysarthria, stutter, aphasia, and health), and form a dataset with a total of <span class=\"ltx_text ltx_font_bold\">2,000</span> speech samples. The selected speech samples range from 2 to 15 seconds in duration and are resampled to 16 kHz. Each sample is converted into a Mel-spectrogram using a hop size of 256 with a window size of 1,024. The resulting Mel-Spectrogram has approximately <span class=\"ltx_text ltx_font_bold\">63</span> frames per second, where each frame corresponds to approximately <span class=\"ltx_text ltx_font_bold\">16ms</span> of speech signal.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "classification",
                    "aphasia",
                    "each",
                    "stuttering",
                    "impairment",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system across three dimensions to comprehensively assess both its technical performance and practical utility. Specifically, <span class=\"ltx_text ltx_font_bold\">recognition accuracy</span>, which we evaluate whether the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can correctly identify different types of speech impairments; <span class=\"ltx_text ltx_font_bold\">refinement performance</span>, which we evaluate whether the refined speech by LLM is clear and easy to understand; and <span class=\"ltx_text ltx_font_bold\">latency</span>, which we evaluate whether the communication between the user and the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> is sufficiently fast and seamless to support everyday interaction.</p>\n\n",
                "matched_terms": [
                    "across",
                    "llm",
                    "accuracy",
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the classification task, we randomly select 10% of the speech samples from each impairment class as the test set and use the remaining 90% for training. A deep learning model is then trained on this dataset, and we report performance in terms of class-level <span class=\"ltx_text ltx_font_bold\">accuracy</span>, <span class=\"ltx_text ltx_font_bold\">F1-score</span>, and <span class=\"ltx_text ltx_font_bold\">AUC</span>.\nThese metrics are standard metrics for evaluating classification models, and are widely applied in domains such as facial recognition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taigman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib57\" title=\"\">2014</a>)</cite> and image classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib18\" title=\"\">2009</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "standard",
                    "model",
                    "f1score",
                    "classification",
                    "each",
                    "impairment",
                    "accuracy",
                    "auc",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the refinement evaluation, we conduct two types of experiments. The first is a text-based evaluation. We use the text from the SNIPS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite> dataset as the ground-truth intention and simulate impaired text for each type of speech impairment, using the original text as the reference. The impaired text is then passed through the LLM refinement algorithm. After refinement, we compute semantic similarity between the refined text and the original intention using <span class=\"ltx_text ltx_font_bold\">BERT</span> score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib66\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">cosine score</span>, and <span class=\"ltx_text ltx_font_bold\">BLEU</span> score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib40\" title=\"\">2002</a>)</cite>. These metrics are widely used to evaluate the quality of language model outputs, including tasks such as machine translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib60\" title=\"\">2017</a>; Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib40\" title=\"\">2002</a>)</cite> and text generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib66\" title=\"\">2019</a>)</cite>. They measure the similarity between pairs of text; in our context, a higher similarity between the refined text and the original intention indicates that the refined text is clearer to be understood and closer to the true intention, and thus the refinement is more effective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "each",
                    "llm",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second experiment is a speech-based evaluation. We feed the impaired speech samples into an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> to obtain transcriptions, which are then processed by the LLM refinement cycle.\nThe refined text is subsequently converted back into speech. Since no ground-truth intention is available for the impaired speech samples, we instead conduct a human evaluation. Several human participants are presented with a shuffled list of impaired and refined speech and asked to rate the clarity of the speech and CMOS (Comparison Mean Opinion Score), where we provide the original impaired speech and refined speech to human listeners to judge which one is better. These subjective evaluation methods are widely adopted in the speech generation community for assessing the perceptual quality of TTS models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib62\" title=\"\">2017</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib53\" title=\"\">2018</a>; Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib46\" title=\"\">2019</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib33\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>. In our context, they serve as an effective metric to indicate the overall quality and intelligibility of the refined speech. Additionally, alongside subjective evaluation, we conduct objective evaluation using our trained SIR model. Specifically, we measure the proportion of successfully recovered samples, where a recovery is defined as the SIR model classifying the refined speech as healthy. Lastly, we evaluate the latency of our system to assess its practicality for real-world applications. We build a simulation environment where a edge device communicates with the server to complete the full interaction loop. This process includes transmitting the speech signal, recognizing the impairment type and transcribing the speech, refining the text, generating the refined speech, and transmitting the output back to the user&#8217;s device. We measure the average end-to-end response time across multiple trials to determine whether the system operates seamlessly and efficiently enough to support everyday user interaction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "healthy",
                    "model",
                    "llm",
                    "overall",
                    "mean",
                    "asr",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our trained SIR models (CNN+LSTM and Transformer) consistently outperform the baseline methods across all impairment classes. The Transformer model, in particular, achieves nearly perfect recognition with average recognition accuracy over 90% and ROC-AUC values exceeding 0.99. This superior performance can be attributed to its ability to directly learn acoustic&#8211;temporal representations from Mel-Spectrograms, allowing it to capture subtle articulatory and prosodic cues that characterize different impairments. The end-to-end optimization ensures that the learned embeddings are well aligned with the classification objective, resulting in robust and generalizable recognition. These findings indicate that specialized SIR models are essential to achieve reliable classification, as they are better suited to handle the nuanced variability in pathological speech compared to general-purpose alignment models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "over",
                    "across",
                    "rocauc",
                    "classes",
                    "transformer",
                    "model",
                    "all",
                    "classification",
                    "impairment",
                    "accuracy",
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the baseline approaches exhibit clear limitations. CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib21\" title=\"\">2023</a>)</cite> and ParaCLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib30\" title=\"\">2024</a>)</cite>, while effective in audio-text alignment tasks, struggle in SIR because they were not trained to discriminate pathological speech conditions; their embeddings focus on broad semantic alignment rather than fine-grained acoustic features. Similarly, the ASR+LLM baseline shows moderate improvements but is constrained by the error compounding of two components: ASR mis-transcriptions of impaired speech and the LLM&#8217;s reliance on textual cues alone, which discard essential acoustic markers of impairment. Together, these results provide strong evidence that dedicated acoustic models are necessary for accurate SIR. Our experimental results demonstrate that the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can accurately recognize speech impairments, which answers <span class=\"ltx_text ltx_font_bold\">RQ1</span>.</p>\n\n",
                "matched_terms": [
                    "paraclap",
                    "models",
                    "clap",
                    "impairment",
                    "asr",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T2\" title=\"Table 2 &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the text-based evaluation for refining impaired text. In this experiment, we mainly evaluate the ability of LLM to reconstruct the intention from the impaired text. Specifically, given an intention text from the Snips dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite>, we first use LLM to synthesize impaired text&#160;<math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> for each type of speech impairment given the intention&#160;<math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> as input (we use GPT-4.1). Then we use different LLMs from various sources to refine the impaired text. Then we compare the semantic similarity between the refined text<math alttext=\"T^{\\prime}=\\mathrm{LLMRefine(T)}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mi>LLMRefine</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">T</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}=\\mathrm{LLMRefine(T)}</annotation></semantics></math>, with the intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. A higher similarity score indicates better reconstruction of the intention.\nAs a baseline, we include grammar correction systems, which represent a lightweight form of text refinement often used in AAC applications to improve intelligibility. We implement both a rule-based grammar correction system (LanguageTool&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>) and a neural grammar correction model (Gramformer, based on&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>). These serve as strong baselines for comparison with our speech-refinement pipeline.</p>\n\n",
                "matched_terms": [
                    "model",
                    "llm",
                    "each",
                    "impairment",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T2\" title=\"Table 2 &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> demonstrate that LLMs can effectively refine impaired text to better align with the intention. Across all impairment classes, the refined outputs achieve substantial improvements over the impaired baseline in terms of semantic similarity, as reflected by higher BERT score, BLEU, Cosine Similarity, and IMS values. This confirms that LLM-based refinement is capable of recovering much of the speaker&#8217;s original intention. Among the models evaluated, Gemini 2.5 consistently outperforms other LLMs, achieving the highest refinement performance across all metrics and impairment classes. This indicates stronger robustness in handling diverse impairment patterns. Moreover, incorporating the impairment class (<math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) into the refinement process yields further gains, particularly in challenging cases such as aphasia, where knowledge of the impairment type provides additional context to guide reconstruction. This aligns with our expectation that specifying the impairment condition would help the model focus on relevant linguistic features and thereby improve similarity to the original intention. Hence, this experiment partially demonstrates <span class=\"ltx_text ltx_font_bold\">RQ2</span>, where the LLM can effectively refine the impaired <span class=\"ltx_text ltx_font_bold\">text</span> to a clearer form that is easier to be understood.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "models",
                    "classes",
                    "model",
                    "llm",
                    "all",
                    "aphasia",
                    "results",
                    "impairment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T3\" title=\"Table 3 &#8227; 7.2. Refinement from Impaired Speech &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the speech-based evaluation for refinement of impaired speech. In this experiment, we evaluate the ability of the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> to refine the impaired speech in an end-to-end manner. Given an impaired speech <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p3.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, we pass it through the entire <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> and let the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> refine and produce the refined speech&#160;<math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p3.m2\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>.\nPurely acoustic-based baselines&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>; Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite>, which focus on signal-level modifications such as repetition removal or silence trimming, achieve only marginal or even degraded performance compared to the original impaired speech. This decline arises because these approaches overlook semantic consistency. The impaired speech signal is already challenging to interpret, and further acoustic alterations often distort meaning and naturalness. Text-based baselines employing grammar correction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>; Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite> perform more competitively, particularly for dysarthric speech, where linguistic structure remains intact but articulation is weak. In such cases, accurate transcription alone (ASR + TTS) already yields speech of comparable clarity to our full agent, indicating that dysarthric speech primarily suffer from articulatory, not semantic, impairment. In contrast, grammar correction performs poorly on stuttering and aphasia, as these impairments introduce disruptions or omissions at the semantic level that rule-based correction cannot resolve. LLMs, however, can reconstruct the underlying intent beyond syntactic adjustment, producing refined text that is both fluent and semantically easy to be understood. These performance gains are particularly evident in the stuttering and aphasia scenarios, where the LLM effectively removes repetitions, restores disrupted sentence structures, and reconstructs missing or fragmented content to improve overall clarity. Taken together, these findings confirm that the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> is effective at enhancing the clarity of impaired speech, thereby directly addressing <span class=\"ltx_text ltx_font_bold\">RQ2</span>.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "overall",
                    "aphasia",
                    "stuttering",
                    "impairment",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we evaluate the efficiency and latency of the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>.\nWe report both <span class=\"ltx_text ltx_font_italic\">refinement latency</span> (comparison between different LLMs to complete a refinement) and <span class=\"ltx_text ltx_font_italic\">model-wise latency</span> (ASR, SIR, Speech refinement, and TTS). Our experimental result is presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.F4\" title=\"Figure 4 &#8227; 7.3. Efficiency Analysis &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.F3\" title=\"Figure 3 &#8227; 7.3. Efficiency Analysis &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The ASR and LLM models account for most of the inference time, with the speech recognition model taking <span class=\"ltx_text ltx_font_bold\">38.5%</span> and the LLM refinement model taking <span class=\"ltx_text ltx_font_bold\">47.3%</span>. Nevertheless, the overall speech refinement pipeline achieves near real-time performance across all components. On average, refining a sentence of approximately 11.5 seconds in duration requires only <span class=\"ltx_text ltx_font_bold\">0.91 seconds</span> of processing time, yielding a real-time factor (RTF) of <span class=\"ltx_text ltx_font_bold\">0.08</span>, far below the real-time threshold (RTF = 1). It keeps the latency well within the acceptable range for natural human communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jacoby et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib29\" title=\"\">2024</a>)</cite>. This indicates that the proposed approach is practically viable for real-world use, enabling fluid and responsive interaction. Consequently, the efficiency evaluation provides a clear answer to <span class=\"ltx_text ltx_font_bold\">RQ3</span>, demonstrating that the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can reliably meet the latency requirements of everyday conversations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "model",
                    "llm",
                    "overall",
                    "all",
                    "asr",
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented a mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that integrates the reasoning capabilities of LLMs with advanced speech processing to enable real-time assistive communication for individuals with speech impairments. Our system demonstrates that low-latency and practically deployable assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can be realized through a carefully designed client&#8211;server architecture. By combining impairment recognition, adaptive refinement, and seamless mobile integration, the agent delivers personalized, intelligible, and context-aware communication support. Evaluation on real-world data confirms both technical robustness and positive user experience, suggesting that the proposed approach is feasible and effective in everyday use. Looking ahead, several directions remain open for exploration. While the current system performs reliably, occasional hallucinations from the language model can lead to subtle deviations from the speaker&#8217;s intention. Future work could focus on enhancing semantic consistency and intent preservation. Moreover, the present agent primarily serves as a passive assistant that refines impaired speech into clearer expressions. Extending its capability toward a more interactive, training-oriented role could transform it into an adaptive partner that helps users practice, monitor, and improve their speech over time. Future research may also explore multilingual and personalized extensions, enabling the agent to support diverse linguistic, emotional, and contextual variations while maintaining natural and trustworthy communication.</p>\n\n",
                "matched_terms": [
                    "over",
                    "model",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the perceptual evaluation protocol for assessing refined speech quality. Two complementary rating schemes were used to capture absolute and relative listener judgments. The Clarity (1&#8211;5) score measures overall intelligibility, considering pronunciation, fluency, and ease of understanding. The Comparison Mean Opinion Score (C-MOS, &#8211;3 to +3) assesses the perceived improvement from the original to the refined speech, reflecting how effectively the refinement enhances clarity and fluency.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The user first taps the record button (Figure 5a) to start a session, and the mobile device records speech until the user presses stop (Figure 5b). The recorded speech is then sent to the server for transcription via the ASR model (Figure 5c), after which the LLM refines the text (Figure 5d). The text is converted to speech, transmitted to edge device and assists user for communication (Figure 5e).</p>\n\n",
                "matched_terms": [
                    "model",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This prompt defines how the large language model (LLM) performs speech refinement for impaired speech in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S4.SS4\" title=\"4.4. Speech Refinement &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>. Two variants are used: one without explicit impairment context and another conditioned on the detected impairment type. In both cases, the LLM reconstructs incomplete or unclear sentences into coherent and fluent expressions while preserving the speaker&#8217;s original intent. When an impairment class is provided, additional contextual cues (e.g., slurred articulation in dysarthria, repetition in stuttering, or lexical omission in aphasia) guide the refinement process, enabling the LLM to adapt its rewriting strategy to different impairment patterns.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "llm",
                    "aphasia",
                    "stuttering",
                    "impairment",
                    "different"
                ]
            }
        ]
    },
    "S7.T2": {
        "caption": "Table 2. Text-based refinement evaluation using BERT score, Cosine Similarity, and BLEU. The best score is highlighted in\ngreen, and the second-best score is highlighted in\nred. All results are averaged over five runs with different seeds, reported as mean ±\\pm standard deviation.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Dysarthria</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Stuttering</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Aphasia</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">CosSim</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">CosSim</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CosSim</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Impaired</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.794<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.049<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.045<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.859<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.118<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.819<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.868<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.139<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.412<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LanguageTool&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">0.858<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">0.183<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.282<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.19</td>\n<td class=\"ltx_td ltx_align_center\">0.862<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">0.147<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.786<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.22</td>\n<td class=\"ltx_td ltx_align_center\">0.866<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">0.137<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.10</td>\n<td class=\"ltx_td ltx_align_center\">0.401<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Gramformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">0.801<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">0.061<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.063<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.10</td>\n<td class=\"ltx_td ltx_align_center\">0.872<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">0.198<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.833<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m26\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.21</td>\n<td class=\"ltx_td ltx_align_center\">0.870<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center\">0.150<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m28\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.12</td>\n<td class=\"ltx_td ltx_align_center\">0.417<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m29\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\"><span class=\"ltx_text ltx_font_bold\">Refined without impairment class (w/o <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m30\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">GPT-4.1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.940<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m31\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.608<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m32\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.662<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m33\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.961<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m34\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.771<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m35\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.861<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m36\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.915<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m37\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.278<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m38\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.458<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m39\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Gemini 2.5</th>\n<td class=\"ltx_td ltx_align_center\">0.917<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m40\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.07</td>\n<td class=\"ltx_td ltx_align_center\">0.491<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m41\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.533<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m42\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.40</td>\n<td class=\"ltx_td ltx_align_center\">0.931<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m43\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.07</td>\n<td class=\"ltx_td ltx_align_center\">0.616<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m44\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.659<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m45\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.41</td>\n<td class=\"ltx_td ltx_align_center\">0.900<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m46\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center\">0.247<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m47\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.25</td>\n<td class=\"ltx_td ltx_align_center\">0.387<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m48\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Qwen 3</th>\n<td class=\"ltx_td ltx_align_center\">0.931<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m49\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.06</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.558<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m50\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.612<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m51\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.32</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.967<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m52\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.771<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m53\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.861<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m54\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.19</td>\n<td class=\"ltx_td ltx_align_center\">0.906<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m55\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.281<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m56\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.24</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.476<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m57\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Gemma3 4B</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.935<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m58\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center\">0.518<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m59\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.604<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m60\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.30</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.966<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m61\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">0.752<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m62\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.856<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m63\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.19</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.908<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m64\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">0.254<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m65\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.22</td>\n<td class=\"ltx_td ltx_align_center\">0.440<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m66\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"10\"><span class=\"ltx_text ltx_font_bold\">Refined with impairment class (w <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m67\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">GPT-4.1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.950<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m68\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.650<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m69\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.709<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m70\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.969<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m71\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.815<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m72\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.881<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m73\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.916<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m74\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.277<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m75\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.456<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m76\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Gemini 2.5</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.954<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m77\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.693<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m78\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.745<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m79\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.28</td>\n<td class=\"ltx_td ltx_align_center\">0.962<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m80\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center\">0.792<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m81\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.833<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m82\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.28</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.915<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m83\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.293<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m84\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.24</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.465<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m85\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Qwen 3</th>\n<td class=\"ltx_td ltx_align_center\">0.945<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m86\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.05</td>\n<td class=\"ltx_td ltx_align_center\">0.611<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m87\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.675<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m88\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.29</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.974<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m89\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.829<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m90\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.892<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m91\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.15</td>\n<td class=\"ltx_td ltx_align_center\">0.908<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m92\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.279<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m93\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.24</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>green!200.472<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m94\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Gemma3 4B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.941<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m95\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.574<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m96\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.656<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m97\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span>red!200.970<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m98\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.785<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m99\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.873<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m100\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.908<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m101\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.253<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m102\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.437<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T2.m103\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>.23</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "0147±pm10",
            "0183±pm15",
            "0139±pm10",
            "0786±pm22",
            "0931±pm06",
            "evaluation",
            "cellcolorred200279±pm24",
            "seeds",
            "0656±pm29",
            "0440±pm23",
            "0049±pm04",
            "0859±pm03",
            "0491±pm39",
            "0908±pm03",
            "qwen",
            "reported",
            "cellcolorred200709±pm27",
            "runs",
            "0833±pm28",
            "over",
            "0945±pm05",
            "cellcolorgreen200829±pm22",
            "0247±pm25",
            "dysarthria",
            "averaged",
            "0574±pm31",
            "results",
            "gemma3",
            "refinement",
            "0518±pm30",
            "0833±pm21",
            "damodaran",
            "0969±pm02",
            "cossim",
            "0533±pm40",
            "0387±pm28",
            "0437±pm23",
            "0675±pm29",
            "0785±pm25",
            "0456±pm23",
            "0858±pm04",
            "0872±pm04",
            "deviation",
            "gpt41",
            "cellcolorgreen200916±pm03",
            "cellcolorred200881±pm18",
            "impairment",
            "0611±pm30",
            "0917±pm07",
            "0277±pm23",
            "0862±pm03",
            "cellcolorred200465±pm24",
            "without",
            "cellcolorred200278±pm24",
            "0961±pm03",
            "mean",
            "languagetool",
            "0819±pm22",
            "0150±pm12",
            "0801±pm03",
            "0412±pm17",
            "0931±pm07",
            "standard",
            "cellcolorred200815±pm24",
            "0198±pm15",
            "±pm",
            "0282±pm19",
            "0962±pm05",
            "cellcolorgreen200472±pm23",
            "0604±pm30",
            "0253±pm23",
            "textbased",
            "0908±pm04",
            "cellcolorgreen200892±pm15",
            "0616±pm41",
            "stuttering",
            "0868±pm02",
            "cellcolorred200908±pm03",
            "cellcolorgreen200915±pm03",
            "cellcolorred200950±pm04",
            "similarity",
            "0792±pm30",
            "cellcolorred200861±pm19",
            "0063±pm10",
            "cellcolorred200650±pm29",
            "0045±pm07",
            "cellcolorgreen200662±pm30",
            "0870±pm02",
            "0401±pm17",
            "0856±pm19",
            "0794±pm03",
            "cellcolorred200935±pm05",
            "0254±pm22",
            "cellcolorred200612±pm32",
            "gramformer",
            "cellcolorgreen200476±pm23",
            "cellcolorred200558±pm33",
            "cosine",
            "0417±pm17",
            "0906±pm04",
            "score",
            "cellcolorgreen200608±pm32",
            "red",
            "0941±pm04",
            "model",
            "five",
            "all",
            "different",
            "impaired",
            "refined",
            "cellcolorgreen200954±pm04",
            "green",
            "cellcolorred200915±pm03",
            "cellcolorgreen200940±pm05",
            "cellcolorgreen200974±pm03",
            "class",
            "cellcolorgreen200281±pm24",
            "aphasia",
            "0061±pm07",
            "cellcolorgreen200693±pm31",
            "cellcolorred200970±pm03",
            "naber",
            "0752±pm27",
            "0137±pm10",
            "highlighted",
            "cellcolorgreen200293±pm24",
            "0659±pm41",
            "0900±pm05",
            "bert",
            "cellcolorgreen200967±pm04",
            "cellcolorred200458±pm23",
            "cellcolorgreen200745±pm28",
            "secondbest",
            "gemini",
            "cellcolorgreen200771±pm27",
            "0866±pm02",
            "cellcolorred200771±pm26",
            "cellcolorgreen200861±pm19",
            "best",
            "0873±pm18",
            "0118±pm07",
            "cellcolorred200966±pm03",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T2\" title=\"Table 2 &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the text-based evaluation for refining impaired text. In this experiment, we mainly evaluate the ability of LLM to reconstruct the intention from the impaired text. Specifically, given an intention text from the Snips dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite>, we first use LLM to synthesize impaired text&#160;<math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> for each type of speech impairment given the intention&#160;<math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> as input (we use GPT-4.1). Then we use different LLMs from various sources to refine the impaired text. Then we compare the semantic similarity between the refined text<math alttext=\"T^{\\prime}=\\mathrm{LLMRefine(T)}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mi>LLMRefine</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">T</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}=\\mathrm{LLMRefine(T)}</annotation></semantics></math>, with the intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. A higher similarity score indicates better reconstruction of the intention.\nAs a baseline, we include grammar correction systems, which represent a lightweight form of text refinement often used in AAC applications to improve intelligibility. We implement both a rule-based grammar correction system (LanguageTool&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>) and a neural grammar correction model (Gramformer, based on&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>). These serve as strong baselines for comparison with our speech-refinement pipeline.</p>\n\n",
            "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T2\" title=\"Table 2 &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> demonstrate that LLMs can effectively refine impaired text to better align with the intention. Across all impairment classes, the refined outputs achieve substantial improvements over the impaired baseline in terms of semantic similarity, as reflected by higher BERT score, BLEU, Cosine Similarity, and IMS values. This confirms that LLM-based refinement is capable of recovering much of the speaker&#8217;s original intention. Among the models evaluated, Gemini 2.5 consistently outperforms other LLMs, achieving the highest refinement performance across all metrics and impairment classes. This indicates stronger robustness in handling diverse impairment patterns. Moreover, incorporating the impairment class (<math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) into the refinement process yields further gains, particularly in challenging cases such as aphasia, where knowledge of the impairment type provides additional context to guide reconstruction. This aligns with our expectation that specifying the impairment condition would help the model focus on relevant linguistic features and thereby improve similarity to the original intention. Hence, this experiment partially demonstrates <span class=\"ltx_text ltx_font_bold\">RQ2</span>, where the LLM can effectively refine the impaired <span class=\"ltx_text ltx_font_bold\">text</span> to a clearer form that is easier to be understood.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech is essential for human communication, yet millions of people face impairments such as dysarthria, stuttering, and aphasia&#8212;conditions that often lead to social isolation and reduced participation.\nDespite recent progress in automatic speech recognition (ASR) and text-to-speech (TTS) technologies, accessible web and mobile infrastructures for users with impaired speech remain limited, hindering the practical adoption of these advances in daily communication.\nTo bridge this gap, we present <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>,\na mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> designed to facilitate people with speech impairments in everyday communication.\nThe system integrates large language model (LLM)&#8211;driven reasoning with advanced speech processing modules, providing adaptive support tailored to diverse impairment types.\nTo ensure real-world practicality, we develop a structured deployment pipeline that enables real-time speech processing on mobile and edge devices, achieving imperceptible latency while maintaining high accuracy and speech quality.\nEvaluation on real-world impaired speech datasets and edge-device latency profiling confirms that SpeechAgent delivers both effective and user-friendly performance, demonstrating its feasibility for personalized, day-to-day assistive communication.\nOur code, dataset, and speech samples are publicly available<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://anonymous.4open.science/r/SpeechAgentDemo-48EE/</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "evaluation",
                    "stuttering",
                    "impairment",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, not everyone can express themselves fluently through speech. Many individuals experience impairments that make their speech fragmented, unclear, or difficult to follow. Such challenges often arise from clinical speech and language disorders that directly affect articulation, fluency, or coherence <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamaguchi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib26\" title=\"\">2010</a>; Eichorn and Fabus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib20\" title=\"\">2024</a>)</cite>.\nAmong the most prevalent are <span class=\"ltx_text ltx_font_italic\">(i) dysarthria</span>, which arises from motor impairments and results in slurred or slow speech due to reduced control of the articulatory muscles&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enderby, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib23\" title=\"\">2013</a>)</cite>; <span class=\"ltx_text ltx_font_italic\">(ii) stuttering</span>, characterized by involuntary repetitions or prolongations that disrupt the natural flow of speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Prasse and Kikano, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib43\" title=\"\">2008</a>)</cite>; and <span class=\"ltx_text ltx_font_italic\">(iii) aphasia</span>, typically caused by neurological damage such as stroke, which impairs a person&#8217;s ability to produce or comprehend language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damasio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib16\" title=\"\">1992</a>; Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "results",
                    "aphasia",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These disorders extend beyond the clinical symptoms, shaping how people interact and are perceived in everyday communication.\nIndividuals with speech impairments often struggle to make themselves understood, facing frequent communication breakdowns that result in misunderstandings, interruptions, or dismissals. Such experiences can be discouraging and might,\nover time, restrict participation in social, educational, and professional contexts,\nleading to isolation and reduced quality of life&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McCormack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib37\" title=\"\">2010</a>; Bashir and Scavuzzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib7\" title=\"\">1992</a>)</cite>. Addressing such barriers is therefore essential. This motivates the design of our <span class=\"ltx_text ltx_font_bold\">mobile SpeechAgent</span>, designed to assist individuals with dysarthria, stuttering, and aphasia in achieving clearer and more effective communication.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "over",
                    "aphasia",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research in speech and language technologies has aimed to support individuals with speech impairment. Traditional approaches include augmentative and alternative communication (AAC) devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beukelman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib9\" title=\"\">1998</a>; Schlosser and Wendt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib49\" title=\"\">2008</a>)</cite>, rule-based grammar correction systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sidorov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib55\" title=\"\">2013</a>; Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>, and computer-assisted therapy programs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Chapelle and Chapelle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib12\" title=\"\">2001</a>)</cite>.\nWhile these tools have provided valuable support in structured clinical or educational domains, they are often rigid and rely heavily on predefined templates or rule mappings. Such designs perform well for predictable, domain-limited input, but they struggle with the fragmented, ambiguous, and dynamic pattern of real-world speech.</p>\n\n",
                "matched_terms": [
                    "naber",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In clinical environments, technological interventions have typically emphasized long-term rehabilitation rather than live conversational repair. For instance, computer-mediated aphasia programs focus on vocabulary relearning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>, while stuttering management tools provide pacing strategies or fluency feedback&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez and Stoeckle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib41\" title=\"\">2016</a>)</cite>. Similarly, interactive therapy software often motivates patients with corrective prompts or gamified exercises&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Rodr&#237;guez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib47\" title=\"\">2008</a>)</cite>. Although these methods are effective in structured therapeutic environments, they are not designed to adapt flexibly to open-ended, everyday usage as well.\nRecent research has explored\ntraining automatic speech recognition (ASR) models to better transcribe impaired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vinotha et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib61\" title=\"\">2024</a>; Lea et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib32\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib28\" title=\"\">2025</a>)</cite>, or developing detection system and classifying different types of speech impairments&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sekhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib51\" title=\"\">2022</a>; Al-Banna et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib4\" title=\"\">2022</a>)</cite>. While these approaches improve the ability to recognize or analyze disordered speech, they are primarily diagnostic in nature. They do not provide direct communicative assistance to the user in real time, nor do they help transform impaired speech into a clearer, more comprehensible form during everyday interaction.\nTo the best of our knowledge, there is currently no assistive communication tool that offers real-time refinement of impaired speech for practical, everyday communication.\nTo address this gap, we introduce a mobile assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that functions as an assistive tool for spontaneous communication. The system refines impaired speech into a clearer form that preserves the speaker&#8217;s intention, allowing users to communicate easily in everyday conversations.</p>\n\n",
                "matched_terms": [
                    "best",
                    "aphasia",
                    "stuttering",
                    "refinement",
                    "different",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> advances beyond traditional speech-impairment refinement pipelines<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>; Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite> by integrating perception, reasoning, and generation into a unified agent framework.\nIt combines open-sourced and custom-trained models to not only recognize disordered speech but also understand and restore the speaker&#8217;s communicative intention. It contains a robust, general-purpose ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> transcribes speech reliably across diverse acoustic conditions, while a dedicated impairment recognition model captures speaker-specific disorder patterns, enabling adaptive, context-aware processing for downstream refinement. Building upon these perceptual foundations, the agent employs large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib3\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib58\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib65\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib59\" title=\"\">2025</a>)</cite> as cognitive refinement engines that reason about the meaning behind fragmented, ambiguous, or disfluent utterances.\nRather than performing surface-level grammatical correction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>; Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>, the LLM reconstructs semantically coherent and faithful text that preserves the speaker&#8217;s intent.\nFinally, a natural and expressive text-to-speech (TTS) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>; Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib45\" title=\"\">2020</a>)</cite> vocalizes the refined text, completing a closed perception&#8211;reasoning generation loop that transforms impaired speech into clear, intelligible, and easy to understand speech.\nThe main contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "refined",
                    "damodaran",
                    "model",
                    "impairment",
                    "refinement",
                    "impaired",
                    "naber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend the system with mechanisms to recognize different types of speech impairments and adapt the agent&#8217;s refinement strategies accordingly.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Impairments.</span>\nSpeech impairments arise from diverse neurological, developmental, or physiological conditions and encompass a wide range of disorders affecting clarity, fluency, and intelligibility. This work focuses on three representative types: dysarthria, stuttering, and aphasia. Dysarthria is an acoustic impairment in which speech sounds are slurred, strained, or unusually slow due to weakened motor control of articulatory muscles, often resulting from stroke, cerebral palsy, traumatic brain injury, or degenerative diseases such as Parkinson&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enderby, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib23\" title=\"\">2013</a>; Pinto et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib42\" title=\"\">2004</a>)</cite>.\nStuttering, by contrast, disrupts speech rhythm through involuntary repetitions (e.g., &#8220;b-b-b-book&#8221;), prolongations (&#8220;ssssun&#8221;), or silent blocks, producing hesitation and tension perceptible even in short utterances. It has both developmental and neurological origins and is associated with atypical timing and coordination in neural circuits governing speech planning and execution&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conture and Wolk, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib14\" title=\"\">1990</a>; Bloodstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nAphasia differs from the above as it primarily involves linguistic and cognitive deficits in language production and comprehension&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ellis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib22\" title=\"\">1983</a>; Wiener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib63\" title=\"\">2004</a>; Damasio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib16\" title=\"\">1992</a>)</cite>. Although articulation and prosody may remain intact, the semantic coherence of speech is disrupted, making it difficult for listeners to grasp the intended meaning.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "aphasia",
                    "stuttering",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Assistive Tools for Speech Impairments.</span>\nResearch in speech and language technologies has long aimed to assist individuals with communication difficulties. Traditional tools largely rely on rule-based correction, including speech therapy software&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shipley and McAfee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib54\" title=\"\">2023</a>)</cite>, augmentative and alternative communication (AAC) devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beukelman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib9\" title=\"\">1998</a>; Schlosser and Wendt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib49\" title=\"\">2008</a>)</cite>, and computer-assisted language learning (CALL) platforms&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chapelle and Chapelle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib12\" title=\"\">2001</a>)</cite>. While useful within their respective domains, these systems operate on fixed rules or phrase banks, offering predictable corrections but limited adaptability. Because they depend on explicit error&#8211;correction mappings, such approaches struggle with the variability and ambiguity of spontaneous or impaired speech. In clinical contexts, technological supports have primarily targeted rehabilitation rather than real-time conversational repair. Programs for aphasia focus on relearning vocabulary and syntax&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>, while stuttering management systems provide pacing or fluency feedback&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez and Stoeckle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib41\" title=\"\">2016</a>)</cite>. Other interactive therapy tools use monitoring, feedback, or gamified exercises to motivate patients&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Rodr&#237;guez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib47\" title=\"\">2008</a>)</cite>. Although effective in structured therapeutic environments, these systems are not designed to adapt flexibly to the open-ended nature of everyday communication.</p>\n\n",
                "matched_terms": [
                    "aphasia",
                    "impaired",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem of speech refinement can be formally defined as finding a transformation <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> that maps an impaired speech <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> to a refined version <math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>, guided by the speaker&#8217;s latent communicative intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. Since the true intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> is unobservable, we introduce a latent variable <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m6\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> to represent the model&#8217;s internal understanding of the speaker&#8217;s intent. <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m7\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> serves as an implicit cognitive state formed by the LLM&#8217;s reasoning process, capturing semantic and pragmatic cues that guide refinement.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "refined",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"p(S^{\\prime}\\mid S,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>S</mi><mo>&#8242;</mo></msup><mo>&#8739;</mo><mrow><mi>S</mi><mo>,</mo><mi>Z</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(S^{\\prime}\\mid S,Z)</annotation></semantics></math> models the refinement process conditioned on both the impaired input <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and the latent intention state <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m3\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>. In essence, <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m4\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> acts as the LLM&#8217;s hidden conceptual representation of what the speaker <em class=\"ltx_emph ltx_font_italic\">means</em>, serving as an internal bridge between acoustic evidence and communicative intent that guides the generation of <math alttext=\"S^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m5\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\ast}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> follows a structured workflow to refine impaired speech. Specifically, given an impaired speech input <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, the system first identifies the speech impairment class <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. The signal is then transcribed into text <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. At this stage, the raw transcript <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> may deviate significantly from the true communicative intent <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>.\nWe therefore model intent inference as a conditional distribution <math alttext=\"P(I\\mid T,C)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mrow><mi>T</mi><mo>,</mo><mi>C</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(I\\mid T,C)</annotation></semantics></math>, where the LLM leverages both the noisy transcript and the impairment class to approximate the underlying intent. Based on the inferred intent, the system generates a refined transcript <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>, modeled as <math alttext=\"P(T^{\\prime}\\mid I,C)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&#8739;</mo><mrow><mi>I</mi><mo>,</mo><mi>C</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(T^{\\prime}\\mid I,C)</annotation></semantics></math>, such that <math alttext=\"T^{\\prime}\\approx I\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&#8776;</mo><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}\\approx I</annotation></semantics></math>. Finally, the refined transcript <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted back into speech <math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>, yielding a refined speech that is easier for listeners to understand the intention. Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#alg1\" title=\"Algorithm 1 &#8227; 4.1. Framework Overview &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall workflow.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "class",
                    "model",
                    "impairment",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain a fixed-length speech-level embedding, we apply a pooling operation over the temporal dimension, <math alttext=\"h=\\mathrm{pool}(H)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mrow><mi>pool</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h=\\mathrm{pool}(H)</annotation></semantics></math>\nwhere <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m10\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> denotes the pooled embedding and <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m11\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> is the hidden dimensionality. In practice, <math alttext=\"\\mathrm{Pool}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>Pool</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Pool}(\\cdot)</annotation></semantics></math> can be implemented as mean pooling or attention-based pooling over time. This speech-level embedding is then projected into a classification logit:</p>\n\n",
                "matched_terms": [
                    "over",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|C|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|C|</annotation></semantics></math> is the number of speech impairment classes. The logits <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m14\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> are converted into a probability distribution over impairment categories using the softmax function:</p>\n\n",
                "matched_terms": [
                    "over",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The predicted class is then given by <math alttext=\"\\hat{y}=\\arg\\max_{y\\in C}p(y\\mid\\mathrm{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><msub><mi>max</mi><mrow><mi>y</mi><mo>&#8712;</mo><mi>C</mi></mrow></msub><mo lspace=\"0.167em\">&#8289;</mo><mi>p</mi></mrow></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8739;</mo><mi mathvariant=\"normal\">S</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{y}=\\arg\\max_{y\\in C}p(y\\mid\\mathrm{S})</annotation></semantics></math>, corresponding to the most likely class of speech impairment.</p>\n\n",
                "matched_terms": [
                    "class",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the impaired text <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> transcribed by the ASR model and the recognized class of speech impairment <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>, the next step is to enhance its clarity and coherence through a LLM refinement model. This model directly rewrites the transcription into a more fluent and comprehensible form, conditioned on the impairment type <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. Formally, the refinement can be expressed as :</p>\n\n",
                "matched_terms": [
                    "class",
                    "model",
                    "impairment",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> denotes the refined text. The prompt used for this refinement is presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#A4\" title=\"Appendix D Prompt Design &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Then, the refined text <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted to natural speech <math alttext=\"S^{\\prime}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>S</mi><mo>&#8242;</mo></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">S^{\\prime}\\in\\mathbb{R}^{L}</annotation></semantics></math>. The TTS model provides the agent with the ability to &#8220;speak,&#8221; completing the full reasoning and acting&#160;(ReACT) pipeline. The refined text <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted into a phoneme sequence through a grapheme-to-phoneme conversion function <math alttext=\"\\mathrm{G2P}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mi>G2P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{G2P}(\\cdot)</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "refined",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"X\\in\\mathbb{R}^{d\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>d</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{d\\times L}</annotation></semantics></math> denotes the phoneme embedding sequence. We incorporate a speaking style embedding <math alttext=\"Style\\in\\mathbb{R}^{d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi>s</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">Style\\in\\mathbb{R}^{d_{s}}</annotation></semantics></math>, which can be derived from descriptive text prompts, allowing flexible control over the speaking style of generated speech. The TTS model then conditions on both the phoneme embeddings <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and the speaking style embedding <math alttext=\"Style\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">Style</annotation></semantics></math> to generate the speech signal:</p>\n\n",
                "matched_terms": [
                    "over",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> adopts a client&#8211;server architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Berson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib8\" title=\"\">1992</a>)</cite>, as illustrated in Figure 2. The mobile edge device acts as the primary user interface, while computation-intensive models are offloaded to a cloud server hosting large-scale models. All communication occurs through secure RESTful APIs over HTTPS, and the server integrates third-party services for monitoring, analytics, and authentication.</p>\n\n",
                "matched_terms": [
                    "over",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The server executes all computation-intensive models through a modular pipeline of models and services. Incoming speech recordings from the mobile app are first processed by a speech-impairment recognition model, which identifies the type of impairment. Next, an ASR system <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> transcribes the impaired speech into text. The transcribed text is then refined by LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib3\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib58\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib59\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib65\" title=\"\">2025</a>)</cite> that interprets, clarifies, and reformulates the text while preserving semantic fidelity. The refined text is subsequently passed to a TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>, which converts it into natural, fluent speech. The final audio output is transmitted back to the edge device.\nThis server-side pipeline, exposed through RESTful APIs, allows seamless integration with the client application and external third-party APIs.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "model",
                    "all",
                    "impairment",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most prior speech impairment-related research has focused on evaluating narrow, well-defined subtasks. For example, studies on speech recognition for impaired speakers typically report word error rate (WER) as the main metric to measure transcription quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mirella, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib38\" title=\"\">2024</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib56\" title=\"\">2024</a>)</cite>. Similarly, research on speech impairment detection or classification often relies on standard accuracy or F1 scores to assess model performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheikh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib52\" title=\"\">2021</a>)</cite>. While these task-specific evaluations provide valuable insights, they do not capture the broader goal of refining impaired speech into a clearer and more communicative form. To systematically benchmark our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>, we design an evaluation framework that integrates both speech and text datasets, rigorous preprocessing, and multidimensional performance analysis. Our goal is to capture not only the technical accuracy of impairment recognition and text refinement, but also the practical usability of the system in real-world applications.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "model",
                    "evaluation",
                    "impairment",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two categories of datasets to evaluate the performance of the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>. First is the impaired speech dataset. To capture diverse speech impairments, we combine multiple sources. For dysarthria, we select 21 hours of English speech from the TORGO dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rudzicz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib48\" title=\"\">2012</a>)</cite>, which contains eight speakers with dysarthria and seven healthy speakers. For stuttering, we select the UCLASS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Howell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib27\" title=\"\">2009</a>)</cite>, which contains recordings of speakers with stutter. For aphasia, we select speech samples from AphasiaBank&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MacWhinney et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib36\" title=\"\">2011</a>)</cite>. Additionally, we adopt a pure text dataset, on top of the impaired speech dataset, we select the SNIPS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite>, a task-oriented text dataset related to common daily-life commands such as weather, transport, reminders, and music. These texts are representative of typical user intents in conversational AI applications and thus provide a practical foundation for evaluating the robustness of the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> in refining the impaired speech.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "aphasia",
                    "impaired",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although numerous studies investigate the automatic recognition of dysarthria&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aboeitta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib2\" title=\"\">2025</a>)</cite>, stuttering&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheikh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib52\" title=\"\">2021</a>)</cite>, and aphasia&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Azevedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib6\" title=\"\">2024</a>)</cite>, most approaches treat the task as a binary classification problem, detecting only a single impairment type at a time. Our goal is to equip the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> with the ability to recognize multiple impairments simultaneously, which naturally formulates the task as multi-class classification. Due to there is no suitable open-source dataset or model for this setting, we construct our own dataset and train a deep learning model that takes spectrograms as input to classify different types of speech impairment. Specifically, we randomly select <span class=\"ltx_text ltx_font_bold\">500</span> speech samples from each type of speech impairment (dysarthria, stutter, aphasia, and health), and form a dataset with a total of <span class=\"ltx_text ltx_font_bold\">2,000</span> speech samples. The selected speech samples range from 2 to 15 seconds in duration and are resampled to 16 kHz. Each sample is converted into a Mel-spectrogram using a hop size of 256 with a window size of 1,024. The resulting Mel-Spectrogram has approximately <span class=\"ltx_text ltx_font_bold\">63</span> frames per second, where each frame corresponds to approximately <span class=\"ltx_text ltx_font_bold\">16ms</span> of speech signal.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "aphasia",
                    "stuttering",
                    "impairment",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system across three dimensions to comprehensively assess both its technical performance and practical utility. Specifically, <span class=\"ltx_text ltx_font_bold\">recognition accuracy</span>, which we evaluate whether the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can correctly identify different types of speech impairments; <span class=\"ltx_text ltx_font_bold\">refinement performance</span>, which we evaluate whether the refined speech by LLM is clear and easy to understand; and <span class=\"ltx_text ltx_font_bold\">latency</span>, which we evaluate whether the communication between the user and the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> is sufficiently fast and seamless to support everyday interaction.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "refined",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the classification task, we randomly select 10% of the speech samples from each impairment class as the test set and use the remaining 90% for training. A deep learning model is then trained on this dataset, and we report performance in terms of class-level <span class=\"ltx_text ltx_font_bold\">accuracy</span>, <span class=\"ltx_text ltx_font_bold\">F1-score</span>, and <span class=\"ltx_text ltx_font_bold\">AUC</span>.\nThese metrics are standard metrics for evaluating classification models, and are widely applied in domains such as facial recognition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taigman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib57\" title=\"\">2014</a>)</cite> and image classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib18\" title=\"\">2009</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "class",
                    "standard",
                    "model",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the refinement evaluation, we conduct two types of experiments. The first is a text-based evaluation. We use the text from the SNIPS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite> dataset as the ground-truth intention and simulate impaired text for each type of speech impairment, using the original text as the reference. The impaired text is then passed through the LLM refinement algorithm. After refinement, we compute semantic similarity between the refined text and the original intention using <span class=\"ltx_text ltx_font_bold\">BERT</span> score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib66\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">cosine score</span>, and <span class=\"ltx_text ltx_font_bold\">BLEU</span> score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib40\" title=\"\">2002</a>)</cite>. These metrics are widely used to evaluate the quality of language model outputs, including tasks such as machine translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib60\" title=\"\">2017</a>; Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib40\" title=\"\">2002</a>)</cite> and text generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib66\" title=\"\">2019</a>)</cite>. They measure the similarity between pairs of text; in our context, a higher similarity between the refined text and the original intention indicates that the refined text is clearer to be understood and closer to the true intention, and thus the refinement is more effective.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "similarity",
                    "score",
                    "textbased",
                    "model",
                    "evaluation",
                    "impairment",
                    "refinement",
                    "bert",
                    "impaired",
                    "cosine",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second experiment is a speech-based evaluation. We feed the impaired speech samples into an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> to obtain transcriptions, which are then processed by the LLM refinement cycle.\nThe refined text is subsequently converted back into speech. Since no ground-truth intention is available for the impaired speech samples, we instead conduct a human evaluation. Several human participants are presented with a shuffled list of impaired and refined speech and asked to rate the clarity of the speech and CMOS (Comparison Mean Opinion Score), where we provide the original impaired speech and refined speech to human listeners to judge which one is better. These subjective evaluation methods are widely adopted in the speech generation community for assessing the perceptual quality of TTS models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib62\" title=\"\">2017</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib53\" title=\"\">2018</a>; Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib46\" title=\"\">2019</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib33\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>. In our context, they serve as an effective metric to indicate the overall quality and intelligibility of the refined speech. Additionally, alongside subjective evaluation, we conduct objective evaluation using our trained SIR model. Specifically, we measure the proportion of successfully recovered samples, where a recovery is defined as the SIR model classifying the refined speech as healthy. Lastly, we evaluate the latency of our system to assess its practicality for real-world applications. We build a simulation environment where a edge device communicates with the server to complete the full interaction loop. This process includes transmitting the speech signal, recognizing the impairment type and transcribing the speech, refining the text, generating the refined speech, and transmitting the output back to the user&#8217;s device. We measure the average end-to-end response time across multiple trials to determine whether the system operates seamlessly and efficiently enough to support everyday user interaction.</p>\n\n",
                "matched_terms": [
                    "score",
                    "refined",
                    "model",
                    "mean",
                    "evaluation",
                    "impairment",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RO1</span>: Can the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> accurately recognize the class of speech impairment?</p>\n\n",
                "matched_terms": [
                    "class",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S6.T1\" title=\"Table 1 &#8227; 6.1. Dataset &amp; Reprocessing &#8227; 6. Benchmarking and Evaluation &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the performance of our speech recognition model. Since there is a lack of prior work directly addressing this multi-modal classification task, we benchmark our approach against two open-source baselines: CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib21\" title=\"\">2023</a>)</cite> and ParaCLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib30\" title=\"\">2024</a>)</cite>, both general-purpose language&#8211;audio alignment models. For these baselines, we follow a text&#8211;audio similarity approach to make classification. Each speech sample is compared against candidate text prompts of the form <span class=\"ltx_text ltx_font_italic\">&#8220;a speech from [dysarthria, stutter, aphasia, healthy] speaker&#8221;</span>. We then compute the similarity between the speech embedding and each candidate text embedding, and assign the label corresponding to the most similar text.\nAdditionally, we explore an alternative ASR+LLM baseline. In this approach, the impaired speech is first transcribed using an ASR model, and the resulting transcript is then passed to an LLM, which judges and classifies the impaired speech into one of the speech impairment classes.\nIn contrast, for the CNN+LSTM and Transformer approaches, we directly train the models to encode Mel-Spectrograms into speech embeddings, which are then optimized using the method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S4.SS2\" title=\"4.2. Speech Impairment Recognition &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. A linear classification layer is subsequently applied to predict the speaker condition label from these embeddings. Our trained SIR models will be released upon paper acceptance to encourage reproduction.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "dysarthria",
                    "model",
                    "aphasia",
                    "impairment",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our trained SIR models (CNN+LSTM and Transformer) consistently outperform the baseline methods across all impairment classes. The Transformer model, in particular, achieves nearly perfect recognition with average recognition accuracy over 90% and ROC-AUC values exceeding 0.99. This superior performance can be attributed to its ability to directly learn acoustic&#8211;temporal representations from Mel-Spectrograms, allowing it to capture subtle articulatory and prosodic cues that characterize different impairments. The end-to-end optimization ensures that the learned embeddings are well aligned with the classification objective, resulting in robust and generalizable recognition. These findings indicate that specialized SIR models are essential to achieve reliable classification, as they are better suited to handle the nuanced variability in pathological speech compared to general-purpose alignment models.</p>\n\n",
                "matched_terms": [
                    "over",
                    "model",
                    "all",
                    "impairment",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the baseline approaches exhibit clear limitations. CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib21\" title=\"\">2023</a>)</cite> and ParaCLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib30\" title=\"\">2024</a>)</cite>, while effective in audio-text alignment tasks, struggle in SIR because they were not trained to discriminate pathological speech conditions; their embeddings focus on broad semantic alignment rather than fine-grained acoustic features. Similarly, the ASR+LLM baseline shows moderate improvements but is constrained by the error compounding of two components: ASR mis-transcriptions of impaired speech and the LLM&#8217;s reliance on textual cues alone, which discard essential acoustic markers of impairment. Together, these results provide strong evidence that dedicated acoustic models are necessary for accurate SIR. Our experimental results demonstrate that the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can accurately recognize speech impairments, which answers <span class=\"ltx_text ltx_font_bold\">RQ1</span>.</p>\n\n",
                "matched_terms": [
                    "impaired",
                    "results",
                    "impairment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T3\" title=\"Table 3 &#8227; 7.2. Refinement from Impaired Speech &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the speech-based evaluation for refinement of impaired speech. In this experiment, we evaluate the ability of the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> to refine the impaired speech in an end-to-end manner. Given an impaired speech <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p3.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, we pass it through the entire <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> and let the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> refine and produce the refined speech&#160;<math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p3.m2\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>.\nPurely acoustic-based baselines&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>; Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite>, which focus on signal-level modifications such as repetition removal or silence trimming, achieve only marginal or even degraded performance compared to the original impaired speech. This decline arises because these approaches overlook semantic consistency. The impaired speech signal is already challenging to interpret, and further acoustic alterations often distort meaning and naturalness. Text-based baselines employing grammar correction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>; Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite> perform more competitively, particularly for dysarthric speech, where linguistic structure remains intact but articulation is weak. In such cases, accurate transcription alone (ASR + TTS) already yields speech of comparable clarity to our full agent, indicating that dysarthric speech primarily suffer from articulatory, not semantic, impairment. In contrast, grammar correction performs poorly on stuttering and aphasia, as these impairments introduce disruptions or omissions at the semantic level that rule-based correction cannot resolve. LLMs, however, can reconstruct the underlying intent beyond syntactic adjustment, producing refined text that is both fluent and semantically easy to be understood. These performance gains are particularly evident in the stuttering and aphasia scenarios, where the LLM effectively removes repetitions, restores disrupted sentence structures, and reconstructs missing or fragmented content to improve overall clarity. Taken together, these findings confirm that the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> is effective at enhancing the clarity of impaired speech, thereby directly addressing <span class=\"ltx_text ltx_font_bold\">RQ2</span>.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "damodaran",
                    "textbased",
                    "evaluation",
                    "aphasia",
                    "stuttering",
                    "impairment",
                    "refinement",
                    "impaired",
                    "naber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we evaluate the efficiency and latency of the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>.\nWe report both <span class=\"ltx_text ltx_font_italic\">refinement latency</span> (comparison between different LLMs to complete a refinement) and <span class=\"ltx_text ltx_font_italic\">model-wise latency</span> (ASR, SIR, Speech refinement, and TTS). Our experimental result is presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.F4\" title=\"Figure 4 &#8227; 7.3. Efficiency Analysis &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.F3\" title=\"Figure 3 &#8227; 7.3. Efficiency Analysis &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The ASR and LLM models account for most of the inference time, with the speech recognition model taking <span class=\"ltx_text ltx_font_bold\">38.5%</span> and the LLM refinement model taking <span class=\"ltx_text ltx_font_bold\">47.3%</span>. Nevertheless, the overall speech refinement pipeline achieves near real-time performance across all components. On average, refining a sentence of approximately 11.5 seconds in duration requires only <span class=\"ltx_text ltx_font_bold\">0.91 seconds</span> of processing time, yielding a real-time factor (RTF) of <span class=\"ltx_text ltx_font_bold\">0.08</span>, far below the real-time threshold (RTF = 1). It keeps the latency well within the acceptable range for natural human communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jacoby et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib29\" title=\"\">2024</a>)</cite>. This indicates that the proposed approach is practically viable for real-world use, enabling fluid and responsive interaction. Consequently, the efficiency evaluation provides a clear answer to <span class=\"ltx_text ltx_font_bold\">RQ3</span>, demonstrating that the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can reliably meet the latency requirements of everyday conversations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "evaluation",
                    "refinement",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented a mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that integrates the reasoning capabilities of LLMs with advanced speech processing to enable real-time assistive communication for individuals with speech impairments. Our system demonstrates that low-latency and practically deployable assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can be realized through a carefully designed client&#8211;server architecture. By combining impairment recognition, adaptive refinement, and seamless mobile integration, the agent delivers personalized, intelligible, and context-aware communication support. Evaluation on real-world data confirms both technical robustness and positive user experience, suggesting that the proposed approach is feasible and effective in everyday use. Looking ahead, several directions remain open for exploration. While the current system performs reliably, occasional hallucinations from the language model can lead to subtle deviations from the speaker&#8217;s intention. Future work could focus on enhancing semantic consistency and intent preservation. Moreover, the present agent primarily serves as a passive assistant that refines impaired speech into clearer expressions. Extending its capability toward a more interactive, training-oriented role could transform it into an adaptive partner that helps users practice, monitor, and improve their speech over time. Future research may also explore multilingual and personalized extensions, enabling the agent to support diverse linguistic, emotional, and contextual variations while maintaining natural and trustworthy communication.</p>\n\n",
                "matched_terms": [
                    "over",
                    "model",
                    "evaluation",
                    "impairment",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the perceptual evaluation protocol for assessing refined speech quality. Two complementary rating schemes were used to capture absolute and relative listener judgments. The Clarity (1&#8211;5) score measures overall intelligibility, considering pronunciation, fluency, and ease of understanding. The Comparison Mean Opinion Score (C-MOS, &#8211;3 to +3) assesses the perceived improvement from the original to the refined speech, reflecting how effectively the refinement enhances clarity and fluency.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "score",
                    "mean",
                    "evaluation",
                    "refinement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison Mean Opinion Score (C-MOS, &#8211;3 to +3)</span>\nIn this task, you will be presented with two speech samples. Please listen carefully to both and rate how the refined speech compares to the original.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "refined",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This prompt defines how the large language model (LLM) performs speech refinement for impaired speech in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S4.SS4\" title=\"4.4. Speech Refinement &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>. Two variants are used: one without explicit impairment context and another conditioned on the detected impairment type. In both cases, the LLM reconstructs incomplete or unclear sentences into coherent and fluent expressions while preserving the speaker&#8217;s original intent. When an impairment class is provided, additional contextual cues (e.g., slurred articulation in dysarthria, repetition in stuttering, or lexical omission in aphasia) guide the refinement process, enabling the LLM to adapt its rewriting strategy to different impairment patterns.</p>\n\n",
                "matched_terms": [
                    "without",
                    "class",
                    "dysarthria",
                    "model",
                    "aphasia",
                    "stuttering",
                    "impairment",
                    "refinement",
                    "different",
                    "impaired"
                ]
            }
        ]
    },
    "S7.T3": {
        "caption": "Table 3. Speech-based refinement evaluation. Human participants rate clarity (Likert scale 1–5) and comparison mean opinion score (C-MOS) between impaired and refined speech.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Dysarthria</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Stuttering</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Aphasia</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Clarity</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">C-MOS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Recover</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Clarity</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">C-MOS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Recover</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Clarity</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">C-MOS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Recover</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Impaired</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.83</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.0</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">2.67</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.0</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.83</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.0</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">ASR + TTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">AutomaticCorrection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">1.17</td>\n<td class=\"ltx_td ltx_align_center\">-1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">10.0</td>\n<td class=\"ltx_td ltx_align_center\">1.33</td>\n<td class=\"ltx_td ltx_align_center\">-0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">1.67</td>\n<td class=\"ltx_td ltx_align_center\">-0.67</td>\n<td class=\"ltx_td ltx_align_center\">10.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CleanVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">1.50</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">2.50</td>\n<td class=\"ltx_td ltx_align_center\">-0.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">1.67</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LanguageTool&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">4.50</td>\n<td class=\"ltx_td ltx_align_center\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">50.0</td>\n<td class=\"ltx_td ltx_align_center\">4.17</td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">10.0</td>\n<td class=\"ltx_td ltx_align_center\">3.67</td>\n<td class=\"ltx_td ltx_align_center\">1.17</td>\n<td class=\"ltx_td ltx_align_center\">20.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Gramformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">4.83</td>\n<td class=\"ltx_td ltx_align_center\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">50.0</td>\n<td class=\"ltx_td ltx_align_center\">4.17</td>\n<td class=\"ltx_td ltx_align_center\">1.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n<td class=\"ltx_td ltx_align_center\">1.33</td>\n<td class=\"ltx_td ltx_align_center\">40.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">SpeechAgent</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">5.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">60.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">10.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">2.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">40.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "1–5",
            "refined",
            "evaluation",
            "opinion",
            "stuttering",
            "comparison",
            "cleanvoice",
            "participants",
            "speechagent",
            "tts",
            "rate",
            "aphasia",
            "between",
            "likert",
            "speechbased",
            "recover",
            "naber",
            "clarity",
            "scale",
            "dysarthria",
            "automaticcorrection",
            "gramformer",
            "mean",
            "languagetool",
            "asr",
            "speech",
            "refinement",
            "cmos",
            "score",
            "damodaran",
            "model",
            "human",
            "arjun",
            "impaired"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T3\" title=\"Table 3 &#8227; 7.2. Refinement from Impaired Speech &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the speech-based evaluation for refinement of impaired speech. In this experiment, we evaluate the ability of the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> to refine the impaired speech in an end-to-end manner. Given an impaired speech <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p3.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, we pass it through the entire <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> and let the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> refine and produce the refined speech&#160;<math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p3.m2\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>.\nPurely acoustic-based baselines&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>; Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite>, which focus on signal-level modifications such as repetition removal or silence trimming, achieve only marginal or even degraded performance compared to the original impaired speech. This decline arises because these approaches overlook semantic consistency. The impaired speech signal is already challenging to interpret, and further acoustic alterations often distort meaning and naturalness. Text-based baselines employing grammar correction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>; Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite> perform more competitively, particularly for dysarthric speech, where linguistic structure remains intact but articulation is weak. In such cases, accurate transcription alone (ASR + TTS) already yields speech of comparable clarity to our full agent, indicating that dysarthric speech primarily suffer from articulatory, not semantic, impairment. In contrast, grammar correction performs poorly on stuttering and aphasia, as these impairments introduce disruptions or omissions at the semantic level that rule-based correction cannot resolve. LLMs, however, can reconstruct the underlying intent beyond syntactic adjustment, producing refined text that is both fluent and semantically easy to be understood. These performance gains are particularly evident in the stuttering and aphasia scenarios, where the LLM effectively removes repetitions, restores disrupted sentence structures, and reconstructs missing or fragmented content to improve overall clarity. Taken together, these findings confirm that the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> is effective at enhancing the clarity of impaired speech, thereby directly addressing <span class=\"ltx_text ltx_font_bold\">RQ2</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech is essential for human communication, yet millions of people face impairments such as dysarthria, stuttering, and aphasia&#8212;conditions that often lead to social isolation and reduced participation.\nDespite recent progress in automatic speech recognition (ASR) and text-to-speech (TTS) technologies, accessible web and mobile infrastructures for users with impaired speech remain limited, hindering the practical adoption of these advances in daily communication.\nTo bridge this gap, we present <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>,\na mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> designed to facilitate people with speech impairments in everyday communication.\nThe system integrates large language model (LLM)&#8211;driven reasoning with advanced speech processing modules, providing adaptive support tailored to diverse impairment types.\nTo ensure real-world practicality, we develop a structured deployment pipeline that enables real-time speech processing on mobile and edge devices, achieving imperceptible latency while maintaining high accuracy and speech quality.\nEvaluation on real-world impaired speech datasets and edge-device latency profiling confirms that SpeechAgent delivers both effective and user-friendly performance, demonstrating its feasibility for personalized, day-to-day assistive communication.\nOur code, dataset, and speech samples are publicly available<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://anonymous.4open.science/r/SpeechAgentDemo-48EE/</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "tts",
                    "dysarthria",
                    "model",
                    "evaluation",
                    "human",
                    "stuttering",
                    "asr",
                    "speech",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, not everyone can express themselves fluently through speech. Many individuals experience impairments that make their speech fragmented, unclear, or difficult to follow. Such challenges often arise from clinical speech and language disorders that directly affect articulation, fluency, or coherence <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamaguchi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib26\" title=\"\">2010</a>; Eichorn and Fabus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib20\" title=\"\">2024</a>)</cite>.\nAmong the most prevalent are <span class=\"ltx_text ltx_font_italic\">(i) dysarthria</span>, which arises from motor impairments and results in slurred or slow speech due to reduced control of the articulatory muscles&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enderby, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib23\" title=\"\">2013</a>)</cite>; <span class=\"ltx_text ltx_font_italic\">(ii) stuttering</span>, characterized by involuntary repetitions or prolongations that disrupt the natural flow of speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Prasse and Kikano, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib43\" title=\"\">2008</a>)</cite>; and <span class=\"ltx_text ltx_font_italic\">(iii) aphasia</span>, typically caused by neurological damage such as stroke, which impairs a person&#8217;s ability to produce or comprehend language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damasio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib16\" title=\"\">1992</a>; Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "aphasia",
                    "speech",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These disorders extend beyond the clinical symptoms, shaping how people interact and are perceived in everyday communication.\nIndividuals with speech impairments often struggle to make themselves understood, facing frequent communication breakdowns that result in misunderstandings, interruptions, or dismissals. Such experiences can be discouraging and might,\nover time, restrict participation in social, educational, and professional contexts,\nleading to isolation and reduced quality of life&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McCormack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib37\" title=\"\">2010</a>; Bashir and Scavuzzo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib7\" title=\"\">1992</a>)</cite>. Addressing such barriers is therefore essential. This motivates the design of our <span class=\"ltx_text ltx_font_bold\">mobile SpeechAgent</span>, designed to assist individuals with dysarthria, stuttering, and aphasia in achieving clearer and more effective communication.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "dysarthria",
                    "aphasia",
                    "stuttering",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research in speech and language technologies has aimed to support individuals with speech impairment. Traditional approaches include augmentative and alternative communication (AAC) devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beukelman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib9\" title=\"\">1998</a>; Schlosser and Wendt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib49\" title=\"\">2008</a>)</cite>, rule-based grammar correction systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sidorov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib55\" title=\"\">2013</a>; Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>, and computer-assisted therapy programs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Chapelle and Chapelle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib12\" title=\"\">2001</a>)</cite>.\nWhile these tools have provided valuable support in structured clinical or educational domains, they are often rigid and rely heavily on predefined templates or rule mappings. Such designs perform well for predictable, domain-limited input, but they struggle with the fragmented, ambiguous, and dynamic pattern of real-world speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In clinical environments, technological interventions have typically emphasized long-term rehabilitation rather than live conversational repair. For instance, computer-mediated aphasia programs focus on vocabulary relearning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>, while stuttering management tools provide pacing strategies or fluency feedback&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez and Stoeckle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib41\" title=\"\">2016</a>)</cite>. Similarly, interactive therapy software often motivates patients with corrective prompts or gamified exercises&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Rodr&#237;guez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib47\" title=\"\">2008</a>)</cite>. Although these methods are effective in structured therapeutic environments, they are not designed to adapt flexibly to open-ended, everyday usage as well.\nRecent research has explored\ntraining automatic speech recognition (ASR) models to better transcribe impaired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vinotha et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib61\" title=\"\">2024</a>; Lea et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib32\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib28\" title=\"\">2025</a>)</cite>, or developing detection system and classifying different types of speech impairments&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sekhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib51\" title=\"\">2022</a>; Al-Banna et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib4\" title=\"\">2022</a>)</cite>. While these approaches improve the ability to recognize or analyze disordered speech, they are primarily diagnostic in nature. They do not provide direct communicative assistance to the user in real time, nor do they help transform impaired speech into a clearer, more comprehensible form during everyday interaction.\nTo the best of our knowledge, there is currently no assistive communication tool that offers real-time refinement of impaired speech for practical, everyday communication.\nTo address this gap, we introduce a mobile assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that functions as an assistive tool for spontaneous communication. The system refines impaired speech into a clearer form that preserves the speaker&#8217;s intention, allowing users to communicate easily in everyday conversations.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "aphasia",
                    "asr",
                    "stuttering",
                    "speech",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> advances beyond traditional speech-impairment refinement pipelines<cite class=\"ltx_cite ltx_citemacro_citep\">(Cleanvoice AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib13\" title=\"\">2025</a>; Arjun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib5\" title=\"\">2020</a>)</cite> by integrating perception, reasoning, and generation into a unified agent framework.\nIt combines open-sourced and custom-trained models to not only recognize disordered speech but also understand and restore the speaker&#8217;s communicative intention. It contains a robust, general-purpose ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> transcribes speech reliably across diverse acoustic conditions, while a dedicated impairment recognition model captures speaker-specific disorder patterns, enabling adaptive, context-aware processing for downstream refinement. Building upon these perceptual foundations, the agent employs large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib3\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib58\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib65\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib59\" title=\"\">2025</a>)</cite> as cognitive refinement engines that reason about the meaning behind fragmented, ambiguous, or disfluent utterances.\nRather than performing surface-level grammatical correction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>; Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>, the LLM reconstructs semantically coherent and faithful text that preserves the speaker&#8217;s intent.\nFinally, a natural and expressive text-to-speech (TTS) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>; Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib45\" title=\"\">2020</a>)</cite> vocalizes the refined text, completing a closed perception&#8211;reasoning generation loop that transforms impaired speech into clear, intelligible, and easy to understand speech.\nThe main contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "refined",
                    "speechagent",
                    "damodaran",
                    "tts",
                    "model",
                    "asr",
                    "speech",
                    "refinement",
                    "arjun",
                    "impaired",
                    "naber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that leverages LLM-based reasoning and advanced speech techniques to support users with speech impairments in everyday communication.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend the system with mechanisms to recognize different types of speech impairments and adapt the agent&#8217;s refinement strategies accordingly.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "refinement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We release a benchmark suite and evaluation pipeline for impaired-speech systems, combining both system-level (latency, throughput, scalability) and user-level (clarity, usability) metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "clarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Impairments.</span>\nSpeech impairments arise from diverse neurological, developmental, or physiological conditions and encompass a wide range of disorders affecting clarity, fluency, and intelligibility. This work focuses on three representative types: dysarthria, stuttering, and aphasia. Dysarthria is an acoustic impairment in which speech sounds are slurred, strained, or unusually slow due to weakened motor control of articulatory muscles, often resulting from stroke, cerebral palsy, traumatic brain injury, or degenerative diseases such as Parkinson&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Enderby, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib23\" title=\"\">2013</a>; Pinto et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib42\" title=\"\">2004</a>)</cite>.\nStuttering, by contrast, disrupts speech rhythm through involuntary repetitions (e.g., &#8220;b-b-b-book&#8221;), prolongations (&#8220;ssssun&#8221;), or silent blocks, producing hesitation and tension perceptible even in short utterances. It has both developmental and neurological origins and is associated with atypical timing and coordination in neural circuits governing speech planning and execution&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conture and Wolk, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib14\" title=\"\">1990</a>; Bloodstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nAphasia differs from the above as it primarily involves linguistic and cognitive deficits in language production and comprehension&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ellis et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib22\" title=\"\">1983</a>; Wiener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib63\" title=\"\">2004</a>; Damasio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib16\" title=\"\">1992</a>)</cite>. Although articulation and prosody may remain intact, the semantic coherence of speech is disrupted, making it difficult for listeners to grasp the intended meaning.</p>\n\n",
                "matched_terms": [
                    "clarity",
                    "dysarthria",
                    "aphasia",
                    "stuttering",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Assistive Tools for Speech Impairments.</span>\nResearch in speech and language technologies has long aimed to assist individuals with communication difficulties. Traditional tools largely rely on rule-based correction, including speech therapy software&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shipley and McAfee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib54\" title=\"\">2023</a>)</cite>, augmentative and alternative communication (AAC) devices&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Beukelman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib9\" title=\"\">1998</a>; Schlosser and Wendt, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib49\" title=\"\">2008</a>)</cite>, and computer-assisted language learning (CALL) platforms&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chapelle and Chapelle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib12\" title=\"\">2001</a>)</cite>. While useful within their respective domains, these systems operate on fixed rules or phrase banks, offering predictable corrections but limited adaptability. Because they depend on explicit error&#8211;correction mappings, such approaches struggle with the variability and ambiguity of spontaneous or impaired speech. In clinical contexts, technological supports have primarily targeted rehabilitation rather than real-time conversational repair. Programs for aphasia focus on relearning vocabulary and syntax&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Brady et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib11\" title=\"\">2016</a>)</cite>, while stuttering management systems provide pacing or fluency feedback&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Perez and Stoeckle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib41\" title=\"\">2016</a>)</cite>. Other interactive therapy tools use monitoring, feedback, or gamified exercises to motivate patients&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Grossinho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib25\" title=\"\">2014</a>; Rodr&#237;guez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib47\" title=\"\">2008</a>)</cite>. Although effective in structured therapeutic environments, these systems are not designed to adapt flexibly to the open-ended nature of everyday communication.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "aphasia",
                    "impaired",
                    "stuttering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most prior systems either focus on therapeutic training in clinical settings or rely on rigid rule-based correction, which limits adaptability to spontaneous and dynamic communication. As a result, individuals suffering from speech impairment often lack access to tools that can flexibly support them in ordinary conversations, where clarity and intent alignment are most essential.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "clarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human communication can be viewed as the process of expressing an underlying communicative intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> through a speech signal <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>. Formally, the speaker produces speech according to a generative process <math alttext=\"p(S\\mid I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mi>I</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(S\\mid I)</annotation></semantics></math>, while the listener attempts to recover the intention by estimating <math alttext=\"p(I\\mid S)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mi>S</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(I\\mid S)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "human",
                    "recover"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For healthy speakers, the mapping between <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> and <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> is generally straightforward. The speech carries sufficient information and is clear enough for listeners to infer the intention with high accuracy, <math alttext=\"p(I\\mid S)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mi>S</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(I\\mid S)</annotation></semantics></math> is well aligned with the true <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. In contrast, for speakers with speech impairments, who produce speech with <em class=\"ltx_emph ltx_font_italic\">acoustic distortions</em> (e.g., disfluency, articulation errors) or <em class=\"ltx_emph ltx_font_italic\">logical distortions</em> degrade this mapping, making <math alttext=\"p(I\\mid S)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mi>S</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(I\\mid S)</annotation></semantics></math> less reliable and making it difficult for listeners to easily understand <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m6\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> from <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m7\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem of speech refinement can be formally defined as finding a transformation <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> that maps an impaired speech <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> to a refined version <math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>, guided by the speaker&#8217;s latent communicative intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. Since the true intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> is unobservable, we introduce a latent variable <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m6\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> to represent the model&#8217;s internal understanding of the speaker&#8217;s intent. <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m7\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> serves as an implicit cognitive state formed by the LLM&#8217;s reasoning process, capturing semantic and pragmatic cues that guide refinement.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "refinement",
                    "refined",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"p(S^{\\prime}\\mid S,Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>S</mi><mo>&#8242;</mo></msup><mo>&#8739;</mo><mrow><mi>S</mi><mo>,</mo><mi>Z</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(S^{\\prime}\\mid S,Z)</annotation></semantics></math> models the refinement process conditioned on both the impaired input <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and the latent intention state <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m3\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>. In essence, <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m4\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> acts as the LLM&#8217;s hidden conceptual representation of what the speaker <em class=\"ltx_emph ltx_font_italic\">means</em>, serving as an internal bridge between acoustic evidence and communicative intent that guides the generation of <math alttext=\"S^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m5\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\ast}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "refinement",
                    "impaired",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> follows a structured workflow to refine impaired speech. Specifically, given an impaired speech input <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, the system first identifies the speech impairment class <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. The signal is then transcribed into text <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. At this stage, the raw transcript <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> may deviate significantly from the true communicative intent <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>.\nWe therefore model intent inference as a conditional distribution <math alttext=\"P(I\\mid T,C)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>I</mi><mo>&#8739;</mo><mrow><mi>T</mi><mo>,</mo><mi>C</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(I\\mid T,C)</annotation></semantics></math>, where the LLM leverages both the noisy transcript and the impairment class to approximate the underlying intent. Based on the inferred intent, the system generates a refined transcript <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>, modeled as <math alttext=\"P(T^{\\prime}\\mid I,C)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&#8739;</mo><mrow><mi>I</mi><mo>,</mo><mi>C</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(T^{\\prime}\\mid I,C)</annotation></semantics></math>, such that <math alttext=\"T^{\\prime}\\approx I\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&#8776;</mo><mi>I</mi></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}\\approx I</annotation></semantics></math>. Finally, the refined transcript <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted back into speech <math alttext=\"S^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><msup><mi>S</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">S^{\\prime}</annotation></semantics></math>, yielding a refined speech that is easier for listeners to understand the intention. Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#alg1\" title=\"Algorithm 1 &#8227; 4.1. Framework Overview &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall workflow.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "speechagent",
                    "model",
                    "speech",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SIR model gives our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> the ability to understand how speech deviates from healthy patterns. Its goal is to detect impairment-related acoustic or linguistic distortions given a speech input. The input to the SIR model is a waveform <math alttext=\"S\\in\\mathbb{R}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>l</mi></msup></mrow><annotation encoding=\"application/x-tex\">S\\in\\mathbb{R}^{l}</annotation></semantics></math>, represented as a one-dimensional array with <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> time points. The waveform is segmented into overlapping frames and transformed into a log-Mel spectrogram&#160;<math alttext=\"\\mathrm{MelSpec}\\in\\mathbb{R}^{F\\times\\mathcal{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>MelSpec</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathrm{MelSpec}\\in\\mathbb{R}^{F\\times\\mathcal{T}}</annotation></semantics></math>.\n<math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math> is the number of temporal frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> is the number of frequency bins. The Mel-spectrogram is passed through a seq2seq deep neural encoder <math alttext=\"\\mathrm{Encoder}_{\\theta}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>Encoder</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Encoder}_{\\theta}(\\cdot)</annotation></semantics></math>, parameterized by <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>, that processes the input as a sequence of hidden frame-level embeddings <math alttext=\"H_{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>C</mi></msub><annotation encoding=\"application/x-tex\">H_{C}</annotation></semantics></math>.:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechagent",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Automatic Speech Recognition (ASR) model gives our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> the ability to &#8220;listen&#8221; and transcribe speech into text. The input to the ASR model is log-Mel spectrogram&#160;<math alttext=\"\\mathrm{MelSpec}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>MelSpec</mi><annotation encoding=\"application/x-tex\">\\mathrm{MelSpec}</annotation></semantics></math>.\nThe Mel-spectrogram is then passed through an seq2seq encoder network <math alttext=\"\\mathrm{Encoder}_{\\phi}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>Encoder</mi><mi>&#981;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Encoder}_{\\phi}(\\cdot)</annotation></semantics></math>, parameterized by <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#981;</mi><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math>, which maps the input into a sequence of latent embeddings:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechagent",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the impaired text <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> transcribed by the ASR model and the recognized class of speech impairment <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>, the next step is to enhance its clarity and coherence through a LLM refinement model. This model directly rewrites the transcription into a more fluent and comprehensible form, conditioned on the impairment type <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. Formally, the refinement can be expressed as :</p>\n\n",
                "matched_terms": [
                    "clarity",
                    "model",
                    "asr",
                    "speech",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m1\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> denotes the refined text. The prompt used for this refinement is presented in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#A4\" title=\"Appendix D Prompt Design &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>. Then, the refined text <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted to natural speech <math alttext=\"S^{\\prime}\\in\\mathbb{R}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>S</mi><mo>&#8242;</mo></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">S^{\\prime}\\in\\mathbb{R}^{L}</annotation></semantics></math>. The TTS model provides the agent with the ability to &#8220;speak,&#8221; completing the full reasoning and acting&#160;(ReACT) pipeline. The refined text <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is converted into a phoneme sequence through a grapheme-to-phoneme conversion function <math alttext=\"\\mathrm{G2P}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p2.m5\" intent=\":literal\"><semantics><mrow><mi>G2P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{G2P}(\\cdot)</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "refined",
                    "tts",
                    "model",
                    "speech",
                    "refinement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"X\\in\\mathbb{R}^{d\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>d</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{d\\times L}</annotation></semantics></math> denotes the phoneme embedding sequence. We incorporate a speaking style embedding <math alttext=\"Style\\in\\mathbb{R}^{d_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi>s</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">Style\\in\\mathbb{R}^{d_{s}}</annotation></semantics></math>, which can be derived from descriptive text prompts, allowing flexible control over the speaking style of generated speech. The TTS model then conditions on both the phoneme embeddings <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and the speaking style embedding <math alttext=\"Style\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p3.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">Style</annotation></semantics></math> to generate the speech signal:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ ParaStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib34\" title=\"\">2025a</a>)</cite>, a neural text-to-speech architecture that jointly models prosody and paralinguistic style for expressive and natural-sounding speech generation. By directly synthesizing waveforms in an end-to-end manner, this TTS model removes the dependency on intermediate spectrogram representations and external vocoders, thereby enhancing both generation quality and computational efficiency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The server executes all computation-intensive models through a modular pipeline of models and services. Incoming speech recordings from the mobile app are first processed by a speech-impairment recognition model, which identifies the type of impairment. Next, an ASR system <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> transcribes the impaired speech into text. The transcribed text is then refined by LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib3\" title=\"\">2023</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib58\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib59\" title=\"\">2025</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib65\" title=\"\">2025</a>)</cite> that interprets, clarifies, and reformulates the text while preserving semantic fidelity. The refined text is subsequently passed to a TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>, which converts it into natural, fluent speech. The final audio output is transmitted back to the edge device.\nThis server-side pipeline, exposed through RESTful APIs, allows seamless integration with the client application and external third-party APIs.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "tts",
                    "model",
                    "asr",
                    "speech",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most prior speech impairment-related research has focused on evaluating narrow, well-defined subtasks. For example, studies on speech recognition for impaired speakers typically report word error rate (WER) as the main metric to measure transcription quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mirella, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib38\" title=\"\">2024</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib56\" title=\"\">2024</a>)</cite>. Similarly, research on speech impairment detection or classification often relies on standard accuracy or F1 scores to assess model performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheikh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib52\" title=\"\">2021</a>)</cite>. While these task-specific evaluations provide valuable insights, they do not capture the broader goal of refining impaired speech into a clearer and more communicative form. To systematically benchmark our <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>, we design an evaluation framework that integrates both speech and text datasets, rigorous preprocessing, and multidimensional performance analysis. Our goal is to capture not only the technical accuracy of impairment recognition and text refinement, but also the practical usability of the system in real-world applications.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "rate",
                    "model",
                    "evaluation",
                    "speech",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two categories of datasets to evaluate the performance of the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>. First is the impaired speech dataset. To capture diverse speech impairments, we combine multiple sources. For dysarthria, we select 21 hours of English speech from the TORGO dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rudzicz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib48\" title=\"\">2012</a>)</cite>, which contains eight speakers with dysarthria and seven healthy speakers. For stuttering, we select the UCLASS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Howell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib27\" title=\"\">2009</a>)</cite>, which contains recordings of speakers with stutter. For aphasia, we select speech samples from AphasiaBank&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(MacWhinney et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib36\" title=\"\">2011</a>)</cite>. Additionally, we adopt a pure text dataset, on top of the impaired speech dataset, we select the SNIPS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite>, a task-oriented text dataset related to common daily-life commands such as weather, transport, reminders, and music. These texts are representative of typical user intents in conversational AI applications and thus provide a practical foundation for evaluating the robustness of the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> in refining the impaired speech.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "dysarthria",
                    "aphasia",
                    "stuttering",
                    "speech",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although numerous studies investigate the automatic recognition of dysarthria&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aboeitta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib2\" title=\"\">2025</a>)</cite>, stuttering&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sheikh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib52\" title=\"\">2021</a>)</cite>, and aphasia&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Azevedo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib6\" title=\"\">2024</a>)</cite>, most approaches treat the task as a binary classification problem, detecting only a single impairment type at a time. Our goal is to equip the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> with the ability to recognize multiple impairments simultaneously, which naturally formulates the task as multi-class classification. Due to there is no suitable open-source dataset or model for this setting, we construct our own dataset and train a deep learning model that takes spectrograms as input to classify different types of speech impairment. Specifically, we randomly select <span class=\"ltx_text ltx_font_bold\">500</span> speech samples from each type of speech impairment (dysarthria, stutter, aphasia, and health), and form a dataset with a total of <span class=\"ltx_text ltx_font_bold\">2,000</span> speech samples. The selected speech samples range from 2 to 15 seconds in duration and are resampled to 16 kHz. Each sample is converted into a Mel-spectrogram using a hop size of 256 with a window size of 1,024. The resulting Mel-Spectrogram has approximately <span class=\"ltx_text ltx_font_bold\">63</span> frames per second, where each frame corresponds to approximately <span class=\"ltx_text ltx_font_bold\">16ms</span> of speech signal.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "dysarthria",
                    "model",
                    "aphasia",
                    "stuttering",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system across three dimensions to comprehensively assess both its technical performance and practical utility. Specifically, <span class=\"ltx_text ltx_font_bold\">recognition accuracy</span>, which we evaluate whether the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can correctly identify different types of speech impairments; <span class=\"ltx_text ltx_font_bold\">refinement performance</span>, which we evaluate whether the refined speech by LLM is clear and easy to understand; and <span class=\"ltx_text ltx_font_bold\">latency</span>, which we evaluate whether the communication between the user and the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> is sufficiently fast and seamless to support everyday interaction.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "speechagent",
                    "between",
                    "speech",
                    "refinement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the classification task, we randomly select 10% of the speech samples from each impairment class as the test set and use the remaining 90% for training. A deep learning model is then trained on this dataset, and we report performance in terms of class-level <span class=\"ltx_text ltx_font_bold\">accuracy</span>, <span class=\"ltx_text ltx_font_bold\">F1-score</span>, and <span class=\"ltx_text ltx_font_bold\">AUC</span>.\nThese metrics are standard metrics for evaluating classification models, and are widely applied in domains such as facial recognition&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Taigman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib57\" title=\"\">2014</a>)</cite> and image classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib18\" title=\"\">2009</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the refinement evaluation, we conduct two types of experiments. The first is a text-based evaluation. We use the text from the SNIPS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite> dataset as the ground-truth intention and simulate impaired text for each type of speech impairment, using the original text as the reference. The impaired text is then passed through the LLM refinement algorithm. After refinement, we compute semantic similarity between the refined text and the original intention using <span class=\"ltx_text ltx_font_bold\">BERT</span> score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib66\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">cosine score</span>, and <span class=\"ltx_text ltx_font_bold\">BLEU</span> score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib40\" title=\"\">2002</a>)</cite>. These metrics are widely used to evaluate the quality of language model outputs, including tasks such as machine translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib60\" title=\"\">2017</a>; Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib40\" title=\"\">2002</a>)</cite> and text generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib66\" title=\"\">2019</a>)</cite>. They measure the similarity between pairs of text; in our context, a higher similarity between the refined text and the original intention indicates that the refined text is clearer to be understood and closer to the true intention, and thus the refinement is more effective.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "score",
                    "model",
                    "evaluation",
                    "between",
                    "speech",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second experiment is a speech-based evaluation. We feed the impaired speech samples into an ASR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib44\" title=\"\">2023</a>)</cite> to obtain transcriptions, which are then processed by the LLM refinement cycle.\nThe refined text is subsequently converted back into speech. Since no ground-truth intention is available for the impaired speech samples, we instead conduct a human evaluation. Several human participants are presented with a shuffled list of impaired and refined speech and asked to rate the clarity of the speech and CMOS (Comparison Mean Opinion Score), where we provide the original impaired speech and refined speech to human listeners to judge which one is better. These subjective evaluation methods are widely adopted in the speech generation community for assessing the perceptual quality of TTS models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib62\" title=\"\">2017</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib53\" title=\"\">2018</a>; Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib46\" title=\"\">2019</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib33\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib35\" title=\"\">2025b</a>)</cite>. In our context, they serve as an effective metric to indicate the overall quality and intelligibility of the refined speech. Additionally, alongside subjective evaluation, we conduct objective evaluation using our trained SIR model. Specifically, we measure the proportion of successfully recovered samples, where a recovery is defined as the SIR model classifying the refined speech as healthy. Lastly, we evaluate the latency of our system to assess its practicality for real-world applications. We build a simulation environment where a edge device communicates with the server to complete the full interaction loop. This process includes transmitting the speech signal, recognizing the impairment type and transcribing the speech, refining the text, generating the refined speech, and transmitting the output back to the user&#8217;s device. We measure the average end-to-end response time across multiple trials to determine whether the system operates seamlessly and efficiently enough to support everyday user interaction.</p>\n\n",
                "matched_terms": [
                    "participants",
                    "refined",
                    "speechbased",
                    "clarity",
                    "tts",
                    "score",
                    "rate",
                    "model",
                    "mean",
                    "evaluation",
                    "impaired",
                    "opinion",
                    "asr",
                    "human",
                    "speech",
                    "refinement",
                    "comparison",
                    "cmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RO1</span>: Can the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> accurately recognize the class of speech impairment?</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ2</span>: Can the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> effectively refine the impaired speech into a clearer and easier-to-understand form?</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechagent",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S6.T1\" title=\"Table 1 &#8227; 6.1. Dataset &amp; Reprocessing &#8227; 6. Benchmarking and Evaluation &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the performance of our speech recognition model. Since there is a lack of prior work directly addressing this multi-modal classification task, we benchmark our approach against two open-source baselines: CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib21\" title=\"\">2023</a>)</cite> and ParaCLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib30\" title=\"\">2024</a>)</cite>, both general-purpose language&#8211;audio alignment models. For these baselines, we follow a text&#8211;audio similarity approach to make classification. Each speech sample is compared against candidate text prompts of the form <span class=\"ltx_text ltx_font_italic\">&#8220;a speech from [dysarthria, stutter, aphasia, healthy] speaker&#8221;</span>. We then compute the similarity between the speech embedding and each candidate text embedding, and assign the label corresponding to the most similar text.\nAdditionally, we explore an alternative ASR+LLM baseline. In this approach, the impaired speech is first transcribed using an ASR model, and the resulting transcript is then passed to an LLM, which judges and classifies the impaired speech into one of the speech impairment classes.\nIn contrast, for the CNN+LSTM and Transformer approaches, we directly train the models to encode Mel-Spectrograms into speech embeddings, which are then optimized using the method described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S4.SS2\" title=\"4.2. Speech Impairment Recognition &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. A linear classification layer is subsequently applied to predict the speaker condition label from these embeddings. Our trained SIR models will be released upon paper acceptance to encourage reproduction.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "aphasia",
                    "asr",
                    "between",
                    "speech",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our trained SIR models (CNN+LSTM and Transformer) consistently outperform the baseline methods across all impairment classes. The Transformer model, in particular, achieves nearly perfect recognition with average recognition accuracy over 90% and ROC-AUC values exceeding 0.99. This superior performance can be attributed to its ability to directly learn acoustic&#8211;temporal representations from Mel-Spectrograms, allowing it to capture subtle articulatory and prosodic cues that characterize different impairments. The end-to-end optimization ensures that the learned embeddings are well aligned with the classification objective, resulting in robust and generalizable recognition. These findings indicate that specialized SIR models are essential to achieve reliable classification, as they are better suited to handle the nuanced variability in pathological speech compared to general-purpose alignment models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the baseline approaches exhibit clear limitations. CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Elizalde et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib21\" title=\"\">2023</a>)</cite> and ParaCLAP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib30\" title=\"\">2024</a>)</cite>, while effective in audio-text alignment tasks, struggle in SIR because they were not trained to discriminate pathological speech conditions; their embeddings focus on broad semantic alignment rather than fine-grained acoustic features. Similarly, the ASR+LLM baseline shows moderate improvements but is constrained by the error compounding of two components: ASR mis-transcriptions of impaired speech and the LLM&#8217;s reliance on textual cues alone, which discard essential acoustic markers of impairment. Together, these results provide strong evidence that dedicated acoustic models are necessary for accurate SIR. Our experimental results demonstrate that the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can accurately recognize speech impairments, which answers <span class=\"ltx_text ltx_font_bold\">RQ1</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechagent",
                    "impaired",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T2\" title=\"Table 2 &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the text-based evaluation for refining impaired text. In this experiment, we mainly evaluate the ability of LLM to reconstruct the intention from the impaired text. Specifically, given an intention text from the Snips dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Coucke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib15\" title=\"\">2018</a>)</cite>, we first use LLM to synthesize impaired text&#160;<math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> for each type of speech impairment given the intention&#160;<math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> as input (we use GPT-4.1). Then we use different LLMs from various sources to refine the impaired text. Then we compare the semantic similarity between the refined text<math alttext=\"T^{\\prime}=\\mathrm{LLMRefine(T)}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mi>LLMRefine</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">T</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}=\\mathrm{LLMRefine(T)}</annotation></semantics></math>, with the intention <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p1.m4\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>. A higher similarity score indicates better reconstruction of the intention.\nAs a baseline, we include grammar correction systems, which represent a lightweight form of text refinement often used in AAC applications to improve intelligibility. We implement both a rule-based grammar correction system (LanguageTool&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naber et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib39\" title=\"\">2003</a>)</cite>) and a neural grammar correction model (Gramformer, based on&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Damodaran, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib17\" title=\"\">2020</a>)</cite>). These serve as strong baselines for comparison with our speech-refinement pipeline.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "score",
                    "damodaran",
                    "model",
                    "gramformer",
                    "evaluation",
                    "impaired",
                    "languagetool",
                    "between",
                    "speech",
                    "refinement",
                    "comparison",
                    "naber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.T2\" title=\"Table 2 &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> demonstrate that LLMs can effectively refine impaired text to better align with the intention. Across all impairment classes, the refined outputs achieve substantial improvements over the impaired baseline in terms of semantic similarity, as reflected by higher BERT score, BLEU, Cosine Similarity, and IMS values. This confirms that LLM-based refinement is capable of recovering much of the speaker&#8217;s original intention. Among the models evaluated, Gemini 2.5 consistently outperforms other LLMs, achieving the highest refinement performance across all metrics and impairment classes. This indicates stronger robustness in handling diverse impairment patterns. Moreover, incorporating the impairment class (<math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS2.p2.m1\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) into the refinement process yields further gains, particularly in challenging cases such as aphasia, where knowledge of the impairment type provides additional context to guide reconstruction. This aligns with our expectation that specifying the impairment condition would help the model focus on relevant linguistic features and thereby improve similarity to the original intention. Hence, this experiment partially demonstrates <span class=\"ltx_text ltx_font_bold\">RQ2</span>, where the LLM can effectively refine the impaired <span class=\"ltx_text ltx_font_bold\">text</span> to a clearer form that is easier to be understood.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "score",
                    "model",
                    "aphasia",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we evaluate the efficiency and latency of the proposed <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span>.\nWe report both <span class=\"ltx_text ltx_font_italic\">refinement latency</span> (comparison between different LLMs to complete a refinement) and <span class=\"ltx_text ltx_font_italic\">model-wise latency</span> (ASR, SIR, Speech refinement, and TTS). Our experimental result is presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.F4\" title=\"Figure 4 &#8227; 7.3. Efficiency Analysis &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S7.F3\" title=\"Figure 3 &#8227; 7.3. Efficiency Analysis &#8227; 7. Result and Discussion &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The ASR and LLM models account for most of the inference time, with the speech recognition model taking <span class=\"ltx_text ltx_font_bold\">38.5%</span> and the LLM refinement model taking <span class=\"ltx_text ltx_font_bold\">47.3%</span>. Nevertheless, the overall speech refinement pipeline achieves near real-time performance across all components. On average, refining a sentence of approximately 11.5 seconds in duration requires only <span class=\"ltx_text ltx_font_bold\">0.91 seconds</span> of processing time, yielding a real-time factor (RTF) of <span class=\"ltx_text ltx_font_bold\">0.08</span>, far below the real-time threshold (RTF = 1). It keeps the latency well within the acceptable range for natural human communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jacoby et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#bib.bib29\" title=\"\">2024</a>)</cite>. This indicates that the proposed approach is practically viable for real-world use, enabling fluid and responsive interaction. Consequently, the efficiency evaluation provides a clear answer to <span class=\"ltx_text ltx_font_bold\">RQ3</span>, demonstrating that the <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can reliably meet the latency requirements of everyday conversations.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "tts",
                    "model",
                    "evaluation",
                    "human",
                    "asr",
                    "between",
                    "speech",
                    "refinement",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented a mobile <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> that integrates the reasoning capabilities of LLMs with advanced speech processing to enable real-time assistive communication for individuals with speech impairments. Our system demonstrates that low-latency and practically deployable assistive <span class=\"ltx_text ltx_font_bold\">SpeechAgent</span> can be realized through a carefully designed client&#8211;server architecture. By combining impairment recognition, adaptive refinement, and seamless mobile integration, the agent delivers personalized, intelligible, and context-aware communication support. Evaluation on real-world data confirms both technical robustness and positive user experience, suggesting that the proposed approach is feasible and effective in everyday use. Looking ahead, several directions remain open for exploration. While the current system performs reliably, occasional hallucinations from the language model can lead to subtle deviations from the speaker&#8217;s intention. Future work could focus on enhancing semantic consistency and intent preservation. Moreover, the present agent primarily serves as a passive assistant that refines impaired speech into clearer expressions. Extending its capability toward a more interactive, training-oriented role could transform it into an adaptive partner that helps users practice, monitor, and improve their speech over time. Future research may also explore multilingual and personalized extensions, enabling the agent to support diverse linguistic, emotional, and contextual variations while maintaining natural and trustworthy communication.</p>\n\n",
                "matched_terms": [
                    "speechagent",
                    "model",
                    "evaluation",
                    "speech",
                    "refinement",
                    "impaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the perceptual evaluation protocol for assessing refined speech quality. Two complementary rating schemes were used to capture absolute and relative listener judgments. The Clarity (1&#8211;5) score measures overall intelligibility, considering pronunciation, fluency, and ease of understanding. The Comparison Mean Opinion Score (C-MOS, &#8211;3 to +3) assesses the perceived improvement from the original to the refined speech, reflecting how effectively the refinement enhances clarity and fluency.</p>\n\n",
                "matched_terms": [
                    "1–5",
                    "refined",
                    "clarity",
                    "score",
                    "mean",
                    "evaluation",
                    "opinion",
                    "speech",
                    "refinement",
                    "comparison",
                    "cmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Clarity (1&#8211;5)</span>\nThis score evaluates overall speech clarity, considering both the acoustic quality (pronunciation, fluency, ease of listening) and the semantic quality (sentence flow, coherence, and whether the speaker&#8217;s intent is understandable).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "1–5",
                    "score",
                    "clarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison Mean Opinion Score (C-MOS, &#8211;3 to +3)</span>\nIn this task, you will be presented with two speech samples. Please listen carefully to both and rate how the refined speech compares to the original.</p>\n\n",
                "matched_terms": [
                    "refined",
                    "score",
                    "rate",
                    "mean",
                    "opinion",
                    "speech",
                    "comparison",
                    "cmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instructions to Participants:</span>\nFor each question, you will hear two speech samples (A and B).\nPlease compare them and select a score from &#8211;3 to +3, according to the scale above.</p>\n\n",
                "matched_terms": [
                    "participants",
                    "score",
                    "scale",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The user first taps the record button (Figure 5a) to start a session, and the mobile device records speech until the user presses stop (Figure 5b). The recorded speech is then sent to the server for transcription via the ASR model (Figure 5c), after which the LLM refines the text (Figure 5d). The text is converted to speech, transmitted to edge device and assists user for communication (Figure 5e).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This prompt defines how the large language model (LLM) performs speech refinement for impaired speech in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20113v1#S4.SS4\" title=\"4.4. Speech Refinement &#8227; 4. Methodology &#8227; SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance\"><span class=\"ltx_text ltx_ref_tag\">4.4</span></a>. Two variants are used: one without explicit impairment context and another conditioned on the detected impairment type. In both cases, the LLM reconstructs incomplete or unclear sentences into coherent and fluent expressions while preserving the speaker&#8217;s original intent. When an impairment class is provided, additional contextual cues (e.g., slurred articulation in dysarthria, repetition in stuttering, or lexical omission in aphasia) guide the refinement process, enabling the LLM to adapt its rewriting strategy to different impairment patterns.</p>\n\n",
                "matched_terms": [
                    "dysarthria",
                    "model",
                    "aphasia",
                    "stuttering",
                    "speech",
                    "refinement",
                    "impaired"
                ]
            }
        ]
    }
}