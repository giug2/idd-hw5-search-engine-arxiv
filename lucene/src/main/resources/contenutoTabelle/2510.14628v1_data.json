{
    "S3.T1": {
        "source_file": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
        "caption": "Table 1: Objective and Subjective Evaluation Results Comparison of RLAIF-SPA with Baselines on WER, SIM-O, CMOS, Emotion MOS, and Speech Emotion Recognition across Two Datasets.",
        "body": "Model\nObjective\nSubjective\nSpeech Emotion Recognition\n\n\nModel\nWER↓\\downarrow\nSIM-O↑\\uparrow\nCMOS↑\\uparrow\nEmotion MOS↑\\uparrow\nNeutral↑\\uparrow\nHappy↑\\uparrow\nSad↑\\uparrow\nAngry↑\\uparrow\nSurprised↑\\uparrow\n\n\nLibriSpeech test-clean (En)\n\n\n\nChat-TTS\n7.85\n0.66\n3.56\n3.43\n0.51\n0.21\n0.11\n0.01\n0.03\n\n\nMegaTTS3\n6.90\n0.71\n3.83\n3.82\n0.72\n0.18\n0.26\n0.03\n0.00\n\n\nRLAIF-SPA\n5.80\n0.72\n3.98\n3.86\n0.77\n0.43\n0.29\n0.01\n0.03\n\n\nESD (Zh)\n\n\n\nChat-TTS\n6.89\n0.70\n3.87\n3.71\n0.76\n0.56\n0.19\n0.01\n0.03\n\n\nMegaTTS3\n5.26\n0.72\n4.02\n3.78\n0.81\n0.36\n0.24\n0.00\n0.03\n\n\nRLAIF-SPA\n4.01\n0.74\n4.16\n3.90\n0.78\n0.47\n0.33\n0.01\n0.09",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" colspan=\"10\"><span class=\"ltx_rule\" style=\"width:100%;height:0.9pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech Emotion Recognition</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SIM-O<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emotion MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Neutral<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Happy<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sad<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Angry<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Surprised<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LibriSpeech test-clean (En)</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chat-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MegaTTS3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.03</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RLAIF-SPA</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.29</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">ESD (Zh)</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chat-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MegaTTS3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RLAIF-SPA</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" colspan=\"10\"><span class=\"ltx_rule\" style=\"width:100%;height:0.9pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "recognition",
            "angry↑uparrow",
            "cmos",
            "neutral↑uparrow",
            "wer",
            "chattts",
            "speech",
            "happy↑uparrow",
            "subjective",
            "datasets",
            "rlaifspa",
            "emotion",
            "simo↑uparrow",
            "esd",
            "simo",
            "evaluation",
            "objective",
            "results",
            "sad↑uparrow",
            "wer↓downarrow",
            "comparison",
            "surprised↑uparrow",
            "mos",
            "testclean",
            "two",
            "across",
            "librispeech",
            "mos↑uparrow",
            "megatts3",
            "model",
            "cmos↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the effectiveness of RLAIF-SPA by comparing it against two strong baselines, Chat-TTS and MegaTTS3. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the comparative performance of the models, detailing the results from both objective (WER, SIM-O) and subjective (CMOS, Emotion MOS) evaluation metrics.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the <span class=\"ltx_text ltx_font_bold\">RLAIF-SPA</span> framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages <span class=\"ltx_text ltx_font_italic\">Prosodic Label Alignment</span> to enhance expressive quality by jointly considering semantic accuracy and prosodic&#8211;emotional alignment along four fine-grained dimensions: <span class=\"ltx_text ltx_font_italic\">Structure, Emotion, Speed,</span> and <span class=\"ltx_text ltx_font_italic\">Tone</span>. In addition, it incorporates <span class=\"ltx_text ltx_font_italic\">Semantic Accuracy Feedback</span> to ensure the generation of clear and accurate speech. Experiments on the LibriSpeech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "librispeech",
                    "wer",
                    "evaluation",
                    "chattts",
                    "speech",
                    "rlaifspa",
                    "emotion",
                    "model",
                    "simo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advancements in Text-To-Speech (TTS) synthesis have enabled the generation of speech with near-human quality in neutral styles </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Nevertheless, human interaction is not solely about informational exchange but also involves the subtle expression of emotion. These emotional signals profoundly impact listener engagement and understanding, a factor of particular consequence in applications like conversational agents, audiobooks, and virtual assistants. In such contexts, a lack of emotional expressiveness can result in speech that sounds monotonous or flat, thereby diminishing its effectiveness </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Therefore, TTS models with controllable emotional capabilities play a crucial role in bridging this gap and have become a major focus of research.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Early approaches to emotional TTS often rely on using explicit, coarse-grained emotion labels as conditional inputs to guide the synthesis process </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These models learn to associate discrete labels with specific acoustic features, such as adjusting pitch and speed to reflect a target emotion </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. While these techniques provide some levels of control, they have notable limitations. First, discrete emotion labels drastically oversimplify the vast and nuanced spectrum of human expression, failing to capture subtle emotional variations. Second, creating large-scale, high-quality datasets with manual emotion annotations is prohibitively expensive and time-consuming, hindering model scalability and performance on the emotional speech synthesis task </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "emotion",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To overcome these constraints, recent work has adopted Reinforcement Learning (RL) to directly optimize perceptual objectives from human feedback </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. By training on pairwise preference data, these models learn human preferences for emotional expression, generating more nuanced and varied speech without relying on predefined labels </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, these RL-based methods introduce their own challenges. Relying on extensive human feedback is costly and time-consuming, and the subjective nature of emotional preferences often results in noisy, unreliable reward signals. Moreover, the optimization target is typically a single, holistic preference score. This score assesses overall quality but fails to provide distinct feedback on individual prosodic attributes like rhythm, pitch, and pace. Consequently, these RL-based model lacks the fine-grained control to optimize the emotional speech synthesis models for precise and targeted emotional expression.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "model",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RLAIF-SPA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\" style=\"font-size:111%;\">1</span></span><span class=\"ltx_text ltx_font_medium\" style=\"font-size:111%;\">The code is available at https://github.com/Zoe-Mango/RLAIF-SPA.</span></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a framework that incorporates a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">einforcement </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">L</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">earning from </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AI</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eedback (RLAIF) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mechanism for </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">emantic-</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">P</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">rosodic </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment into Text-To-Speech synthesis. Our approach jointly optimizes emotional expressiveness and intelligibility without manual annotations or preference scoring by enabling the model to generate its own reward signals. Specifically, RLAIF-SPA leverages two core components for AI Feedback. First, for expressiveness, it employs </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Prosodic Label Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where the model&#8217;s output is judged against automatically generated labels along four fine-grained dimensions: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure, Emotion, Speed,</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This provides a structured, detailed, and consistent reward for emotional quality, directly addressing the need for fine-grained control. Second, for clarity, it uses </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Semantic Accuracy Feedback</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which assesses the consistency between the transcribed output and the original input text. By combining these two signals, the AI Feedback mechanism provides a stable and highly scalable optimization target. Experimental results on the LibriSpeech and ESD datasets demonstrate the effectiveness of RLAIF-SPA in both enhancing speech intelligibility and emotional expressiveness. Specifically, RLAIF-SPA significantly reduces word error rate compared to strong baseline models, while also achieving higher speaker similarity and improved emotional alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "speech",
                    "datasets",
                    "results",
                    "rlaifspa",
                    "emotion",
                    "esd",
                    "two",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This section details the proposed RLAIF-SPA framework. We first outline the overall training strategy and optimization objective, then elaborate on the two core components of our AI Feedback mechanism: Prosodic Label Alignment and Semantic Accuracy Feedback, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "two",
                    "rlaifspa",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The optimization process of RLAIF-SPA can be formulated as a multi-objective optimization problem, which is guided by AI Feedbacks. Both feedback signals are from two key objectives: emotional expressiveness and speech intelligibility. Our primary goal is to enhance prosodic-emotional label alignment while concurrently minimizing the Word Error Rate (WER), thereby producing speech that is both emotionally expressive and highly intelligible.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "rlaifspa",
                    "speech",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given an input text </span>\n  <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the policy model generates a speech sample </span>\n  <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This sample is then evaluated using a composite reward function, </span>\n  <math alttext=\"R(s_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R(s_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which balances two key objectives. The first objective, prosodic-emotional alignment, is assessed through a form of speech understanding, while the second, intelligibility, is measured via automatic speech recognition. The reward is computed as a weighted linear combination of the two optimization objectives:</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "objective",
                    "two",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"R_{\\text{label}}(s_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">R</mi>\n          <mtext mathsize=\"0.900em\">label</mtext>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R_{\\text{label}}(s_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the reward for prosodic-emotional alignment (Sections </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Prosodic Label Alignment &#8227; 2 Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and </span>\n  <math alttext=\"R_{\\text{wer}}(s_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">R</mi>\n          <mtext mathsize=\"0.900em\">wer</mtext>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">R_{\\text{wer}}(s_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a penalty term based on the WER of the generated speech (Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Semantic Accuracy Feedback &#8227; 2 Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). The non-negative hyperparameters </span>\n  <math alttext=\"\\alpha_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\alpha_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\alpha_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> balance the trade-off between expressiveness and intelligibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To optimize our policy model, we adopt Group Relative Policy Optimization (GRPO)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which evaluates the relative quality of multiple candidates within a group of generated outputs, rather than assessing each output in isolation. This group-wise comparison is particularly effective for the nuances of speech synthesis. Unlike methods that rely on noisy absolute reward scores or simple binary preferences, GRPO can better leverage the subtle, multi-dimensional feedback from our prosodic-emotional labels, allowing the model to learn complex trade-offs, such as preferring a speech sample with superior emotional tone over one that is rhythmically perfect but emotionally flat. By integrating GRPO with our AI Feedback mechanism, RLAIF-SPA can effectively optimize both emotional depth and intelligibility. This provides a robust framework for policy refinement that relies on group-relative feedback, leading to a stable, efficient, and low-supervision training process.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "speech",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Within the GRPO framework, for a given input text </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the policy </span>\n  <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generates a group of </span>\n  <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">G</mi>\n      <annotation encoding=\"application/x-tex\">G</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> candidate speech outputs </span>\n  <math alttext=\"\\{s_{i}\\}^{G}_{i=1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">G</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{s_{i}\\}^{G}_{i=1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The model&#8217;s parameters </span>\n  <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#952;</mi>\n      <annotation encoding=\"application/x-tex\">\\theta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are then updated to maximize the following objective function:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The first component of the AI Feedback mechanism is prosodic-emotional label matching. To guide the model toward generating emotionally expressive speech, we adopt a fine-grained labeling strategy that annotates speech along four distinct prosodic-emotional dimensions: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure, Emotion, Speed,</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These four labels represent key and complementary aspects of emotional expression, thereby forming a comprehensive yet manageable framework. The </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> label guides the model in capturing the organization and rhythm of speech&#8211;for example, distinguishing between questions and statements or modeling emotional progression in the narrative&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib24\" title=\"\">24</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;thereby enabling the generation of speech that is both emotionally varied and contextually coherent. The </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Emotion</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> label serves as the core determinant of the overall emotional state (e.g., happy, sad) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Meanwhile, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Speed</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provide fine-grained control over prosodic characteristics that reflect emotional intensity and color. In detail, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Speed</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adjusts the delivery pace, a strong indicator of arousal (e.g., faster for excitement, slower for sadness) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> controls pitch variation, which is crucial for conveying subtle emotional nuances like the rising intonation in a surprised voice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To implement this strategy at scale, we employ an automated labeling process where a LLM generates the target prosodic-emotional labels for the entire training dataset. This method bypasses the need for costly and time-consuming manual annotation, ensuring efficient and consistent labeling across large volumes of data. During the AI Feedback phase, the model&#8217;s generated speech is evaluated for alignment with these LLM-generated target labels. A reward is granted when the predicted prosodic characteristics match the ground truth labels, thereby guiding the model toward producing more emotionally expressive and accurate speech.\nThe reward function for this prosodic-emotional label alignment is formally defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The second key component of the AI Feedback mechanism is designed to ensure speech intelligibility. This is achieved by quantifying the semantic accuracy of the synthesized speech. Let </span>\n  <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> be the original input text and </span>\n  <math alttext=\"s_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> be a generated speech sample. We employ an Automatic Speech Recognition (ASR) model to obtain the transcription </span>\n  <math alttext=\"\\text{ASR}(s_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">ASR</mtext>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{ASR}(s_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where the </span>\n  <math alttext=\"\\text{WER}(t_{i},ASR(s_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mrow>\n            <mi mathsize=\"0.900em\">A</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">S</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">s</mi>\n                <mi mathsize=\"0.900em\">i</mi>\n              </msub>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{WER}(t_{i},ASR(s_{i}))</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> function calculates the standard word error rate between the two input texts. This value serves as a direct cost within our composite reward function (Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.1 Training Strategy and Optimization Framework &#8227; 2 Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), effectively penalizing any deviation from the source text. By integrating this semantic accuracy penalty, the framework ensures that any improvements in emotional expressiveness do not come at the expense of clarity or content accuracy, leading to speech that is both articulate and emotionally resonant.</span>\n</p>\n\n",
                "matched_terms": [
                    "two",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we describe the datasets, baseline, experimental details and evaluation metrics in our experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets and Baselines.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nOur model is trained on a subset of 1,000 utterances from the LibriSpeech dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which we annotate with four prosodic-emotional labels (Structure, Emotion, Speed, Tone) using GPT-4o. This targeted set is strategically chosen to demonstrate the data efficiency of our reward-based optimization for learning emotional alignment. For a comprehensive evaluation, we test on two datasets: the LibriSpeech test-clean set for general speech quality and the ESD dataset</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for targeted emotional analysis. The ESD, while smaller, is more emotionally expressive, containing data from 10 speakers across 5 emotions. We compare RLAIF-SPA against two strong, publicly available baselines: MegaTTS3 </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/bytedance/MegaTTS3</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Chat-TTS</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/2noise/ChatTTS</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "across",
                    "librispeech",
                    "evaluation",
                    "speech",
                    "testclean",
                    "datasets",
                    "rlaifspa",
                    "megatts3",
                    "emotion",
                    "esd",
                    "two",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experimental Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nOur backbone LLM is based on the MiniCPM-O 2.6</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/openbmb/MiniCPM-o-2_6</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework. It employs a Whisper-Medium-300M encoder as the speech tokenizer to generate latent representations, Qwen2.5-7B-Instruct as the core LLM, and a Chat-TTS vocoder as the speech detokenizer. We fine-tune the model using GRPO with our AI Feedback mechanism to enhance emotional expressiveness. The reward signal is computed automatically: Whisper-Large-v3</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/openai/whisper</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> calculates the WER for intelligibility, while Qwen2-Audio</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\">\n    <sup class=\"ltx_note_mark\">6</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://github.com/QwenLM/Qwen2-Audio</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> assesses alignment with our four prosodic-emotional labels. The weights for the WER penalty and label reward are set to </span>\n  <math alttext=\"\\alpha_{1}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{1}=0.3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha_{2}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.7</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{2}=0.7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with uniform weights (</span>\n  <math alttext=\"w_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mi mathsize=\"0.900em\">k</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">w_{k}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) applied across all label dimensions. To ensure consistency, all components are initialized from their pre-trained MiniCPM-O 2.6 checkpoints. The model is trained for 7 epochs.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "wer",
                    "chattts",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate our model&#8217;s performance using a combination of objective and subjective metrics to assess both intelligibility and emotional expressiveness. For objective assessment, we measure three key aspects. To assess intelligibility, we compute the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Word Error Rate (WER)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by transcribing the synthesized speech with Whisper-Large-v3 and comparing it to the original text. We use the WavLM-Large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model to calculate </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Speaker Similarity (SIM-O)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the generated sample and the prompt, where a higher score in the range of </span>\n  <math alttext=\"[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[-1,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicates greater similarity. Finally, speech emotion recognition is evaluated using the emotion2vec model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to verify the accuracy of the generated emotion categories. For subjective evaluation, we conduct human listening tests. We measure speech naturalness using the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Mean Opinion Score (MOS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, specifically assessing </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">CMOS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for overall quality (clarity, naturalness, and high-frequency details) and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Emotion MOS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the similarity of emotion between the generated and ground-truth audio. For these tests, 40 random samples were evaluated by at least 20 participants each. Additionally, we conduct </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">AB Preference Tests</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, asking 20 listeners to select the better sample between RLAIF-SPA and the baselines (Chat-TTS, MegaTTS3) based on emotional expressiveness and overall quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "recognition",
                    "cmos",
                    "wer",
                    "evaluation",
                    "chattts",
                    "speech",
                    "subjective",
                    "objective",
                    "mos",
                    "rlaifspa",
                    "megatts3",
                    "emotion",
                    "model",
                    "simo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in the results, RLAIF-SPA significantly outperforms both baselines in terms of WER. This substantial improvement in intelligibility is a direct consequence of our methodology, which explicitly incorporates a WER-based penalty into the AI Feedback mechanism. By directly optimizing for semantic accuracy alongside emotional expressiveness, our model is guided to produce speech that is not only emotionally rich but also highly intelligible and precise in its articulation.</span>\n</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "wer",
                    "speech",
                    "results",
                    "rlaifspa",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond intelligibility, RLAIF-SPA excels in emotional expressiveness and overall speech quality. This advancement is principally driven by our fine-grained, label-driven reward component, which enables the model to precisely modulate prosodic nuances across four dimensions: Structure, Emotion, Speed, and Tone. The model&#8217;s superiority is validated through a suite of evaluations. Objectively, it achieves higher speaker similarity (i.e., SIM-O) and greater accuracy in automatic speech emotion recognition, confirming that the synthesized emotions are distinct and well-aligned with their targets. Subjectively, listeners award RLAIF-SPA higher ratings for both overall quality (i.e., CMOS) and emotional fidelity (i.e., Emotion MOS). These findings are corroborated by AB preference tests (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3 Experimental Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), in which a significant majority of participants prefer RLAIF-SPA for its compelling balance of clarity and rich emotional nuance.</span>\n</p>\n\n",
                "matched_terms": [
                    "recognition",
                    "cmos",
                    "across",
                    "speech",
                    "mos",
                    "rlaifspa",
                    "emotion",
                    "model",
                    "simo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To isolate and verify the contributions of the key components within our framework, we conduct an ablation study, with results presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We examine two variants of our model: one without the GRPO strategy and another without the fine-grained label rewards.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "two",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Removing the GRPO component leads to a substantial increase in WER and a drop in speaker similarity. The reason may lie in that GRPO provides a stable learning signal by evaluating the relative quality of candidates within a group. Since the rewards are derived from AI feedback, optimizing without GRPO relies on synthesized, noisy and high-variance signals from individual samples, resulting in unstable policy updates and hindering consistent convergence. This instability directly degrades the model&#8217;s ability to maintain articulatory precision and timbral consistency, thus causing the higher WER and lower speaker similarity. Similarly, removing the prosodic-emotional label rewards also results in significant performance degradation. This occurs because, without these fine-grained rewards, the optimization objective defaults to minimizing WER alone. Consequently, the model tends to generate highly intelligible but emotionally monotonous speech, lacking the guidance necessary to modulate prosody across the critical dimensions of Structure, Emotion, Speed, and Tone.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "wer",
                    "speech",
                    "objective",
                    "results",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents RLAIF-SPA, a novel framework that autonomously optimizes for both emotional expressiveness and intelligibility in speech synthesis. By employing a AI Feedback mechanism with GRPO to enforce fine-grained prosodic consistency and semantic accuracy, RLAIF-SPA significantly outperforms strong baseline models on the LibriSpeech and ESD datasets. Crucially, our work demonstrates the feasibility of generating emotionally rich and highly intelligible speech without reliance on costly manual annotations, paving the way for more scalable and data-efficient emotional TTS systems. Future work will focus on refining the reward mechanism and assessing the framework&#8217;s scalability across a broader range of acoustic environments and languages. Another promising direction involves modeling how a speaker&#8217;s transient emotional state dynamically shapes prosody.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "librispeech",
                    "speech",
                    "datasets",
                    "rlaifspa",
                    "esd"
                ]
            }
        ]
    },
    "S3.F2.pic1": {
        "source_file": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
        "caption": "",
        "body": "Win\n\n\n\n\n  Loss\n\n\n\n\n  Tie",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_rule ltx_framed ltx_framed_rectangle\" style=\"width:11.4pt;height:7.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;Win</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_rule ltx_framed ltx_framed_rectangle\" style=\"width:11.4pt;height:7.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;Loss</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_rule ltx_framed ltx_framed_rectangle\" style=\"width:11.4pt;height:7.1pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;Tie</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "loss",
            "tie",
            "win"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": []
    },
    "S4.T2": {
        "source_file": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
        "caption": "Table 2: Ablation Study on Model Performance.",
        "body": "Model\n\nWER↓\\downarrow\n\n\nSIM-O↑\\uparrow\n\n\nCMOS↑\\uparrow\n\n\nEmotion MOS↑\\uparrow\n\n\n\nRLAIF-SPA\n5.80\n0.72\n3.98\n3.86\n\n\nw/o GRPO\n11.32\n0.65\n3.51\n3.40\n\n\nw/o label\n8.89\n0.63\n3.60\n3.17",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" colspan=\"5\"><span class=\"ltx_rule\" style=\"width:100%;height:0.9pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SIM-O</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CMOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Emotion MOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">RLAIF-SPA</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.86</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o GRPO</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.51</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.40</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o label</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.17</span></td>\n</tr>\n</tbody>\n<tfoot class=\"ltx_tfoot\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" colspan=\"5\"><span class=\"ltx_rule\" style=\"width:100%;height:0.9pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></th>\n</tr>\n</tfoot>\n</table>\n\n",
        "informative_terms_identified": [
            "wer↓downarrow",
            "performance",
            "ablation",
            "study",
            "label",
            "mos↑uparrow",
            "rlaifspa",
            "emotion",
            "grpo",
            "simo↑uparrow",
            "model",
            "cmos↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To isolate and verify the contributions of the key components within our framework, we conduct an ablation study, with results presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Ablation Study &#8227; 4 Experimental Results &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We examine two variants of our model: one without the GRPO strategy and another without the fine-grained label rewards.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the <span class=\"ltx_text ltx_font_bold\">RLAIF-SPA</span> framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages <span class=\"ltx_text ltx_font_italic\">Prosodic Label Alignment</span> to enhance expressive quality by jointly considering semantic accuracy and prosodic&#8211;emotional alignment along four fine-grained dimensions: <span class=\"ltx_text ltx_font_italic\">Structure, Emotion, Speed,</span> and <span class=\"ltx_text ltx_font_italic\">Tone</span>. In addition, it incorporates <span class=\"ltx_text ltx_font_italic\">Semantic Accuracy Feedback</span> to ensure the generation of clear and accurate speech. Experiments on the LibriSpeech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "rlaifspa",
                    "label",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Early approaches to emotional TTS often rely on using explicit, coarse-grained emotion labels as conditional inputs to guide the synthesis process </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These models learn to associate discrete labels with specific acoustic features, such as adjusting pitch and speed to reflect a target emotion </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. While these techniques provide some levels of control, they have notable limitations. First, discrete emotion labels drastically oversimplify the vast and nuanced spectrum of human expression, failing to capture subtle emotional variations. Second, creating large-scale, high-quality datasets with manual emotion annotations is prohibitively expensive and time-consuming, hindering model scalability and performance on the emotional speech synthesis task </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RLAIF-SPA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\" style=\"font-size:111%;\">1</span></span><span class=\"ltx_text ltx_font_medium\" style=\"font-size:111%;\">The code is available at https://github.com/Zoe-Mango/RLAIF-SPA.</span></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a framework that incorporates a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">einforcement </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">L</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">earning from </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AI</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">eedback (RLAIF) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mechanism for </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">emantic-</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">P</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">rosodic </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment into Text-To-Speech synthesis. Our approach jointly optimizes emotional expressiveness and intelligibility without manual annotations or preference scoring by enabling the model to generate its own reward signals. Specifically, RLAIF-SPA leverages two core components for AI Feedback. First, for expressiveness, it employs </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Prosodic Label Alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where the model&#8217;s output is judged against automatically generated labels along four fine-grained dimensions: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure, Emotion, Speed,</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This provides a structured, detailed, and consistent reward for emotional quality, directly addressing the need for fine-grained control. Second, for clarity, it uses </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Semantic Accuracy Feedback</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which assesses the consistency between the transcribed output and the original input text. By combining these two signals, the AI Feedback mechanism provides a stable and highly scalable optimization target. Experimental results on the LibriSpeech and ESD datasets demonstrate the effectiveness of RLAIF-SPA in both enhancing speech intelligibility and emotional expressiveness. Specifically, RLAIF-SPA significantly reduces word error rate compared to strong baseline models, while also achieving higher speaker similarity and improved emotional alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "rlaifspa",
                    "label",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This section details the proposed RLAIF-SPA framework. We first outline the overall training strategy and optimization objective, then elaborate on the two core components of our AI Feedback mechanism: Prosodic Label Alignment and Semantic Accuracy Feedback, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "label"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The optimization process of RLAIF-SPA can be formulated as a multi-objective optimization problem, which is guided by AI Feedbacks. Both feedback signals are from two key objectives: emotional expressiveness and speech intelligibility. Our primary goal is to enhance prosodic-emotional label alignment while concurrently minimizing the Word Error Rate (WER), thereby producing speech that is both emotionally expressive and highly intelligible.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "label"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To optimize our policy model, we adopt Group Relative Policy Optimization (GRPO)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which evaluates the relative quality of multiple candidates within a group of generated outputs, rather than assessing each output in isolation. This group-wise comparison is particularly effective for the nuances of speech synthesis. Unlike methods that rely on noisy absolute reward scores or simple binary preferences, GRPO can better leverage the subtle, multi-dimensional feedback from our prosodic-emotional labels, allowing the model to learn complex trade-offs, such as preferring a speech sample with superior emotional tone over one that is rhythmically perfect but emotionally flat. By integrating GRPO with our AI Feedback mechanism, RLAIF-SPA can effectively optimize both emotional depth and intelligibility. This provides a robust framework for policy refinement that relies on group-relative feedback, leading to a stable, efficient, and low-supervision training process.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "model",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The first component of the AI Feedback mechanism is prosodic-emotional label matching. To guide the model toward generating emotionally expressive speech, we adopt a fine-grained labeling strategy that annotates speech along four distinct prosodic-emotional dimensions: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure, Emotion, Speed,</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These four labels represent key and complementary aspects of emotional expression, thereby forming a comprehensive yet manageable framework. The </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> label guides the model in capturing the organization and rhythm of speech&#8211;for example, distinguishing between questions and statements or modeling emotional progression in the narrative&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib24\" title=\"\">24</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;thereby enabling the generation of speech that is both emotionally varied and contextually coherent. The </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Emotion</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> label serves as the core determinant of the overall emotional state (e.g., happy, sad) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Meanwhile, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Speed</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provide fine-grained control over prosodic characteristics that reflect emotional intensity and color. In detail, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Speed</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adjusts the delivery pace, a strong indicator of arousal (e.g., faster for excitement, slower for sadness) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> controls pitch variation, which is crucial for conveying subtle emotional nuances like the rising intonation in a surprised voice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "label",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To implement this strategy at scale, we employ an automated labeling process where a LLM generates the target prosodic-emotional labels for the entire training dataset. This method bypasses the need for costly and time-consuming manual annotation, ensuring efficient and consistent labeling across large volumes of data. During the AI Feedback phase, the model&#8217;s generated speech is evaluated for alignment with these LLM-generated target labels. A reward is granted when the predicted prosodic characteristics match the ground truth labels, thereby guiding the model toward producing more emotionally expressive and accurate speech.\nThe reward function for this prosodic-emotional label alignment is formally defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "label"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"m_{k}(s_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mi mathsize=\"0.900em\">k</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m_{k}(s_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a binary function indicating a match for each of the four prosodic-emotional labels (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Structure</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Emotion</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Speed</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Tone</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and </span>\n  <math alttext=\"w_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mi mathsize=\"0.900em\">k</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">w_{k}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the corresponding weight for each label dimension.</span>\n</p>\n\n",
                "matched_terms": [
                    "label",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets and Baselines.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nOur model is trained on a subset of 1,000 utterances from the LibriSpeech dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which we annotate with four prosodic-emotional labels (Structure, Emotion, Speed, Tone) using GPT-4o. This targeted set is strategically chosen to demonstrate the data efficiency of our reward-based optimization for learning emotional alignment. For a comprehensive evaluation, we test on two datasets: the LibriSpeech test-clean set for general speech quality and the ESD dataset</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for targeted emotional analysis. The ESD, while smaller, is more emotionally expressive, containing data from 10 speakers across 5 emotions. We compare RLAIF-SPA against two strong, publicly available baselines: MegaTTS3 </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/bytedance/MegaTTS3</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Chat-TTS</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/2noise/ChatTTS</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Experimental Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nOur backbone LLM is based on the MiniCPM-O 2.6</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://huggingface.co/openbmb/MiniCPM-o-2_6</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework. It employs a Whisper-Medium-300M encoder as the speech tokenizer to generate latent representations, Qwen2.5-7B-Instruct as the core LLM, and a Chat-TTS vocoder as the speech detokenizer. We fine-tune the model using GRPO with our AI Feedback mechanism to enhance emotional expressiveness. The reward signal is computed automatically: Whisper-Large-v3</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/openai/whisper</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> calculates the WER for intelligibility, while Qwen2-Audio</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\">\n    <sup class=\"ltx_note_mark\">6</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://github.com/QwenLM/Qwen2-Audio</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> assesses alignment with our four prosodic-emotional labels. The weights for the WER penalty and label reward are set to </span>\n  <math alttext=\"\\alpha_{1}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{1}=0.3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha_{2}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.7</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{2}=0.7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with uniform weights (</span>\n  <math alttext=\"w_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mi mathsize=\"0.900em\">k</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">w_{k}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) applied across all label dimensions. To ensure consistency, all components are initialized from their pre-trained MiniCPM-O 2.6 checkpoints. The model is trained for 7 epochs.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "label",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate our model&#8217;s performance using a combination of objective and subjective metrics to assess both intelligibility and emotional expressiveness. For objective assessment, we measure three key aspects. To assess intelligibility, we compute the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Word Error Rate (WER)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by transcribing the synthesized speech with Whisper-Large-v3 and comparing it to the original text. We use the WavLM-Large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model to calculate </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Speaker Similarity (SIM-O)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the generated sample and the prompt, where a higher score in the range of </span>\n  <math alttext=\"[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[-1,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicates greater similarity. Finally, speech emotion recognition is evaluated using the emotion2vec model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to verify the accuracy of the generated emotion categories. For subjective evaluation, we conduct human listening tests. We measure speech naturalness using the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Mean Opinion Score (MOS)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, specifically assessing </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">CMOS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for overall quality (clarity, naturalness, and high-frequency details) and </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Emotion MOS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the similarity of emotion between the generated and ground-truth audio. For these tests, 40 random samples were evaluated by at least 20 participants each. Additionally, we conduct </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">AB Preference Tests</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, asking 20 listeners to select the better sample between RLAIF-SPA and the baselines (Chat-TTS, MegaTTS3) based on emotional expressiveness and overall quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "model",
                    "emotion",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the effectiveness of RLAIF-SPA by comparing it against two strong baselines, Chat-TTS and MegaTTS3. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the comparative performance of the models, detailing the results from both objective (WER, SIM-O) and subjective (CMOS, Emotion MOS) evaluation metrics.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "emotion",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in the results, RLAIF-SPA significantly outperforms both baselines in terms of WER. This substantial improvement in intelligibility is a direct consequence of our methodology, which explicitly incorporates a WER-based penalty into the AI Feedback mechanism. By directly optimizing for semantic accuracy alongside emotional expressiveness, our model is guided to produce speech that is not only emotionally rich but also highly intelligible and precise in its articulation.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond intelligibility, RLAIF-SPA excels in emotional expressiveness and overall speech quality. This advancement is principally driven by our fine-grained, label-driven reward component, which enables the model to precisely modulate prosodic nuances across four dimensions: Structure, Emotion, Speed, and Tone. The model&#8217;s superiority is validated through a suite of evaluations. Objectively, it achieves higher speaker similarity (i.e., SIM-O) and greater accuracy in automatic speech emotion recognition, confirming that the synthesized emotions are distinct and well-aligned with their targets. Subjectively, listeners award RLAIF-SPA higher ratings for both overall quality (i.e., CMOS) and emotional fidelity (i.e., Emotion MOS). These findings are corroborated by AB preference tests (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14628v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3 Experimental Methodology &#8227; RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), in which a significant majority of participants prefer RLAIF-SPA for its compelling balance of clarity and rich emotional nuance.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Removing the GRPO component leads to a substantial increase in WER and a drop in speaker similarity. The reason may lie in that GRPO provides a stable learning signal by evaluating the relative quality of candidates within a group. Since the rewards are derived from AI feedback, optimizing without GRPO relies on synthesized, noisy and high-variance signals from individual samples, resulting in unstable policy updates and hindering consistent convergence. This instability directly degrades the model&#8217;s ability to maintain articulatory precision and timbral consistency, thus causing the higher WER and lower speaker similarity. Similarly, removing the prosodic-emotional label rewards also results in significant performance degradation. This occurs because, without these fine-grained rewards, the optimization objective defaults to minimizing WER alone. Consequently, the model tends to generate highly intelligible but emotionally monotonous speech, lacking the guidance necessary to modulate prosody across the critical dimensions of Structure, Emotion, Speed, and Tone.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "label",
                    "grpo",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents RLAIF-SPA, a novel framework that autonomously optimizes for both emotional expressiveness and intelligibility in speech synthesis. By employing a AI Feedback mechanism with GRPO to enforce fine-grained prosodic consistency and semantic accuracy, RLAIF-SPA significantly outperforms strong baseline models on the LibriSpeech and ESD datasets. Crucially, our work demonstrates the feasibility of generating emotionally rich and highly intelligible speech without reliance on costly manual annotations, paving the way for more scalable and data-efficient emotional TTS systems. Future work will focus on refining the reward mechanism and assessing the framework&#8217;s scalability across a broader range of acoustic environments and languages. Another promising direction involves modeling how a speaker&#8217;s transient emotional state dynamically shapes prosody.</span>\n</p>\n\n",
                "matched_terms": [
                    "rlaifspa",
                    "grpo"
                ]
            }
        ]
    }
}