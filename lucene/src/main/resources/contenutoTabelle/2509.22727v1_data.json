{
    "S3.T1": {
        "caption": "Table 1: Comparison of Training Data Scale Across TTS Systems",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Data Scale</span><span class=\"ltx_text\" style=\"font-size:90%;\">/hours</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Edge TTS</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib19\" title=\"\">19</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">150k</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen-TTS</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">3,000k</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ours</span><span class=\"ltx_text\" style=\"font-size:90%;\"> (stage 1&amp;2)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.9pt;padding-bottom:-0.9pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.7k</span><span class=\"ltx_text\" style=\"font-size:90%;\"> Mandarin+</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.4k</span><span class=\"ltx_text\" style=\"font-size:90%;\"> dialects</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "qwentts20",
            "tts18",
            "comparison",
            "tts",
            "systems",
            "stage",
            "∼sim150k",
            "training",
            "system",
            "∼sim07k",
            "across",
            "scale",
            "dialects",
            "scalehours",
            "cosyvoice219",
            "ours",
            "edge",
            "data",
            "∼sim3000k",
            "mandarin04k"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Dialect speech embodies rich cultural and linguistic diversity, yet building text-to-speech (TTS) systems for dialects remains challenging due to scarce data, inconsistent orthographies, and complex phonetic variation. To address these issues, we present DiaMoE-TTS, a unified IPA-based framework that standardizes phonetic representations and resolves grapheme-to-phoneme ambiguities. Built upon the F5-TTS architecture, the system introduces a dialect-aware Mixture-of-Experts (MoE) to model phonological differences and employs parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and Conditioning Adapters for rapid transfer to new dialects. Unlike approaches dependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable, open-data-driven synthesis. Experiments demonstrate natural and expressive speech generation, achieving zero-shot performance on unseen dialects and specialized domains such as Peking Opera with only a few hours of data. Code and resources are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/GiantAILab/DiaMoE-TTS\" title=\"\">https://github.com/GiantAILab/DiaMoE-TTS</a></p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems",
                    "dialects",
                    "data",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text-to-speech (TTS) models have advanced rapidly in recent years<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib5\" title=\"\">5</a>]</cite>, demonstrating remarkable capabilities in generating natural and expressive speech. Dialect TTS, a vital extension of speech synthesis, is culturally and linguistically significant across China&#8217;s diverse dialects and many regional languages in Europe, yet progress is hindered by data scarcity, inconsistent orthographies, and front-end modeling challenges. Public resources for dialects are largely automatic speech recognition (ASR)-oriented rather than TTS-ready; to our knowledge, there is no unified <em class=\"ltx_emph ltx_font_italic\">open-data</em> pipeline that enables end-to-end zero-shot dialect TTS.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "across",
                    "data",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we leverage linguistic expertise to construct a novel multilingual speech dataset for Chinese dialects. Built upon existing open-source ASR corpora, our dataset introduces a unified International Phonetic Alphabet (IPA)-based pronunciation lexicon<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib6\" title=\"\">6</a>]</cite>, providing aligned text, IPA phonemes, and speech pairs. This resource enables consistent modeling across diverse dialects and serves as a foundation for open-data-driven, zero-shot multilingual and multi-dialect TTS research.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "across",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the above, we propose a unified framework for low-resource dialect TTS that leverages both cross-lingual pre-training and phonetically grounded modeling. We standardize the pronunciation modeling across dialects using the IPA, constructing a harmonized text-IPA-speech alignment dataset covering multiple Chinese dialects.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "across",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method has reached a performance almost comparable to closed-source or industrial-scale systems. We will also open-source the dataset construction methodology, a multidialect dataset from public data, and full training/inference scripts to support reproducible research.</p>\n\n",
                "matched_terms": [
                    "data",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">IPA is a standardized phonetic system, enabling precise and unambiguous representation of human speech. In early TTS systems, IPA plays a crucial role<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib7\" title=\"\">7</a>]</cite>, offering accurate phonetic guidance and reducing mispronunciations. Its consistency also makes it essential for building unified multilingual TTS models<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib9\" title=\"\">9</a>]</cite>. As shown in the figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 F5-TTS: A Lightweight and Efficient High-Quality TTS Model &#8227; 2 Related Work &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we have built a unified dialect front-end system based on IPA. Different types of dialect share a set of identifiers, providing a prerequisite for the expansion of low-resource dialect TTS.</p>\n\n",
                "matched_terms": [
                    "system",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a unified framework for dialect TTS, built upon a multi-stage training strategy that progressively extends the model&#8217;s capability from Mandarin to Chinese dialectal speech and finally enables adaptation to low-resource novel dialects. The proposed framework is trained in four stages.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 0: Initialization with F5-TTS checkpoint</span>\n<br class=\"ltx_break\"/>The training starts from a pre-trained F5-TTS checkpoint, trained on large-scale Mandarin and English data. This provides a strong prior for high-quality speech generation and serves as the foundation for downstream adaptation.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1 &amp; 2: Joint Training on Multidialect IPA Data</span>\n<br class=\"ltx_break\"/>In both Stage 1 and Stage 2, the model is trained on a unified dataset of IPA-aligned text-speech pairs covering Standard Mandarin and multiple Chinese dialects. It is designed to align diverse input representations (e.g., pinyin, characters) with a unified IPA phoneme space. In stage 2, we add an extra dialect-style MoE module<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib16\" title=\"\">16</a>]</cite> after Text Embedding to enable the model to automatically learn and capture the relevant information of dialect styles, thereby weakening the phenomenon of style averaging.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "stage",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Adaptation to Low-Resource Dialects</span>\n<br class=\"ltx_break\"/>Building upon the multidialect-capable model, Stage 3 performs rapid adaptation to new, low-resource dialects. We adopt a parameter-efficient fine-tuning (PEFT) approach<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib17\" title=\"\">17</a>]</cite> in which only LoRA and Conditioning Adapters are trained, while the backbone model remains frozen. Furthermore, data augmentation methods are also introduced to expand the small training set. This enables speech synthesis with only a few hours of data, preserving multilingual knowledge and mitigating catastrophic forgetting.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "stage",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This staged training paradigm effectively balances model capacity, data efficiency, and generalization, enabling rapid expansion to new dialects in zero-shot or few-shot speech synthesis.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our method employs a single, shared text encoder that assumes relatively stable grapheme-to-phoneme mappings. However, the joint training of multiple dialects can lead to the homogenization of styles, and without additional guidance, the expression of dialectal styles will be weakened. To address this, we incorporate a residual MoE architecture into the text embedding module:</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the scarcity of high-quality speech data for many dialects&#8212;often limited to less than ten hours&#8212;we further investigate the generalization of our framework to extremely low-resource dialects. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib17\" title=\"\">17</a>]</cite>, We incorporate a Conditioning Adapter into the text embedding module and apply LoRA to the query and value projections within the attention layers of the input embedding and DiT blocks. The MoE structure is kept frozen during fine-tuning to preserve its learned routing behavior and avoid overfitting under limited data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also use acoustic data augmentation techniques to expand the limited training data of the new dialect without altering its dialectal style. Specifically, for each original utterance, we apply pitch and time-scale modifications with factors of 0.85, 0.9, 0.95, 1.05, 1.1, and 1.15. By combining with this simple and effective augmentation method, the zero-shot speech synthesis capability of this model can be extended to a new dialect without affecting the original dialect synthesis ability.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize the Common Voice Cantonese dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib21\" title=\"\">21</a>]</cite>, the Emilia Mandarin dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib22\" title=\"\">22</a>]</cite>, dialectal data from the KeSpeech corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib23\" title=\"\">23</a>]</cite> and a open-source Sourthern Min dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib24\" title=\"\">24</a>]</cite>. In addition, we incorporate commercially acquired speech data in the Shanghai and Tianjin dialects to broaden the coverage of dialect. For low-resource fine-tuning, we employ approximately 3 hours of professionally recorded Peking Opera recitation, covering both Jingbai and Yunbai styles, along with an equal amount of Nanjing dialect data for comparison.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "data",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the AdamW optimizer for the model in all experiments. In stages 1 and 2, a peak learning rate of 7.5e-5 is employed, with a warm-up of 2k steps and a linearly decayed learning rate setting over 200K total training steps. The batch size is set to 28k frames per GPU. In Stage 3, only the LoRA (rank=16, <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>=1) and Conditioning Adapters are updated, while the main model is frozen. We trained the model for 100K steps with a learning rate of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This evaluation covers both objective metrics and subjective listening tests, comparing our system against several state-of-the-art commercial TTS models including Edge TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib18\" title=\"\">18</a>]</cite>, CosyVoice2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib19\" title=\"\">19</a>]</cite>, and Qwen-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib20\" title=\"\">20</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cosyvoice219",
                    "edge",
                    "tts",
                    "qwentts20",
                    "tts18",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows our method&#8217;s performance across all dialects, including low-resource varieties like Peking Opera (Jingbai and Yunbai) and Nanjing. Our approach effectively learns from limited data via continual learning, whereas direct fine-tuning leads to significant forgetting of prior knowledge.</p>\n\n",
                "matched_terms": [
                    "across",
                    "data",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> also displays the WER, MOS and UTMOSv2. While our system falls slightly behind commercial baselines in certain metrics, this observation warrants further clarification. Notably, the primary objective of our method is not to surpass highly optimized proprietary systems, but rather to demonstrate a feasible framework for training with minimal resources. In addition, the apparent performance gap can be attributed to the broader dialectal coverage and the added functionality of voice cloning supported by our TTS model&#8212;capabilities that are absent in most commercial counterparts. Supporting a wide range of dialects and enabling personalized synthesis inevitably increases the modeling complexity, which in turn may lead to a modest degradation in performance on individual dialects.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "systems",
                    "dialects",
                    "training",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a unified, open-source framework for low-resource dialect TTS, leveraging IPA-based modeling, dialect-aware MoE, and parameter-efficient adaptation to address data scarcity and phonetic variation. Datasets and code are released to ensure reproducibility. The framework provides a scalable foundation extendable to other language families. Future work will expand it with higher-quality data and broader dialect coverage, advancing open-data-driven multilingual speech synthesis.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tts"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Main Results on WER, MOS and UTMOSv2 Scores Across Dialects",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Metric</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">YUE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">CD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">XA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ZZ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">TJ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">NAN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SJZ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">NJ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Jingbai</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Yunbai</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\"><span class=\"ltx_text ltx_font_bold\">WER</span> /%</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">76.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">76.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">37.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">33.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">29.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">20.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">92.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">36.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">31.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">39.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">68.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Edge TTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib19\" title=\"\">19</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">59.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Qwen TTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">48.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Average</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">31.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">48.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.34</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MOS</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Edge TTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib19\" title=\"\">19</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.22</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.06</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Qwen TTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Average</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"5\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">UTMOSv2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Edge TTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib19\" title=\"\">19</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.99</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Qwen TTS </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.77</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.31</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Average</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.11</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.18</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Diff.</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours vs Comp. Avg. (WER)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">44.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">27.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">29.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">32.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">25.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours vs Comp. Avg. (MOS)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-1.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-1.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.86</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours vs Comp. Avg. (UTMOSv2)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-0.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "diff",
            "yue",
            "wer",
            "jingbai",
            "mos",
            "tts",
            "average",
            "qwen",
            "scores",
            "across",
            "avg",
            "main",
            "dialects",
            "cosyvoice2",
            "sjz",
            "results",
            "metric",
            "ours",
            "edge",
            "yunbai",
            "model",
            "nan",
            "comp",
            "utmosv2"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows our method&#8217;s performance across all dialects, including low-resource varieties like Peking Opera (Jingbai and Yunbai) and Nanjing. Our approach effectively learns from limited data via continual learning, whereas direct fine-tuning leads to significant forgetting of prior knowledge.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> also displays the WER, MOS and UTMOSv2. While our system falls slightly behind commercial baselines in certain metrics, this observation warrants further clarification. Notably, the primary objective of our method is not to surpass highly optimized proprietary systems, but rather to demonstrate a feasible framework for training with minimal resources. In addition, the apparent performance gap can be attributed to the broader dialectal coverage and the added functionality of voice cloning supported by our TTS model&#8212;capabilities that are absent in most commercial counterparts. Supporting a wide range of dialects and enabling personalized synthesis inevitably increases the modeling complexity, which in turn may lead to a modest degradation in performance on individual dialects.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Dialect speech embodies rich cultural and linguistic diversity, yet building text-to-speech (TTS) systems for dialects remains challenging due to scarce data, inconsistent orthographies, and complex phonetic variation. To address these issues, we present DiaMoE-TTS, a unified IPA-based framework that standardizes phonetic representations and resolves grapheme-to-phoneme ambiguities. Built upon the F5-TTS architecture, the system introduces a dialect-aware Mixture-of-Experts (MoE) to model phonological differences and employs parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and Conditioning Adapters for rapid transfer to new dialects. Unlike approaches dependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable, open-data-driven synthesis. Experiments demonstrate natural and expressive speech generation, achieving zero-shot performance on unseen dialects and specialized domains such as Peking Opera with only a few hours of data. Code and resources are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/GiantAILab/DiaMoE-TTS\" title=\"\">https://github.com/GiantAILab/DiaMoE-TTS</a></p>\n\n",
                "matched_terms": [
                    "dialects",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text-to-speech (TTS) models have advanced rapidly in recent years<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib5\" title=\"\">5</a>]</cite>, demonstrating remarkable capabilities in generating natural and expressive speech. Dialect TTS, a vital extension of speech synthesis, is culturally and linguistically significant across China&#8217;s diverse dialects and many regional languages in Europe, yet progress is hindered by data scarcity, inconsistent orthographies, and front-end modeling challenges. Public resources for dialects are largely automatic speech recognition (ASR)-oriented rather than TTS-ready; to our knowledge, there is no unified <em class=\"ltx_emph ltx_font_italic\">open-data</em> pipeline that enables end-to-end zero-shot dialect TTS.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "across",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we leverage linguistic expertise to construct a novel multilingual speech dataset for Chinese dialects. Built upon existing open-source ASR corpora, our dataset introduces a unified International Phonetic Alphabet (IPA)-based pronunciation lexicon<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib6\" title=\"\">6</a>]</cite>, providing aligned text, IPA phonemes, and speech pairs. This resource enables consistent modeling across diverse dialects and serves as a foundation for open-data-driven, zero-shot multilingual and multi-dialect TTS research.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "across",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the above, we propose a unified framework for low-resource dialect TTS that leverages both cross-lingual pre-training and phonetically grounded modeling. We standardize the pronunciation modeling across dialects using the IPA, constructing a harmonized text-IPA-speech alignment dataset covering multiple Chinese dialects.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "across",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib10\" title=\"\">10</a>]</cite> is a fully non-autoregressive TTS model built upon <span class=\"ltx_text ltx_font_bold\">Optimal Transport Conditional Flow Matching</span> (OT-CFM)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib11\" title=\"\">11</a>]</cite> and a <span class=\"ltx_text ltx_font_bold\">Diffusion Transformer</span> (DiT) backbone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib12\" title=\"\">12</a>]</cite>. It synthesizes speech in the reference speaker&#8217;s voice given a reference audio, its transcript, and target text. The model encodes text using ConvNeXt V2 blocks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib13\" title=\"\">13</a>]</cite>, conditions on masked Mel-spectrograms and flow-matching latents, and employs DiT layers to progressively denoise latent spectrograms, effectively capturing long-range prosody and speech structure.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a unified framework for dialect TTS, built upon a multi-stage training strategy that progressively extends the model&#8217;s capability from Mandarin to Chinese dialectal speech and finally enables adaptation to low-resource novel dialects. The proposed framework is trained in four stages.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1 &amp; 2: Joint Training on Multidialect IPA Data</span>\n<br class=\"ltx_break\"/>In both Stage 1 and Stage 2, the model is trained on a unified dataset of IPA-aligned text-speech pairs covering Standard Mandarin and multiple Chinese dialects. It is designed to align diverse input representations (e.g., pinyin, characters) with a unified IPA phoneme space. In stage 2, we add an extra dialect-style MoE module<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib16\" title=\"\">16</a>]</cite> after Text Embedding to enable the model to automatically learn and capture the relevant information of dialect styles, thereby weakening the phenomenon of style averaging.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Adaptation to Low-Resource Dialects</span>\n<br class=\"ltx_break\"/>Building upon the multidialect-capable model, Stage 3 performs rapid adaptation to new, low-resource dialects. We adopt a parameter-efficient fine-tuning (PEFT) approach<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib17\" title=\"\">17</a>]</cite> in which only LoRA and Conditioning Adapters are trained, while the backbone model remains frozen. Furthermore, data augmentation methods are also introduced to expand the small training set. This enables speech synthesis with only a few hours of data, preserving multilingual knowledge and mitigating catastrophic forgetting.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This staged training paradigm effectively balances model capacity, data efficiency, and generalization, enabling rapid expansion to new dialects in zero-shot or few-shot speech synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This enables learning of dialect-aware text representation, allowing the model to adaptively generate prosodic patterns that align with the target dialect. In addition, the sparse activation of experts ensures scalability and computational efficiency, making it feasible to support a wide range of dialects within a unified framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize the Common Voice Cantonese dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib21\" title=\"\">21</a>]</cite>, the Emilia Mandarin dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib22\" title=\"\">22</a>]</cite>, dialectal data from the KeSpeech corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib23\" title=\"\">23</a>]</cite> and a open-source Sourthern Min dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib24\" title=\"\">24</a>]</cite>. In addition, we incorporate commercially acquired speech data in the Shanghai and Tianjin dialects to broaden the coverage of dialect. For low-resource fine-tuning, we employ approximately 3 hours of professionally recorded Peking Opera recitation, covering both Jingbai and Yunbai styles, along with an equal amount of Nanjing dialect data for comparison.</p>\n\n",
                "matched_terms": [
                    "yunbai",
                    "dialects",
                    "jingbai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the AdamW optimizer for the model in all experiments. In stages 1 and 2, a peak learning rate of 7.5e-5 is employed, with a warm-up of 2k steps and a linearly decayed learning rate setting over 200K total training steps. The batch size is set to 28k frames per GPU. In Stage 3, only the LoRA (rank=16, <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>=1) and Conditioning Adapters are updated, while the main model is frozen. We trained the model for 100K steps with a learning rate of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "main",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt a combined objective and subjective evaluation framework. For objective evaluation, we report word error rate (WER) using FireRedASR<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib25\" title=\"\">25</a>]</cite> as the evaluation ASR model; and UTMOSv2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib26\" title=\"\">26</a>]</cite>, a learned metric for predicting speech naturalness. For subjective evaluation, we conduct human listening tests using mean opinion score (MOS).</p>\n\n",
                "matched_terms": [
                    "mos",
                    "metric",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we evaluate our model on multiple Chinese dialects. The abbreviations used in the tables correspond to the following dialects: YUE (Cantonese), SH (Shanghai), CD (Chengdu), XA (Xi&#8217;an), ZZ (Zhengzhou), TJ (Tianjin), NAN (Southern Min), SJZ (Shijiazhuang), NJ (Nanjing), Jingbai and Yunbai (two pronunciation types of Peking Opera).</p>\n\n",
                "matched_terms": [
                    "yue",
                    "yunbai",
                    "model",
                    "dialects",
                    "nan",
                    "sjz",
                    "jingbai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This evaluation covers both objective metrics and subjective listening tests, comparing our system against several state-of-the-art commercial TTS models including Edge TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib18\" title=\"\">18</a>]</cite>, CosyVoice2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib19\" title=\"\">19</a>]</cite>, and Qwen-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib20\" title=\"\">20</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "edge",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first evaluates the impact of the MoE. We compare a model with MoE against a baseline without it. The Second evaluates the impact of phonetic representation, we compare models using IPA and pinyin as input, both with the MoE architecture enabled. According to the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it effectively proves that adding MoE and adopting IPA will achieve better results.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Ablation Study on MOS and WER",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MoE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Input Rep.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Dialects</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding:-0.85pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\" style=\"padding:-0.85pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column\" style=\"padding:-0.85pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">CD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">XA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ZZ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SJZ</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MOS</span></th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o MoE</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">IPA</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o IPA</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">pinyin</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.16</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">IPA</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.22</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER</span><span class=\"ltx_text\" style=\"font-size:80%;\">/%</span>\n</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.85pt 4.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o MoE</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">IPA</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">45.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">41.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">41.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o IPA</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">pinyin</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">93.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">93.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">90.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">90.49</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">IPA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">33.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">29.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">36.83</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "mos",
            "wer",
            "study",
            "×times",
            "model",
            "dialects",
            "ablation",
            "rep",
            "sjz",
            "moe",
            "input",
            "pinyin",
            "ipa"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The first evaluates the impact of the MoE. We compare a model with MoE against a baseline without it. The Second evaluates the impact of phonetic representation, we compare models using IPA and pinyin as input, both with the MoE architecture enabled. According to the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, it effectively proves that adding MoE and adopting IPA will achieve better results.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Dialect speech embodies rich cultural and linguistic diversity, yet building text-to-speech (TTS) systems for dialects remains challenging due to scarce data, inconsistent orthographies, and complex phonetic variation. To address these issues, we present DiaMoE-TTS, a unified IPA-based framework that standardizes phonetic representations and resolves grapheme-to-phoneme ambiguities. Built upon the F5-TTS architecture, the system introduces a dialect-aware Mixture-of-Experts (MoE) to model phonological differences and employs parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and Conditioning Adapters for rapid transfer to new dialects. Unlike approaches dependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable, open-data-driven synthesis. Experiments demonstrate natural and expressive speech generation, achieving zero-shot performance on unseen dialects and specialized domains such as Peking Opera with only a few hours of data. Code and resources are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/GiantAILab/DiaMoE-TTS\" title=\"\">https://github.com/GiantAILab/DiaMoE-TTS</a></p>\n\n",
                "matched_terms": [
                    "moe",
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we leverage linguistic expertise to construct a novel multilingual speech dataset for Chinese dialects. Built upon existing open-source ASR corpora, our dataset introduces a unified International Phonetic Alphabet (IPA)-based pronunciation lexicon<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib6\" title=\"\">6</a>]</cite>, providing aligned text, IPA phonemes, and speech pairs. This resource enables consistent modeling across diverse dialects and serves as a foundation for open-data-driven, zero-shot multilingual and multi-dialect TTS research.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "ipa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the above, we propose a unified framework for low-resource dialect TTS that leverages both cross-lingual pre-training and phonetically grounded modeling. We standardize the pronunciation modeling across dialects using the IPA, constructing a harmonized text-IPA-speech alignment dataset covering multiple Chinese dialects.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "ipa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">&#8226; A unified IPA-based front-end that replaces conventional pinyin or character inputs with explicit phonetic sequences, ensuring precise pronunciation modeling for Chinese dialects.</p>\n\n",
                "matched_terms": [
                    "pinyin",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1 &amp; 2: Joint Training on Multidialect IPA Data</span>\n<br class=\"ltx_break\"/>In both Stage 1 and Stage 2, the model is trained on a unified dataset of IPA-aligned text-speech pairs covering Standard Mandarin and multiple Chinese dialects. It is designed to align diverse input representations (e.g., pinyin, characters) with a unified IPA phoneme space. In stage 2, we add an extra dialect-style MoE module<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib16\" title=\"\">16</a>]</cite> after Text Embedding to enable the model to automatically learn and capture the relevant information of dialect styles, thereby weakening the phenomenon of style averaging.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects",
                    "moe",
                    "input",
                    "pinyin",
                    "ipa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Adaptation to Low-Resource Dialects</span>\n<br class=\"ltx_break\"/>Building upon the multidialect-capable model, Stage 3 performs rapid adaptation to new, low-resource dialects. We adopt a parameter-efficient fine-tuning (PEFT) approach<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib17\" title=\"\">17</a>]</cite> in which only LoRA and Conditioning Adapters are trained, while the backbone model remains frozen. Furthermore, data augmentation methods are also introduced to expand the small training set. This enables speech synthesis with only a few hours of data, preserving multilingual knowledge and mitigating catastrophic forgetting.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This staged training paradigm effectively balances model capacity, data efficiency, and generalization, enabling rapid expansion to new dialects in zero-shot or few-shot speech synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our method employs a single, shared text encoder that assumes relatively stable grapheme-to-phoneme mappings. However, the joint training of multiple dialects can lead to the homogenization of styles, and without additional guidance, the expression of dialectal styles will be weakened. To address this, we incorporate a residual MoE architecture into the text embedding module:</p>\n\n",
                "matched_terms": [
                    "moe",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"p_{\\text{IPA}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>IPA</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{IPA}}</annotation></semantics></math> indicates input IPA frontend sequence, <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> and <math alttext=\"h^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msup><mi>h</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">h^{\\prime}</annotation></semantics></math> are the original and the dialect-aware feature, respectively.</p>\n\n",
                "matched_terms": [
                    "input",
                    "ipa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our MoE module comprises multiple expert networks, each specialized in capturing the phonological characteristics of specific dialects. A learnable gating mechanism dynamically routes input sequences to the most relevant experts based on contextual and linguistic cues. As a guide, we introduce an auxiliary dialect classification loss to encourage the MoE gate. The gate logits are computed from a mean-pooled representation of the input sequence:</p>\n\n",
                "matched_terms": [
                    "input",
                    "moe",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This enables learning of dialect-aware text representation, allowing the model to adaptively generate prosodic patterns that align with the target dialect. In addition, the sparse activation of experts ensures scalability and computational efficiency, making it feasible to support a wide range of dialects within a unified framework.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the scarcity of high-quality speech data for many dialects&#8212;often limited to less than ten hours&#8212;we further investigate the generalization of our framework to extremely low-resource dialects. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib17\" title=\"\">17</a>]</cite>, We incorporate a Conditioning Adapter into the text embedding module and apply LoRA to the query and value projections within the attention layers of the input embedding and DiT blocks. The MoE structure is kept frozen during fine-tuning to preserve its learned routing behavior and avoid overfitting under limited data.</p>\n\n",
                "matched_terms": [
                    "input",
                    "moe",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt a combined objective and subjective evaluation framework. For objective evaluation, we report word error rate (WER) using FireRedASR<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib25\" title=\"\">25</a>]</cite> as the evaluation ASR model; and UTMOSv2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#bib.bib26\" title=\"\">26</a>]</cite>, a learned metric for predicting speech naturalness. For subjective evaluation, we conduct human listening tests using mean opinion score (MOS).</p>\n\n",
                "matched_terms": [
                    "mos",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we evaluate our model on multiple Chinese dialects. The abbreviations used in the tables correspond to the following dialects: YUE (Cantonese), SH (Shanghai), CD (Chengdu), XA (Xi&#8217;an), ZZ (Zhengzhou), TJ (Tianjin), NAN (Southern Min), SJZ (Shijiazhuang), NJ (Nanjing), Jingbai and Yunbai (two pronunciation types of Peking Opera).</p>\n\n",
                "matched_terms": [
                    "study",
                    "sjz",
                    "model",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22727v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Low-resource Dialect Adaptation Method &#8227; 3 Approach &#8227; DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> also displays the WER, MOS and UTMOSv2. While our system falls slightly behind commercial baselines in certain metrics, this observation warrants further clarification. Notably, the primary objective of our method is not to surpass highly optimized proprietary systems, but rather to demonstrate a feasible framework for training with minimal resources. In addition, the apparent performance gap can be attributed to the broader dialectal coverage and the added functionality of voice cloning supported by our TTS model&#8212;capabilities that are absent in most commercial counterparts. Supporting a wide range of dialects and enabling personalized synthesis inevitably increases the modeling complexity, which in turn may lead to a modest degradation in performance on individual dialects.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "dialects",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the superiority of our method, we conduct two ablation studies on four dialects.</p>\n\n",
                "matched_terms": [
                    "dialects",
                    "ablation"
                ]
            }
        ]
    }
}