{
    "S2.T1": {
        "caption": "Table 1: Comparison with related datasets.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Has human interaction</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Setting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Total duration [h]</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Ego4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Daily tasks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3,670</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">EgoExo4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib9\" title=\"\">9</a>]</cite>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Skilled activity</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1,286</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Epic-Kitchen-100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>]</cite>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Cooking</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Assembly101 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib22\" title=\"\">22</a>]</cite>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Toy assembly</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">167</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Holoassist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Instruction</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">166</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">EgoCom <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Group conversation</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">38.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">AEA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib16\" title=\"\">16</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Daily tasks</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">7.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">EgoInstruct (ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Instruction</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "daily",
            "toy",
            "setting",
            "assembly101",
            "group",
            "has",
            "ego4d",
            "instruction",
            "comparison",
            "activity",
            "egoexo4d",
            "cooking",
            "tasks",
            "epickitchen100",
            "egoinstruct",
            "related",
            "egocom",
            "interaction",
            "assembly",
            "conversation",
            "datasets",
            "ours",
            "holoassist",
            "total",
            "duration",
            "human",
            "skilled",
            "dataset",
            "aea"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To analyze tasks and interactions involving multiple people, time-synchronized multi-view datasets are essential. Egocom <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib18\" title=\"\">18</a>]</cite> captures natural conversations in activities like high-fives and card games, providing egocentric footage along with speech recognition annotations and benchmarks for conversation turn prediction. The Aria Everyday Activities dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib16\" title=\"\">16</a>]</cite> collects daily-life tasks such as making coffee or tidying a room, including tasks performed solo or by two people. Although several synchronized multi-person datasets exist, none specifically focuses on face-to-face instruction. Our dataset is designed to fill this gap (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Egocentric Video Datasets &#8227; 2 Related work &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "instruction",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Against this backdrop, egocentric video has attracted considerable attention. Compared to third-person footage, which captures full-body pose and the surrounding environment, egocentric video records the scene from the camera wearer&#8217;s viewpoint. Analyzing egocentric videos that include natural conversation, nods, and hand gestures offers the potential to reveal intentions and motivations underlying human behavior. Because egocentric videos are recorded with head- or body-mounted wearable cameras, they inherently encode the wearer&#8217;s viewpoint and environment, as well as cues to interest and attention. This makes it possible to capture crucial information such as expert gaze patterns and attentional strategies. Even as experts retire, egocentric recordings can preserve rich information; analyzing such videos may also allow novices to virtually experience an expert&#8217;s perspective and deepen their understanding during training.</p>\n\n",
                "matched_terms": [
                    "has",
                    "conversation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While many studies have tackled action and procedure understanding from egocentric video <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib19\" title=\"\">19</a>]</cite>, most do not target face-to-face instructional scenes. In instructional settings, beyond verbal directions from the instructor, there are diverse interactions: nonverbal communication including gestures, learners&#8217; backchanneling and questions, and more. Understanding these interactional elements holistically is essential for improving training effectiveness. However, modeling interaction is difficult since human behavior changes fluidly through mutual influence. Natural interactions that include conversation and bodily motion rarely conform to a single consistent template. Learning from data using machine learning is therefore appealing, yet current research on interaction analysis faces two issues: i) a shortage of datasets specifically capturing face-to-face scenes and ii) limited techniques for analyzing face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "human",
                    "conversation",
                    "interaction",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are many egocentric datasets from a worker&#8217;s viewpoint <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>]</cite>, but most focus on single-person tasks and lack genuine face-to-face interaction. Although the number of egocentric datasets is increasing, large-scale multimodal datasets that capture time-synchronized egocentric views from multiple people engaged in co-present interactions are scarce. In particular, we are not aware of datasets that focus on instruction or collaborative task execution.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "instruction",
                    "datasets",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "interaction",
                    "tasks",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this dataset, we define two fundamental tasks as the first step toward a holistic understanding of instructional interactions. The first is procedural step segmentation, which aims to segment task progress into steps from both the instructor&#8217;s and the learner&#8217;s viewpoints, thereby extracting procedural information useful for skill transfer. The second is conversation-state classification of utterances, for example, &#8220;the learner asks the instructor a question&#8221; or &#8220;the instructor gives instructions.&#8221; Classifying conversation states structures the interaction flow and helps analyze instruction; it can surface difficult points and important tricks while enabling the learner to gradually acquire skills, including tacit knowledge.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "tasks",
                    "dataset",
                    "conversation",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a new egocentric dataset of face-to-face instructional scenes. We define two tasks (procedural step segmentation and conversation-state classification) and provide ground-truth annotations.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A large number of egocentric datasets have been proposed <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib22\" title=\"\">22</a>]</cite>. EPIC-KITCHENS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>]</cite> focuses on activities in the kitchen, and Ego4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>]</cite> provides a large-scale egocentric benchmark with more than 3,670 hours of everyday activities. Ego-Exo4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib9\" title=\"\">9</a>]</cite> captures skilled activities with time-synchronized first- and third-person views. However, these datasets mainly focus on single-person tasks and do not target multi-person interaction; they typically include egocentric views only from the task performer.</p>\n\n",
                "matched_terms": [
                    "egoexo4d",
                    "interaction",
                    "tasks",
                    "skilled",
                    "ego4d",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "related",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "has",
                    "interaction",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Research has also focused on speech in interactive settings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib4\" title=\"\">4</a>]</cite>. HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite> categorizes utterances and assigns labels within the dataset. Saupp&#233; et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib21\" title=\"\">21</a>]</cite> proposed conversational design patterns for human-robot interaction and, in doing so, observed human-to-human interactions across scenarios, identifying seven recurring patterns including question-answer pairs. Porfirio et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib4\" title=\"\">4</a>]</cite> further defined utterance states within seven patterns such as conversation, collaboration, instruction, interview, and storytelling. Our definition of the conversation state is partly based on this instructional template.</p>\n\n",
                "matched_terms": [
                    "holoassist",
                    "interaction",
                    "dataset",
                    "has",
                    "conversation",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most closely related work to ours is HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>. It introduces a large-scale egocentric human interaction dataset in which two people collaboratively complete physical manipulation tasks. Their setting features remote instruction and is motivated by the goal of building an AI assistant that intervenes only when the performer makes a mistake. As a result, the instructor does not intervene proactively but provides help only after errors occur. In contrast, we focus on face-to-face instructional scenes in which the instructor teaches the task step by step and engages in proactive nonverbal communication. Accordingly, our dataset exhibits distinct characteristics relative to HoloAssist, particularly in setting, instructional goals, and interaction dynamics.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "holoassist",
                    "related",
                    "interaction",
                    "setting",
                    "tasks",
                    "human",
                    "dataset",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, many egocentric datasets have been released and there is extensive research on understanding human behavior. There is also growing work on interaction scenes that focuses on attended objects, subtle movements, and utterance content. However, few egocentric datasets capture multiple people simultaneously. In particular, to our knowledge there is no egocentric dataset for instructional scenes that records both the instructor&#8217;s and the learner&#8217;s viewpoints for the purpose of completing a task. Moreover, little prior work examines both linguistic and non-linguistic directives in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "human",
                    "dataset",
                    "instruction",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our goal is to capture the details of interaction in face-to-face instructional scenes by constructing a new dataset and advancing interaction analysis through conversation-state classification and procedural step segmentation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing egocentric datasets provide only the task performer&#8217;s viewpoint for single-person tasks. In contrast, we focus on the interaction between two participants by recording both the instructor and the learner as they face each other while the learner performs the task under instruction. To support interaction analysis, in addition to step annotations, we also provide conversation transcripts and conversation-state annotations.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "tasks",
                    "conversation",
                    "instruction",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "total",
                    "tasks",
                    "assembly",
                    "dataset",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step Labels</span>\nThe four tasks in our dataset follow HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>, anticipating future use of HoloAssist data in pretraining or augmentation. Since the original tasks have different step granularities, we redefined step labels so that the granularity is consistent across all four tasks. The step definitions are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T4\" title=\"Table 4 &#8227; 3.2 Annotation &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "holoassist",
                    "dataset",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Utterances and Conversation States</span>\nWe transcribed the utterances of both the instructor and the learner and added timestamps because utterance timing is crucial for interaction analysis. Conversations were conducted in participants&#8217; native language and transcribed in that language. Based on Porfirio et al.&#8217;s template for conversational state transitions in instructional scenes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib4\" title=\"\">4</a>]</cite> and Wang et al.&#8217;s definitions <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>, we defined a set of conversation-state labels. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T5\" title=\"Table 5 &#8227; 3.2 Annotation &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of annotated utterances, and Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F5\" title=\"Figure 5 &#8227; 3.2 Annotation &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes the conversation-state classes and their distribution.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated two tasks on our dataset: step segmentation from the learner&#8217;s viewpoint and conversation-state classification for utterances exchanged between the instructor and the learner.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "conversation",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "tasks",
                    "dataset",
                    "conversation",
                    "instruction",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work includes scaling up the dataset and tackling more advanced tasks for understanding instructional scenes. As a first step, we built a dataset and focused on fundamental tasks in this paper. Examples of more advanced tasks include recognizing deictic expressions such as &#8220;this&#8221; and &#8220;here,&#8221; estimating the learner&#8217;s level of understanding, and extracting tips and tacit knowledge that are not written in manuals; we will then evaluate the extent to which MLLMs can advance on these fronts. Another direction is to exploit information we recorded but did not use in this paper (third-person views, gaze, and SLAM-based 3D trajectories) and to devise effective ways to incorporate these signals into MLLMs to enable deeper understanding.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tasks"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Task-wise dataset statistics for EgoInstruct.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Positional relationship</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"># steps</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"># sessions</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Average duration</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Cart assembly</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">face-to-face</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">10</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">11m21s</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Chair assembly</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">face-to-face</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">10</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3m49s</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GoPro battery replacement</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">face-to-face</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">10</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3m3s</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Printer toner replacement</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">side by side</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">5m41s</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "relationship",
            "side",
            "facetoface",
            "printer",
            "taskwise",
            "gopro",
            "sessions",
            "average",
            "statistics",
            "chair",
            "egoinstruct",
            "5m41s",
            "dataset",
            "assembly",
            "cart",
            "11m21s",
            "positional",
            "task",
            "toner",
            "steps",
            "duration",
            "3m3s",
            "3m49s",
            "battery",
            "replacement"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are many egocentric datasets from a worker&#8217;s viewpoint <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>]</cite>, but most focus on single-person tasks and lack genuine face-to-face interaction. Although the number of egocentric datasets is increasing, large-scale multimodal datasets that capture time-synchronized egocentric views from multiple people engaged in co-present interactions are scarce. In particular, we are not aware of datasets that focus on instruction or collaborative task execution.</p>\n\n",
                "matched_terms": [
                    "task",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "sessions",
                    "task",
                    "facetoface",
                    "assembly",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this dataset, we define two fundamental tasks as the first step toward a holistic understanding of instructional interactions. The first is procedural step segmentation, which aims to segment task progress into steps from both the instructor&#8217;s and the learner&#8217;s viewpoints, thereby extracting procedural information useful for skill transfer. The second is conversation-state classification of utterances, for example, &#8220;the learner asks the instructor a question&#8221; or &#8220;the instructor gives instructions.&#8221; Classifying conversation states structures the interaction flow and helps analyze instruction; it can surface difficult points and important tricks while enabling the learner to gradually acquire skills, including tacit knowledge.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our experiments, we annotate procedural steps from both viewpoints, and we also annotate the utterance spans, their transcripts, and the conversation state (e.g., &#8220;question from learner,&#8221; &#8220;instruction from instructor&#8221;). Our goal is to provide a foundation for the analysis of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a new egocentric dataset of face-to-face instructional scenes. We define two tasks (procedural step segmentation and conversation-state classification) and provide ground-truth annotations.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze tasks and interactions involving multiple people, time-synchronized multi-view datasets are essential. Egocom <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib18\" title=\"\">18</a>]</cite> captures natural conversations in activities like high-fives and card games, providing egocentric footage along with speech recognition annotations and benchmarks for conversation turn prediction. The Aria Everyday Activities dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib16\" title=\"\">16</a>]</cite> collects daily-life tasks such as making coffee or tidying a room, including tasks performed solo or by two people. Although several synchronized multi-person datasets exist, none specifically focuses on face-to-face instruction. Our dataset is designed to fill this gap (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Egocentric Video Datasets &#8227; 2 Related work &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most closely related work to ours is HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>. It introduces a large-scale egocentric human interaction dataset in which two people collaboratively complete physical manipulation tasks. Their setting features remote instruction and is motivated by the goal of building an AI assistant that intervenes only when the performer makes a mistake. As a result, the instructor does not intervene proactively but provides help only after errors occur. In contrast, we focus on face-to-face instructional scenes in which the instructor teaches the task step by step and engages in proactive nonverbal communication. Accordingly, our dataset exhibits distinct characteristics relative to HoloAssist, particularly in setting, instructional goals, and interaction dynamics.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, many egocentric datasets have been released and there is extensive research on understanding human behavior. There is also growing work on interaction scenes that focuses on attended objects, subtle movements, and utterance content. However, few egocentric datasets capture multiple people simultaneously. In particular, to our knowledge there is no egocentric dataset for instructional scenes that records both the instructor&#8217;s and the learner&#8217;s viewpoints for the purpose of completing a task. Moreover, little prior work examines both linguistic and non-linguistic directives in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our goal is to capture the details of interaction in face-to-face instructional scenes by constructing a new dataset and advancing interaction analysis through conversation-state classification and procedural step segmentation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "task",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "dataset",
                    "facetoface"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "task",
                    "cart",
                    "toner",
                    "chair",
                    "assembly",
                    "printer",
                    "battery",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "task",
                    "cart",
                    "toner",
                    "chair",
                    "assembly",
                    "printer",
                    "battery",
                    "replacement",
                    "gopro"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Recording devices and settings.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Device</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"># cameras</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">View</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Resolution</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">FPS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Aria Glass</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">FPV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1400x1400</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GoPro HERO11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">TPV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">4K (Wide)</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">60</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cameras",
            "fps",
            "hero11",
            "fpv",
            "1400x1400",
            "device",
            "devices",
            "aria",
            "view",
            "recording",
            "settings",
            "gopro",
            "glass",
            "wide",
            "tpv",
            "resolution"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Both the instructor and the learner wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite> to record egocentric video. Through the Aria API, we can obtain gaze information and head pose from the camera trajectory, which we expect will be valuable for future interaction analysis. Additionally, we recorded two fixed third-person views with GoPro HERO11 cameras to enable multi-view analyses in future work, although these third-person views are not used in the experiments in this paper. All four cameras were time-synchronized using QR timecodes. Camera settings are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T3\" title=\"Table 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "cameras",
                    "gopro",
                    "recording"
                ]
            }
        ]
    },
    "S3.F3": {
        "caption": "(a) GoPro use",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<div class=\"ltx_block ltx_minipage ltx_align_top\" style=\"width:130.1pt;\">\n<img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"140\" id=\"S3.F3.g1\" src=\"x3.png\" width=\"249\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_block\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span><span class=\"ltx_text\" style=\"font-size:90%;\">GoPro use</span></figcaption>\n</div>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<div class=\"ltx_block ltx_minipage ltx_align_top\" style=\"width:130.1pt;\">\n<img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"140\" id=\"S3.F3.g2\" src=\"x4.png\" width=\"249\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_block\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span><span class=\"ltx_text\" style=\"font-size:90%;\">Printer toner replacement</span></figcaption>\n</div>\n<div class=\"ltx_block ltx_minipage ltx_align_top\" style=\"width:130.1pt;\">\n<img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"140\" id=\"S3.F3.g3\" src=\"x5.png\" width=\"249\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_block\"><span class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span><span class=\"ltx_text\" style=\"font-size:90%;\">Furniture assembly</span></figcaption>\n</div>\n</td>\n<td class=\"ltx_td\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "use",
            "toner",
            "printer",
            "furniture",
            "assembly",
            "replacement",
            "gopro"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "assembly",
                    "furniture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "toner",
                    "assembly",
                    "printer",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "toner",
                    "assembly",
                    "printer",
                    "replacement",
                    "gopro"
                ]
            }
        ]
    },
    "S3.T4": {
        "caption": "Table 4: Tasks and steps in the EgoInstruct dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Task</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Step</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Cart assembly</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">assemble two sidebars</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">assemble base frame</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attach the sidebar</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attach the lower tray</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attach the middle tray</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attach the upper tray</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attach the four wheels</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Chair assembly</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">assemble the legs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attatch the seat to the legs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">fasten with a screw</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GoPro battery replacement</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">replace the battery</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">replace the microSD card</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">turn on power</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">turn off power</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">attach the peg</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">detach the peg</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Printer toner replacement</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">put paper</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">print in color</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">copy in black and white</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">replace cartridge</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "four",
            "print",
            "copy",
            "tray",
            "attach",
            "printer",
            "microsd",
            "gopro",
            "two",
            "frame",
            "battery",
            "put",
            "sidebar",
            "step",
            "base",
            "black",
            "cartridge",
            "color",
            "screw",
            "chair",
            "sidebars",
            "egoinstruct",
            "tasks",
            "turn",
            "legs",
            "replace",
            "off",
            "power",
            "card",
            "attatch",
            "assemble",
            "assembly",
            "lower",
            "middle",
            "seat",
            "peg",
            "cart",
            "detach",
            "white",
            "task",
            "wheels",
            "toner",
            "steps",
            "upper",
            "paper",
            "dataset",
            "replacement",
            "fasten"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step Labels</span>\nThe four tasks in our dataset follow HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>, anticipating future use of HoloAssist data in pretraining or augmentation. Since the original tasks have different step granularities, we redefined step labels so that the granularity is consistent across all four tasks. The step definitions are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T4\" title=\"Table 4 &#8227; 3.2 Annotation &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tasks",
                    "step",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are many egocentric datasets from a worker&#8217;s viewpoint <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>]</cite>, but most focus on single-person tasks and lack genuine face-to-face interaction. Although the number of egocentric datasets is increasing, large-scale multimodal datasets that capture time-synchronized egocentric views from multiple people engaged in co-present interactions are scarce. In particular, we are not aware of datasets that focus on instruction or collaborative task execution.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset",
                    "tasks",
                    "assembly",
                    "paper",
                    "battery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this dataset, we define two fundamental tasks as the first step toward a holistic understanding of instructional interactions. The first is procedural step segmentation, which aims to segment task progress into steps from both the instructor&#8217;s and the learner&#8217;s viewpoints, thereby extracting procedural information useful for skill transfer. The second is conversation-state classification of utterances, for example, &#8220;the learner asks the instructor a question&#8221; or &#8220;the instructor gives instructions.&#8221; Classifying conversation states structures the interaction flow and helps analyze instruction; it can surface difficult points and important tricks while enabling the learner to gradually acquire skills, including tacit knowledge.</p>\n\n",
                "matched_terms": [
                    "step",
                    "task",
                    "steps",
                    "tasks",
                    "dataset",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a new egocentric dataset of face-to-face instructional scenes. We define two tasks (procedural step segmentation and conversation-state classification) and provide ground-truth annotations.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tasks",
                    "step",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A large number of egocentric datasets have been proposed <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib22\" title=\"\">22</a>]</cite>. EPIC-KITCHENS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib3\" title=\"\">3</a>]</cite> focuses on activities in the kitchen, and Ego4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib8\" title=\"\">8</a>]</cite> provides a large-scale egocentric benchmark with more than 3,670 hours of everyday activities. Ego-Exo4D <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib9\" title=\"\">9</a>]</cite> captures skilled activities with time-synchronized first- and third-person views. However, these datasets mainly focus on single-person tasks and do not target multi-person interaction; they typically include egocentric views only from the task performer.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze tasks and interactions involving multiple people, time-synchronized multi-view datasets are essential. Egocom <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib18\" title=\"\">18</a>]</cite> captures natural conversations in activities like high-fives and card games, providing egocentric footage along with speech recognition annotations and benchmarks for conversation turn prediction. The Aria Everyday Activities dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib16\" title=\"\">16</a>]</cite> collects daily-life tasks such as making coffee or tidying a room, including tasks performed solo or by two people. Although several synchronized multi-person datasets exist, none specifically focuses on face-to-face instruction. Our dataset is designed to fill this gap (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Egocentric Video Datasets &#8227; 2 Related work &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "card",
                    "tasks",
                    "turn",
                    "dataset",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video understanding encompasses downstream tasks such as procedure recognition, step recognition, and step prediction. Among large-scale instructional corpora, HowTo100M <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib17\" title=\"\">17</a>]</cite> collects YouTube how-to videos, and subsequent work builds procedure knowledge graphs to infer the structured organization of tasks demonstrated in videos.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are also methods that use egocentric video to discover object-use procedures. For example, Damen et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib2\" title=\"\">2</a>]</cite> proposed a method to discover how objects are used from first-person recordings of predefined tasks. They leverage gaze to locate task-relevant objects, characterize their appearance and usage, and retrieve video snippets to explain how a recognized object was used previously. Although Paprika <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib28\" title=\"\">28</a>]</cite> and You-Do, I-Learn <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib2\" title=\"\">2</a>]</cite> focus on procedure discovery, they assume single-actor task execution rather than interactive scenes. In contrast, we focus on the conversations that occur while an instructor teaches a learner.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The most closely related work to ours is HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>. It introduces a large-scale egocentric human interaction dataset in which two people collaboratively complete physical manipulation tasks. Their setting features remote instruction and is motivated by the goal of building an AI assistant that intervenes only when the performer makes a mistake. As a result, the instructor does not intervene proactively but provides help only after errors occur. In contrast, we focus on face-to-face instructional scenes in which the instructor teaches the task step by step and engages in proactive nonverbal communication. Accordingly, our dataset exhibits distinct characteristics relative to HoloAssist, particularly in setting, instructional goals, and interaction dynamics.</p>\n\n",
                "matched_terms": [
                    "step",
                    "task",
                    "tasks",
                    "dataset",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed above, many egocentric datasets have been released and there is extensive research on understanding human behavior. There is also growing work on interaction scenes that focuses on attended objects, subtle movements, and utterance content. However, few egocentric datasets capture multiple people simultaneously. In particular, to our knowledge there is no egocentric dataset for instructional scenes that records both the instructor&#8217;s and the learner&#8217;s viewpoints for the purpose of completing a task. Moreover, little prior work examines both linguistic and non-linguistic directives in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our goal is to capture the details of interaction in face-to-face instructional scenes by constructing a new dataset and advancing interaction analysis through conversation-state classification and procedural step segmentation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing egocentric datasets provide only the task performer&#8217;s viewpoint for single-person tasks. In contrast, we focus on the interaction between two participants by recording both the instructor and the learner as they face each other while the learner performs the task under instruction. To support interaction analysis, in addition to step annotations, we also provide conversation transcripts and conversation-state annotations.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "step",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "four",
                    "task",
                    "cart",
                    "toner",
                    "dataset",
                    "chair",
                    "tasks",
                    "printer",
                    "assembly",
                    "battery",
                    "replacement",
                    "gopro",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both the instructor and the learner wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite> to record egocentric video. Through the Aria API, we can obtain gaze information and head pose from the camera trajectory, which we expect will be valuable for future interaction analysis. Additionally, we recorded two fixed third-person views with GoPro HERO11 cameras to enable multi-view analyses in future work, although these third-person views are not used in the experiments in this paper. All four cameras were time-synchronized using QR timecodes. Camera settings are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T3\" title=\"Table 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "four",
                    "gopro",
                    "paper",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We annotated steps separately for the instructor and the learner because the timing differs by viewpoint. For the instructor, a step starts when the instructor begins explaining it. For the learner, a step starts when the learner begins executing the step after listening to the instructor. We therefore provide distinct timestamps for steps from both viewpoints (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Annotation &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "steps",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated two tasks on our dataset: step segmentation from the learner&#8217;s viewpoint and conversation-state classification for utterances exchanged between the instructor and the learner.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tasks",
                    "step",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "task",
                    "steps",
                    "step",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASFormer</span>\nWe assessed how well step segmentation could be predicted from egocentric video when training on a subset of the dataset. We performed 5-fold cross-validation: in each fold, we trained on eight participants and tested on the remaining two. Only the learner&#8217;s egocentric video was used; third-person views were excluded. We trained with an initial learning rate of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> using the Adam optimizer for 20 epochs on a single NVIDIA A100-SXM4-40GB GPU.\nWe used I3D features extracted with a 3D ResNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib23\" title=\"\">23</a>]</cite> pretrained on Kinetics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "step",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F6\" title=\"Figure 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> visualizes example segmentations. Compared to MLLMs, ASFormer tended to over-segment steps. We hypothesize that ASFormer primarily relied on visual pattern matching to actions seen during training, whereas MLLMs leveraged broad prior knowledge acquired during pretraining to produce more reasonable segmentations. We also observed that boundaries became more accurate when combining modalities. Purely visual input often lacks sufficient cues, and utterances that indicate the next step are not synchronized with the exact timing of the physical action. Multimodal fusion was therefore necessary for accurate boundaries.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset",
                    "lower"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "step",
                    "steps",
                    "tasks",
                    "dataset",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work includes scaling up the dataset and tackling more advanced tasks for understanding instructional scenes. As a first step, we built a dataset and focused on fundamental tasks in this paper. Examples of more advanced tasks include recognizing deictic expressions such as &#8220;this&#8221; and &#8220;here,&#8221; estimating the learner&#8217;s level of understanding, and extracting tips and tacit knowledge that are not written in manuals; we will then evaluate the extent to which MLLMs can advance on these fronts. Another direction is to exploit information we recorded but did not use in this paper (third-person views, gaze, and SLAM-based 3D trajectories) and to devise effective ways to incorporate these signals into MLLMs to enable deeper understanding.</p>\n\n",
                "matched_terms": [
                    "paper",
                    "tasks",
                    "step",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "step",
                    "task",
                    "cart",
                    "toner",
                    "chair",
                    "assembly",
                    "printer",
                    "battery",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.F8\" title=\"Figure 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> visualizes the results of step segmentation on &#8220;chair assembly&#8221; and &#8220;Printer toner replacement&#8221; tasks.</p>\n\n",
                "matched_terms": [
                    "toner",
                    "step",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "task",
                    "cart",
                    "toner",
                    "chair",
                    "assembly",
                    "printer",
                    "battery",
                    "replacement",
                    "gopro"
                ]
            }
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Step segmentation results on EgoInstruct across input modalities.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Video</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Edit</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">ASFormer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">55.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">50.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">41.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">42.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">79.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">80.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">72.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">58.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">86.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">58.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">94.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">91.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">85.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">89.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">83.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">91.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">87.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">73.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">89.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">75.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">81.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">73.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">55.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">81.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">62.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">89.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">86.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">72.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">88.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">77.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">95.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">92.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">85.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">92.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">85.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "f150",
            "edit",
            "step",
            "f125",
            "audio",
            "gemini25pro",
            "egoinstruct",
            "input",
            "asformer",
            "text",
            "across",
            "f110",
            "results",
            "gpt4o",
            "video",
            "model",
            "acc",
            "segmentation",
            "modalities"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "step",
                    "audio",
                    "video",
                    "segmentation",
                    "modalities",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this dataset, we define two fundamental tasks as the first step toward a holistic understanding of instructional interactions. The first is procedural step segmentation, which aims to segment task progress into steps from both the instructor&#8217;s and the learner&#8217;s viewpoints, thereby extracting procedural information useful for skill transfer. The second is conversation-state classification of utterances, for example, &#8220;the learner asks the instructor a question&#8221; or &#8220;the instructor gives instructions.&#8221; Classifying conversation states structures the interaction flow and helps analyze instruction; it can surface difficult points and important tricks while enabling the learner to gradually acquire skills, including tacit knowledge.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a new egocentric dataset of face-to-face instructional scenes. We define two tasks (procedural step segmentation and conversation-state classification) and provide ground-truth annotations.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Video understanding encompasses downstream tasks such as procedure recognition, step recognition, and step prediction. Among large-scale instructional corpora, HowTo100M <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib17\" title=\"\">17</a>]</cite> collects YouTube how-to videos, and subsequent work builds procedure knowledge graphs to infer the structured organization of tasks demonstrated in videos.</p>\n\n",
                "matched_terms": [
                    "video",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "segmentation",
                    "model",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "segmentation",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our goal is to capture the details of interaction in face-to-face instructional scenes by constructing a new dataset and advancing interaction analysis through conversation-state classification and procedural step segmentation.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step Labels</span>\nThe four tasks in our dataset follow HoloAssist <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib24\" title=\"\">24</a>]</cite>, anticipating future use of HoloAssist data in pretraining or augmentation. Since the original tasks have different step granularities, we redefined step labels so that the granularity is consistent across all four tasks. The step definitions are provided in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T4\" title=\"Table 4 &#8227; 3.2 Annotation &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated two tasks on our dataset: step segmentation from the learner&#8217;s viewpoint and conversation-state classification for utterances exchanged between the instructor and the learner.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "step",
                    "gemini25pro",
                    "segmentation",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASFormer</span>\nWe assessed how well step segmentation could be predicted from egocentric video when training on a subset of the dataset. We performed 5-fold cross-validation: in each fold, we trained on eight participants and tested on the remaining two. Only the learner&#8217;s egocentric video was used; third-person views were excluded. We trained with an initial learning rate of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> using the Adam optimizer for 20 epochs on a single NVIDIA A100-SXM4-40GB GPU.\nWe used I3D features extracted with a 3D ResNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib23\" title=\"\">23</a>]</cite> pretrained on Kinetics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "segmentation",
                    "step",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "step",
                    "gpt4o",
                    "audio",
                    "video",
                    "model",
                    "gemini25pro",
                    "modalities",
                    "input",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F6\" title=\"Figure 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> visualizes example segmentations. Compared to MLLMs, ASFormer tended to over-segment steps. We hypothesize that ASFormer primarily relied on visual pattern matching to actions seen during training, whereas MLLMs leveraged broad prior knowledge acquired during pretraining to produce more reasonable segmentations. We also observed that boundaries became more accurate when combining modalities. Purely visual input often lacks sufficient cues, and utterances that indicate the next step are not synchronized with the exact timing of the physical action. Multimodal fusion was therefore necessary for accurate boundaries.</p>\n\n",
                "matched_terms": [
                    "step",
                    "results",
                    "modalities",
                    "input",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "step",
                    "gemini25pro",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "modalities",
                    "segmentation",
                    "step",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.F8\" title=\"Figure 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> visualizes the results of step segmentation on &#8220;chair assembly&#8221; and &#8220;Printer toner replacement&#8221; tasks.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S7.F9\" title=\"Figure 9 &#8227; 7.1 Step Segmentation &#8227; 7 MLLM Prompt &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the prompt used for procedural step segmentation.</p>\n\n",
                "matched_terms": [
                    "segmentation",
                    "step"
                ]
            }
        ]
    },
    "S4.T7": {
        "caption": "Table 7: Performance comparison of conversation type classification.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Recall</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1 score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Without context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">BERT</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.74</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.72</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.74</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">With context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.78</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.84</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score",
            "type",
            "gpt4o",
            "without",
            "context",
            "gemini25pro",
            "acc",
            "classification",
            "recall",
            "precision",
            "bert",
            "conversation",
            "comparison",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "without",
                    "classification",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this dataset, we define two fundamental tasks as the first step toward a holistic understanding of instructional interactions. The first is procedural step segmentation, which aims to segment task progress into steps from both the instructor&#8217;s and the learner&#8217;s viewpoints, thereby extracting procedural information useful for skill transfer. The second is conversation-state classification of utterances, for example, &#8220;the learner asks the instructor a question&#8221; or &#8220;the instructor gives instructions.&#8221; Classifying conversation states structures the interaction flow and helps analyze instruction; it can surface difficult points and important tricks while enabling the learner to gradually acquire skills, including tacit knowledge.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "comparison",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "performance",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "conversation",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "without",
                    "context",
                    "gemini25pro",
                    "bert",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nOverall, labels corresponding to instructional speech, such as <em class=\"ltx_emph ltx_font_italic\">instruction</em> and <em class=\"ltx_emph ltx_font_italic\">additional instruction</em>, were often predicted correctly, likely because these utterances were longer and contained enough information within a single sentence. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F7\" title=\"Figure 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows qualitative examples. Without context, we observed errors such as answering before the question was asked or swapping speaker roles, but these were largely resolved when the full conversational context was provided.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context",
                    "classification",
                    "conversation",
                    "performance"
                ]
            }
        ]
    },
    "S6.T8.st1": {
        "caption": "(a) Cart assembly",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Video</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st1.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Edit</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">ASFormer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">68.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">62.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">57.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">53.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">91.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">72.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">62.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">46.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">82.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">49.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">97.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">96.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">94.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">95.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">85.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">89.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">88.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">83.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">93.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">79.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">75.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">70.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">52.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">84.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">58.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">84.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">83.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">70.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">86.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">72.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">99.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">97.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">97.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">98.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">88.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt4o",
            "f125",
            "f110",
            "audio",
            "video",
            "model",
            "gemini25pro",
            "acc",
            "text",
            "assembly",
            "f150",
            "edit",
            "asformer",
            "cart"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "video",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "video",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "model",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "cart",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASFormer</span>\nWe assessed how well step segmentation could be predicted from egocentric video when training on a subset of the dataset. We performed 5-fold cross-validation: in each fold, we trained on eight participants and tested on the remaining two. Only the learner&#8217;s egocentric video was used; third-person views were excluded. We trained with an initial learning rate of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> using the Adam optimizer for 20 epochs on a single NVIDIA A100-SXM4-40GB GPU.\nWe used I3D features extracted with a 3D ResNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib23\" title=\"\">23</a>]</cite> pretrained on Kinetics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "audio",
                    "video",
                    "model",
                    "gemini25pro",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro",
                    "edit",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "cart",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "cart",
                    "assembly"
                ]
            }
        ]
    },
    "S6.T8.st2": {
        "caption": "(b) Chair assembly",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Video</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st2.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Edit</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">ASFormer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">66.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">66.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">66.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">56.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">95.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">64.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">38.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">69.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">51.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">80.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">70.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">56.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">67.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">77.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">80.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">75.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">53.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">71.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">75.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">71.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">55.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">30.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">60.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">64.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">76.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">69.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">43.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">70.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">70.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">84.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">76.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">65.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">73.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">82.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt4o",
            "f125",
            "f110",
            "audio",
            "video",
            "model",
            "gemini25pro",
            "chair",
            "acc",
            "assembly",
            "f150",
            "edit",
            "asformer",
            "text"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "video",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "video",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "model",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "chair",
                    "video",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASFormer</span>\nWe assessed how well step segmentation could be predicted from egocentric video when training on a subset of the dataset. We performed 5-fold cross-validation: in each fold, we trained on eight participants and tested on the remaining two. Only the learner&#8217;s egocentric video was used; third-person views were excluded. We trained with an initial learning rate of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> using the Adam optimizer for 20 epochs on a single NVIDIA A100-SXM4-40GB GPU.\nWe used I3D features extracted with a 3D ResNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib23\" title=\"\">23</a>]</cite> pretrained on Kinetics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "audio",
                    "video",
                    "model",
                    "gemini25pro",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro",
                    "edit",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "chair",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "chair",
                    "assembly"
                ]
            }
        ]
    },
    "S6.T8.st3": {
        "caption": "(c) GoPro battery replacement",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Video</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st3.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st3.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Edit</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">ASFormer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">34.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">21.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">8.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">27.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">51.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">90.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">82.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">98.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">93.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">97.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">87.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">97.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">97.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">86.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">96.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">79.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">91.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">85.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">71.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">86.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">73.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">95.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">95.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">82.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">97.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">81.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">98.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">95.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">88.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">97.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">85.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt4o",
            "f125",
            "edit",
            "f110",
            "audio",
            "video",
            "model",
            "gemini25pro",
            "acc",
            "text",
            "f150",
            "battery",
            "asformer",
            "replacement",
            "gopro"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "video",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We therefore present a new egocentric dataset targeting face-to-face instructional scenes (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We collected 38 sessions totaling 8 hours in which an instructor teaches tasks such as furniture assembly or replacing a camera battery while a learner executes the task. All participants wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite>, which provide, in addition to first-person video, gaze information, skeletal keypoints for the wrist and palm, and head pose estimated from SLAM-based camera localization. Having synchronized egocentric views and these signals from both the instructor and the learner is a distinctive feature compared with prior work. Although we do not analyze gaze or head pose in this paper, we expect gaze (which indicates where the instructor and learner look while acting) and relative 3D positions obtained by SLAM to serve as important cues for future interaction analysis.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "model",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "video",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both the instructor and the learner wore Aria Glasses <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib6\" title=\"\">6</a>]</cite> to record egocentric video. Through the Aria API, we can obtain gaze information and head pose from the camera trajectory, which we expect will be valuable for future interaction analysis. Additionally, we recorded two fixed third-person views with GoPro HERO11 cameras to enable multi-view analyses in future work, although these third-person views are not used in the experiments in this paper. All four cameras were time-synchronized using QR timecodes. Camera settings are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T3\" title=\"Table 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASFormer</span>\nWe assessed how well step segmentation could be predicted from egocentric video when training on a subset of the dataset. We performed 5-fold cross-validation: in each fold, we trained on eight participants and tested on the remaining two. Only the learner&#8217;s egocentric video was used; third-person views were excluded. We trained with an initial learning rate of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> using the Adam optimizer for 20 epochs on a single NVIDIA A100-SXM4-40GB GPU.\nWe used I3D features extracted with a 3D ResNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib23\" title=\"\">23</a>]</cite> pretrained on Kinetics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "audio",
                    "video",
                    "model",
                    "gemini25pro",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro",
                    "edit",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "replacement",
                    "gopro"
                ]
            }
        ]
    },
    "S6.T8.st4": {
        "caption": "(d) Printer toner replacement",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Video</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Text</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>25</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1<math alttext=\"@\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T8.st4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">@</mi><annotation encoding=\"application/x-tex\">@</annotation></semantics></math>50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Edit</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">ASFormer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">50.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">50.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">35.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">33.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">79.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">87.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">73.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">60.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">94.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">52.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">96.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">82.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">90.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">70.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">88.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">82.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">65.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">94.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">54.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">98.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">92.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">83.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">90.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">85.5</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt4o",
            "f125",
            "f110",
            "audio",
            "video",
            "model",
            "toner",
            "gemini25pro",
            "acc",
            "printer",
            "f150",
            "edit",
            "asformer",
            "replacement",
            "text"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "video",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand behavior, content, and meaning in such scenes, methods must integrate verbal and nonverbal communication because multiple modalities (language, speech, and vision) are involved simultaneously. Recently, multimodal large language models (MLLMs) that jointly process images, audio, and text have emerged, offering a new avenue for this challenge and enabling more holistic scene understanding. Using our dataset, we therefore evaluate MLLMs alongside conventional specialized models to clarify how well current machine learning models understand face-to-face instruction. In our experiments, MLLMs outperform task-specific models without additional fine-tuning, indicating their potential for comprehensive understanding of face-to-face instructional scenes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Action segmentation temporally partitions a video by predicting a class label for each frame. As there is already a large body of work on this task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib5\" title=\"\">5</a>]</cite>, we briefly review representative methods. Lea et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib13\" title=\"\">13</a>]</cite> introduced hierarchical temporal convolutions for efficient computation. MS-TCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib14\" title=\"\">14</a>]</cite> extends this line with dilated temporal convolutions, enabling long-range modeling at high temporal resolution. Building on the observation that action segmentation is closely related to language tasks, Yi et al. proposed a Transformer-based model, ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "model",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Temporal segmentation models such as ASFormer have improved the detection of procedural steps and action intervals. However, most prior work targets single-person activities and has not been applied to scenarios with rich interaction such as face-to-face instruction. In this work, we use these methods together with MLLMs to analyze procedures and actions that unfold between an instructor and a learner from egocentric video.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "replacement",
                    "toner",
                    "printer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "asformer",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASFormer</span>\nWe assessed how well step segmentation could be predicted from egocentric video when training on a subset of the dataset. We performed 5-fold cross-validation: in each fold, we trained on eight participants and tested on the remaining two. Only the learner&#8217;s egocentric video was used; third-person views were excluded. We trained with an initial learning rate of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math> using the Adam optimizer for 20 epochs on a single NVIDIA A100-SXM4-40GB GPU.\nWe used I3D features extracted with a 3D ResNet backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib23\" title=\"\">23</a>]</cite> pretrained on Kinetics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib1\" title=\"\">1</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "asformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "audio",
                    "video",
                    "model",
                    "gemini25pro",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro",
                    "edit",
                    "asformer",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "replacement",
                    "toner",
                    "printer"
                ]
            }
        ]
    },
    "S6.T9.st1": {
        "caption": "(a) Cart assembly",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Recall</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1 score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Without context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">BERT</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.69</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.63</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.67</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">With context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.70</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.77</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score",
            "gpt4o",
            "without",
            "context",
            "gemini25pro",
            "acc",
            "recall",
            "assembly",
            "precision",
            "bert",
            "cart"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "cart",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "without",
                    "context",
                    "gemini25pro",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "without",
                    "bert",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nOverall, labels corresponding to instructional speech, such as <em class=\"ltx_emph ltx_font_italic\">instruction</em> and <em class=\"ltx_emph ltx_font_italic\">additional instruction</em>, were often predicted correctly, likely because these utterances were longer and contained enough information within a single sentence. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F7\" title=\"Figure 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows qualitative examples. Without context, we observed errors such as answering before the question was asked or swapping speaker roles, but these were largely resolved when the full conversational context was provided.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "cart",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "cart",
                    "assembly"
                ]
            }
        ]
    },
    "S6.T9.st2": {
        "caption": "(b) Chair assembly",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Recall</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1 score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Without context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">BERT</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.70</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.74</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">With context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.81</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score",
            "gpt4o",
            "without",
            "context",
            "gemini25pro",
            "chair",
            "acc",
            "recall",
            "assembly",
            "precision",
            "bert"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "chair",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "without",
                    "context",
                    "gemini25pro",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "without",
                    "bert",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nOverall, labels corresponding to instructional speech, such as <em class=\"ltx_emph ltx_font_italic\">instruction</em> and <em class=\"ltx_emph ltx_font_italic\">additional instruction</em>, were often predicted correctly, likely because these utterances were longer and contained enough information within a single sentence. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F7\" title=\"Figure 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows qualitative examples. Without context, we observed errors such as answering before the question was asked or swapping speaker roles, but these were largely resolved when the full conversational context was provided.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "chair",
                    "assembly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "chair",
                    "assembly"
                ]
            }
        ]
    },
    "S6.T9.st3": {
        "caption": "(c) GoPro battery replacement",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Recall</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1 score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Without context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">BERT</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.79</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.75</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">With context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.81</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.85</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score",
            "gpt4o",
            "without",
            "context",
            "gemini25pro",
            "acc",
            "recall",
            "precision",
            "battery",
            "bert",
            "replacement",
            "gopro"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "without",
                    "context",
                    "gemini25pro",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "without",
                    "bert",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nOverall, labels corresponding to instructional speech, such as <em class=\"ltx_emph ltx_font_italic\">instruction</em> and <em class=\"ltx_emph ltx_font_italic\">additional instruction</em>, were often predicted correctly, likely because these utterances were longer and contained enough information within a single sentence. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F7\" title=\"Figure 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows qualitative examples. Without context, we observed errors such as answering before the question was asked or swapping speaker roles, but these were largely resolved when the full conversational context was provided.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "replacement",
                    "gopro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "battery",
                    "replacement",
                    "gopro"
                ]
            }
        ]
    },
    "S6.T9.st4": {
        "caption": "(d) Printer toner replacement",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Acc</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Recall</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">F1 score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Without context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">BERT</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.79</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.78</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">With context</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.83</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Gemini-2.5-pro</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">0.87</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "score",
            "gpt4o",
            "without",
            "toner",
            "context",
            "gemini25pro",
            "acc",
            "recall",
            "printer",
            "precision",
            "bert",
            "replacement"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T9.st4\" title=\"Table 9(d) &#8227; Table 9 &#8227; 6.2 Conversation-state classification &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">9(d)</span></a> shows the conversation-state classification results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification.\nUsing this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models.\nSince face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner.\nAccordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text.\nThis evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes.\nIn experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset includes videos in which an instructor teaches one of the four tasks (cart assembly, chair assembly, GoPro battery replacement, printer toner replacement) listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.T2\" title=\"Table 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> to a learner, who then executes the task. Ten learners (6 male, 4 female) and two instructors (1 male, 1 female) participated. The spatial layout of the instructor, the learner, and the cameras is shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Before recording, instructors were given time to review the task procedures and, if necessary, consult an instruction sheet. To ensure visibility from both viewpoints, all tasks except printer toner replacement were performed face-to-face. Only the printer toner replacement was performed side by side. Each participant completed all four tasks, yielding 38 video sessions and a total of 8 hours of footage. Learners&#8217; prior experience levels for each task are summarized in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S3.F3\" title=\"Figure 3 &#8227; 3 Dataset Construction &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "replacement",
                    "toner",
                    "printer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For step segmentation, we followed the standard action segmentation protocol, predicting the temporal intervals of procedural steps for each task. We compared ASFormer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib26\" title=\"\">26</a>]</cite>, a state-of-the-art method for action segmentation, against two MLLMs, Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MLLM Setup</span>\nFor Gemini-2.5-pro, we evaluated multiple input modalities: video only, audio only, text only, video+audio, and video+audio+text. We used the same test set as ASFormer to ensure a fair comparison. Gemini-2.5-pro was evaluated in a zero-shot setting with no training data. For a model comparison, we also evaluated GPT-4o under the text-only condition with the same test data. The prompts are provided in the supplementary material. The set of step labels was assumed known in the prompts, and the model was asked to output the start and end times of each step along with the label.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T6\" title=\"Table 6 &#8227; 4.1 Step Segmentation &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> summarizes the results. With video-only input, zero-shot Gemini outperformed the trained ASFormer on F1 and Edit metrics, although ASFormer achieved higher frame-wise accuracy due to over-segmentation. When both models were evaluated under the same input modality (text only), Gemini-2.5-pro outperformed GPT-4o. Its performance was strong across individual modalities and improved further when modalities were combined; the best results were obtained with video+audio+text. These findings indicate that MLLMs effectively integrated multimodal cues to segment steps in face-to-face instruction.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step toward comprehensive understanding of face-to-face instruction, we classified the conversation state for each utterance. All utterances in the dataset are labeled with conversation-state annotations. We took the utterance text as input and predicted the conversation-state label. We compared a widely used NLP baseline, BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#bib.bib12\" title=\"\">12</a>]</cite>, with the LLMs Gemini-2.5-pro and GPT-4o.</p>\n\n",
                "matched_terms": [
                    "bert",
                    "gpt4o",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM</span>\nGPT-4o and Gemini-2.5-pro were used without additional fine-tuning. Following the in-context learning paradigm, we supplied in-context exemplars from eight participants, namely pairs of utterances with conversation-state labels, and performed inference on the remaining two participants&#8217; test set. We evaluated two settings: <em class=\"ltx_emph ltx_font_italic\">without context</em>, where each utterance was predicted independently as in BERT, and <em class=\"ltx_emph ltx_font_italic\">with context</em>, where we provided the entire conversation sequence in the test set and the model predicted all utterances jointly with access to surrounding context. Prompts are given in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "gpt4o",
                    "without",
                    "context",
                    "gemini25pro",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Results</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.T7\" title=\"Table 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the results. Without context, BERT achieved higher performance. This can be interpreted as a trade-off between BERT&#8217;s task specialization and the LLMs&#8217; broader world knowledge. With fine-tuning on a specific dataset, BERT could replicate task-specific labeling rules precisely, which was advantageous when clear criteria and adherence to given examples were more important than outside knowledge. LLMs possess strong internal world models from pretraining. When this prior knowledge conflicted with task-specific rules, the model sometimes favored its own &#8220;common sense,&#8221; leading to reasonable but task-inconsistent labels and thus lower accuracy.</p>\n\n",
                "matched_terms": [
                    "without",
                    "bert",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Qualitative Results</span>\nOverall, labels corresponding to instructional speech, such as <em class=\"ltx_emph ltx_font_italic\">instruction</em> and <em class=\"ltx_emph ltx_font_italic\">additional instruction</em>, were often predicted correctly, likely because these utterances were longer and contained enough information within a single sentence. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S4.F7\" title=\"Figure 7 &#8227; 4.2 Conversation-State Classification &#8227; 4 Experiments &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows qualitative examples. Without context, we observed errors such as answering before the question was asked or swapping speaker roles, but these were largely resolved when the full conversational context was provided.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a new egocentric dataset of face-to-face instruction and analyzed instructional interactions using it. Existing datasets rarely focus on instruction, making interaction analysis in instructional scenes difficult. Our dataset provides egocentric views from both the instructor and the learner along with annotations for conversation states and procedural steps. We evaluated models, including multimodal large language models (MLLMs), on two tasks: procedural step segmentation and conversation-state classification. For step segmentation, MLLMs produced accurate boundaries by leveraging multiple modalities. For conversation-state classification, LLMs achieved strong performance when provided with conversational context. These results suggest that MLLMs can contribute to holistic understanding of face-to-face instruction even without additional fine-tuning.</p>\n\n",
                "matched_terms": [
                    "without",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22019v1#S6.T8.st4\" title=\"Table 8(d) &#8227; Table 8 &#8227; 6.1 Step Segmentation &#8227; 6 Other Results &#8227; EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking\"><span class=\"ltx_text ltx_ref_tag\">8(d)</span></a> shows the step segmentation results on each task: cart assembly, chair assembly, GoPro battery replacement, and printer toner replacement.</p>\n\n",
                "matched_terms": [
                    "replacement",
                    "toner",
                    "printer"
                ]
            }
        ]
    }
}