{
    "S3.T1": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 1: Fine interleaving improves over coarse interleaving.",
        "body": "Interleaving Granularity\nText Understanding (T→T{\\text{T}}{\\rightarrow}{\\text{T}})\nSQA (S→T{\\text{S}}{\\rightarrow}{\\text{T}}) acc (%)\n\n\nCoreEN\nMMLU\nSWQ\nSTQ\nSLQ\nAvg\n\n\nCoarse\n60.4\n63.9\n42.5\n26.6\n43.6\n37.6\n\n\nFine\n60.4\n64.1\n42.7\n32.2\n47.3\n40.7",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Interleaving Granularity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">SQA (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>) acc (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">CoreEN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SWQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">STQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SLQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Coarse</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Fine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">64.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">42.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">32.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">47.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">40.7</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "acc",
            "sqa",
            "fine",
            "over",
            "s→ttextsrightarrowtextt",
            "text",
            "swq",
            "slq",
            "improves",
            "mmlu",
            "stq",
            "t→ttexttrightarrowtextt",
            "granularity",
            "avg",
            "coarse",
            "understanding",
            "interleaving",
            "coreen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
            "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "interleaving",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "s→ttextsrightarrowtextt",
                    "text",
                    "swq",
                    "slq",
                    "stq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> To ensure our speech-text pretraining recipe does not degrade base language modeling performance, we evaluate on <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> standard text benchmarks spanning across general knowledge, math and coding: <span class=\"ltx_text ltx_font_italic\">MMLU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib59\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">CoreEN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Busbridge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib19\" title=\"\">2025</a>)</cite> (consisting of <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> benchmarks&#8212;<span class=\"ltx_text ltx_font_italic\">ARC-Easy</span> and\n<span class=\"ltx_text ltx_font_italic\">ARC-Challenge</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib24\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">HellaSwag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib171\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Lambada</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Paperno et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib106\" title=\"\">2016</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">PIQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib13\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SciQ</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Welbl et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib154\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">TriviaQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib66\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">WebQuestions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib11\" title=\"\">2013</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">WinoGrande</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib122\" title=\"\">2021</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">GSM-8k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib25\" title=\"\">2021</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">HumanEval</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib21\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "mmlu",
                    "t→ttexttrightarrowtextt",
                    "understanding",
                    "coreen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "over",
                    "improves",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T3\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find deterministic sampling boosts SQA performance by <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>% on average over stochastic sampling.\nWe posit that the number of modality switches during training affects the SQA performance&#8212;in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F4\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the distribution of modality switches occuring during interleaved training, finding that stochastic sampling switches modalities quite infrequently, whereas the deterministic approach has a higher number of modality switches during training. Indeed, the expected number of modality switches for a sample consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> chunks is <math alttext=\"{n}{-}{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{n}{-}{1}</annotation></semantics></math> for deterministic sampling and <math alttext=\"\\frac{{n}{-}{1}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m4\" intent=\":literal\"><semantics><mfrac><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{{n}{-}{1}}{2}</annotation></semantics></math> for stochastic sampling. By frequently switching modalities more often, deterministic sampling likely enables more effective cross-modal learning, thereby improving downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "fine",
                    "text",
                    "coarse",
                    "interleaving"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "interleaving",
                    "fine",
                    "over",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "interleaving",
                    "sqa",
                    "fine",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "swq",
                    "sqa",
                    "stq",
                    "slq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "interleaving"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Coarse interleaving.</span> Here, we aim to have relatively long audio-text chunks. To do this, we continually merge consecutive audio segments based on the diarization outputs while they have the same speakerID. While merging the segments, we concatenate the corresponding text transcriptions of each audio segment, separated by a white-space, to yield the merged text transcription for the merged audio.</p>\n\n",
                "matched_terms": [
                    "text",
                    "interleaving",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Fine interleaving.</span> Since the original diarized output segments already yield relatively short chunks, we do not apply any post-processing on the output segments and directly use them as our audio-text chunks for interleaved training.</p>\n\n",
                "matched_terms": [
                    "fine",
                    "interleaving"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "s→ttextsrightarrowtextt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "swq",
                    "slq",
                    "stq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "text",
                    "mmlu"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 2: Synthetic speech-text interleaved data improves over web-crawl data.",
        "body": "Data Mix\nText Understanding (T→T{\\text{T}}{\\rightarrow}{\\text{T}})\nSQA (S→T{\\text{S}}{\\rightarrow}{\\text{T}}) acc (%)\n\n\nCoreEN\nMMLU\nSWQ\nSTQ\nSLQ\nAvg\n\n\nWeb-crawl 100100%\n60.4\n64.1\n42.7\n32.2\n47.3\n40.7\n\n\nWeb-crawl 5353% + Krist 4747%\n60.8\n64.8\n43.4\n29.2\n52.0\n41.5\n\n\nWeb-crawl 6666% + Quest 3434%\n60.4\n66.2\n42.7\n34.7\n66.3\n47.9\n\n\nWeb-crawl 5959% + Quest 66% + Krist 3535%\n60.7\n65.9\n43.8\n31.5\n51.0\n42.1\n\n\nWeb-crawl 4040% + Quest 2727% + Krist 3333%\n60.6\n65.7\n43.3\n31.7\n49.3\n41.4",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Data Mix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">SQA (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>) acc (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">CoreEN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SWQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">STQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SLQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Web-crawl <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Web-crawl <math alttext=\"53\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mn>53</mn><annotation encoding=\"application/x-tex\">53</annotation></semantics></math>% + Krist <math alttext=\"47\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mn>47</mn><annotation encoding=\"application/x-tex\">47</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">60.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">29.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Web-crawl <math alttext=\"66\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mn>66</mn><annotation encoding=\"application/x-tex\">66</annotation></semantics></math>% + Quest <math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><mn>34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\">42.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">34.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">66.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">47.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">43.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">51.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Web-crawl <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% + Quest <math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m12\" intent=\":literal\"><semantics><mn>27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math>% + Krist <math alttext=\"33\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m13\" intent=\":literal\"><semantics><mn>33</mn><annotation encoding=\"application/x-tex\">33</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">60.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">65.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">31.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">49.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">41.4</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "over",
            "text",
            "swq",
            "webcrawl",
            "avg",
            "sqa",
            "s→ttextsrightarrowtextt",
            "mix",
            "mmlu",
            "interleaved",
            "acc",
            "t→ttexttrightarrowtextt",
            "krist",
            "synthetic",
            "speechtext",
            "slq",
            "improves",
            "stq",
            "understanding",
            "data",
            "coreen",
            "quest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
            "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
            "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "text",
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "over",
                    "text",
                    "interleaved",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "text",
                    "improves",
                    "interleaved",
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "text",
                    "interleaved",
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "s→ttextsrightarrowtextt",
                    "text",
                    "swq",
                    "slq",
                    "stq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> To ensure our speech-text pretraining recipe does not degrade base language modeling performance, we evaluate on <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> standard text benchmarks spanning across general knowledge, math and coding: <span class=\"ltx_text ltx_font_italic\">MMLU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib59\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">CoreEN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Busbridge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib19\" title=\"\">2025</a>)</cite> (consisting of <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> benchmarks&#8212;<span class=\"ltx_text ltx_font_italic\">ARC-Easy</span> and\n<span class=\"ltx_text ltx_font_italic\">ARC-Challenge</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib24\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">HellaSwag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib171\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Lambada</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Paperno et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib106\" title=\"\">2016</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">PIQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib13\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SciQ</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Welbl et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib154\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">TriviaQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib66\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">WebQuestions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib11\" title=\"\">2013</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">WinoGrande</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib122\" title=\"\">2021</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">GSM-8k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib25\" title=\"\">2021</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">HumanEval</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib21\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "mmlu",
                    "t→ttexttrightarrowtextt",
                    "understanding",
                    "coreen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "interleaved",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "data",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "improves",
                    "interleaved",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "interleaved",
                    "quest",
                    "data",
                    "krist",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "interleaved",
                    "data",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "interleaved",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "data",
                    "sqa",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T3\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find deterministic sampling boosts SQA performance by <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>% on average over stochastic sampling.\nWe posit that the number of modality switches during training affects the SQA performance&#8212;in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F4\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the distribution of modality switches occuring during interleaved training, finding that stochastic sampling switches modalities quite infrequently, whereas the deterministic approach has a higher number of modality switches during training. Indeed, the expected number of modality switches for a sample consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> chunks is <math alttext=\"{n}{-}{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{n}{-}{1}</annotation></semantics></math> for deterministic sampling and <math alttext=\"\\frac{{n}{-}{1}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m4\" intent=\":literal\"><semantics><mfrac><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{{n}{-}{1}}{2}</annotation></semantics></math> for stochastic sampling. By frequently switching modalities more often, deterministic sampling likely enables more effective cross-modal learning, thereby improving downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "interleaved",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "interleaved",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "sqa",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "data",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">TTS and ASR-style Conversations</span>: We convert utterances from ASR/TTS datasets into natural conversation, in which users ask assistants to either transcribe a given audio (ASR) or synthesize a given text (TTS). We also include instruction-following TTS data where users ask to synthesize text responses with specific instructions (e.g., synthesize speech in a given volume, pace, style or emotion).</p>\n\n",
                "matched_terms": [
                    "text",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "synthetic",
                    "text",
                    "data",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "data",
                    "over",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sqa",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "text",
                    "webcrawl",
                    "improves",
                    "quest",
                    "data",
                    "krist",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "synthetic",
                    "improves",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, by enabling broader coverage of topic domains, our synthetic datasets help to (1) close the distribution mismatch between the raw web-crawled data and the downstream evaluation datasets, and (2) enhance the diversity of our pretraining data distribution. Our findings extend prior work in the language space that have discussed the importance of training data diversity and domain coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib98\" title=\"\">2025a</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib151\" title=\"\">2025c</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> to the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sqa",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "swq",
                    "slq",
                    "stq",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "text",
                    "interleaved",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide more details about each step in our data processing pipeline for converting web-crawled audio into interleaved speech-text format. We highlight all the components in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "data",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Chunking.</span> The last step in our pipeline is the interleaved chunking stage, which constructs the final audio-text chunks used for interleaved training. As described in the main text, we study two chunking strategies:</p>\n\n",
                "matched_terms": [
                    "text",
                    "interleaved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extraction prompt for <span class=\"ltx_text ltx_font_italic\">Krist</span>.</span> To extract and lightly rewrite the text content from the HTML using <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span>, we use the following prompt:</p>\n\n",
                "matched_terms": [
                    "text",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speechtext",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "interleaved",
                    "understanding",
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "s→ttextsrightarrowtextt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "over",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "mix",
                    "swq",
                    "slq",
                    "webcrawl",
                    "stq",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our experiments were at the <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B parameter scale trained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens (roughly <math alttext=\"{\\sim}{{3.81}{\\times}{10^{22}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\sim}{{3.81}{\\times}{10^{22}}}</annotation></semantics></math> FLOPs). While our results are strong (outperforming models that are <math alttext=\"3{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">3{\\times}</annotation></semantics></math> the size, trained for similar compute budgets), it would still be interesting to explore if our data-centric strategies would hold at larger model scales. While recent papers like <cite class=\"ltx_cite ltx_citemacro_citet\">Nezhurina et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib96\" title=\"\">2025</a>)</cite>, DataComp-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite>, HoneyBee <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> and DataComp-CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>)</cite> suggest transferability of data curation methods across model scales, recent work in language and vision-language modeling has posited that there may be trade-offs when applying data curation across different model sizes and compute budgets <cite class=\"ltx_cite ltx_citemacro_citep\">(Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Goyal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib52\" title=\"\">2024</a>)</cite>. To the best of our knowledge, no existing work showcases such trade-offs in the SpeechLM community. It would be an interesting direction to explore the interaction of data recipes with model scale and compute budget.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "data",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "speechtext",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext",
                    "mix"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 3: Deterministic speech-text sampling improves over stochastic sampling.",
        "body": "Sampling scheme\nText Understanding (T→T{\\text{T}}{\\rightarrow}{\\text{T}})\nSQA (S→T{\\text{S}}{\\rightarrow}{\\text{T}}) acc (%)\n\n\nCoreEN\nMMLU\nSWQ\nSTQ\nSLQ\nAvg\n\n\nStochastic\n60.6\n65.7\n43.3\n31.7\n49.3\n41.4\n\n\nDeterministic\n60.1\n65.2\n44.2\n31.2\n51.7\n42.4",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Sampling scheme</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">SQA (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>) acc (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">CoreEN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SWQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">STQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SLQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Stochastic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">60.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">65.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Deterministic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">65.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">44.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">31.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">51.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">42.4</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "over",
            "text",
            "swq",
            "scheme",
            "avg",
            "sqa",
            "s→ttextsrightarrowtextt",
            "mmlu",
            "stochastic",
            "acc",
            "t→ttexttrightarrowtextt",
            "deterministic",
            "speechtext",
            "sampling",
            "slq",
            "improves",
            "stq",
            "understanding",
            "coreen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T3\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find deterministic sampling boosts SQA performance by <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>% on average over stochastic sampling.\nWe posit that the number of modality switches during training affects the SQA performance&#8212;in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F4\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the distribution of modality switches occuring during interleaved training, finding that stochastic sampling switches modalities quite infrequently, whereas the deterministic approach has a higher number of modality switches during training. Indeed, the expected number of modality switches for a sample consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> chunks is <math alttext=\"{n}{-}{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{n}{-}{1}</annotation></semantics></math> for deterministic sampling and <math alttext=\"\\frac{{n}{-}{1}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m4\" intent=\":literal\"><semantics><mfrac><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{{n}{-}{1}}{2}</annotation></semantics></math> for stochastic sampling. By frequently switching modalities more often, deterministic sampling likely enables more effective cross-modal learning, thereby improving downstream SQA performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "speechtext",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "sampling",
                    "text",
                    "improves",
                    "stochastic",
                    "deterministic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "s→ttextsrightarrowtextt",
                    "text",
                    "swq",
                    "slq",
                    "stq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> To ensure our speech-text pretraining recipe does not degrade base language modeling performance, we evaluate on <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> standard text benchmarks spanning across general knowledge, math and coding: <span class=\"ltx_text ltx_font_italic\">MMLU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib59\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">CoreEN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Busbridge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib19\" title=\"\">2025</a>)</cite> (consisting of <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> benchmarks&#8212;<span class=\"ltx_text ltx_font_italic\">ARC-Easy</span> and\n<span class=\"ltx_text ltx_font_italic\">ARC-Challenge</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib24\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">HellaSwag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib171\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Lambada</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Paperno et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib106\" title=\"\">2016</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">PIQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib13\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SciQ</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Welbl et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib154\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">TriviaQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib66\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">WebQuestions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib11\" title=\"\">2013</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">WinoGrande</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib122\" title=\"\">2021</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">GSM-8k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib25\" title=\"\">2021</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">HumanEval</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib21\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "mmlu",
                    "t→ttexttrightarrowtextt",
                    "understanding",
                    "coreen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "over",
                    "mmlu",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Sampling.</span> In the first variant (used in all our previous experiments), at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, we randomly sample the chunk-modality with <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math> probability.\nThe modality sampling at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is independent of all other chunks <math alttext=\"{j}{\\neq}{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>&#8800;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">{j}{\\neq}{i}</annotation></semantics></math>.\nWe always start with an audio chunk <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m5\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>, to ensure that there is at least <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> audio chunk in our training sequence.</p>\n\n",
                "matched_terms": [
                    "stochastic",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic Sampling.</span>\nWhile the stochastic variant allows flexibility and potentially offers better generalization,\nit can restrict the number of <span class=\"ltx_text ltx_font_italic\">modality switches</span> during training.\nHence, we test a deterministic approach, where we alternate between audio and text modalities at each chunk, i.e. we formulate the training sequence as <math alttext=\"{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>A</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}</annotation></semantics></math>.\nThis <span class=\"ltx_text ltx_font_italic\">maximizes the number of modality switches</span> for a given sample.\nHere too, we always start with <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sampling",
                    "stochastic",
                    "deterministic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "speechtext",
                    "improves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "swq",
                    "sqa",
                    "stq",
                    "slq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "speechtext",
                    "sampling",
                    "text",
                    "deterministic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext",
                    "understanding",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sqa",
                    "s→ttextsrightarrowtextt",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "swq",
                    "slq",
                    "stq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechtext"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 4: Our data-centric methods also work for understanding-only SpeechLM",
        "body": "Method\nText Understanding (T→T{\\text{T}}{\\rightarrow}{\\text{T}})\nSQA (S→T{\\text{S}}{\\rightarrow}{\\text{T}}) acc (%)\n\n\nCoreEN\nMMLU\nSWQ\nSTQ\nSLQ\nAvg\n\n\nBaseline (w/o loss-masking)\n60.4\n63.9\n42.5\n26.6\n43.6\n40.7\n\n\n\n++ all data interventions\n60.1\n65.2\n44.2\n31.2\n51.7\n42.4\n\n\nBaseline (w/ loss-masking)\n61.7\n66.5\n45.9\n34.0\n47.7\n42.5\n\n\n\n++ all data interventions\n61.8\n67.3\n45.7\n44.6\n65.0\n51.8",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">SQA (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>) acc (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">CoreEN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SWQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">STQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">SLQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (w/o loss-masking)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m3\" intent=\":literal\"><semantics><mo>+</mo><annotation encoding=\"application/x-tex\">+</annotation></semantics></math> all data interventions</td>\n<td class=\"ltx_td ltx_align_center\">60.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">65.2</td>\n<td class=\"ltx_td ltx_align_center\">44.2</td>\n<td class=\"ltx_td ltx_align_center\">31.2</td>\n<td class=\"ltx_td ltx_align_center\">51.7</td>\n<td class=\"ltx_td ltx_align_center\">42.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Baseline (w/ loss-masking)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">66.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">\n<math alttext=\"+\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m4\" intent=\":literal\"><semantics><mo>+</mo><annotation encoding=\"application/x-tex\">+</annotation></semantics></math> all data interventions</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">61.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">67.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">45.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">44.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">51.8</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "understandingonly",
            "text",
            "swq",
            "avg",
            "also",
            "our",
            "sqa",
            "s→ttextsrightarrowtextt",
            "datacentric",
            "all",
            "mmlu",
            "work",
            "baseline",
            "methods",
            "acc",
            "interventions",
            "t→ttexttrightarrowtextt",
            "lossmasking",
            "slq",
            "speechlm",
            "stq",
            "method",
            "understanding",
            "data",
            "coreen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datacentric",
                    "text",
                    "speechlm",
                    "work",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-based assistants are now widely deployed <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib102\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib26\" title=\"\">2025</a>)</cite>,\nlargely riding on the progress of text-only foundation models <cite class=\"ltx_cite ltx_citemacro_citep\">(Bommasani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib14\" title=\"\">2021</a>)</cite>.\nYet, purely textual interactions are inherently limiting for real-world assistants that must operate in open, hands-free settings. Voice provides a natural, low-friction interface for human&#8211;AI interaction, and recent work therefore emphasizes Spoken Question-Answering (SQA) <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>&#8212;where a question is asked in audio and the system must produce spoken or textual answers&#8212;as a core capability for end-to-end speech language models (SpeechLMs).</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "work"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "sqa",
                    "datacentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datacentric",
                    "text",
                    "interventions",
                    "speechlm",
                    "work",
                    "also",
                    "methods",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speechlm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Foundation Models.</span> Pretraining data quality is pivotal for driving performance of foundation models.\nEfforts like Gopher <cite class=\"ltx_cite ltx_citemacro_citep\">(Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>)</cite>, T5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib114\" title=\"\">2020</a>)</cite>, Nemotron-CC <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>)</cite>, FineWeb <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>)</cite>, DCLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and OLMo-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite> significantly emphasize the benefits of strong data processing, curation and filtering for language data. In computer vision, Dinov2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>)</cite>, Dinov3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, AIMv2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib41\" title=\"\">2025</a>)</cite> and Web-SSL <cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib38\" title=\"\">2025</a>)</cite> showcased the high impact that careful data curation has on model quality.\nSimilar results on the importance of data-centric research have been shown in vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib39\" title=\"\">2023a</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib150\" title=\"\">2025b</a>)</cite> and reasoning-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib81\" title=\"\">2025d</a>; Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib94\" title=\"\">2025</a>)</cite> foundation modeling literature.\nOwing to the paucity of such data-centric research in the speech-language domain, we aim to close this gap through a set of controlled data ablations, demonstrating the strong utility of data-centric approaches for boosting SpeechLM quality.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speechlm",
                    "datacentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datacentric",
                    "text",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "s→ttextsrightarrowtextt",
                    "text",
                    "swq",
                    "all",
                    "slq",
                    "stq",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> To ensure our speech-text pretraining recipe does not degrade base language modeling performance, we evaluate on <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> standard text benchmarks spanning across general knowledge, math and coding: <span class=\"ltx_text ltx_font_italic\">MMLU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib59\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">CoreEN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Busbridge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib19\" title=\"\">2025</a>)</cite> (consisting of <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> benchmarks&#8212;<span class=\"ltx_text ltx_font_italic\">ARC-Easy</span> and\n<span class=\"ltx_text ltx_font_italic\">ARC-Challenge</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib24\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">HellaSwag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib171\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Lambada</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Paperno et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib106\" title=\"\">2016</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">PIQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib13\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SciQ</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Welbl et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib154\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">TriviaQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib66\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">WebQuestions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib11\" title=\"\">2013</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">WinoGrande</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib122\" title=\"\">2021</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">GSM-8k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib25\" title=\"\">2021</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">HumanEval</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib21\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "mmlu",
                    "t→ttexttrightarrowtextt",
                    "understanding",
                    "coreen",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Architecture.</span> We conduct all our experiments with a <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3.8B-parameter SpeechLM, consisting of two major components: a speech tokenizer and a pretrained language model. Our speech tokenizer consists of a speech encoder with conformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib55\" title=\"\">2020</a>)</cite> blocks followed by a finite scalar quantizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib90\" title=\"\">2023</a>)</cite> that outputs discrete speech tokens. We initialize our language model with the dense 3B base-LM from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> that has a context-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "all",
                    "speechlm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "also",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "also",
                    "lossmasking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "all",
                    "data",
                    "sqa",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "mmlu",
                    "work",
                    "also",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "data",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Sampling.</span> In the first variant (used in all our previous experiments), at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, we randomly sample the chunk-modality with <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math> probability.\nThe modality sampling at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is independent of all other chunks <math alttext=\"{j}{\\neq}{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>&#8800;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">{j}{\\neq}{i}</annotation></semantics></math>.\nWe always start with an audio chunk <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m5\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>, to ensure that there is at least <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> audio chunk in our training sequence.</p>\n\n",
                "matched_terms": [
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datacentric",
                    "understandingonly",
                    "text",
                    "speechlm",
                    "also",
                    "methods",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "datacentric",
                    "interventions",
                    "all",
                    "also",
                    "methods",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training setup.</span>\nFor the purpose of this study, we started from our base model checkpoints and conducted supervised fine-tuning (SFT). Our SFT data consisted of the following categories:</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">TTS and ASR-style Conversations</span>: We convert utterances from ASR/TTS datasets into natural conversation, in which users ask assistants to either transcribe a given audio (ASR) or synthesize a given text (TTS). We also include instruction-following TTS data where users ask to synthesize text responses with specific instructions (e.g., synthesize speech in a given volume, pace, style or emotion).</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluations.</span>\nWe evaluated SFT models for both <span class=\"ltx_text ltx_font_italic\">text response quality</span> <span class=\"ltx_text ltx_font_italic\">and audio response quality</span>. To evaluate text response quality, we use two eval sets: <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span> and <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. The first is obtained by synthesizing the alpaca evaluation dataset (<cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib80\" title=\"\">2023</a>)</cite>). On top of <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span>, we added various background noise with a SNR randomly sampled from 5 to 15 dB. This produces <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. During the evaluation, 804 spoken alpaca questions were fed in, and the model&#8217;s text response, <math alttext=\"\\texttt{T}^{\\texttt{a}}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p7.m1\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}^{\\texttt{a}}_{1}</annotation></semantics></math>, is extracted. These text responses are pair-wise compared with the responses generated from a performant internal baseline model using the standard evaluation protocol with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> as the judge model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate audio response quality, we work with several third-party vendors to collect diversified user prompts in audio. For multi-turn dialogue evaluation, we adopt the last-turn-with-context strategy to evaluate the last turn&#8217;s assistant response, while the previous assistant responses are generated by <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio</span> and fed in as context. In total, we constructed <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p8.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> evaluation sets, each having a different focus, such as knowledge-rich, multi-turn, long-context, and challenging speech environments. We also notice pair-wise comparison of audio is often harder than text, in which judges (LLM or human) cannot tell which response is better. In order to reduce variance of judge scores, we ask the judge to output whether audio response A is better than, worse than or tied with audio response B. The auto-grading prompt template we used is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A8\" title=\"Appendix H Prompt template for GPT-4o-audio in auto eval &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "work",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datacentric",
                    "text",
                    "interventions",
                    "also",
                    "methods",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "datacentric",
                    "interventions",
                    "baseline",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "interventions",
                    "also",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "text",
                    "also",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Web-crawled data is highly skewed</span> and is majorly comprised of <span class=\"ltx_text ltx_font_italic\">entertainment</span>, <span class=\"ltx_text ltx_font_italic\">sports and fitness</span>, <span class=\"ltx_text ltx_font_italic\">religion</span> and <span class=\"ltx_text ltx_font_italic\">social life</span> domains. This is not surprising given that most of our web-crawled audio data is sourced from podcasts, interviews, talk-shows and monologues.</p>\n\n",
                "matched_terms": [
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, by enabling broader coverage of topic domains, our synthetic datasets help to (1) close the distribution mismatch between the raw web-crawled data and the downstream evaluation datasets, and (2) enhance the diversity of our pretraining data distribution. Our findings extend prior work in the language space that have discussed the importance of training data diversity and domain coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib98\" title=\"\">2025a</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib151\" title=\"\">2025c</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> to the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "data",
                    "work",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sqa",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "all",
                    "sqa",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "swq",
                    "all",
                    "slq",
                    "stq",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equipped with our key data-centric insights from the previous sections, we now train a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM, called <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>.\nWe use the same training configuration as before, with <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> sequence length\ntrained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens.\nWe compare against SoTA speech-language base models including Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>)</cite>, and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe additionally compare two post-trained models&#8212;Voxtral-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> and GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>&#8212;with the caveat that having undergone instruction-tuning, they are not directly comparable to base models <cite class=\"ltx_cite ltx_citemacro_citep\">(Dominguez-Olmedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib32\" title=\"\">2024</a>)</cite>. To ensure our training recipe does not degrade language performance, we also compare against strong open-weights base language models on standard text-only benchmarks.</p>\n\n",
                "matched_terms": [
                    "speechlm",
                    "also",
                    "our",
                    "datacentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "all",
                    "sqa",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datacentric",
                    "text",
                    "speechlm",
                    "work",
                    "also",
                    "methods",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide more details about each step in our data processing pipeline for converting web-crawled audio into interleaved speech-text format. We highlight all the components in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription Generation.</span> Next, we aim to provide paired text annotations for all of the raw audio in our corpus. For this, we first used the Whisper model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> to transcribe the raw audio from each of the diarized output chunks. However, we noticed that the Whisper model transcriptions can tend to be quite noisy and contain some hallucinations. To ensure cleaner transcriptions, we use a post-processing transcription ensembling approach called ROVER <cite class=\"ltx_cite ltx_citemacro_citep\">(Fiscus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib42\" title=\"\">1997</a>)</cite> used in prior works performing transcription cleaning <cite class=\"ltx_cite ltx_citemacro_citep\">(Jalalvand et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib64\" title=\"\">2015</a>)</cite>. We first obtain additional speech transcriptions from an internal SIRI transcription model and <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\" title=\"\">Nvidia-Parakeet-TDT-CTC</a>. We then apply the ROVER post-processing method using the three candidate transcriptions from Whisper, SIRI and Parakeet. We use the ensembled transcription as our text annotations for subsequent steps. We provide some examples of the individual model-based transcriptions and the final ROVER-ensembled transcriptions below:</p>\n\n",
                "matched_terms": [
                    "text",
                    "all",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Chunking.</span> The last step in our pipeline is the interleaved chunking stage, which constructs the final audio-text chunks used for interleaved training. As described in the main text, we study two chunking strategies:</p>\n\n",
                "matched_terms": [
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all",
                    "also",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our models are <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter transformer-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib146\" title=\"\">2017</a>)</cite> speech-language models. We use a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> for all our experiments. Our models use a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens. We train for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps in total, yielding a total of <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for our training runs. Using the standard <math alttext=\"{6}{N}{D}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m6\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">{6}{N}{D}</annotation></semantics></math> rule <cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib69\" title=\"\">2020</a>)</cite>, this equates to about <math alttext=\"{3.81}{\\times}{{10}^{22}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">{3.81}{\\times}{{10}^{22}}</annotation></semantics></math>FLOPs (note that this estimate is a rough lower bound since we do not count the FLOPs associated with the speech tokenizer in this estimate).\nWe only tune the language model weights while keep the speech tokenizer frozen.\nWe use a cosine-decay learning rate schedule with <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m8\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> steps of linear-warmup.\nWe use the AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib86\" title=\"\">2017</a>)</cite> optimizer with <math alttext=\"{\\beta_{1}}{=}{0.9}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{1}}{=}{0.9}</annotation></semantics></math> and <math alttext=\"{\\beta_{2}}{=}{0.95}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{2}}{=}{0.95}</annotation></semantics></math>, a peak learning rate of <math alttext=\"{3}{e}{-}{4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">{3}{e}{-}{4}</annotation></semantics></math>, weight decay of <math alttext=\"{1}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{1}{e}{-}{5}</annotation></semantics></math> and clip gradients to a max norm of <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m13\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. We use the <span class=\"ltx_text ltx_font_typewriter\">axlearn</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib75\" title=\"\">2025</a>)</cite> codebase for all our experiments using <span class=\"ltx_text ltx_font_typewriter\">jax</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bradbury et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib16\" title=\"\">2021</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">pygrain</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ritter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib117\" title=\"\">2023</a>)</cite> for dataloading. One training run takes approximately <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m14\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math> days on <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m15\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> TPU-v6e chips.</p>\n\n",
                "matched_terms": [
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper, we briefly described some related work in speech-language pretraining. Further, we focused on situating our work in the SpeechLM literature and emphasized the lack of data-centric research in speech-language pretraining. Here, we provide a deeper dive into SpeechLMs and reference some related data-centric work that does exist in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "work",
                    "speechlm",
                    "our",
                    "datacentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "understanding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "datacentric",
                    "text",
                    "speechlm",
                    "work",
                    "understanding",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "s→ttextsrightarrowtextt",
                    "text",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below, we also provide a few examples from each evaluation dataset, with the question (in text), choices, and the ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "text",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we compute the completion log-probability for each of the <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> answer choices. We normalize the completion log-probability by answer length to prevent biasing against long answer choices. A question is marked correct if the model assigns highest normalized log-probability to the ground-truth answer. We use standard accuracy metric (random chance level is <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m2\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%) for reporting results. For running all our model evaluations, we use a fork of <span class=\"ltx_text ltx_font_typewriter\">lm-eval-harness</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib45\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each test sample <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, let <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math> represent the question tokens in text and audio modality respectively.\nThat is, the tokenized representation of <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> is <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and the tokenized representation of <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> is <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math>.\nFor brevity, let us denote these tokenized representations as <math alttext=\"t_{1:m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{1:m}</annotation></semantics></math> and <math alttext=\"a_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{1:n}</annotation></semantics></math>.\nNote that since the length of the question tokens in text and audio modalities might differ, it is possible that <math alttext=\"{n}\\neq{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8800;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">{n}\\neq{m}</annotation></semantics></math>.\nLet <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math> represent the ground-truth answer tokens in text modality i.e. the tokenized representation of <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> is <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math>.\nAgain, for brevity, we denote this as <math alttext=\"g_{1:o}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m14\" intent=\":literal\"><semantics><msub><mi>g</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">g_{1:o}</annotation></semantics></math>.\nLet <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m15\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> be the vocabulary of the SpeechLM.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "interventions",
                    "all",
                    "also",
                    "methods",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"In J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we showcase the topic distribution of Spoken-Web-Questions. Similar to the takeaways in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we find that some of the topics that Spoken-Web-Questions contains are severely under-represented in the web-crawled dataset while being represented adequately in the synthetic datasets. This further corroborates our findings that synthetic datasets help close the distribution mismatch between the web-crawled dataset and the evaluation datasets. Our findings regarding the under-representation of concepts in web-crawled datasets have also been echoed in the language and vision domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiedemer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib157\" title=\"\">2025</a>; Parashar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib107\" title=\"\">2024</a>; Elazar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib37\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib67\" title=\"\">2023</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib144\" title=\"\">2024</a>; Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib184\" title=\"\">2024</a>; Samuel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib123\" title=\"\">2024</a>; Dodge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib31\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "also",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "all",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "swq",
                    "slq",
                    "stq",
                    "baseline",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "also",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "method",
                    "also",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While we conducted extensive experiments to study the three data-centric questions outlined in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, there are still a few limitations in our work that can be improved upon:</p>\n\n",
                "matched_terms": [
                    "work",
                    "our",
                    "datacentric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our experiments were at the <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B parameter scale trained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens (roughly <math alttext=\"{\\sim}{{3.81}{\\times}{10^{22}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\sim}{{3.81}{\\times}{10^{22}}}</annotation></semantics></math> FLOPs). While our results are strong (outperforming models that are <math alttext=\"3{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">3{\\times}</annotation></semantics></math> the size, trained for similar compute budgets), it would still be interesting to explore if our data-centric strategies would hold at larger model scales. While recent papers like <cite class=\"ltx_cite ltx_citemacro_citet\">Nezhurina et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib96\" title=\"\">2025</a>)</cite>, DataComp-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite>, HoneyBee <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> and DataComp-CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>)</cite> suggest transferability of data curation methods across model scales, recent work in language and vision-language modeling has posited that there may be trade-offs when applying data curation across different model sizes and compute budgets <cite class=\"ltx_cite ltx_citemacro_citep\">(Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Goyal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib52\" title=\"\">2024</a>)</cite>. To the best of our knowledge, no existing work showcases such trade-offs in the SpeechLM community. It would be an interesting direction to explore the interaction of data recipes with model scale and compute budget.</p>\n\n",
                "matched_terms": [
                    "datacentric",
                    "all",
                    "speechlm",
                    "work",
                    "methods",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the focus of our work was mainly on improving spoken question-answering capabilities of SpeechLMs, all our experiments used the standard benchmarks that are prevalent in the literature for our task of interest <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>. We therefore did not explore how our models would perform on more targeted tasks like automatic speech recognition, emotion recognition or text-to-speech synthesis. One caveat preventing us from a direct comparison on such tasks is that we do not employ any task-specific training, unlike other SpeechLMs that explicitly add in a task-specific component into their training mixture (e.g., ASR-specific training datasets) <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "work",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "also",
                    "understanding",
                    "methods",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "text",
                    "interventions",
                    "all",
                    "mmlu",
                    "also",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our training runs initialize the language model backbone for our SpeechLM using a pretrained base-LM. This is the standard recipe used by almost all the existing foundation SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>. However, recent work in the vision-language literature has advocated for full native multimodal pretraining from scratch <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>)</cite>, where both the language model and the modality-specific encoder/tokenizer are trained from scratch. It would be interesting to explore if our data-centric methods also enable more efficient SpeechLM pretraining from scratch in the future.</p>\n\n",
                "matched_terms": [
                    "datacentric",
                    "all",
                    "speechlm",
                    "work",
                    "also",
                    "methods",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "datacentric",
                    "all",
                    "speechlm",
                    "methods",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "our"
                ]
            }
        ]
    },
    "S3.T5": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 5: Comparison of model’s text/audio response quality after SFT.",
        "body": "Pretrain ckpt\nText quality111Length-controlled (Dubois et al., 2024) win rates in % against the reference model.\nAudio Quality222Win (Tie) rates in % against the reference model.\n\n\nspoken-alpaca\nnoisy-alpaca\nEval 1\nEval 2\nEval 3\nEval 4\nEval 5\n\n\ncoarse\n42.6\n45.2\n37.4 (17.2)\n33.3 (24.1)\n34.3 (18.1)\n37.0 (16.3)\n38.8 (16.9)\n\n\nfine\n44.3\n47.3\n39.9 (18.5)\n33.8 (23.7)\n36.4 (11.6)\n38.0 (16.9)\n\n41.9 (20.7)\n\n\nfine + syn\n47.4\n48.8\n\n41.1 (17.1)\n\n36.6 (23.1)\n\n40.1 (18.7)\n\n39.4 (16.9)\n39.3 (16.8)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Pretrain ckpt</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Text quality<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">Length-controlled </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text ltx_font_medium\">(</span>Dubois et al.<span class=\"ltx_text ltx_font_medium\">, </span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib35\" title=\"\">2024</a><span class=\"ltx_text ltx_font_medium\">)</span></cite><span class=\"ltx_text ltx_font_medium\"> win rates in % against the reference model.</span></span></span></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Audio Quality<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><span class=\"ltx_text ltx_font_medium\">Win (Tie) rates in % against the reference model.</span></span></span></span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">spoken-alpaca</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">noisy-alpaca</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Eval 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Eval 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Eval 3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Eval 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Eval 5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">coarse</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">45.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.4 (17.2)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.3 (24.1)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.3 (18.1)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.0 (16.3)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.8 (16.9)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">fine</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.9 (18.5)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.8 (23.7)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.4 (11.6)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.0 (16.9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">41.9</span> (20.7)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">fine + syn</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">47.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">48.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">41.1</span> (17.1)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">36.6</span> (23.1)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">40.1</span> (18.7)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">39.4</span> (16.9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.3 (16.8)</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "quality",
            "text",
            "textaudio",
            "response",
            "against",
            "spokenalpaca",
            "noisyalpaca",
            "quality111lengthcontrolled",
            "reference",
            "win",
            "tie",
            "quality222win",
            "coarse",
            "fine",
            "model",
            "model’s",
            "rates",
            "eval",
            "dubois",
            "ckpt",
            "sft",
            "pretrain",
            "after",
            "comparison",
            "audio",
            "syn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "pretrain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "text",
                    "after",
                    "audio",
                    "pretrain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Foundation Models.</span> Pretraining data quality is pivotal for driving performance of foundation models.\nEfforts like Gopher <cite class=\"ltx_cite ltx_citemacro_citep\">(Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>)</cite>, T5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib114\" title=\"\">2020</a>)</cite>, Nemotron-CC <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>)</cite>, FineWeb <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>)</cite>, DCLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and OLMo-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite> significantly emphasize the benefits of strong data processing, curation and filtering for language data. In computer vision, Dinov2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>)</cite>, Dinov3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, AIMv2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib41\" title=\"\">2025</a>)</cite> and Web-SSL <cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib38\" title=\"\">2025</a>)</cite> showcased the high impact that careful data curation has on model quality.\nSimilar results on the importance of data-centric research have been shown in vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib39\" title=\"\">2023a</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib150\" title=\"\">2025b</a>)</cite> and reasoning-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib81\" title=\"\">2025d</a>; Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib94\" title=\"\">2025</a>)</cite> foundation modeling literature.\nOwing to the paucity of such data-centric research in the speech-language domain, we aim to close this gap through a set of controlled data ablations, demonstrating the strong utility of data-centric approaches for boosting SpeechLM quality.</p>\n\n",
                "matched_terms": [
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "fine",
                    "coarse",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic Sampling.</span>\nWhile the stochastic variant allows flexibility and potentially offers better generalization,\nit can restrict the number of <span class=\"ltx_text ltx_font_italic\">modality switches</span> during training.\nHence, we test a deterministic approach, where we alternate between audio and text modalities at each chunk, i.e. we formulate the training sequence as <math alttext=\"{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>A</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}</annotation></semantics></math>.\nThis <span class=\"ltx_text ltx_font_italic\">maximizes the number of modality switches</span> for a given sample.\nHere too, we always start with <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "after",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training setup.</span>\nFor the purpose of this study, we started from our base model checkpoints and conducted supervised fine-tuning (SFT). Our SFT data consisted of the following categories:</p>\n\n",
                "matched_terms": [
                    "sft",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Question-and-Answer Conversations</span>: We start from about <math alttext=\"1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mn>1.5</mn><annotation encoding=\"application/x-tex\">1.5</annotation></semantics></math> million question-answer conversations in text between users and simulated assistants (more details can be found from Section 4.3.2 in <cite class=\"ltx_cite ltx_citemacro_citet\">Gunter et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>)</cite>). We filter out conversations that are not suitable for spoken dialogues (e.g., conversations involving coding or large chunks of math equations) and rewrite the assistant responses to make them more concise. We then use both <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span> to synthesize text conversations into speech. About <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> millon spoken dialogues are generated in this manner. Further, to improve the robustness to voice variations and background noises on the user side, we mined about <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m3\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math>k speech segments whose transcription indicates that it is a question that can be answered with the given context. We then generate the text response and synthesize it in speech. Both mining and response generation is done by querying <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span>, and the speech synthesizing is done via <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "response"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">TTS and ASR-style Conversations</span>: We convert utterances from ASR/TTS datasets into natural conversation, in which users ask assistants to either transcribe a given audio (ASR) or synthesize a given text (TTS). We also include instruction-following TTS data where users ask to synthesize text responses with specific instructions (e.g., synthesize speech in a given volume, pace, style or emotion).</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike pretraining, we found it useful to explicitly generate the chain-of-thought trajectory, i.e., before the model generates assistant&#8217;s audio response for the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th turn, <math alttext=\"\\texttt{A}_{t}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m2\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{A}_{t}^{\\texttt{a}}</annotation></semantics></math>, we ask the model to generate text tokens for what the user has said in the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th turn, <math alttext=\"\\texttt{T}_{t}^{\\texttt{u}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m4\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}_{t}^{\\texttt{u}}</annotation></semantics></math>, and what assistant would say in text, <math alttext=\"\\texttt{T}_{t}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m5\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}_{t}^{\\texttt{a}}</annotation></semantics></math>. Therefore, for a <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>-turn conversation, <math alttext=\"(\\texttt{A}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{u}}),(\\texttt{A}_{1}^{\\texttt{a}},\\texttt{T}_{1}^{\\texttt{a}}),\\cdots,(\\texttt{A}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{u}}),(\\texttt{A}_{T}^{\\texttt{a}},\\texttt{T}_{T}^{\\texttt{a}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m7\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\texttt{A}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{u}}),(\\texttt{A}_{1}^{\\texttt{a}},\\texttt{T}_{1}^{\\texttt{a}}),\\cdots,(\\texttt{A}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{u}}),(\\texttt{A}_{T}^{\\texttt{a}},\\texttt{T}_{T}^{\\texttt{a}})</annotation></semantics></math>, we formulate a sequence, <math alttext=\"\\underline{\\texttt{A}_{1}^{\\texttt{u}}},\\texttt{T}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{a}},\\texttt{A}_{1}^{\\texttt{a}},\\cdots,\\underline{\\texttt{A}_{T}^{\\texttt{u}}},\\texttt{T}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{a}},\\texttt{A}_{T}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m8\" intent=\":literal\"><semantics><mrow><munder accentunder=\"true\"><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"true\">&#175;</mo></munder><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><munder accentunder=\"true\"><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"true\">&#175;</mo></munder><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup></mrow><annotation encoding=\"application/x-tex\">\\underline{\\texttt{A}_{1}^{\\texttt{u}}},\\texttt{T}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{a}},\\texttt{A}_{1}^{\\texttt{a}},\\cdots,\\underline{\\texttt{A}_{T}^{\\texttt{u}}},\\texttt{T}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{a}},\\texttt{A}_{T}^{\\texttt{a}}</annotation></semantics></math>.\nThe loss from users&#8217; audio tokens (those marked with underlines) are masked out during training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "response",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "fine",
                    "model",
                    "sft",
                    "coarse",
                    "syn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluations.</span>\nWe evaluated SFT models for both <span class=\"ltx_text ltx_font_italic\">text response quality</span> <span class=\"ltx_text ltx_font_italic\">and audio response quality</span>. To evaluate text response quality, we use two eval sets: <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span> and <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. The first is obtained by synthesizing the alpaca evaluation dataset (<cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib80\" title=\"\">2023</a>)</cite>). On top of <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span>, we added various background noise with a SNR randomly sampled from 5 to 15 dB. This produces <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. During the evaluation, 804 spoken alpaca questions were fed in, and the model&#8217;s text response, <math alttext=\"\\texttt{T}^{\\texttt{a}}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p7.m1\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}^{\\texttt{a}}_{1}</annotation></semantics></math>, is extracted. These text responses are pair-wise compared with the responses generated from a performant internal baseline model using the standard evaluation protocol with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> as the judge model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "quality",
                    "spokenalpaca",
                    "text",
                    "noisyalpaca",
                    "eval",
                    "sft",
                    "model’s",
                    "response",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate audio response quality, we work with several third-party vendors to collect diversified user prompts in audio. For multi-turn dialogue evaluation, we adopt the last-turn-with-context strategy to evaluate the last turn&#8217;s assistant response, while the previous assistant responses are generated by <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio</span> and fed in as context. In total, we constructed <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p8.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> evaluation sets, each having a different focus, such as knowledge-rich, multi-turn, long-context, and challenging speech environments. We also notice pair-wise comparison of audio is often harder than text, in which judges (LLM or human) cannot tell which response is better. In order to reduce variance of judge scores, we ask the judge to output whether audio response A is better than, worse than or tied with audio response B. The auto-grading prompt template we used is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A8\" title=\"Appendix H Prompt template for GPT-4o-audio in auto eval &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "text",
                    "response",
                    "comparison",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fine",
                    "coarse",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fine",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "eval",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "after",
                    "model",
                    "against"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription Generation.</span> Next, we aim to provide paired text annotations for all of the raw audio in our corpus. For this, we first used the Whisper model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> to transcribe the raw audio from each of the diarized output chunks. However, we noticed that the Whisper model transcriptions can tend to be quite noisy and contain some hallucinations. To ensure cleaner transcriptions, we use a post-processing transcription ensembling approach called ROVER <cite class=\"ltx_cite ltx_citemacro_citep\">(Fiscus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib42\" title=\"\">1997</a>)</cite> used in prior works performing transcription cleaning <cite class=\"ltx_cite ltx_citemacro_citep\">(Jalalvand et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib64\" title=\"\">2015</a>)</cite>. We first obtain additional speech transcriptions from an internal SIRI transcription model and <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\" title=\"\">Nvidia-Parakeet-TDT-CTC</a>. We then apply the ROVER post-processing method using the three candidate transcriptions from Whisper, SIRI and Parakeet. We use the ensembled transcription as our text annotations for subsequent steps. We provide some examples of the individual model-based transcriptions and the final ROVER-ensembled transcriptions below:</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Coarse interleaving.</span> Here, we aim to have relatively long audio-text chunks. To do this, we continually merge consecutive audio segments based on the diarization outputs while they have the same speakerID. While merging the segments, we concatenate the corresponding text transcriptions of each audio segment, separated by a white-space, to yield the merged text transcription for the merged audio.</p>\n\n",
                "matched_terms": [
                    "text",
                    "coarse",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we compute the completion log-probability for each of the <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> answer choices. We normalize the completion log-probability by answer length to prevent biasing against long answer choices. A question is marked correct if the model assigns highest normalized log-probability to the ground-truth answer. We use standard accuracy metric (random chance level is <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m2\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%) for reporting results. For running all our model evaluations, we use a fork of <span class=\"ltx_text ltx_font_typewriter\">lm-eval-harness</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib45\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "against"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start with a spoken question-answering test set. Each test sample consists of (<math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math>, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m2\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math>, <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math>) triplets, where <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> denotes the spoken question in audio modality, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m5\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> denotes the question in text modality, and <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m6\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> denotes the ground-truth answer in text modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to measure the divergence between the token-wise teacher-forced <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams &amp; Zipser, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib159\" title=\"\">1989</a>)</cite> conditional probability distributions of the audio and text modality. That is, we compare the next&#8211;token distributions under audio vs. text question conditioning, evaluated along the same ground&#8211;truth (GT) answer path (the answer is always in text modality).</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each test sample <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, let <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math> represent the question tokens in text and audio modality respectively.\nThat is, the tokenized representation of <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> is <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and the tokenized representation of <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> is <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math>.\nFor brevity, let us denote these tokenized representations as <math alttext=\"t_{1:m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{1:m}</annotation></semantics></math> and <math alttext=\"a_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{1:n}</annotation></semantics></math>.\nNote that since the length of the question tokens in text and audio modalities might differ, it is possible that <math alttext=\"{n}\\neq{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8800;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">{n}\\neq{m}</annotation></semantics></math>.\nLet <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math> represent the ground-truth answer tokens in text modality i.e. the tokenized representation of <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> is <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math>.\nAgain, for brevity, we denote this as <math alttext=\"g_{1:o}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m14\" intent=\":literal\"><semantics><msub><mi>g</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">g_{1:o}</annotation></semantics></math>.\nLet <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m15\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> be the vocabulary of the SpeechLM.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "after",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            }
        ]
    },
    "S5.T6": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 6: Spoken Question-Answering (S→T{\\text{S}}{\\rightarrow}{\\text{T}}) comparison. We report results for SoTA SpeechLMs and SpeLangy. Where possible, we report results obtained using the pretrained base models (if no base models are publicly released, we evaluate the post-trained checkpoints and make a note of this in the table).",
        "body": "Type\nModel\n# Params\nSWQ\nSTQ\nSLQ\nAverage\n\n\nBase\nKimi-Audio\n10.5B\n44.0\n33.8\n47.0\n41.6\n\n\nQwen-Audio\n8.4B\n45.7\n30.3\n46.0\n40.7\n\n\nQwen-2-Audio\n8.4B\n45.7\n33.4\n47.0\n42.0\n\n\nSpeLangy\n3.8B\n45.7\n44.6\n65.0\n51.8\n\n\nSFT\nVoxtral-mini\n4.7B\n41.6\n46.6\n65.3\n51.2\n\n\nGLM-4-Voice\n9.9B\n43.3\n52.4\n64.7\n53.4",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Type</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\"># Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">SWQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">STQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">SLQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Base</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Kimi-Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Qwen-Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">45.7</span></td>\n<td class=\"ltx_td ltx_align_center\">30.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">46.0</td>\n<td class=\"ltx_td ltx_align_center\">40.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Qwen-2-Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.4B</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">45.7</span></td>\n<td class=\"ltx_td ltx_align_center\">33.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">47.0</td>\n<td class=\"ltx_td ltx_align_center\">42.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\">3.8B</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_bold\">45.7</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_bold\">44.6</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_bold\">65.0</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\"><span class=\"ltx_text ltx_font_bold\">51.8</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SFT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Voxtral-mini</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">51.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">GLM-4-Voice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">9.9B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">52.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">64.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.4</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "105b",
            "qwenaudio",
            "evaluate",
            "spoken",
            "type",
            "swq",
            "pretrained",
            "s→ttextsrightarrowtextt",
            "glm4voice",
            "possible",
            "84b",
            "released",
            "where",
            "base",
            "38b",
            "sota",
            "qwen2audio",
            "questionanswering",
            "spelangy",
            "results",
            "voxtralmini",
            "obtained",
            "report",
            "model",
            "params",
            "99b",
            "posttrained",
            "47b",
            "kimiaudio",
            "average",
            "speechlms",
            "checkpoints",
            "models",
            "slq",
            "stq",
            "sft",
            "note",
            "make",
            "comparison",
            "publicly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "questionanswering",
                    "models",
                    "released",
                    "speechlms",
                    "spelangy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-based assistants are now widely deployed <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib102\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib26\" title=\"\">2025</a>)</cite>,\nlargely riding on the progress of text-only foundation models <cite class=\"ltx_cite ltx_citemacro_citep\">(Bommasani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib14\" title=\"\">2021</a>)</cite>.\nYet, purely textual interactions are inherently limiting for real-world assistants that must operate in open, hands-free settings. Voice provides a natural, low-friction interface for human&#8211;AI interaction, and recent work therefore emphasizes Spoken Question-Answering (SQA) <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>&#8212;where a question is asked in audio and the system must produce spoken or textual answers&#8212;as a core capability for end-to-end speech language models (SpeechLMs).</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechlms",
                    "questionanswering",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "average",
                    "models",
                    "spelangy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "questionanswering",
                    "spoken",
                    "models",
                    "kimiaudio",
                    "speechlms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Foundation Models.</span> Pretraining data quality is pivotal for driving performance of foundation models.\nEfforts like Gopher <cite class=\"ltx_cite ltx_citemacro_citep\">(Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>)</cite>, T5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib114\" title=\"\">2020</a>)</cite>, Nemotron-CC <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>)</cite>, FineWeb <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>)</cite>, DCLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and OLMo-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite> significantly emphasize the benefits of strong data processing, curation and filtering for language data. In computer vision, Dinov2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>)</cite>, Dinov3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, AIMv2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib41\" title=\"\">2025</a>)</cite> and Web-SSL <cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib38\" title=\"\">2025</a>)</cite> showcased the high impact that careful data curation has on model quality.\nSimilar results on the importance of data-centric research have been shown in vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib39\" title=\"\">2023a</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib150\" title=\"\">2025b</a>)</cite> and reasoning-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib81\" title=\"\">2025d</a>; Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib94\" title=\"\">2025</a>)</cite> foundation modeling literature.\nOwing to the paucity of such data-centric research in the speech-language domain, we aim to close this gap through a set of controlled data ablations, demonstrating the strong utility of data-centric approaches for boosting SpeechLM quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "s→ttextsrightarrowtextt",
                    "spoken",
                    "model",
                    "swq",
                    "slq",
                    "stq",
                    "where",
                    "questionanswering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> To ensure our speech-text pretraining recipe does not degrade base language modeling performance, we evaluate on <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> standard text benchmarks spanning across general knowledge, math and coding: <span class=\"ltx_text ltx_font_italic\">MMLU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib59\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">CoreEN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Busbridge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib19\" title=\"\">2025</a>)</cite> (consisting of <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> benchmarks&#8212;<span class=\"ltx_text ltx_font_italic\">ARC-Easy</span> and\n<span class=\"ltx_text ltx_font_italic\">ARC-Challenge</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib24\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">HellaSwag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib171\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Lambada</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Paperno et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib106\" title=\"\">2016</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">PIQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib13\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SciQ</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Welbl et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib154\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">TriviaQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib66\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">WebQuestions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib11\" title=\"\">2013</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">WinoGrande</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib122\" title=\"\">2021</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">GSM-8k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib25\" title=\"\">2021</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">HumanEval</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib21\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "base",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Architecture.</span> We conduct all our experiments with a <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3.8B-parameter SpeechLM, consisting of two major components: a speech tokenizer and a pretrained language model. Our speech tokenizer consists of a speech encoder with conformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib55\" title=\"\">2020</a>)</cite> blocks followed by a finite scalar quantizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib90\" title=\"\">2023</a>)</cite> that outputs discrete speech tokens. We initialize our language model with the dense 3B base-LM from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> that has a context-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "average",
                    "results",
                    "note",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "questionanswering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "possible",
                    "questionanswering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "results",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T3\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find deterministic sampling boosts SQA performance by <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>% on average over stochastic sampling.\nWe posit that the number of modality switches during training affects the SQA performance&#8212;in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F4\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the distribution of modality switches occuring during interleaved training, finding that stochastic sampling switches modalities quite infrequently, whereas the deterministic approach has a higher number of modality switches during training. Indeed, the expected number of modality switches for a sample consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> chunks is <math alttext=\"{n}{-}{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{n}{-}{1}</annotation></semantics></math> for deterministic sampling and <math alttext=\"\\frac{{n}{-}{1}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m4\" intent=\":literal\"><semantics><mfrac><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{{n}{-}{1}}{2}</annotation></semantics></math> for stochastic sampling. By frequently switching modalities more often, deterministic sampling likely enables more effective cross-modal learning, thereby improving downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "average",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "average",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "speechlms",
                    "model",
                    "results",
                    "checkpoints"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training setup.</span>\nFor the purpose of this study, we started from our base model checkpoints and conducted supervised fine-tuning (SFT). Our SFT data consisted of the following categories:</p>\n\n",
                "matched_terms": [
                    "sft",
                    "base",
                    "model",
                    "checkpoints"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Question-and-Answer Conversations</span>: We start from about <math alttext=\"1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mn>1.5</mn><annotation encoding=\"application/x-tex\">1.5</annotation></semantics></math> million question-answer conversations in text between users and simulated assistants (more details can be found from Section 4.3.2 in <cite class=\"ltx_cite ltx_citemacro_citet\">Gunter et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>)</cite>). We filter out conversations that are not suitable for spoken dialogues (e.g., conversations involving coding or large chunks of math equations) and rewrite the assistant responses to make them more concise. We then use both <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span> to synthesize text conversations into speech. About <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> millon spoken dialogues are generated in this manner. Further, to improve the robustness to voice variations and background noises on the user side, we mined about <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m3\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math>k speech segments whose transcription indicates that it is a question that can be answered with the given context. We then generate the text response and synthesize it in speech. Both mining and response generation is done by querying <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span>, and the speech synthesizing is done via <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span>.</p>\n\n",
                "matched_terms": [
                    "make",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Conversations with emotion and general audio understanding knowledge</span>: Here,\nwe include spoken conversations where users ask assistants questions that require emotion, sound and music understanding. As before, we generate such conversations using <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> and synthesize speech using <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "obtained",
                    "model",
                    "sft",
                    "pretrained",
                    "checkpoints"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluations.</span>\nWe evaluated SFT models for both <span class=\"ltx_text ltx_font_italic\">text response quality</span> <span class=\"ltx_text ltx_font_italic\">and audio response quality</span>. To evaluate text response quality, we use two eval sets: <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span> and <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. The first is obtained by synthesizing the alpaca evaluation dataset (<cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib80\" title=\"\">2023</a>)</cite>). On top of <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span>, we added various background noise with a SNR randomly sampled from 5 to 15 dB. This produces <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. During the evaluation, 804 spoken alpaca questions were fed in, and the model&#8217;s text response, <math alttext=\"\\texttt{T}^{\\texttt{a}}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p7.m1\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}^{\\texttt{a}}_{1}</annotation></semantics></math>, is extracted. These text responses are pair-wise compared with the responses generated from a performant internal baseline model using the standard evaluation protocol with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> as the judge model.</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "model",
                    "spoken",
                    "obtained",
                    "models",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate audio response quality, we work with several third-party vendors to collect diversified user prompts in audio. For multi-turn dialogue evaluation, we adopt the last-turn-with-context strategy to evaluate the last turn&#8217;s assistant response, while the previous assistant responses are generated by <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio</span> and fed in as context. In total, we constructed <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p8.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> evaluation sets, each having a different focus, such as knowledge-rich, multi-turn, long-context, and challenging speech environments. We also notice pair-wise comparison of audio is often harder than text, in which judges (LLM or human) cannot tell which response is better. In order to reduce variance of judge scores, we ask the judge to output whether audio response A is better than, worse than or tied with audio response B. The auto-grading prompt template we used is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A8\" title=\"Appendix H Prompt template for GPT-4o-audio in auto eval &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluate",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "obtained",
                    "model",
                    "models",
                    "sft",
                    "posttrained",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "make",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "model",
                    "report",
                    "swq",
                    "slq",
                    "possible",
                    "stq",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "results",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equipped with our key data-centric insights from the previous sections, we now train a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM, called <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>.\nWe use the same training configuration as before, with <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> sequence length\ntrained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens.\nWe compare against SoTA speech-language base models including Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>)</cite>, and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe additionally compare two post-trained models&#8212;Voxtral-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> and GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>&#8212;with the caveat that having undergone instruction-tuning, they are not directly comparable to base models <cite class=\"ltx_cite ltx_citemacro_citep\">(Dominguez-Olmedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib32\" title=\"\">2024</a>)</cite>. To ensure our training recipe does not degrade language performance, we also compare against strong open-weights base language models on standard text-only benchmarks.</p>\n\n",
                "matched_terms": [
                    "qwenaudio",
                    "glm4voice",
                    "models",
                    "base",
                    "posttrained",
                    "kimiaudio",
                    "sota",
                    "qwen2audio",
                    "spelangy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "models",
                    "pretrained",
                    "questionanswering",
                    "results",
                    "spelangy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "report",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "results",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our models are <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter transformer-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib146\" title=\"\">2017</a>)</cite> speech-language models. We use a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> for all our experiments. Our models use a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens. We train for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps in total, yielding a total of <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for our training runs. Using the standard <math alttext=\"{6}{N}{D}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m6\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">{6}{N}{D}</annotation></semantics></math> rule <cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib69\" title=\"\">2020</a>)</cite>, this equates to about <math alttext=\"{3.81}{\\times}{{10}^{22}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">{3.81}{\\times}{{10}^{22}}</annotation></semantics></math>FLOPs (note that this estimate is a rough lower bound since we do not count the FLOPs associated with the speech tokenizer in this estimate).\nWe only tune the language model weights while keep the speech tokenizer frozen.\nWe use a cosine-decay learning rate schedule with <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m8\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> steps of linear-warmup.\nWe use the AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib86\" title=\"\">2017</a>)</cite> optimizer with <math alttext=\"{\\beta_{1}}{=}{0.9}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{1}}{=}{0.9}</annotation></semantics></math> and <math alttext=\"{\\beta_{2}}{=}{0.95}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{2}}{=}{0.95}</annotation></semantics></math>, a peak learning rate of <math alttext=\"{3}{e}{-}{4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">{3}{e}{-}{4}</annotation></semantics></math>, weight decay of <math alttext=\"{1}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{1}{e}{-}{5}</annotation></semantics></math> and clip gradients to a max norm of <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m13\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. We use the <span class=\"ltx_text ltx_font_typewriter\">axlearn</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib75\" title=\"\">2025</a>)</cite> codebase for all our experiments using <span class=\"ltx_text ltx_font_typewriter\">jax</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bradbury et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib16\" title=\"\">2021</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">pygrain</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ritter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib117\" title=\"\">2023</a>)</cite> for dataloading. One training run takes approximately <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m14\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math> days on <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m15\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> TPU-v6e chips.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechlms",
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechlms",
                    "model",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "s→ttextsrightarrowtextt",
                    "evaluate",
                    "spoken",
                    "model",
                    "models",
                    "where",
                    "speechlms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we compute the completion log-probability for each of the <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> answer choices. We normalize the completion log-probability by answer length to prevent biasing against long answer choices. A question is marked correct if the model assigns highest normalized log-probability to the ground-truth answer. We use standard accuracy metric (random chance level is <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m2\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%) for reporting results. For running all our model evaluations, we use a fork of <span class=\"ltx_text ltx_font_typewriter\">lm-eval-harness</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib45\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start with a spoken question-answering test set. Each test sample consists of (<math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math>, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m2\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math>, <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math>) triplets, where <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> denotes the spoken question in audio modality, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m5\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> denotes the question in text modality, and <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m6\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> denotes the ground-truth answer in text modality.</p>\n\n",
                "matched_terms": [
                    "questionanswering",
                    "spoken",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each test sample <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, let <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math> represent the question tokens in text and audio modality respectively.\nThat is, the tokenized representation of <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> is <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and the tokenized representation of <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> is <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math>.\nFor brevity, let us denote these tokenized representations as <math alttext=\"t_{1:m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{1:m}</annotation></semantics></math> and <math alttext=\"a_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{1:n}</annotation></semantics></math>.\nNote that since the length of the question tokens in text and audio modalities might differ, it is possible that <math alttext=\"{n}\\neq{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8800;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">{n}\\neq{m}</annotation></semantics></math>.\nLet <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math> represent the ground-truth answer tokens in text modality i.e. the tokenized representation of <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> is <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math>.\nAgain, for brevity, we denote this as <math alttext=\"g_{1:o}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m14\" intent=\":literal\"><semantics><msub><mi>g</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">g_{1:o}</annotation></semantics></math>.\nLet <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m15\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> be the vocabulary of the SpeechLM.</p>\n\n",
                "matched_terms": [
                    "possible",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "obtained",
                    "report",
                    "models",
                    "speechlms",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "released"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "swq",
                    "slq",
                    "model",
                    "stq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "possible",
                    "model",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "model",
                    "spoken",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While research in optimal ways to do test-set contamination in language models is still nascent, many works take the alternate approach of building benchmarks that are by construction non-contaminated <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib48\" title=\"\">2024</a>; White et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib156\" title=\"\">2024</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib174\" title=\"\">2025</a>; Wildman et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib158\" title=\"\">2025</a>; Jain et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib63\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib178\" title=\"\">2024b</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib167\" title=\"\">2023</a>; Srivastava et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib130\" title=\"\">2024</a>)</cite>. We note that there is a huge gap in such robust evaluations in the speech-language modeling community, and striving for better benchmarks would enable stronger significance in results, while diminishing the impacts of train-test contamination on downstream model performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "results",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our experiments were at the <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B parameter scale trained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens (roughly <math alttext=\"{\\sim}{{3.81}{\\times}{10^{22}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\sim}{{3.81}{\\times}{10^{22}}}</annotation></semantics></math> FLOPs). While our results are strong (outperforming models that are <math alttext=\"3{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">3{\\times}</annotation></semantics></math> the size, trained for similar compute budgets), it would still be interesting to explore if our data-centric strategies would hold at larger model scales. While recent papers like <cite class=\"ltx_cite ltx_citemacro_citet\">Nezhurina et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib96\" title=\"\">2025</a>)</cite>, DataComp-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite>, HoneyBee <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> and DataComp-CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>)</cite> suggest transferability of data curation methods across model scales, recent work in language and vision-language modeling has posited that there may be trade-offs when applying data curation across different model sizes and compute budgets <cite class=\"ltx_cite ltx_citemacro_citep\">(Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Goyal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib52\" title=\"\">2024</a>)</cite>. To the best of our knowledge, no existing work showcases such trade-offs in the SpeechLM community. It would be an interesting direction to explore the interaction of data recipes with model scale and compute budget.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the focus of our work was mainly on improving spoken question-answering capabilities of SpeechLMs, all our experiments used the standard benchmarks that are prevalent in the literature for our task of interest <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>. We therefore did not explore how our models would perform on more targeted tasks like automatic speech recognition, emotion recognition or text-to-speech synthesis. One caveat preventing us from a direct comparison on such tasks is that we do not employ any task-specific training, unlike other SpeechLMs that explicitly add in a task-specific component into their training mixture (e.g., ASR-specific training datasets) <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spoken",
                    "questionanswering",
                    "models",
                    "speechlms",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "spoken",
                    "model",
                    "models",
                    "where",
                    "note",
                    "kimiaudio",
                    "questionanswering",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "questionanswering",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our training runs initialize the language model backbone for our SpeechLM using a pretrained base-LM. This is the standard recipe used by almost all the existing foundation SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>. However, recent work in the vision-language literature has advocated for full native multimodal pretraining from scratch <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>)</cite>, where both the language model and the modality-specific encoder/tokenizer are trained from scratch. It would be interesting to explore if our data-centric methods also enable more efficient SpeechLM pretraining from scratch in the future.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "speechlms",
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "speechlms",
                    "model"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 7: Text Understanding (T→T{\\text{T}}{\\rightarrow}{\\text{T}}) comparison. We compare SpeLangy with leading text-only base models of the same size-class. Text-init is the model we start continued-pretraining from. Our model is competitive with all compared models, highlighting that we strongly preserve text-only capabilities after speech-text training.",
        "body": "Model\n# Params\nCoreEN\nMMLU\nGSM8k\nHumanEval\n\n\nText-init\n3B\n62.4\n62.2\n47.1\n29.9\n\n\nGemma-2\n2.6B\n–\n56.1\n30.3\n19.5\n\n\nGemma-3\n4B\n–\n62.8\n38.4\n36.0\n\n\nQwen-2.5\n3B\n–\n65.6\n79.1\n42.1\n\n\nSpeLangy\n3.8B\n61.8\n67.3\n71.9\n37.6",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\"># Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">CoreEN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">GSM8k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">HumanEval</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Text-init</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">62.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Gemma-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.6B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">56.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Gemma-3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4B</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">62.8</td>\n<td class=\"ltx_td ltx_align_center\">38.4</td>\n<td class=\"ltx_td ltx_align_center\">36.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Qwen-2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3B</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">65.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">79.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">42.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6FF;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\" style=\"--ltx-bg-color:#E6E6FF;\">SpeLangy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\">3.8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6FF;\">61.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6FF;\">67.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#E6E6FF;\">71.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#E6E6FF;\">37.6</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "compare",
            "training",
            "text",
            "textonly",
            "gemma3",
            "26b",
            "same",
            "gsm8k",
            "humaneval",
            "leading",
            "qwen25",
            "our",
            "all",
            "mmlu",
            "base",
            "from",
            "highlighting",
            "38b",
            "start",
            "spelangy",
            "model",
            "continuedpretraining",
            "params",
            "t→ttexttrightarrowtextt",
            "textinit",
            "sizeclass",
            "capabilities",
            "speechtext",
            "models",
            "strongly",
            "gemma2",
            "compared",
            "understanding",
            "preserve",
            "after",
            "comparison",
            "coreen",
            "competitive"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "text",
                    "models",
                    "from",
                    "spelangy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-based assistants are now widely deployed <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib102\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib26\" title=\"\">2025</a>)</cite>,\nlargely riding on the progress of text-only foundation models <cite class=\"ltx_cite ltx_citemacro_citep\">(Bommasani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib14\" title=\"\">2021</a>)</cite>.\nYet, purely textual interactions are inherently limiting for real-world assistants that must operate in open, hands-free settings. Voice provides a natural, low-friction interface for human&#8211;AI interaction, and recent work therefore emphasizes Spoken Question-Answering (SQA) <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>&#8212;where a question is asked in audio and the system must produce spoken or textual answers&#8212;as a core capability for end-to-end speech language models (SpeechLMs).</p>\n\n",
                "matched_terms": [
                    "models",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "textonly",
                    "from",
                    "speechtext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "text",
                    "models",
                    "compared",
                    "after",
                    "spelangy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechtext",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Foundation Models.</span> Pretraining data quality is pivotal for driving performance of foundation models.\nEfforts like Gopher <cite class=\"ltx_cite ltx_citemacro_citep\">(Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>)</cite>, T5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib114\" title=\"\">2020</a>)</cite>, Nemotron-CC <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>)</cite>, FineWeb <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>)</cite>, DCLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and OLMo-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite> significantly emphasize the benefits of strong data processing, curation and filtering for language data. In computer vision, Dinov2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>)</cite>, Dinov3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, AIMv2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Fini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib41\" title=\"\">2025</a>)</cite> and Web-SSL <cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib38\" title=\"\">2025</a>)</cite> showcased the high impact that careful data curation has on model quality.\nSimilar results on the importance of data-centric research have been shown in vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib39\" title=\"\">2023a</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib150\" title=\"\">2025b</a>)</cite> and reasoning-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib81\" title=\"\">2025d</a>; Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib94\" title=\"\">2025</a>)</cite> foundation modeling literature.\nOwing to the paucity of such data-centric research in the speech-language domain, we aim to close this gap through a set of controlled data ablations, demonstrating the strong utility of data-centric approaches for boosting SpeechLM quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "text",
                    "textonly",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "all",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Understanding (<math alttext=\"{\\text{T}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">T</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{T}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> To ensure our speech-text pretraining recipe does not degrade base language modeling performance, we evaluate on <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> standard text benchmarks spanning across general knowledge, math and coding: <span class=\"ltx_text ltx_font_italic\">MMLU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib59\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">CoreEN</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gunter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Busbridge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib19\" title=\"\">2025</a>)</cite> (consisting of <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> benchmarks&#8212;<span class=\"ltx_text ltx_font_italic\">ARC-Easy</span> and\n<span class=\"ltx_text ltx_font_italic\">ARC-Challenge</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib24\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">HellaSwag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zellers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib171\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">Lambada</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Paperno et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib106\" title=\"\">2016</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">PIQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bisk et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib13\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">SciQ</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Welbl et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib154\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">TriviaQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib66\" title=\"\">2017</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">WebQuestions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib11\" title=\"\">2013</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">WinoGrande</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakaguchi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib122\" title=\"\">2021</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">GSM-8k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cobbe et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib25\" title=\"\">2021</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">HumanEval</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib21\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "mmlu",
                    "t→ttexttrightarrowtextt",
                    "base",
                    "understanding",
                    "gsm8k",
                    "humaneval",
                    "coreen",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Architecture.</span> We conduct all our experiments with a <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3.8B-parameter SpeechLM, consisting of two major components: a speech tokenizer and a pretrained language model. Our speech tokenizer consists of a speech encoder with conformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib55\" title=\"\">2020</a>)</cite> blocks followed by a finite scalar quantizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib90\" title=\"\">2023</a>)</cite> that outputs discrete speech tokens. We initialize our language model with the dense 3B base-LM from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> that has a context-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "text",
                    "textonly",
                    "continuedpretraining",
                    "base",
                    "from",
                    "preserve",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speechtext",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "textonly",
                    "all",
                    "compared",
                    "from",
                    "same",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "models",
                    "textonly",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "training",
                    "text",
                    "from",
                    "preserve",
                    "start"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "all",
                    "from",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "training",
                    "textonly",
                    "mmlu",
                    "compared",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Sampling.</span> In the first variant (used in all our previous experiments), at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, we randomly sample the chunk-modality with <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math> probability.\nThe modality sampling at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is independent of all other chunks <math alttext=\"{j}{\\neq}{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>&#8800;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">{j}{\\neq}{i}</annotation></semantics></math>.\nWe always start with an audio chunk <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m5\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>, to ensure that there is at least <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> audio chunk in our training sequence.</p>\n\n",
                "matched_terms": [
                    "start",
                    "all",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic Sampling.</span>\nWhile the stochastic variant allows flexibility and potentially offers better generalization,\nit can restrict the number of <span class=\"ltx_text ltx_font_italic\">modality switches</span> during training.\nHence, we test a deterministic approach, where we alternate between audio and text modalities at each chunk, i.e. we formulate the training sequence as <math alttext=\"{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>A</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}</annotation></semantics></math>.\nThis <span class=\"ltx_text ltx_font_italic\">maximizes the number of modality switches</span> for a given sample.\nHere too, we always start with <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "start",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T3\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find deterministic sampling boosts SQA performance by <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>% on average over stochastic sampling.\nWe posit that the number of modality switches during training affects the SQA performance&#8212;in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F4\" title=\"In 3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the distribution of modality switches occuring during interleaved training, finding that stochastic sampling switches modalities quite infrequently, whereas the deterministic approach has a higher number of modality switches during training. Indeed, the expected number of modality switches for a sample consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> chunks is <math alttext=\"{n}{-}{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{n}{-}{1}</annotation></semantics></math> for deterministic sampling and <math alttext=\"\\frac{{n}{-}{1}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p4.m4\" intent=\":literal\"><semantics><mfrac><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{{n}{-}{1}}{2}</annotation></semantics></math> for stochastic sampling. By frequently switching modalities more often, deterministic sampling likely enables more effective cross-modal learning, thereby improving downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "training",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "all",
                    "after",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Post-training setup.</span>\nFor the purpose of this study, we started from our base model checkpoints and conducted supervised fine-tuning (SFT). Our SFT data consisted of the following categories:</p>\n\n",
                "matched_terms": [
                    "base",
                    "from",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Question-and-Answer Conversations</span>: We start from about <math alttext=\"1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mn>1.5</mn><annotation encoding=\"application/x-tex\">1.5</annotation></semantics></math> million question-answer conversations in text between users and simulated assistants (more details can be found from Section 4.3.2 in <cite class=\"ltx_cite ltx_citemacro_citet\">Gunter et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib57\" title=\"\">2024</a>)</cite>). We filter out conversations that are not suitable for spoken dialogues (e.g., conversations involving coding or large chunks of math equations) and rewrite the assistant responses to make them more concise. We then use both <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span> to synthesize text conversations into speech. About <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m2\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> millon spoken dialogues are generated in this manner. Further, to improve the robustness to voice variations and background noises on the user side, we mined about <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m3\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math>k speech segments whose transcription indicates that it is a question that can be answered with the given context. We then generate the text response and synthesize it in speech. Both mining and response generation is done by querying <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span>, and the speech synthesizing is done via <span class=\"ltx_text ltx_font_typewriter\">gpt4o-audio</span>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "start"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">TTS and ASR-style Conversations</span>: We convert utterances from ASR/TTS datasets into natural conversation, in which users ask assistants to either transcribe a given audio (ASR) or synthesize a given text (TTS). We also include instruction-following TTS data where users ask to synthesize text responses with specific instructions (e.g., synthesize speech in a given volume, pace, style or emotion).</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike pretraining, we found it useful to explicitly generate the chain-of-thought trajectory, i.e., before the model generates assistant&#8217;s audio response for the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th turn, <math alttext=\"\\texttt{A}_{t}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m2\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{A}_{t}^{\\texttt{a}}</annotation></semantics></math>, we ask the model to generate text tokens for what the user has said in the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th turn, <math alttext=\"\\texttt{T}_{t}^{\\texttt{u}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m4\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}_{t}^{\\texttt{u}}</annotation></semantics></math>, and what assistant would say in text, <math alttext=\"\\texttt{T}_{t}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m5\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}_{t}^{\\texttt{a}}</annotation></semantics></math>. Therefore, for a <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>-turn conversation, <math alttext=\"(\\texttt{A}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{u}}),(\\texttt{A}_{1}^{\\texttt{a}},\\texttt{T}_{1}^{\\texttt{a}}),\\cdots,(\\texttt{A}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{u}}),(\\texttt{A}_{T}^{\\texttt{a}},\\texttt{T}_{T}^{\\texttt{a}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m7\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\texttt{A}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{u}}),(\\texttt{A}_{1}^{\\texttt{a}},\\texttt{T}_{1}^{\\texttt{a}}),\\cdots,(\\texttt{A}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{u}}),(\\texttt{A}_{T}^{\\texttt{a}},\\texttt{T}_{T}^{\\texttt{a}})</annotation></semantics></math>, we formulate a sequence, <math alttext=\"\\underline{\\texttt{A}_{1}^{\\texttt{u}}},\\texttt{T}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{a}},\\texttt{A}_{1}^{\\texttt{a}},\\cdots,\\underline{\\texttt{A}_{T}^{\\texttt{u}}},\\texttt{T}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{a}},\\texttt{A}_{T}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m8\" intent=\":literal\"><semantics><mrow><munder accentunder=\"true\"><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"true\">&#175;</mo></munder><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><munder accentunder=\"true\"><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"true\">&#175;</mo></munder><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup></mrow><annotation encoding=\"application/x-tex\">\\underline{\\texttt{A}_{1}^{\\texttt{u}}},\\texttt{T}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{a}},\\texttt{A}_{1}^{\\texttt{a}},\\cdots,\\underline{\\texttt{A}_{T}^{\\texttt{u}}},\\texttt{T}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{a}},\\texttt{A}_{T}^{\\texttt{a}}</annotation></semantics></math>.\nThe loss from users&#8217; audio tokens (those marked with underlines) are masked out during training.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "from",
                    "same",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "text",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluations.</span>\nWe evaluated SFT models for both <span class=\"ltx_text ltx_font_italic\">text response quality</span> <span class=\"ltx_text ltx_font_italic\">and audio response quality</span>. To evaluate text response quality, we use two eval sets: <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span> and <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. The first is obtained by synthesizing the alpaca evaluation dataset (<cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib80\" title=\"\">2023</a>)</cite>). On top of <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span>, we added various background noise with a SNR randomly sampled from 5 to 15 dB. This produces <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. During the evaluation, 804 spoken alpaca questions were fed in, and the model&#8217;s text response, <math alttext=\"\\texttt{T}^{\\texttt{a}}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p7.m1\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}^{\\texttt{a}}_{1}</annotation></semantics></math>, is extracted. These text responses are pair-wise compared with the responses generated from a performant internal baseline model using the standard evaluation protocol with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> as the judge model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "compared",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate audio response quality, we work with several third-party vendors to collect diversified user prompts in audio. For multi-turn dialogue evaluation, we adopt the last-turn-with-context strategy to evaluate the last turn&#8217;s assistant response, while the previous assistant responses are generated by <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio</span> and fed in as context. In total, we constructed <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p8.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> evaluation sets, each having a different focus, such as knowledge-rich, multi-turn, long-context, and challenging speech environments. We also notice pair-wise comparison of audio is often harder than text, in which judges (LLM or human) cannot tell which response is better. In order to reduce variance of judge scores, we ask the judge to output whether audio response A is better than, worse than or tied with audio response B. The auto-grading prompt template we used is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A8\" title=\"Appendix H Prompt template for GPT-4o-audio in auto eval &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "text",
                    "textonly",
                    "models",
                    "from",
                    "after",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "start"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "compared",
                    "model",
                    "leading",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "text",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Web-crawled data is highly skewed</span> and is majorly comprised of <span class=\"ltx_text ltx_font_italic\">entertainment</span>, <span class=\"ltx_text ltx_font_italic\">sports and fitness</span>, <span class=\"ltx_text ltx_font_italic\">religion</span> and <span class=\"ltx_text ltx_font_italic\">social life</span> domains. This is not surprising given that most of our web-crawled audio data is sourced from podcasts, interviews, talk-shows and monologues.</p>\n\n",
                "matched_terms": [
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "all",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, by enabling broader coverage of topic domains, our synthetic datasets help to (1) close the distribution mismatch between the raw web-crawled data and the downstream evaluation datasets, and (2) enhance the diversity of our pretraining data distribution. Our findings extend prior work in the language space that have discussed the importance of training data diversity and domain coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib98\" title=\"\">2025a</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib151\" title=\"\">2025c</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> to the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "all",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "same",
                    "after",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equipped with our key data-centric insights from the previous sections, we now train a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM, called <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>.\nWe use the same training configuration as before, with <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> sequence length\ntrained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens.\nWe compare against SoTA speech-language base models including Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>)</cite>, and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe additionally compare two post-trained models&#8212;Voxtral-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> and GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>&#8212;with the caveat that having undergone instruction-tuning, they are not directly comparable to base models <cite class=\"ltx_cite ltx_citemacro_citep\">(Dominguez-Olmedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib32\" title=\"\">2024</a>)</cite>. To ensure our training recipe does not degrade language performance, we also compare against strong open-weights base language models on standard text-only benchmarks.</p>\n\n",
                "matched_terms": [
                    "compare",
                    "speechtext",
                    "training",
                    "textonly",
                    "models",
                    "base",
                    "from",
                    "same",
                    "spelangy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "text",
                    "models",
                    "competitive",
                    "from",
                    "spelangy",
                    "our",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide more details about each step in our data processing pipeline for converting web-crawled audio into interleaved speech-text format. We highlight all the components in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "speechtext",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Raw Audio.</span> We start with a large corpus (<math alttext=\"{&gt;}10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}10</annotation></semantics></math>M hours) of conversational-speech audio crawled from the web. These are sourced from a range of web domains, filtered to remove other audio types like music, ads and background noise.\nOur audio corpus primarily consist of podcasts, interviews and monologue speeches.</p>\n\n",
                "matched_terms": [
                    "start",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription Generation.</span> Next, we aim to provide paired text annotations for all of the raw audio in our corpus. For this, we first used the Whisper model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> to transcribe the raw audio from each of the diarized output chunks. However, we noticed that the Whisper model transcriptions can tend to be quite noisy and contain some hallucinations. To ensure cleaner transcriptions, we use a post-processing transcription ensembling approach called ROVER <cite class=\"ltx_cite ltx_citemacro_citep\">(Fiscus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib42\" title=\"\">1997</a>)</cite> used in prior works performing transcription cleaning <cite class=\"ltx_cite ltx_citemacro_citep\">(Jalalvand et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib64\" title=\"\">2015</a>)</cite>. We first obtain additional speech transcriptions from an internal SIRI transcription model and <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\" title=\"\">Nvidia-Parakeet-TDT-CTC</a>. We then apply the ROVER post-processing method using the three candidate transcriptions from Whisper, SIRI and Parakeet. We use the ensembled transcription as our text annotations for subsequent steps. We provide some examples of the individual model-based transcriptions and the final ROVER-ensembled transcriptions below:</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "all",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved Chunking.</span> The last step in our pipeline is the interleaved chunking stage, which constructs the final audio-text chunks used for interleaved training. As described in the main text, we study two chunking strategies:</p>\n\n",
                "matched_terms": [
                    "text",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Coarse interleaving.</span> Here, we aim to have relatively long audio-text chunks. To do this, we continually merge consecutive audio segments based on the diarization outputs while they have the same speakerID. While merging the segments, we concatenate the corresponding text transcriptions of each audio segment, separated by a white-space, to yield the merged text transcription for the merged audio.</p>\n\n",
                "matched_terms": [
                    "text",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Fine interleaving.</span> Since the original diarized output segments already yield relatively short chunks, we do not apply any post-processing on the output segments and directly use them as our audio-text chunks for interleaved training.</p>\n\n",
                "matched_terms": [
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extraction prompt for <span class=\"ltx_text ltx_font_italic\">Krist</span>.</span> To extract and lightly rewrite the text content from the HTML using <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span>, we use the following prompt:</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only dataset.</span> For our text-only continued pretraining dataset, we use the dataset used in the continual pretraining experiments of <cite class=\"ltx_cite ltx_citemacro_citet\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite>, which roughly comprises of <math alttext=\"2.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mn>2.2</mn><annotation encoding=\"application/x-tex\">2.2</annotation></semantics></math>T tokens.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "all",
                    "speechtext",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "textonly",
                    "all",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our models are <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter transformer-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib146\" title=\"\">2017</a>)</cite> speech-language models. We use a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> for all our experiments. Our models use a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens. We train for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps in total, yielding a total of <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for our training runs. Using the standard <math alttext=\"{6}{N}{D}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m6\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">{6}{N}{D}</annotation></semantics></math> rule <cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib69\" title=\"\">2020</a>)</cite>, this equates to about <math alttext=\"{3.81}{\\times}{{10}^{22}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">{3.81}{\\times}{{10}^{22}}</annotation></semantics></math>FLOPs (note that this estimate is a rough lower bound since we do not count the FLOPs associated with the speech tokenizer in this estimate).\nWe only tune the language model weights while keep the speech tokenizer frozen.\nWe use a cosine-decay learning rate schedule with <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m8\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> steps of linear-warmup.\nWe use the AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib86\" title=\"\">2017</a>)</cite> optimizer with <math alttext=\"{\\beta_{1}}{=}{0.9}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{1}}{=}{0.9}</annotation></semantics></math> and <math alttext=\"{\\beta_{2}}{=}{0.95}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{2}}{=}{0.95}</annotation></semantics></math>, a peak learning rate of <math alttext=\"{3}{e}{-}{4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">{3}{e}{-}{4}</annotation></semantics></math>, weight decay of <math alttext=\"{1}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{1}{e}{-}{5}</annotation></semantics></math> and clip gradients to a max norm of <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m13\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. We use the <span class=\"ltx_text ltx_font_typewriter\">axlearn</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib75\" title=\"\">2025</a>)</cite> codebase for all our experiments using <span class=\"ltx_text ltx_font_typewriter\">jax</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bradbury et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib16\" title=\"\">2021</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">pygrain</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ritter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib117\" title=\"\">2023</a>)</cite> for dataloading. One training run takes approximately <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m14\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math> days on <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m15\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> TPU-v6e chips.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "models",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "training",
                    "models",
                    "understanding",
                    "our",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "training",
                    "text",
                    "models",
                    "from",
                    "understanding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below, we also provide a few examples from each evaluation dataset, with the question (in text), choices, and the ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we compute the completion log-probability for each of the <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> answer choices. We normalize the completion log-probability by answer length to prevent biasing against long answer choices. A question is marked correct if the model assigns highest normalized log-probability to the ground-truth answer. We use standard accuracy metric (random chance level is <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m2\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%) for reporting results. For running all our model evaluations, we use a fork of <span class=\"ltx_text ltx_font_typewriter\">lm-eval-harness</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib45\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start with a spoken question-answering test set. Each test sample consists of (<math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math>, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m2\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math>, <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math>) triplets, where <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> denotes the spoken question in audio modality, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m5\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> denotes the question in text modality, and <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m6\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> denotes the ground-truth answer in text modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "start"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to measure the divergence between the token-wise teacher-forced <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams &amp; Zipser, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib159\" title=\"\">1989</a>)</cite> conditional probability distributions of the audio and text modality. That is, we compare the next&#8211;token distributions under audio vs. text question conditioning, evaluated along the same ground&#8211;truth (GT) answer path (the answer is always in text modality).</p>\n\n",
                "matched_terms": [
                    "text",
                    "same",
                    "compare"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "models",
                    "all",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "all",
                    "from",
                    "same",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "from",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start from the full test set (containing contaminated samples).\nIn our significance test, we test<span class=\"ltx_text ltx_font_italic\"> whether removing contaminated test items reduces accuracy beyond what would be expected under random removal of an equal number of items.</span>\nFormally, for accuracy <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>, the null is:</p>\n\n",
                "matched_terms": [
                    "start",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "training",
                    "all",
                    "from",
                    "same",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "models",
                    "all",
                    "from",
                    "after",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While research in optimal ways to do test-set contamination in language models is still nascent, many works take the alternate approach of building benchmarks that are by construction non-contaminated <cite class=\"ltx_cite ltx_citemacro_citep\">(Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib48\" title=\"\">2024</a>; White et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib156\" title=\"\">2024</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib174\" title=\"\">2025</a>; Wildman et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib158\" title=\"\">2025</a>; Jain et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib63\" title=\"\">2024</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib178\" title=\"\">2024b</a>; Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib167\" title=\"\">2023</a>; Srivastava et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib130\" title=\"\">2024</a>)</cite>. We note that there is a huge gap in such robust evaluations in the speech-language modeling community, and striving for better benchmarks would enable stronger significance in results, while diminishing the impacts of train-test contamination on downstream model performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our experiments were at the <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B parameter scale trained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens (roughly <math alttext=\"{\\sim}{{3.81}{\\times}{10^{22}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\sim}{{3.81}{\\times}{10^{22}}}</annotation></semantics></math> FLOPs). While our results are strong (outperforming models that are <math alttext=\"3{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">3{\\times}</annotation></semantics></math> the size, trained for similar compute budgets), it would still be interesting to explore if our data-centric strategies would hold at larger model scales. While recent papers like <cite class=\"ltx_cite ltx_citemacro_citet\">Nezhurina et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib96\" title=\"\">2025</a>)</cite>, DataComp-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite>, HoneyBee <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> and DataComp-CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>)</cite> suggest transferability of data curation methods across model scales, recent work in language and vision-language modeling has posited that there may be trade-offs when applying data curation across different model sizes and compute budgets <cite class=\"ltx_cite ltx_citemacro_citep\">(Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Goyal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib52\" title=\"\">2024</a>)</cite>. To the best of our knowledge, no existing work showcases such trade-offs in the SpeechLM community. It would be an interesting direction to explore the interaction of data recipes with model scale and compute budget.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "models",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the focus of our work was mainly on improving spoken question-answering capabilities of SpeechLMs, all our experiments used the standard benchmarks that are prevalent in the literature for our task of interest <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>. We therefore did not explore how our models would perform on more targeted tasks like automatic speech recognition, emotion recognition or text-to-speech synthesis. One caveat preventing us from a direct comparison on such tasks is that we do not employ any task-specific training, unlike other SpeechLMs that explicitly add in a task-specific component into their training mixture (e.g., ASR-specific training datasets) <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "models",
                    "all",
                    "from",
                    "comparison",
                    "our",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "model",
                    "textonly",
                    "models",
                    "from",
                    "understanding",
                    "our",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "text",
                    "textonly",
                    "all",
                    "models",
                    "mmlu",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our training runs initialize the language model backbone for our SpeechLM using a pretrained base-LM. This is the standard recipe used by almost all the existing foundation SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>. However, recent work in the vision-language literature has advocated for full native multimodal pretraining from scratch <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>)</cite>, where both the language model and the modality-specific encoder/tokenizer are trained from scratch. It would be interesting to explore if our data-centric methods also enable more efficient SpeechLM pretraining from scratch in the future.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "all",
                    "from",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "continuedpretraining",
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "model",
                    "training",
                    "text",
                    "textonly",
                    "models",
                    "base",
                    "our",
                    "capabilities"
                ]
            }
        ]
    },
    "A3.T8": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 8: Training Data Statistics.",
        "body": "Training dataset\n# Hours\n# Speech tokens\n\n\nWeb-crawl\n\n8.038.03M\n\n361.3361.3B\n\n\nKrist\n\n4.724.72M\n\n212.4212.4B\n\n\nQuest\n\n0.860.86M\n\n3838B",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Training dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\"># Hours</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold ltx_align_center\"># Speech tokens</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Web-crawl</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"8.03\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m1\" intent=\":literal\"><semantics><mn>8.03</mn><annotation encoding=\"application/x-tex\">8.03</annotation></semantics></math>M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"361.3\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m2\" intent=\":literal\"><semantics><mn>361.3</mn><annotation encoding=\"application/x-tex\">361.3</annotation></semantics></math>B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Krist</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"4.72\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m3\" intent=\":literal\"><semantics><mn>4.72</mn><annotation encoding=\"application/x-tex\">4.72</annotation></semantics></math>M</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"212.4\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m4\" intent=\":literal\"><semantics><mn>212.4</mn><annotation encoding=\"application/x-tex\">212.4</annotation></semantics></math>B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Quest</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m5\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"38\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m6\" intent=\":literal\"><semantics><mn>38</mn><annotation encoding=\"application/x-tex\">38</annotation></semantics></math>B</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "training",
            "3838b",
            "webcrawl",
            "dataset",
            "21242124b",
            "803803m",
            "086086m",
            "hours",
            "krist",
            "data",
            "tokens",
            "472472m",
            "36133613b",
            "statistics",
            "quest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "training",
                    "3838b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Architecture.</span> We conduct all our experiments with a <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3.8B-parameter SpeechLM, consisting of two major components: a speech tokenizer and a pretrained language model. Our speech tokenizer consists of a speech encoder with conformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib55\" title=\"\">2020</a>)</cite> blocks followed by a finite scalar quantizer <cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib90\" title=\"\">2023</a>)</cite> that outputs discrete speech tokens. We initialize our language model with the dense 3B base-LM from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> that has a context-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Optimization Details.</span> We train with a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens, for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps. We use the standard next-token prediction objective and compute the loss over both speech and text tokens (we also conduct ablations with loss-masking on the speech tokens in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>).\nWe only tune the language model weights while keeping the speech tokenizer frozen.\nFor more details regarding optimizer, learning rate schedule and training configuration, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A4\" title=\"Appendix D Training Details &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "data",
                    "statistics",
                    "training",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "training",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "hours",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "training",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">TTS and ASR-style Conversations</span>: We convert utterances from ASR/TTS datasets into natural conversation, in which users ask assistants to either transcribe a given audio (ASR) or synthesize a given text (TTS). We also include instruction-following TTS data where users ask to synthesize text responses with specific instructions (e.g., synthesize speech in a given volume, pace, style or emotion).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike pretraining, we found it useful to explicitly generate the chain-of-thought trajectory, i.e., before the model generates assistant&#8217;s audio response for the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th turn, <math alttext=\"\\texttt{A}_{t}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m2\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{A}_{t}^{\\texttt{a}}</annotation></semantics></math>, we ask the model to generate text tokens for what the user has said in the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th turn, <math alttext=\"\\texttt{T}_{t}^{\\texttt{u}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m4\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}_{t}^{\\texttt{u}}</annotation></semantics></math>, and what assistant would say in text, <math alttext=\"\\texttt{T}_{t}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m5\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>t</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}_{t}^{\\texttt{a}}</annotation></semantics></math>. Therefore, for a <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>-turn conversation, <math alttext=\"(\\texttt{A}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{u}}),(\\texttt{A}_{1}^{\\texttt{a}},\\texttt{T}_{1}^{\\texttt{a}}),\\cdots,(\\texttt{A}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{u}}),(\\texttt{A}_{T}^{\\texttt{a}},\\texttt{T}_{T}^{\\texttt{a}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m7\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\texttt{A}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{u}}),(\\texttt{A}_{1}^{\\texttt{a}},\\texttt{T}_{1}^{\\texttt{a}}),\\cdots,(\\texttt{A}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{u}}),(\\texttt{A}_{T}^{\\texttt{a}},\\texttt{T}_{T}^{\\texttt{a}})</annotation></semantics></math>, we formulate a sequence, <math alttext=\"\\underline{\\texttt{A}_{1}^{\\texttt{u}}},\\texttt{T}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{a}},\\texttt{A}_{1}^{\\texttt{a}},\\cdots,\\underline{\\texttt{A}_{T}^{\\texttt{u}}},\\texttt{T}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{a}},\\texttt{A}_{T}^{\\texttt{a}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p4.m8\" intent=\":literal\"><semantics><mrow><munder accentunder=\"true\"><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"true\">&#175;</mo></munder><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><munder accentunder=\"true\"><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo stretchy=\"true\">&#175;</mo></munder><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">u</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><mo>,</mo><msubsup><mtext class=\"ltx_mathvariant_monospace\">A</mtext><mi>T</mi><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup></mrow><annotation encoding=\"application/x-tex\">\\underline{\\texttt{A}_{1}^{\\texttt{u}}},\\texttt{T}_{1}^{\\texttt{u}},\\texttt{T}_{1}^{\\texttt{a}},\\texttt{A}_{1}^{\\texttt{a}},\\cdots,\\underline{\\texttt{A}_{T}^{\\texttt{u}}},\\texttt{T}_{T}^{\\texttt{u}},\\texttt{T}_{T}^{\\texttt{a}},\\texttt{A}_{T}^{\\texttt{a}}</annotation></semantics></math>.\nThe loss from users&#8217; audio tokens (those marked with underlines) are masked out during training.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "data",
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "training",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, by enabling broader coverage of topic domains, our synthetic datasets help to (1) close the distribution mismatch between the raw web-crawled data and the downstream evaluation datasets, and (2) enhance the diversity of our pretraining data distribution. Our findings extend prior work in the language space that have discussed the importance of training data diversity and domain coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib98\" title=\"\">2025a</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib151\" title=\"\">2025c</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> to the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "dataset",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equipped with our key data-centric insights from the previous sections, we now train a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM, called <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>.\nWe use the same training configuration as before, with <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> sequence length\ntrained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens.\nWe compare against SoTA speech-language base models including Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>)</cite>, and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe additionally compare two post-trained models&#8212;Voxtral-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> and GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>&#8212;with the caveat that having undergone instruction-tuning, they are not directly comparable to base models <cite class=\"ltx_cite ltx_citemacro_citep\">(Dominguez-Olmedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib32\" title=\"\">2024</a>)</cite>. To ensure our training recipe does not degrade language performance, we also compare against strong open-weights base language models on standard text-only benchmarks.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "3838b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only dataset.</span> For our text-only continued pretraining dataset, we use the dataset used in the continual pretraining experiments of <cite class=\"ltx_cite ltx_citemacro_citet\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite>, which roughly comprises of <math alttext=\"2.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mn>2.2</mn><annotation encoding=\"application/x-tex\">2.2</annotation></semantics></math>T tokens.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "training",
                    "webcrawl",
                    "krist",
                    "data",
                    "tokens",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our models are <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter transformer-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib146\" title=\"\">2017</a>)</cite> speech-language models. We use a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> for all our experiments. Our models use a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens. We train for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps in total, yielding a total of <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for our training runs. Using the standard <math alttext=\"{6}{N}{D}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m6\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">{6}{N}{D}</annotation></semantics></math> rule <cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib69\" title=\"\">2020</a>)</cite>, this equates to about <math alttext=\"{3.81}{\\times}{{10}^{22}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">{3.81}{\\times}{{10}^{22}}</annotation></semantics></math>FLOPs (note that this estimate is a rough lower bound since we do not count the FLOPs associated with the speech tokenizer in this estimate).\nWe only tune the language model weights while keep the speech tokenizer frozen.\nWe use a cosine-decay learning rate schedule with <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m8\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> steps of linear-warmup.\nWe use the AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib86\" title=\"\">2017</a>)</cite> optimizer with <math alttext=\"{\\beta_{1}}{=}{0.9}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{1}}{=}{0.9}</annotation></semantics></math> and <math alttext=\"{\\beta_{2}}{=}{0.95}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{2}}{=}{0.95}</annotation></semantics></math>, a peak learning rate of <math alttext=\"{3}{e}{-}{4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">{3}{e}{-}{4}</annotation></semantics></math>, weight decay of <math alttext=\"{1}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{1}{e}{-}{5}</annotation></semantics></math> and clip gradients to a max norm of <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m13\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. We use the <span class=\"ltx_text ltx_font_typewriter\">axlearn</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib75\" title=\"\">2025</a>)</cite> codebase for all our experiments using <span class=\"ltx_text ltx_font_typewriter\">jax</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bradbury et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib16\" title=\"\">2021</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">pygrain</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ritter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib117\" title=\"\">2023</a>)</cite> for dataloading. One training run takes approximately <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m14\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math> days on <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m15\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> TPU-v6e chips.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "data",
                    "hours",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "webcrawl",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our experiments were at the <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B parameter scale trained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens (roughly <math alttext=\"{\\sim}{{3.81}{\\times}{10^{22}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\sim}{{3.81}{\\times}{10^{22}}}</annotation></semantics></math> FLOPs). While our results are strong (outperforming models that are <math alttext=\"3{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">3{\\times}</annotation></semantics></math> the size, trained for similar compute budgets), it would still be interesting to explore if our data-centric strategies would hold at larger model scales. While recent papers like <cite class=\"ltx_cite ltx_citemacro_citet\">Nezhurina et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib96\" title=\"\">2025</a>)</cite>, DataComp-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite>, HoneyBee <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> and DataComp-CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>)</cite> suggest transferability of data curation methods across model scales, recent work in language and vision-language modeling has posited that there may be trade-offs when applying data curation across different model sizes and compute budgets <cite class=\"ltx_cite ltx_citemacro_citep\">(Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Goyal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib52\" title=\"\">2024</a>)</cite>. To the best of our knowledge, no existing work showcases such trade-offs in the SpeechLM community. It would be an interesting direction to explore the interaction of data recipes with model scale and compute budget.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tokens",
                    "3838b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the focus of our work was mainly on improving spoken question-answering capabilities of SpeechLMs, all our experiments used the standard benchmarks that are prevalent in the literature for our task of interest <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>. We therefore did not explore how our models would perform on more targeted tasks like automatic speech recognition, emotion recognition or text-to-speech synthesis. One caveat preventing us from a direct comparison on such tasks is that we do not employ any task-specific training, unlike other SpeechLMs that explicitly add in a task-specific component into their training mixture (e.g., ASR-specific training datasets) <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            }
        ]
    },
    "A3.T9": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 9: Data mixture statistics for experiments in table˜2.",
        "body": "Training dataset\nText-only dataset\nWeb-crawl\nKrist\nQuest\n\n\n%mix\n#toks\n#repeats\n%mix\n#toks\n#repeats\n%mix\n#toks\n#repeats\n%mix\n#toks\n#repeats\n\n\nWeb-crawl 100100%\n0.60\n1T\n0.45\n0.40\n670B\n1.85\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nWeb-crawl 5353% + Krist 4747%\n0.60\n1T\n0.45\n0.21\n355B\n0.98\n0.19\n315B\n1.48\n0.00\n0.00\n0.00\n\n\nWeb-crawl 6666% + Quest 3434%\n0.60\n1T\n0.45\n0.26\n442B\n1.22\n0.00\n0.00\n0.00\n0.14\n228\n6.00\n\n\nWeb-crawl 5959% + Quest 66% + Krist 3535%\n0.60\n1T\n0.45\n0.24\n395B\n1.09\n0.14\n232B\n1.10\n0.02\n43B\n1.13\n\n\nWeb-crawl 4040% + Quest 2727% + Krist 3333%\n0.60\n1T\n0.45\n0.16\n267B\n0.74\n0.13\n221B\n1.04\n0.11\n182\n4.79",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Training dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Text-only dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Web-crawl</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Krist</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Quest</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">%mix</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">#toks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#repeats</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">%mix</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">#toks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#repeats</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">%mix</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">#toks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#repeats</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">%mix</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">#toks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">#repeats</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Web-crawl <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">670B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Web-crawl <math alttext=\"53\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m2\" intent=\":literal\"><semantics><mn>53</mn><annotation encoding=\"application/x-tex\">53</annotation></semantics></math>% + Krist <math alttext=\"47\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m3\" intent=\":literal\"><semantics><mn>47</mn><annotation encoding=\"application/x-tex\">47</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">1T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.21</td>\n<td class=\"ltx_td ltx_align_center\">355B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.98</td>\n<td class=\"ltx_td ltx_align_center\">0.19</td>\n<td class=\"ltx_td ltx_align_center\">315B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.48</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Web-crawl <math alttext=\"66\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m4\" intent=\":literal\"><semantics><mn>66</mn><annotation encoding=\"application/x-tex\">66</annotation></semantics></math>% + Quest <math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m5\" intent=\":literal\"><semantics><mn>34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">1T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.26</td>\n<td class=\"ltx_td ltx_align_center\">442B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.22</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.14</td>\n<td class=\"ltx_td ltx_align_center\">228</td>\n<td class=\"ltx_td ltx_align_center\">6.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m6\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m7\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m8\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">1T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.24</td>\n<td class=\"ltx_td ltx_align_center\">395B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.09</td>\n<td class=\"ltx_td ltx_align_center\">0.14</td>\n<td class=\"ltx_td ltx_align_center\">232B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.10</td>\n<td class=\"ltx_td ltx_align_center\">0.02</td>\n<td class=\"ltx_td ltx_align_center\">43B</td>\n<td class=\"ltx_td ltx_align_center\">1.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Web-crawl <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m9\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% + Quest <math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m10\" intent=\":literal\"><semantics><mn>27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math>% + Krist <math alttext=\"33\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m11\" intent=\":literal\"><semantics><mn>33</mn><annotation encoding=\"application/x-tex\">33</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">267B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">221B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">1.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">182</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.79</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "textonly",
            "442b",
            "webcrawl",
            "670b",
            "repeats",
            "395b",
            "statistics",
            "mix",
            "mixture",
            "315b",
            "355b",
            "toks",
            "table˜2",
            "267b",
            "43b",
            "dataset",
            "221b",
            "krist",
            "232b",
            "experiments",
            "data",
            "quest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "experiments",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "experiments",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "training",
                    "mixture",
                    "textonly",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "statistics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "experiments",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "krist",
                    "data",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "table˜2",
                    "training",
                    "mixture",
                    "textonly",
                    "webcrawl",
                    "data",
                    "krist",
                    "repeats",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Sampling.</span> In the first variant (used in all our previous experiments), at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, we randomly sample the chunk-modality with <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math> probability.\nThe modality sampling at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is independent of all other chunks <math alttext=\"{j}{\\neq}{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>&#8800;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">{j}{\\neq}{i}</annotation></semantics></math>.\nWe always start with an audio chunk <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m5\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>, to ensure that there is at least <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> audio chunk in our training sequence.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "table˜2",
                    "training",
                    "experiments",
                    "webcrawl",
                    "data",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "dataset",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "quest",
                    "textonly",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "training",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, by enabling broader coverage of topic domains, our synthetic datasets help to (1) close the distribution mismatch between the raw web-crawled data and the downstream evaluation datasets, and (2) enhance the diversity of our pretraining data distribution. Our findings extend prior work in the language space that have discussed the importance of training data diversity and domain coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib98\" title=\"\">2025a</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib151\" title=\"\">2025c</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> to the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "dataset",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equipped with our key data-centric insights from the previous sections, we now train a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM, called <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>.\nWe use the same training configuration as before, with <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m2\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> sequence length\ntrained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m3\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens.\nWe compare against SoTA speech-language base models including Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>)</cite>, and Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe additionally compare two post-trained models&#8212;Voxtral-mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> and GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>&#8212;with the caveat that having undergone instruction-tuning, they are not directly comparable to base models <cite class=\"ltx_cite ltx_citemacro_citep\">(Dominguez-Olmedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib32\" title=\"\">2024</a>)</cite>. To ensure our training recipe does not degrade language performance, we also compare against strong open-weights base language models on standard text-only benchmarks.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-only dataset.</span> For our text-only continued pretraining dataset, we use the dataset used in the continual pretraining experiments of <cite class=\"ltx_cite ltx_citemacro_citet\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite>, which roughly comprises of <math alttext=\"2.2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mn>2.2</mn><annotation encoding=\"application/x-tex\">2.2</annotation></semantics></math>T tokens.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "experiments",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "training",
                    "webcrawl",
                    "dataset",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our models are <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter transformer-based <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib146\" title=\"\">2017</a>)</cite> speech-language models. We use a global-batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> for all our experiments. Our models use a packed-sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> tokens. We train for <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps in total, yielding a total of <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for our training runs. Using the standard <math alttext=\"{6}{N}{D}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m6\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">{6}{N}{D}</annotation></semantics></math> rule <cite class=\"ltx_cite ltx_citemacro_citep\">(Kaplan et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib69\" title=\"\">2020</a>)</cite>, this equates to about <math alttext=\"{3.81}{\\times}{{10}^{22}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">{3.81}{\\times}{{10}^{22}}</annotation></semantics></math>FLOPs (note that this estimate is a rough lower bound since we do not count the FLOPs associated with the speech tokenizer in this estimate).\nWe only tune the language model weights while keep the speech tokenizer frozen.\nWe use a cosine-decay learning rate schedule with <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m8\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> steps of linear-warmup.\nWe use the AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov &amp; Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib86\" title=\"\">2017</a>)</cite> optimizer with <math alttext=\"{\\beta_{1}}{=}{0.9}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{1}}{=}{0.9}</annotation></semantics></math> and <math alttext=\"{\\beta_{2}}{=}{0.95}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">{\\beta_{2}}{=}{0.95}</annotation></semantics></math>, a peak learning rate of <math alttext=\"{3}{e}{-}{4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">{3}{e}{-}{4}</annotation></semantics></math>, weight decay of <math alttext=\"{1}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{1}{e}{-}{5}</annotation></semantics></math> and clip gradients to a max norm of <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m13\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>. We use the <span class=\"ltx_text ltx_font_typewriter\">axlearn</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib75\" title=\"\">2025</a>)</cite> codebase for all our experiments using <span class=\"ltx_text ltx_font_typewriter\">jax</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bradbury et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib16\" title=\"\">2021</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">pygrain</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ritter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib117\" title=\"\">2023</a>)</cite> for dataloading. One training run takes approximately <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m14\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math> days on <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m15\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> TPU-v6e chips.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "experiments",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "data",
                    "experiments",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "training",
                    "dataset",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "mix",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our experiments were at the <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B parameter scale trained for <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T speech-text tokens (roughly <math alttext=\"{\\sim}{{3.81}{\\times}{10^{22}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mn>3.81</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mn>22</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">{\\sim}{{3.81}{\\times}{10^{22}}}</annotation></semantics></math> FLOPs). While our results are strong (outperforming models that are <math alttext=\"3{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A12.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">3{\\times}</annotation></semantics></math> the size, trained for similar compute budgets), it would still be interesting to explore if our data-centric strategies would hold at larger model scales. While recent papers like <cite class=\"ltx_cite ltx_citemacro_citet\">Nezhurina et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib96\" title=\"\">2025</a>)</cite>, DataComp-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite>, HoneyBee <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> and DataComp-CLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>)</cite> suggest transferability of data curation methods across model scales, recent work in language and vision-language modeling has posited that there may be trade-offs when applying data curation across different model sizes and compute budgets <cite class=\"ltx_cite ltx_citemacro_citep\">(Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Goyal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib52\" title=\"\">2024</a>)</cite>. To the best of our knowledge, no existing work showcases such trade-offs in the SpeechLM community. It would be an interesting direction to explore the interaction of data recipes with model scale and compute budget.</p>\n\n",
                "matched_terms": [
                    "data",
                    "experiments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the focus of our work was mainly on improving spoken question-answering capabilities of SpeechLMs, all our experiments used the standard benchmarks that are prevalent in the literature for our task of interest <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>. We therefore did not explore how our models would perform on more targeted tasks like automatic speech recognition, emotion recognition or text-to-speech synthesis. One caveat preventing us from a direct comparison on such tasks is that we do not employ any task-specific training, unlike other SpeechLMs that explicitly add in a task-specific component into their training mixture (e.g., ASR-specific training datasets) <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "training",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "textonly",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "training",
                    "mix",
                    "mixture",
                    "textonly",
                    "experiments"
                ]
            }
        ]
    },
    "A6.T10": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 10: Details of SQA evaluation datasets.",
        "body": "Evaluation Dataset\nNum. samples\nChance%\nTTS Engine\nAudio Source\n\n\nSpoken-LLaMA-Questions\n300300\n25%25\\%\nGoogle Cloud TTS\nLink\n\n\nSpoken-TriviaQA\n10001000\n25%25\\%\nBaichuan-Audio TTS\nLink\n\n\nSpoken-Web-Questions\n10001000\n25%25\\%\nBaichuan-Audio TTS\nLink",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Evaluation Dataset</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Num. samples</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Chance%</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">TTS Engine</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Audio Source</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Spoken-LLaMA-Questions</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"300\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T10.m1\" intent=\":literal\"><semantics><mn>300</mn><annotation encoding=\"application/x-tex\">300</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T10.m2\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Google Cloud TTS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://github.com/google-research-datasets/LLAMA1-Test-Set/tree/main\" title=\"\">Link</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Spoken-TriviaQA</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T10.m3\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T10.m4\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Baichuan-Audio TTS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/baichuan-inc/OpenAudioBench/tree/main/eval_datas/trivia_qa/audios\" title=\"\">Link</a></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Spoken-Web-Questions</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T10.m5\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T10.m6\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_typewriter\">Baichuan-Audio TTS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/baichuan-inc/OpenAudioBench/tree/main/eval_datas/web_questions/audios\" title=\"\">Link</a></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "source",
            "spokenwebquestions",
            "datasets",
            "details",
            "sqa",
            "tts",
            "spokentriviaqa",
            "chance",
            "spokenllamaquestions",
            "evaluation",
            "google",
            "dataset",
            "engine",
            "link",
            "samples",
            "baichuanaudio",
            "num",
            "cloud",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken Question-Answering (SQA) is a core capability for useful and interactive artificial intelligence systems. Recently, several speech-language models (SpeechLMs) have been released with a specific focus on improving their SQA performance. However, a lack of controlled ablations of pretraining data processing and curation makes it challenging to understand what factors account for performance, despite substantial gains from similar studies in other data modalities. In this work, we address this gap by conducting a data-centric exploration for pretraining SpeechLMs.\nWe focus on three research questions fundamental to speech-language pretraining data: (1) how to <em class=\"ltx_emph ltx_font_italic\">process</em> raw web-crawled audio content for speech-text pretraining, (2) how to <em class=\"ltx_emph ltx_font_italic\">construct</em> synthetic pretraining datasets to augment web-crawled data and (3) how to <em class=\"ltx_emph ltx_font_italic\">interleave</em> (text, audio) segments into training sequences. We apply the insights from our controlled data-centric ablations to pretrain a <math alttext=\"{3.8}\" class=\"ltx_Math\" display=\"inline\" id=\"m12\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">{3.8}</annotation></semantics></math>B-parameter SpeechLM, called <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>, that outperforms models that are up to <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"m13\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"m14\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>% absolute performance. We hope our findings highlight the impact of effective data curation for speech-language pretraining and guide future data-centric exploration in SpeechLMs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sqa",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language-based assistants are now widely deployed <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib102\" title=\"\">2024</a>; Comanici et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib26\" title=\"\">2025</a>)</cite>,\nlargely riding on the progress of text-only foundation models <cite class=\"ltx_cite ltx_citemacro_citep\">(Bommasani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib14\" title=\"\">2021</a>)</cite>.\nYet, purely textual interactions are inherently limiting for real-world assistants that must operate in open, hands-free settings. Voice provides a natural, low-friction interface for human&#8211;AI interaction, and recent work therefore emphasizes Spoken Question-Answering (SQA) <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>&#8212;where a question is asked in audio and the system must produce spoken or textual answers&#8212;as a core capability for end-to-end speech language models (SpeechLMs).</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, <span class=\"ltx_text ltx_font_italic\">speech&#8211;text interleaved pretraining</span>&#8212;next-token prediction over sequences that alternate between speech and text tokens&#8212;has been proposed as a viable strategy to boost SQA performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite>.\nHowever, while these recent works describe modeling\nand optimization\nchoices comprehensively, details of\ntheir\ndata\npipelines are often not shared or evaluated in a controlled setting.\nHow should we process raw audio into trainable speech-text chunks? Can we leverage text-only datasets to go beyond datasets sourced from raw audio? How should we interleave tokens for effective modality alignment? In the current speech-language literature, <span class=\"ltx_text ltx_font_italic\">these data-centric questions remain underexplored</span>. In other domains like language <cite class=\"ltx_cite ltx_citemacro_citep\">(Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>)</cite> and vision <cite class=\"ltx_cite ltx_citemacro_citep\">(Gadre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib43\" title=\"\">2023</a>; Sim&#233;oni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib127\" title=\"\">2025</a>)</cite>, data curation has consistently proven to be a primary driver of performance improvements, yet a large gap exists from the data-centric perspective in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "details",
                    "sqa",
                    "datasets",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sqa",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span>\nMost recent SpeechLMs employ a simple Speech Encoder + Connector + LLM philosophy for conducting joint speech-text training <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite>. Models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken question-answering.\nWhile demonstrating impressive performance, details behind their data curation strategies are however scant.\nThrough our controlled experiments, we aim to fill this gap in the SpeechLM domain by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "details",
                    "baichuanaudio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we address our three key <span class=\"ltx_text ltx_font_italic\">data-centric</span> questions for improving SQA, via controlled experiments: (1) how to <span class=\"ltx_text ltx_font_italic\">process</span> raw web-crawled audio into suitable interleaved speech-text training data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>), (2) how to <span class=\"ltx_text ltx_font_italic\">construct</span> synthetic speech-text datasets seeded from text-only datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>), and (3) how to <span class=\"ltx_text ltx_font_italic\">interleave</span> between speech and text modalities while training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sqa",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "sqa",
                    "source",
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "evaluation",
                    "spokentriviaqa",
                    "datasets",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "details",
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "details",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "details",
                    "sqa",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we showed our three data-centric methods boost SQA significantly. These results were achieved while computing the loss on both audio and text tokens during interleaved training to support a native end-to-end SpeechLM. However, there is also great interest in developing an understanding-only SpeechLM that ingests both audio and text and outputs only text, e.g. the Thinker model in the Thinker-Talker architecture series <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib163\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib164\" title=\"\">b</a>)</cite>.\nIn this vein, many prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> apply loss masking on the audio tokens while doing speech-text interleaved training.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">TTS and ASR-style Conversations</span>: We convert utterances from ASR/TTS datasets into natural conversation, in which users ask assistants to either transcribe a given audio (ASR) or synthesize a given text (TTS). We also include instruction-following TTS data where users ask to synthesize text responses with specific instructions (e.g., synthesize speech in a given volume, pace, style or emotion).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tts",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluations.</span>\nWe evaluated SFT models for both <span class=\"ltx_text ltx_font_italic\">text response quality</span> <span class=\"ltx_text ltx_font_italic\">and audio response quality</span>. To evaluate text response quality, we use two eval sets: <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span> and <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. The first is obtained by synthesizing the alpaca evaluation dataset (<cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib80\" title=\"\">2023</a>)</cite>). On top of <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span>, we added various background noise with a SNR randomly sampled from 5 to 15 dB. This produces <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. During the evaluation, 804 spoken alpaca questions were fed in, and the model&#8217;s text response, <math alttext=\"\\texttt{T}^{\\texttt{a}}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p7.m1\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}^{\\texttt{a}}_{1}</annotation></semantics></math>, is extracted. These text responses are pair-wise compared with the responses generated from a performant internal baseline model using the standard evaluation protocol with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> as the judge model.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate audio response quality, we work with several third-party vendors to collect diversified user prompts in audio. For multi-turn dialogue evaluation, we adopt the last-turn-with-context strategy to evaluate the last turn&#8217;s assistant response, while the previous assistant responses are generated by <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio</span> and fed in as context. In total, we constructed <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p8.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> evaluation sets, each having a different focus, such as knowledge-rich, multi-turn, long-context, and challenging speech environments. We also notice pair-wise comparison of audio is often harder than text, in which judges (LLM or human) cannot tell which response is better. In order to reduce variance of judge scores, we ask the judge to output whether audio response A is better than, worse than or tied with audio response B. The auto-grading prompt template we used is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A8\" title=\"Appendix H Prompt template for GPT-4o-audio in auto eval &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "spokenllamaquestions",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "spokenllamaquestions",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "samples",
                    "evaluation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Therefore, by enabling broader coverage of topic domains, our synthetic datasets help to (1) close the distribution mismatch between the raw web-crawled data and the downstream evaluation datasets, and (2) enhance the diversity of our pretraining data distribution. Our findings extend prior work in the language space that have discussed the importance of training data diversity and domain coverage <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib98\" title=\"\">2025a</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib151\" title=\"\">2025c</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> to the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "samples",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "dataset",
                    "datasets",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "samples",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we studied three data-curation methods for speech-language interleaved pretraining to enhance spoken question-answering (SQA) capabilities. We found fine-grained interleaving of speech-text chunks bringing large gains, while synthetic datasets synthesized from knowledge-rich seed text-datasets also boosted performance. Deterministic sampling of speech-text chunks during interleaved pretraining further improved SQA results. We showed that these data-centric recipes strengthen alignment between the speech and text modalities and broaden domain coverage of pretraining datasets. Distilling these insights, we pretrained a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B-parameter SpeechLM, <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span>, achieving competitive performance with <math alttext=\"{3}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{3}{\\times}</annotation></semantics></math> larger models. We hope our insights motivate more data-centric exploration in the speech-language pretraining domain.</p>\n\n",
                "matched_terms": [
                    "sqa",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide more details about each step in our data processing pipeline for converting web-crawled audio into interleaved speech-text format. We highlight all the components in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "details",
                    "datasets",
                    "dataset",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Language Models.</span> There has been a recent push for training end-to-end SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib7\" title=\"\">2025</a>)</cite>. Early efforts like Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite>, SALMONN <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib132\" title=\"\">2023</a>)</cite>, and LTU-AS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib51\" title=\"\">2023</a>)</cite> employed multi-task pretraining to enable tasks like automatic speech recognition, emotion classification etc. Scaling these principles\nby increasing model-size and training compute <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib22\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Geng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib47\" title=\"\">2025</a>; Kong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib70\" title=\"\">2024</a>; Ghosh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib49\" title=\"\">2025</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite> has yielded continued gains. Further works considered pretraining models with speech understanding and generation capabilities <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib73\" title=\"\">2021</a>; Algayres et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib5\" title=\"\">2023</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Nguyen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib99\" title=\"\">2025b</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>; Rubenstein et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib119\" title=\"\">2023</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib176\" title=\"\">2023</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>)</cite>. More recently, models like Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite>, Step-Audio-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>)</cite>, Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>, GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, and MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> have emerged as strong foundation models that seamlessly perform several tasks, including spoken-question answering. While demonstrating impressive performance, details behind their data curation strategies are scant. Through our controlled experiments, we aim to fill this gap by shedding light on how to effectively construct speech-text pretraining datasets.</p>\n\n",
                "matched_terms": [
                    "details",
                    "baichuanaudio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Curation for Speech-Language Models.</span> Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> was one of the first works to effectively leverage web-scale data for training a multi-task speech-text model, using a dataset of <math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m1\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k hours. Attempting to openly reproduce the original Whisper dataset, <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>)</cite> introduced <span class=\"ltx_text ltx_font_smallcaps\">OLMoASR-POOL</span>, a dataset of <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>M hours of audio and <math alttext=\"17\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m3\" intent=\":literal\"><semantics><mn>17</mn><annotation encoding=\"application/x-tex\">17</annotation></semantics></math>M transcripts.\nThey conducted heuristic-based filtering on their data pool, showcasing benefits on ASR tasks.\n <cite class=\"ltx_cite ltx_citemacro_citet\">Tian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib136\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib110\" title=\"\">2025</a>)</cite> similarly conducted comprehensive studies to understand the effects of data heterogenity, ASR error rate based filtering and LLM-based transcription rephrasing, while training Whisper-style models. However, these efforts were limited to training models that were primarily capable of performing ASR tasks. The data curation literature in the end-to-end SpeechLM literature is much more sparse. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> describes their speech-text dataset construction pipeline, beginning from <math alttext=\"13\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m4\" intent=\":literal\"><semantics><mn>13</mn><annotation encoding=\"application/x-tex\">13</annotation></semantics></math>M audio hours and processing them into speech-text interleaved training data.\nHowever, why certain design decisions were taken remain unanswered.\nContrarily, <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib173\" title=\"\">2024b</a>)</cite> constructed synthetic interleaved data sourced from high-quality text pretraining data, but yet again omit clear details on key design choices.\nMiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> scaled up their training dataset size by an order of magnitude to an unprecedented <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"A5.p3.m5\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math>M hours of audio data. While they showcased the benefits of dataset quantity using few-shot experiments, they did not conduct any explicit controlled experiments to justify the filtering and curation decisions they made.\nIn our work, we aim to fill this gap on the data-centric side of SpeechLMs, by describing and understanding data curation pipelines for speech-text interleaved pretraining through three key questions around interleaved data chunking, synthetic dataset construction and modality sampling schemes during interleaved training.</p>\n\n",
                "matched_terms": [
                    "details",
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below, we also provide a few examples from each evaluation dataset, with the question (in text), choices, and the ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation details.</span>\nWe use log-likelihood based scoring for our evaluation protocol following standard language modeling works <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the following prompt for generating the distractor options for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>.</p>\n\n",
                "matched_terms": [
                    "spokentriviaqa",
                    "spokenllamaquestions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the following prompt template when using <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-audio</span> in our auto evaluation pipeline for audio responses.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "spokentriviaqa",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "datasets",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"In J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we showcase the topic distribution of Spoken-Web-Questions. Similar to the takeaways in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we find that some of the topics that Spoken-Web-Questions contains are severely under-represented in the web-crawled dataset while being represented adequately in the synthetic datasets. This further corroborates our findings that synthetic datasets help close the distribution mismatch between the web-crawled dataset and the evaluation datasets. Our findings regarding the under-representation of concepts in web-crawled datasets have also been echoed in the language and vision domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiedemer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib157\" title=\"\">2025</a>; Parashar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib107\" title=\"\">2024</a>; Elazar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib37\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib67\" title=\"\">2023</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib144\" title=\"\">2024</a>; Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib184\" title=\"\">2024</a>; Samuel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib123\" title=\"\">2024</a>; Dodge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib31\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "evaluation",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "evaluation",
                    "dataset",
                    "spokentriviaqa",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T13\" title=\"Table 13 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T15\" title=\"Table 15 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> summarize results for Spoken-TriviaQA, Spoken-LLaMA-Questions, and Spoken-Web-Questions.\nWe highlight the difference <math alttext=\"\\Delta=\\text{Clean}-\\text{RandMean}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mtext>Clean</mtext><mo>&#8722;</mo><mtext>RandMean</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=\\text{Clean}-\\text{RandMean}</annotation></semantics></math> and give the decision at a significance level <math alttext=\"\\alpha{=}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha{=}0.01</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "baichuanaudio",
                    "evaluation",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audio"
                ]
            }
        ]
    },
    "A9.T11": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 11: Dataset-level means of all divergence metrics b/w conditional next-token distributions.\nWe report the means of all three divergence distributions (as computed in equation˜I.7). FKL represents forward KL-divergence, RKL is reverse KL-divergence and JSD is Jensen-Shannon divergence.",
        "body": "Method\nSpoken-Web-Questions\nSpoken-TriviaQA\nSpoken-LLaMA-Questions\n\n\nFKL\nRKL\nJSD\nFKL\nRKL\nJSD\nFKL\nRKL\nJSD\n\n\nCoarse\n2.07\n2.78\n0.32\n2.97\n3.70\n0.40\n2.57\n3.20\n0.35\n\n\nFine\n1.68\n1.84\n0.27\n2.72\n2.80\n0.36\n2.15\n2.21\n0.30\n\n\nFine + Syn\n1.90\n1.35\n0.24\n2.71\n1.94\n0.31\n2.23\n1.47\n0.27",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Spoken-Web-Questions</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Spoken-TriviaQA</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_bold\">Spoken-LLaMA-Questions</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">FKL</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">RKL</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">JSD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">FKL</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">RKL</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">JSD</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">FKL</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">RKL</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\"><span class=\"ltx_text ltx_font_italic\">JSD</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Coarse</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.07</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.78</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.32</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.97</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">3.70</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.57</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">3.20</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Fine</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.68</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.84</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.27</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.72</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.80</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.36</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.15</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.21</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">Fine + Syn</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.90</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.35</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.71</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.94</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.31</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">2.23</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">1.47</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">0.27</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "computed",
            "spokenwebquestions",
            "datasetlevel",
            "means",
            "equation˜i7",
            "three",
            "jensenshannon",
            "all",
            "metrics",
            "kldivergence",
            "spokentriviaqa",
            "coarse",
            "represents",
            "nexttoken",
            "fine",
            "report",
            "forward",
            "jsd",
            "fkl",
            "spokenllamaquestions",
            "distributions",
            "reverse",
            "divergence",
            "method",
            "conditional",
            "rkl",
            "syn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Over each test set <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> we also report the dataset means across metrics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>:</p>\n\n",
            "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In our work, we aim to close this gap with a systematic, <em class=\"ltx_emph ltx_font_italic\">data-centric</em> study of interleaved pretraining for SQA (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S0.F1\" title=\"In Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nWe first provide a detailed description of our processing pipeline for converting raw audio into speech-text interleaved data (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>). We then study optimal interleaving\nstrategies for speech-text pretraining, finding that fine-grained interleaving (which alternates between speech and text modalities at sentence boundaries)\nimproves alignment of the two modalities (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS3\" title=\"3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>). Building on this, we introduce effective synthetic data methods involving LLM-based rewriting and text-to-speech synthesis to go beyond raw web-crawled audio for pretraining (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>).\nWe also examine two modality-sampling schemes for interleaved training, finding that a deterministic ordering of alternating speech-text chunks is beneficial compared to stochastic modality sampling (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS5\" title=\"3.5 Modality sampling schemes for interleaved training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.5</span></a>).\nFurther, we show our pretraining data interventions also improve models under the audio-understanding only setting (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS6\" title=\"3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.6</span></a>) and after post-training (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS7\" title=\"3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.7</span></a>).\nTo understand <span class=\"ltx_text ltx_font_italic\">why</span> our data-centric methods improve performance, we\nanalyse the modality gap between speech and text distributions (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>) and inspect the topic distributions of web-crawled and synthetic datasets (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS2\" title=\"4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFinally,\nto showcase the efficacy of our data interventions at scale, we pretrain a <math alttext=\"3.8\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mn>3.8</mn><annotation encoding=\"application/x-tex\">3.8</annotation></semantics></math>B SpeechLM (<span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">SpeLangy</span>) that outperforms <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>x larger models by upto <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math>% average SQA performance, across three standard benchmarks.\nTaken together, our results underscore the central role of data curation in speech&#8211;language pretraining and motivate a broader, systematic push toward data-centric exploration.</p>\n\n",
                "matched_terms": [
                    "three",
                    "distributions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "three",
                    "spokenwebquestions",
                    "all",
                    "spokenllamaquestions",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine vs coarse interleaving.</span> Prior speech-text pretraining works <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite> have explored constructing interleaved data from raw audio. However, they do not quantify the importance of <span class=\"ltx_text ltx_font_italic\">interleaving granularity</span> for effective training.\nTo study this, we construct two interleaving variants (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-A)&#8212;(1) <span class=\"ltx_text ltx_font_italic\">coarse interleaving</span>, where we merge multiple consecutive diarized outputs into one if tagged with same speaker-ID, yielding long chunks, and (2) <span class=\"ltx_text ltx_font_italic\">fine interleaving</span>, where we keep all diarized outputs as is without merging, yielding short chunks.\nAs expected, from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F3\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we find coarse interleaving leads to longer chunks (mean-length=<math alttext=\"19.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mn>19.2</mn><annotation encoding=\"application/x-tex\">19.2</annotation></semantics></math>s) compared to fine interleaving (mean-length=<math alttext=\"5.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mn>5.2</mn><annotation encoding=\"application/x-tex\">5.2</annotation></semantics></math>s).\nFrom <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, we note fine interleaving improves SQA performance by <math alttext=\"{3.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>3.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{3.1}{\\%}</annotation></semantics></math> on average, while matching text-only performance.\nThis is a significant finding since the default approach in prior works <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> has been to merge same-speaker diarization outputs, yet our results advocate for more granular interleaving.\nHence, for all our subsequent experiments, we adopt fine interleaving\nfor web-crawled speech-text pretraining\nby default.</p>\n\n",
                "matched_terms": [
                    "all",
                    "fine",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "fine",
                    "coarse",
                    "syn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "coarse",
                    "fine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "fine",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "spokenllamaquestions",
                    "metrics",
                    "distributions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "fine",
                    "spokenllamaquestions",
                    "metrics",
                    "kldivergence",
                    "distributions",
                    "coarse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "all",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "all",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "all",
                    "spokentriviaqa",
                    "spokenllamaquestions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T6\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> we find that our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> outperforms\nKimi-Audio, Qwen-Audio and Qwen-2-Audio by <math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%, <math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>% and <math alttext=\"9.8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mn>9.8</mn><annotation encoding=\"application/x-tex\">9.8</annotation></semantics></math>% on average across the three SQA benchmarks, while being <math alttext=\"{2.8}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>2.8</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.8}{\\times}</annotation></semantics></math>, <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> and <math alttext=\"{2.2}{\\times}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">{2.2}{\\times}</annotation></semantics></math> smaller in size. Further, we obtain competitive performance with the strongly post-trained Voxtral-mini and GLM-4-Voice, <em class=\"ltx_emph ltx_font_italic\">without having undergone any task-specific instruction-tuning</em>. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S5.T7\" title=\"In 5 SpeLangy: Bringing it all together &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the text performance of <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> with the base LM that we initialize from&#8212;we observe large boosts across the board compared to the base-LM, indicating positive text-capability transfer. Further, our model is competitive with Gemma-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib134\" title=\"\">2024</a>)</cite>, Gemma-3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib135\" title=\"\">2025</a>)</cite> and Qwen-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib166\" title=\"\">2024</a>)</cite> models, all of which are leading open-weights models, highlighting the strength of our <span class=\"ltx_text ltx_font_typewriter\">SpeLangy</span> model.</p>\n\n",
                "matched_terms": [
                    "all",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription Generation.</span> Next, we aim to provide paired text annotations for all of the raw audio in our corpus. For this, we first used the Whisper model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> to transcribe the raw audio from each of the diarized output chunks. However, we noticed that the Whisper model transcriptions can tend to be quite noisy and contain some hallucinations. To ensure cleaner transcriptions, we use a post-processing transcription ensembling approach called ROVER <cite class=\"ltx_cite ltx_citemacro_citep\">(Fiscus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib42\" title=\"\">1997</a>)</cite> used in prior works performing transcription cleaning <cite class=\"ltx_cite ltx_citemacro_citep\">(Jalalvand et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib64\" title=\"\">2015</a>)</cite>. We first obtain additional speech transcriptions from an internal SIRI transcription model and <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\" title=\"\">Nvidia-Parakeet-TDT-CTC</a>. We then apply the ROVER post-processing method using the three candidate transcriptions from Whisper, SIRI and Parakeet. We use the ensembled transcription as our text annotations for subsequent steps. We provide some examples of the individual model-based transcriptions and the final ROVER-ensembled transcriptions below:</p>\n\n",
                "matched_terms": [
                    "all",
                    "three",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "all",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "all",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "three",
                    "spokentriviaqa",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the following prompt for generating the distractor options for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>.</p>\n\n",
                "matched_terms": [
                    "spokentriviaqa",
                    "spokenllamaquestions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to measure the divergence between the token-wise teacher-forced <cite class=\"ltx_cite ltx_citemacro_citep\">(Williams &amp; Zipser, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib159\" title=\"\">1989</a>)</cite> conditional probability distributions of the audio and text modality. That is, we compare the next&#8211;token distributions under audio vs. text question conditioning, evaluated along the same ground&#8211;truth (GT) answer path (the answer is always in text modality).</p>\n\n",
                "matched_terms": [
                    "conditional",
                    "divergence",
                    "distributions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{Pr}_{\\theta}({X}{=}{v}|Y)\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p4.m1\" intent=\":literal\"><semantics><mrow><msub><mtext>Pr</mtext><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>X</mi><mo>=</mo><mrow><mi>v</mi><mo fence=\"false\">|</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Pr}_{\\theta}({X}{=}{v}|Y)</annotation></semantics></math> represents the conditional probability distribution for all values <math alttext=\"v\\in V\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p4.m2\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo>&#8712;</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">v\\in V</annotation></semantics></math>, conditioned on the previous context <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p4.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "conditional",
                    "all",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now compute (1) forward KL, (2) reverse KL, and (3) Jensen&#8211;Shannon (JS) divergence at each step <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, between the two next-token distributions:</p>\n\n",
                "matched_terms": [
                    "nexttoken",
                    "divergence",
                    "forward",
                    "distributions",
                    "reverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "all",
                    "spokenllamaquestions",
                    "distributions",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "all",
                    "computed",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T13\" title=\"Table 13 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T15\" title=\"Table 15 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> summarize results for Spoken-TriviaQA, Spoken-LLaMA-Questions, and Spoken-Web-Questions.\nWe highlight the difference <math alttext=\"\\Delta=\\text{Clean}-\\text{RandMean}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mtext>Clean</mtext><mo>&#8722;</mo><mtext>RandMean</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=\\text{Clean}-\\text{RandMean}</annotation></semantics></math> and give the decision at a significance level <math alttext=\"\\alpha{=}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha{=}0.01</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "spokentriviaqa"
                ]
            }
        ]
    },
    "A11.T12": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 12: Proportion of contamination. For each evaluation dataset, we report the proportion of test samples detected as contaminated. We also report the absolute number of matches in brackets.",
        "body": "Evaluation dataset\n% Contamination [# samples]\n\n\nKrist\nQuest\nAll\n\n\nSpoken-Web-Questions\n0.4% [4]\n0.1% [1]\n0.4% [4]\n\n\nSpoken-TriviaQA\n2.2% [22]\n0.8% [8]\n2.5% [25]\n\n\nSpoken-LLaMA-Questions\n6.7% [20]\n0.2% [5]\n7.7% [23]",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Evaluation dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">% Contamination [# samples]</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_italic\">Krist</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_italic\">Quest</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_italic\">All</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Spoken-Web-Questions</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.4% [4]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.1% [1]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.4% [4]</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Spoken-TriviaQA</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">2.2% [22]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.8% [8]</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">2.5% [25]</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Spoken-LLaMA-Questions</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">6.7% [20]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.2% [5]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">7.7% [23]</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "absolute",
            "spokenwebquestions",
            "also",
            "each",
            "test",
            "all",
            "spokentriviaqa",
            "contamination",
            "detected",
            "matches",
            "report",
            "contaminated",
            "spokenllamaquestions",
            "evaluation",
            "dataset",
            "brackets",
            "krist",
            "number",
            "samples",
            "proportion",
            "quest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Spoken Question-Answering (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_bold\">T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).</span> We use three standard benchmarks for SQA where the model is asked questions in speech and is tasked to respond in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>): <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> (SLQ), <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> (SWQ) and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> (STQ). We source all the audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>.\nOur protocol follows standard language modeling pretraining evaluations <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>)</cite> to use an MCQ cloze-format with log-likelihood evaluation for choosing the correct option (we use <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> multiple choices with chance-level accuracy being <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%).\nWe provide more details and examples from each of our evaluation datasets in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6\" title=\"Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "all",
                    "evaluation",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Data.</span> Our base training data mixture consists of web-crawled audio that we process into interleaved speech-text data. We provide more details on how we process audio into our training data format in the next section. We also use the text continued-pretraining dataset from <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib77\" title=\"\">2025b</a>)</cite> to preserve the base-LM&#8217;s text performance. Following prior multimodal works <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>)</cite>, we use a <math alttext=\"60\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">60\\%</annotation></semantics></math> text-only and <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math> speech-text data mixture during interleaved pretraining.</p>\n\n",
                "matched_terms": [
                    "also",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extracting interleaved data from raw audio.</span> We begin with <math alttext=\"{&gt;}{10}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">{&gt;}{10}</annotation></semantics></math>M hours of raw web-crawled audio. To process them into trainable speech-text samples, we follow a multi-stage pipeline (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>), involving <span class=\"ltx_text ltx_font_italic\">speaker diarization</span>, <span class=\"ltx_text ltx_font_italic\">language detection and filtering</span>, <span class=\"ltx_text ltx_font_italic\">paired-transcription generation and filtering</span>, and <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span>.\nOur pipeline yields interleaved training samples <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> consisting of multiple paired speech-text chunks of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is the number of chunks in each sample. We provide more details about each individual processing component along with detailed statistics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1\" title=\"Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">A</span></a>, while focusing on the <span class=\"ltx_text ltx_font_italic\">interleaved chunking</span> component here.</p>\n\n",
                "matched_terms": [
                    "each",
                    "number",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "each",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "all",
                    "samples",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "each",
                    "also",
                    "krist",
                    "proportion",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic Sampling.</span> In the first variant (used in all our previous experiments), at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, we randomly sample the chunk-modality with <math alttext=\"0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><mn>0.5</mn><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math> probability.\nThe modality sampling at each chunk <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is independent of all other chunks <math alttext=\"{j}{\\neq}{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>&#8800;</mo><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">{j}{\\neq}{i}</annotation></semantics></math>.\nWe always start with an audio chunk <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m5\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>, to ensure that there is at least <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> audio chunk in our training sequence.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic Sampling.</span>\nWhile the stochastic variant allows flexibility and potentially offers better generalization,\nit can restrict the number of <span class=\"ltx_text ltx_font_italic\">modality switches</span> during training.\nHence, we test a deterministic approach, where we alternate between audio and text modalities at each chunk, i.e. we formulate the training sequence as <math alttext=\"{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>A</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}</annotation></semantics></math>.\nThis <span class=\"ltx_text ltx_font_italic\">maximizes the number of modality switches</span> for a given sample.\nHere too, we always start with <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "number",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "test",
                    "also",
                    "absolute"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "all",
                    "also",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluations.</span>\nWe evaluated SFT models for both <span class=\"ltx_text ltx_font_italic\">text response quality</span> <span class=\"ltx_text ltx_font_italic\">and audio response quality</span>. To evaluate text response quality, we use two eval sets: <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span> and <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. The first is obtained by synthesizing the alpaca evaluation dataset (<cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib80\" title=\"\">2023</a>)</cite>). On top of <span class=\"ltx_text ltx_font_italic\">spoken-alpaca</span>, we added various background noise with a SNR randomly sampled from 5 to 15 dB. This produces <span class=\"ltx_text ltx_font_italic\">noisy-alpaca</span>. During the evaluation, 804 spoken alpaca questions were fed in, and the model&#8217;s text response, <math alttext=\"\\texttt{T}^{\\texttt{a}}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p7.m1\" intent=\":literal\"><semantics><msubsup><mtext class=\"ltx_mathvariant_monospace\">T</mtext><mn>1</mn><mtext class=\"ltx_mathvariant_monospace\">a</mtext></msubsup><annotation encoding=\"application/x-tex\">\\texttt{T}^{\\texttt{a}}_{1}</annotation></semantics></math>, is extracted. These text responses are pair-wise compared with the responses generated from a performant internal baseline model using the standard evaluation protocol with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-2024-07-18</span> as the judge model.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate audio response quality, we work with several third-party vendors to collect diversified user prompts in audio. For multi-turn dialogue evaluation, we adopt the last-turn-with-context strategy to evaluate the last turn&#8217;s assistant response, while the previous assistant responses are generated by <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio</span> and fed in as context. In total, we constructed <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p8.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> evaluation sets, each having a different focus, such as knowledge-rich, multi-turn, long-context, and challenging speech environments. We also notice pair-wise comparison of audio is often harder than text, in which judges (LLM or human) cannot tell which response is better. In order to reduce variance of judge scores, we ask the judge to output whether audio response A is better than, worse than or tied with audio response B. The auto-grading prompt template we used is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A8\" title=\"Appendix H Prompt template for GPT-4o-audio in auto eval &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">H</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "also",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "also",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "spokenllamaquestions",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "spokenllamaquestions",
                    "also",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "each",
                    "evaluation",
                    "also",
                    "krist",
                    "samples",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "all",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "test",
                    "contaminated",
                    "contamination",
                    "samples",
                    "proportion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "each",
                    "matches",
                    "test",
                    "all",
                    "contaminated",
                    "dataset",
                    "contamination",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "test",
                    "all",
                    "contaminated",
                    "contamination",
                    "number",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "absolute",
                    "spokenwebquestions",
                    "all",
                    "spokenllamaquestions",
                    "spokentriviaqa",
                    "contamination",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide more details about each step in our data processing pipeline for converting web-crawled audio into interleaved speech-text format. We highlight all the components in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A1.F8\" title=\"In Appendix A Preprocessing web-crawled audio as interleaved training data &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription Generation.</span> Next, we aim to provide paired text annotations for all of the raw audio in our corpus. For this, we first used the Whisper model <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib112\" title=\"\">2023</a>)</cite> to transcribe the raw audio from each of the diarized output chunks. However, we noticed that the Whisper model transcriptions can tend to be quite noisy and contain some hallucinations. To ensure cleaner transcriptions, we use a post-processing transcription ensembling approach called ROVER <cite class=\"ltx_cite ltx_citemacro_citep\">(Fiscus, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib42\" title=\"\">1997</a>)</cite> used in prior works performing transcription cleaning <cite class=\"ltx_cite ltx_citemacro_citep\">(Jalalvand et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib64\" title=\"\">2015</a>)</cite>. We first obtain additional speech transcriptions from an internal SIRI transcription model and <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\" title=\"\">Nvidia-Parakeet-TDT-CTC</a>. We then apply the ROVER post-processing method using the three candidate transcriptions from Whisper, SIRI and Parakeet. We use the ensembled transcription as our text annotations for subsequent steps. We provide some examples of the individual model-based transcriptions and the final ROVER-ensembled transcriptions below:</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transcription Filtering.</span> Despite the ROVER post-processing, we still find that a lot of annotations are low-quality including empty transcription texts and containing several repetitions. We filter out samples with such faulty transcriptions. For detecting repetition, we use a heuristic <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p9.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram based approach. We first tokenize each transcription using a pretrained SentencePiece <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib71\" title=\"\">2018</a>)</cite> tokenizer. We then search for unique <math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p9.m2\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math>-gram spans in the tokenized text. If we find that a <math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p9.m3\" intent=\":literal\"><semantics><mn>15</mn><annotation encoding=\"application/x-tex\">15</annotation></semantics></math>-gram span occurs more than <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p9.m4\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> times in the entire sequence, we discard that sample.</p>\n\n",
                "matched_terms": [
                    "each",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question answering prompt for <span class=\"ltx_text ltx_font_italic\">Quest</span>.</span> Finally, we prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to answer with a chain-of-thought to each verified question using:</p>\n\n",
                "matched_terms": [
                    "each",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "each",
                    "report",
                    "number",
                    "all",
                    "dataset",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "each",
                    "report",
                    "number",
                    "all",
                    "also",
                    "krist",
                    "proportion",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to evaluate the <span class=\"ltx_text ltx_font_italic\">speech-to-text transfer</span> capability of SpeechLMs, where the model is asked a question in speech and tasked with responding in text (<math alttext=\"{\\text{S}}{\\rightarrow}{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>S</mtext><mo stretchy=\"false\">&#8594;</mo><mtext>T</mtext></mrow><annotation encoding=\"application/x-tex\">{\\text{S}}{\\rightarrow}{\\text{T}}</annotation></semantics></math>).\nIn the literature, there is a lack of standardized evaluations for this task of Spoken-Question-Answering (SQA). While efforts like Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>)</cite> have open-sourced some evaluation sets, they use different text-to-speech engines and generation parameters for synthesizing the spoken questions, rendering comparisons across different models unfair. Moreover, these datasets only consist of a question and answer, requiring models to generate free-form text outputs. However, prior works in LM evaluation standardization <cite class=\"ltx_cite ltx_citemacro_citep\">(Gu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib53\" title=\"\">2024</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>)</cite> recommend using a <span class=\"ltx_text ltx_font_italic\">cloze-form</span> of MCQ evaluation for evaluating base-models with question-conditioned completion log-probabilities rather than decoding free-form text outputs. The log-probability method removes evaluation confounds such as decoding temperature, sampling method and other decoding parameters, which are known to induce large variance <cite class=\"ltx_cite ltx_citemacro_citep\">(Hochlehnert et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib60\" title=\"\">2025</a>)</cite>. Therefore, we construct a standardized SQA evaluation suite of three datasets&#8212;<span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>. We source the raw audio questions from OpenAudioBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite>. We then prompt <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> with the original text question and answer of each sample to provide a set of three distractor choices (the prompts for generating choices are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A7\" title=\"Appendix G Prompts for generating distractor choices for evaluation sets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">G</span></a>). Hence, our final evaluation datasets consist of a spoken-question and <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> choices, with one correct answer (chance-level is <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m3\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math>). In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A6.T10\" title=\"In Appendix F Details and examples of SQA evaluation datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, we provide details about the number of test samples, the TTS engine used for synthesizing the speech questions, and the links to the original audio source files.</p>\n\n",
                "matched_terms": [
                    "each",
                    "test",
                    "spokenwebquestions",
                    "evaluation",
                    "spokentriviaqa",
                    "number",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below, we also provide a few examples from each evaluation dataset, with the question (in text), choices, and the ground-truth answer.</p>\n\n",
                "matched_terms": [
                    "each",
                    "also",
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each test sample and each answer-choice (out of <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> total choices), we use the following cloze-form to prompt the model:</p>\n\n",
                "matched_terms": [
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we compute the completion log-probability for each of the <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> answer choices. We normalize the completion log-probability by answer length to prevent biasing against long answer choices. A question is marked correct if the model assigns highest normalized log-probability to the ground-truth answer. We use standard accuracy metric (random chance level is <math alttext=\"25\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p7.m2\" intent=\":literal\"><semantics><mn>25</mn><annotation encoding=\"application/x-tex\">25</annotation></semantics></math>%) for reporting results. For running all our model evaluations, we use a fork of <span class=\"ltx_text ltx_font_typewriter\">lm-eval-harness</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib45\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the following prompt for generating the distractor options for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span>.</p>\n\n",
                "matched_terms": [
                    "spokentriviaqa",
                    "spokenllamaquestions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start with a spoken question-answering test set. Each test sample consists of (<math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math>, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m2\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math>, <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math>) triplets, where <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> denotes the spoken question in audio modality, <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m5\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> denotes the question in text modality, and <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.p2.m6\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> denotes the ground-truth answer in text modality.</p>\n\n",
                "matched_terms": [
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each test sample <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, let <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math> represent the question tokens in text and audio modality respectively.\nThat is, the tokenized representation of <math alttext=\"q_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">q_{t}</annotation></semantics></math> is <math alttext=\"\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>t</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>t</mi><mi>m</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{t_{1}}{,}{t_{2}}{\\cdots}{t_{m}}\\}</annotation></semantics></math> and the tokenized representation of <math alttext=\"q_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">q_{a}</annotation></semantics></math> is <math alttext=\"\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>a</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>a</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{a_{1}}{,}{a_{2}}{\\cdots}{a_{n}}\\}</annotation></semantics></math>.\nFor brevity, let us denote these tokenized representations as <math alttext=\"t_{1:m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{1:m}</annotation></semantics></math> and <math alttext=\"a_{1:n}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{1:n}</annotation></semantics></math>.\nNote that since the length of the question tokens in text and audio modalities might differ, it is possible that <math alttext=\"{n}\\neq{m}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8800;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">{n}\\neq{m}</annotation></semantics></math>.\nLet <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math> represent the ground-truth answer tokens in text modality i.e. the tokenized representation of <math alttext=\"gt\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">gt</annotation></semantics></math> is <math alttext=\"\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>g</mi><mn>1</mn></msub><mo>,</mo><mrow><msub><mi>g</mi><mn>2</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>g</mi><mi>o</mi></msub></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{{g_{1}}{,}{g_{2}}\\cdots{g_{o}}\\}</annotation></semantics></math>.\nAgain, for brevity, we denote this as <math alttext=\"g_{1:o}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m14\" intent=\":literal\"><semantics><msub><mi>g</mi><mrow><mn>1</mn><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">g_{1:o}</annotation></semantics></math>.\nLet <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p1.m15\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> be the vocabulary of the SpeechLM.</p>\n\n",
                "matched_terms": [
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a given test sample <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math>, for each answer token <math alttext=\"i\\in\\{{1}{,}{2}{\\cdots}{o}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in\\{{1}{,}{2}{\\cdots}{o}\\}</annotation></semantics></math>, we define the teacher-forced next&#8211;token distributions as:</p>\n\n",
                "matched_terms": [
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over each test set <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"A9.SS0.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> we also report the dataset means across metrics in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>:</p>\n\n",
                "matched_terms": [
                    "each",
                    "report",
                    "test",
                    "dataset",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "report",
                    "test",
                    "spokenwebquestions",
                    "all",
                    "spokenllamaquestions",
                    "spokentriviaqa",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For conducting the topic domain analysis in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we used the topic domain classifier that was released by <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite>. The classifier is a <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\" title=\"\">gte-base-en-v1.5</a> model that was fine-tuned on web-texts annotated by LLaMA models. We used the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/WebOrganizer/FormatClassifier-NoURL\" title=\"\">No-URL</a> version of the classifier that takes only the raw text as input and classifies it into one of 24 output classes. For getting the topic distribution of each of our datasets, we randomly sample <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"A10.SS1.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> examples, concatenate all the text chunks from each example (for web-crawled data, these are the annotated transcriptions while for synthetic data, these are the source text data samples), and use that as input to the topic classifier.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"In J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we showcase the topic distribution of Spoken-Web-Questions. Similar to the takeaways in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we find that some of the topics that Spoken-Web-Questions contains are severely under-represented in the web-crawled dataset while being represented adequately in the synthetic datasets. This further corroborates our findings that synthetic datasets help close the distribution mismatch between the web-crawled dataset and the evaluation datasets. Our findings regarding the under-representation of concepts in web-crawled datasets have also been echoed in the language and vision domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiedemer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib157\" title=\"\">2025</a>; Parashar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib107\" title=\"\">2024</a>; Elazar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib37\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib67\" title=\"\">2023</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib144\" title=\"\">2024</a>; Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib184\" title=\"\">2024</a>; Samuel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib123\" title=\"\">2024</a>; Dodge et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib31\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "also",
                    "dataset",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For all the topic domain analyses we have conducted previously, we used a coarse-level topic classifier that could categorize between 24 different topics. Here, we use a more fine-grained topic classifier that can produce a finer-grained categorization into 67 different topics. We use the <a class=\"ltx_ref ltx_href ltx_font_typewriter\" href=\"https://huggingface.co/kenhktsui/finefineweb-domain-fasttext-classifier\" title=\"\">finefineweb-domain-fasttext-classifier</a>, which is a bi-gram fasttext model that was used for curating the FineFineWeb dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib177\" title=\"\">2024a</a>)</cite>. We use the same procedure as before for annotating our evaluation and training datasets. We plot the fine-grained topic distributions for <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F13\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F14\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">14</span></a> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F15\" title=\"In J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">15</span></a>, along with all training datasets. Across all the plots, our findings from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.F12\" title=\"Figure 12 &#8227; J.2 Topic distribution for Spoken-Web-Questions &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> hold&#8212;our synthetic datasets increase the diversity and topic coverage of our training data distribution, thereby more closely matching the distribution of concepts encompassed in the evaluation datasets. This helps improve model generalization, yielding better downstream performance.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "all",
                    "spokenllamaquestions",
                    "evaluation",
                    "dataset",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "each",
                    "matches",
                    "test",
                    "contaminated",
                    "dataset",
                    "contamination"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start from the full test set (containing contaminated samples).\nIn our significance test, we test<span class=\"ltx_text ltx_font_italic\"> whether removing contaminated test items reduces accuracy beyond what would be expected under random removal of an equal number of items.</span>\nFormally, for accuracy <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>, the null is:</p>\n\n",
                "matched_terms": [
                    "contaminated",
                    "samples",
                    "number",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">i.e., the clean accuracy is not lower than the random-removal distribution. Because the contamination claim is directional (contamination would inflate accuracy), we use a <em class=\"ltx_emph ltx_font_italic\">one-sided</em> test.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "each",
                    "report",
                    "test",
                    "all",
                    "contaminated",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value is appropriate for the hypothesis that contamination inflates accuracy (so clean should be lower if inflation is present).\nWith 100 replicates, the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value granularity is <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Hence, we report <math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math> when no replicate from the bootstrap distribution is as low as the clean accuracy.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "report"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T13\" title=\"Table 13 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T15\" title=\"Table 15 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> summarize results for Spoken-TriviaQA, Spoken-LLaMA-Questions, and Spoken-Web-Questions.\nWe highlight the difference <math alttext=\"\\Delta=\\text{Clean}-\\text{RandMean}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mtext>Clean</mtext><mo>&#8722;</mo><mtext>RandMean</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=\\text{Clean}-\\text{RandMean}</annotation></semantics></math> and give the decision at a significance level <math alttext=\"\\alpha{=}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha{=}0.01</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "spokenwebquestions",
                    "spokenllamaquestions",
                    "spokentriviaqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "krist",
                    "test",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "also",
                    "test",
                    "contamination"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "also",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "also",
                    "test",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, all our evaluations for spoken question-answering used a <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><mn>0</mn></math>-shot prompting strategy i.e. the model would be fed in an input audio question and has to respond in text, with no additional examples in-context. However, many of the text-only evaluations including MMLU and WebQuestions are few-shot / in-context evaluations (MMLU is <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>-shot and WebQuestions is <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>-shot). Evaluating our models&#8217; abilities in the few-shot / in-context setting can further yield important insights on transferability and steerability of our models. Importantly, the few-shot capability has been emphasized to large degrees in both the vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib186\" title=\"\">2022</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib181\" title=\"\">2021b</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib46\" title=\"\">2024b</a>; Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib143\" title=\"\">2023</a>; Alayrac et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib4\" title=\"\">2022</a>; Awadalla et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib8\" title=\"\">2023</a>; Lauren&#231;on et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib74\" title=\"\">2024</a>)</cite> and text-only <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>; Touvron et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib139\" title=\"\">2023</a>; Dong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib33\" title=\"\">2022</a>; Olsson et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib101\" title=\"\">2022</a>)</cite> foundation modeling literature. Recently, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> also described their experimental settings which included few-shot speech-text tasks. Studying the transfer of our data interventions to the few-shot evaluation setting is an important open problem.</p>\n\n",
                "matched_terms": [
                    "all",
                    "also",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our training runs initialize the language model backbone for our SpeechLM using a pretrained base-LM. This is the standard recipe used by almost all the existing foundation SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>. However, recent work in the vision-language literature has advocated for full native multimodal pretraining from scratch <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>)</cite>, where both the language model and the modality-specific encoder/tokenizer are trained from scratch. It would be interesting to explore if our data-centric methods also enable more efficient SpeechLM pretraining from scratch in the future.</p>\n\n",
                "matched_terms": [
                    "all",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "all",
                    "test"
                ]
            }
        ]
    },
    "A11.T13": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 13: One-sided contamination test on STQ (N=1000).",
        "body": "Data mix\nFull (%)\nClean (%)\nRandom mean (95% CI) (%)\n\n𝚫\\boldsymbol{\\Delta} (pp)\nOne-sided pp\nDecision\n\n\nWeb-crawl 5353% + Krist 4747%\n29.20\n29.03\n29.22 [28.77, 29.64]\n−0.19-0.19\n0.32\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 6666% + Quest 3434%\n34.70\n34.56\n34.73 [34.26, 35.28]\n−0.17-0.17\n0.38\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 5959% + Quest 66% + Krist 3535%\n30.80\n30.46\n30.81 [30.36, 31.18]\n−0.35-0.35\n0.09\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 4040% + Quest 2727% + Krist 3333%\n31.70\n31.59\n31.70 [31.28, 32.10]\n−0.11-0.11\n0.41\nFail to reject H0H_{0}",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Data mix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Full (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Clean (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Random mean (95% CI) (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<math alttext=\"\\boldsymbol{\\Delta}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m1\" intent=\":literal\"><semantics><mi>&#120491;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\Delta}</annotation></semantics></math> (pp)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">One-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Decision</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"53\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m3\" intent=\":literal\"><semantics><mn>53</mn><annotation encoding=\"application/x-tex\">53</annotation></semantics></math>% + Krist <math alttext=\"47\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m4\" intent=\":literal\"><semantics><mn>47</mn><annotation encoding=\"application/x-tex\">47</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">29.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">29.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">29.22 [28.77, 29.64]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.19\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.19</mn></mrow><annotation encoding=\"application/x-tex\">-0.19</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"66\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m7\" intent=\":literal\"><semantics><mn>66</mn><annotation encoding=\"application/x-tex\">66</annotation></semantics></math>% + Quest <math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m8\" intent=\":literal\"><semantics><mn>34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">34.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">34.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">34.73 [34.26, 35.28]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.17\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m9\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.17</mn></mrow><annotation encoding=\"application/x-tex\">-0.17</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m10\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m11\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m12\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m13\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">30.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">30.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">30.81 [30.36, 31.18]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m14\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.35</mn></mrow><annotation encoding=\"application/x-tex\">-0.35</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m15\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m16\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% + Quest <math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m17\" intent=\":literal\"><semantics><mn>27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math>% + Krist <math alttext=\"33\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m18\" intent=\":literal\"><semantics><mn>33</mn><annotation encoding=\"application/x-tex\">33</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">31.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">31.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">31.70 [31.28, 32.10]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.11\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m19\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">-0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T13.m20\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "−011011",
            "decision",
            "webcrawl",
            "random",
            "mix",
            "test",
            "𝚫boldsymboldelta",
            "mean",
            "contamination",
            "−017017",
            "−019019",
            "reject",
            "−035035",
            "onesided",
            "clean",
            "n1000",
            "krist",
            "full",
            "stq",
            "fail",
            "h0h0",
            "data",
            "quest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T13\" title=\"Table 13 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T15\" title=\"Table 15 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> summarize results for Spoken-TriviaQA, Spoken-LLaMA-Questions, and Spoken-Web-Questions.\nWe highlight the difference <math alttext=\"\\Delta=\\text{Clean}-\\text{RandMean}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mtext>Clean</mtext><mo>&#8722;</mo><mtext>RandMean</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=\\text{Clean}-\\text{RandMean}</annotation></semantics></math> and give the decision at a significance level <math alttext=\"\\alpha{=}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha{=}0.01</annotation></semantics></math>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "clean",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "webcrawl",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hence test if our three data strategies also transfer to this audio-loss-masked setting. From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T4\" title=\"In 3.6 Our data-centric lessons transfer to understanding-only SpeechLMs &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we find this indeed to be the case (<math alttext=\"9.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mn>9.3</mn><annotation encoding=\"application/x-tex\">9.3</annotation></semantics></math>% average SQA lift). Further, we find absolute SQA performance improves significantly with loss-masking (<math alttext=\"51.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mn>51.8</mn><annotation encoding=\"application/x-tex\">51.8</annotation></semantics></math>% with loss masking vs. <math alttext=\"42.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m3\" intent=\":literal\"><semantics><mn>42.4</mn><annotation encoding=\"application/x-tex\">42.4</annotation></semantics></math>% without).\nThis result corroborates prior results <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>)</cite> suggesting that, for small scale models there is an inherent modality conflict between audio and text tokens, which can lead to regressions when computing loss on both speech and text modalities.</p>\n\n",
                "matched_terms": [
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, all our data-centric methods were only tested for the speech-text interleaved pretraining phase.\nOur model checkpoints are all hence inherently base-models, and cannot be used in an assistant-like manner.\nHowever, since most real-world usecases of SpeechLMs are for chat-assistant purposes <cite class=\"ltx_cite ltx_citemacro_citep\">(Ouyang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib104\" title=\"\">2022</a>; Taori et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib133\" title=\"\">2023</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib84\" title=\"\">2023</a>)</cite>, it is imperative that our data-centric methods\nalso transfer the gains after instruction-tuning. Here, we test whether our data interventions induce better post-training results.</p>\n\n",
                "matched_terms": [
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "data",
                    "webcrawl",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "data",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we aim to better understand why our data interventions (fine chunking + synthetic data mixing) improve over a baseline with coarse chunking and no synthetic data.\nOne plausible hypothesis is that <span class=\"ltx_text ltx_font_italic\">fine interleaving and synthetic data close the gap between the model&#8217;s audio-conditioned output distribution and text-conditioned output distribution</span>. Since we initialize from a well-trained language model, ensuring the audio-conditioned output distribution matches the distribution of text-conditioned outputs enables <span class=\"ltx_text ltx_font_italic\">strong modality alignment</span>. We now test if our data-centric approaches close this distribution gap.</p>\n\n",
                "matched_terms": [
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "full",
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "random",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "test",
                    "stq",
                    "contamination",
                    "clean",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "random",
                    "test",
                    "full",
                    "onesided",
                    "contamination",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "random",
                    "mean",
                    "contamination",
                    "clean",
                    "data",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "data",
                    "krist",
                    "webcrawl",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "mix",
                    "full",
                    "webcrawl",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start from the full test set (containing contaminated samples).\nIn our significance test, we test<span class=\"ltx_text ltx_font_italic\"> whether removing contaminated test items reduces accuracy beyond what would be expected under random removal of an equal number of items.</span>\nFormally, for accuracy <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>, the null is:</p>\n\n",
                "matched_terms": [
                    "random",
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">i.e., the clean accuracy is not lower than the random-removal distribution. Because the contamination claim is directional (contamination would inflate accuracy), we use a <em class=\"ltx_emph ltx_font_italic\">one-sided</em> test.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "clean",
                    "test",
                    "onesided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "random",
                    "mix",
                    "test",
                    "full",
                    "onesided",
                    "mean",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value is appropriate for the hypothesis that contamination inflates accuracy (so clean should be lower if inflation is present).\nWith 100 replicates, the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value granularity is <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Hence, we report <math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math> when no replicate from the bootstrap distribution is as low as the clean accuracy.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "random",
                    "mix",
                    "test",
                    "webcrawl",
                    "stq",
                    "onesided",
                    "contamination",
                    "clean",
                    "data",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "data",
                    "full",
                    "test"
                ]
            }
        ]
    },
    "A11.T14": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 14: One-sided contamination test on SLQ (N=300).",
        "body": "Training mix\nFull (%)\nClean (%)\nRandom mean (95% CI) (%)\n\n𝚫\\boldsymbol{\\Delta} (pp)\nOne-sided pp\nDecision\n\n\nWeb-crawl 5353% + Krist 4747%\n52.00\n50.54\n52.16 [50.54, 53.62]\n−1.62-1.62\n0.10\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 6666% + Quest 3434%\n66.33\n66.79\n66.34 [64.62, 68.06]\n+0.45+0.45\n0.82\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 5959% + Quest 66% + Krist 3535%\n50.33\n48.01\n50.33 [48.91, 51.62]\n−2.32-2.32\n<0.01<0.01\nReject H0H_{0}\n\n\nWeb-crawl 4040% + Quest 2727% + Krist 3333%\n49.33\n47.29\n49.43 [47.65, 50.90]\n−2.14-2.14\n0.02\nFail to reject H0H_{0}",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Training mix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Full (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Clean (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Random mean (95% CI) (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<math alttext=\"\\boldsymbol{\\Delta}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m1\" intent=\":literal\"><semantics><mi>&#120491;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\Delta}</annotation></semantics></math> (pp)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">One-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Decision</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"53\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m3\" intent=\":literal\"><semantics><mn>53</mn><annotation encoding=\"application/x-tex\">53</annotation></semantics></math>% + Krist <math alttext=\"47\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m4\" intent=\":literal\"><semantics><mn>47</mn><annotation encoding=\"application/x-tex\">47</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">52.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">50.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">52.16 [50.54, 53.62]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-1.62\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.62</mn></mrow><annotation encoding=\"application/x-tex\">-1.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"66\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m7\" intent=\":literal\"><semantics><mn>66</mn><annotation encoding=\"application/x-tex\">66</annotation></semantics></math>% + Quest <math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m8\" intent=\":literal\"><semantics><mn>34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">66.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">66.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">66.34 [64.62, 68.06]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"+0.45\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m9\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">+0.45</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m10\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m11\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m12\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m13\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">50.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">48.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">50.33 [48.91, 51.62]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-2.32\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m14\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.32</mn></mrow><annotation encoding=\"application/x-tex\">-2.32</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"&lt;0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m15\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">&lt;0.01</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m16\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m17\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% + Quest <math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m18\" intent=\":literal\"><semantics><mn>27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math>% + Krist <math alttext=\"33\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m19\" intent=\":literal\"><semantics><mn>33</mn><annotation encoding=\"application/x-tex\">33</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">49.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">47.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">49.43 [47.65, 50.90]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-2.14\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m20\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2.14</mn></mrow><annotation encoding=\"application/x-tex\">-2.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T14.m21\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "−232232",
            "−214214",
            "decision",
            "webcrawl",
            "random",
            "mix",
            "test",
            "𝚫boldsymboldelta",
            "mean",
            "contamination",
            "reject",
            "n300",
            "onesided",
            "clean",
            "krist",
            "full",
            "−162162",
            "slq",
            "fail",
            "h0h0",
            "quest"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "krist",
                    "clean",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic Sampling.</span>\nWhile the stochastic variant allows flexibility and potentially offers better generalization,\nit can restrict the number of <span class=\"ltx_text ltx_font_italic\">modality switches</span> during training.\nHence, we test a deterministic approach, where we alternate between audio and text modalities at each chunk, i.e. we formulate the training sequence as <math alttext=\"{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>A</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}</annotation></semantics></math>.\nThis <span class=\"ltx_text ltx_font_italic\">maximizes the number of modality switches</span> for a given sample.\nHere too, we always start with <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "random",
                    "training",
                    "webcrawl",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "test",
                    "slq",
                    "contamination",
                    "clean",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "random",
                    "test",
                    "full",
                    "onesided",
                    "contamination",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "random",
                    "mean",
                    "contamination",
                    "clean",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "training",
                    "full",
                    "mix",
                    "webcrawl",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start from the full test set (containing contaminated samples).\nIn our significance test, we test<span class=\"ltx_text ltx_font_italic\"> whether removing contaminated test items reduces accuracy beyond what would be expected under random removal of an equal number of items.</span>\nFormally, for accuracy <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>, the null is:</p>\n\n",
                "matched_terms": [
                    "random",
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">i.e., the clean accuracy is not lower than the random-removal distribution. Because the contamination claim is directional (contamination would inflate accuracy), we use a <em class=\"ltx_emph ltx_font_italic\">one-sided</em> test.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "clean",
                    "test",
                    "onesided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "random",
                    "training",
                    "test",
                    "mix",
                    "full",
                    "onesided",
                    "mean",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value is appropriate for the hypothesis that contamination inflates accuracy (so clean should be lower if inflation is present).\nWith 100 replicates, the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value granularity is <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Hence, we report <math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math> when no replicate from the bootstrap distribution is as low as the clean accuracy.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "random",
                    "mix",
                    "test",
                    "slq",
                    "webcrawl",
                    "onesided",
                    "contamination",
                    "clean",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our training runs initialize the language model backbone for our SpeechLM using a pretrained base-LM. This is the standard recipe used by almost all the existing foundation SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>. However, recent work in the vision-language literature has advocated for full native multimodal pretraining from scratch <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>)</cite>, where both the language model and the modality-specific encoder/tokenizer are trained from scratch. It would be interesting to explore if our data-centric methods also enable more efficient SpeechLM pretraining from scratch in the future.</p>\n\n",
                "matched_terms": [
                    "training",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "training",
                    "mix"
                ]
            }
        ]
    },
    "A11.T15": {
        "source_file": "Data-Centric Lessons To Improve Speech-Language Pretraining",
        "caption": "Table 15: One-sided contamination test on SWQ (N=1000).",
        "body": "Training mix\nFull (%)\nClean (%)\nRandom mean (95% CI) (%)\n\n𝚫\\boldsymbol{\\Delta} (pp)\nOne-sided pp\nDecision\n\n\nWeb-crawl 5353% + Krist 4747%\n43.40\n43.27\n43.39 [43.22, 43.57]\n−0.12-0.12\n0.23\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 6666% + Quest 3434%\n42.70\n42.57\n42.69 [42.47, 42.87]\n−0.12-0.12\n0.20\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 5959% + Quest 66% + Krist 3535%\n43.80\n43.67\n43.80 [43.62, 43.98]\n−0.13-0.13\n0.19\nFail to reject H0H_{0}\n\n\n\nWeb-crawl 4040% + Quest 2727% + Krist 3333%\n43.30\n43.17\n43.29 [43.07, 43.47]\n−0.12-0.12\n0.23\nFail to reject H0H_{0}",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Training mix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Full (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Clean (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Random mean (95% CI) (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<math alttext=\"\\boldsymbol{\\Delta}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m1\" intent=\":literal\"><semantics><mi>&#120491;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\Delta}</annotation></semantics></math> (pp)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">One-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Decision</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"53\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m3\" intent=\":literal\"><semantics><mn>53</mn><annotation encoding=\"application/x-tex\">53</annotation></semantics></math>% + Krist <math alttext=\"47\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m4\" intent=\":literal\"><semantics><mn>47</mn><annotation encoding=\"application/x-tex\">47</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.39 [43.22, 43.57]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.12\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">-0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"66\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m7\" intent=\":literal\"><semantics><mn>66</mn><annotation encoding=\"application/x-tex\">66</annotation></semantics></math>% + Quest <math alttext=\"34\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m8\" intent=\":literal\"><semantics><mn>34</mn><annotation encoding=\"application/x-tex\">34</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">42.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">42.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">42.69 [42.47, 42.87]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.12\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m9\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">-0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m10\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m11\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m12\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m13\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.80 [43.62, 43.98]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.13\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m14\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.13</mn></mrow><annotation encoding=\"application/x-tex\">-0.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m15\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Web-crawl <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m16\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% + Quest <math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m17\" intent=\":literal\"><semantics><mn>27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math>% + Krist <math alttext=\"33\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m18\" intent=\":literal\"><semantics><mn>33</mn><annotation encoding=\"application/x-tex\">33</annotation></semantics></math>%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">43.29 [43.07, 43.47]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><math alttext=\"-0.12\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m19\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">-0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">Fail to reject <math alttext=\"H_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.T15.m20\" intent=\":literal\"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">H_{0}</annotation></semantics></math>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "swq",
            "decision",
            "webcrawl",
            "random",
            "mix",
            "test",
            "𝚫boldsymboldelta",
            "mean",
            "contamination",
            "reject",
            "onesided",
            "clean",
            "n1000",
            "krist",
            "full",
            "−013013",
            "fail",
            "−012012",
            "h0h0",
            "quest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T13\" title=\"Table 13 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T15\" title=\"Table 15 &#8227; Results and interpretation. &#8227; K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> summarize results for Spoken-TriviaQA, Spoken-LLaMA-Questions, and Spoken-Web-Questions.\nWe highlight the difference <math alttext=\"\\Delta=\\text{Clean}-\\text{RandMean}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mtext>Clean</mtext><mo>&#8722;</mo><mtext>RandMean</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=\\text{Clean}-\\text{RandMean}</annotation></semantics></math> and give the decision at a significance level <math alttext=\"\\alpha{=}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha{=}0.01</annotation></semantics></math>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While web-crawled datasets offer massive volume, they often have poor <span class=\"ltx_text ltx_font_italic\">domain coverage</span>&#8212;their data distribution does not reflect the highest-priority domains for downstream deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Baack, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib9\" title=\"\">2024</a>; Longpre et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib85\" title=\"\">2024</a>)</cite>. Often, sufficient data from many core domains simply does not exist or is hard to crawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib183\" title=\"\">2024c</a>; Fang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib40\" title=\"\">2023b</a>; Kydl&#237;&#269;ek et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib72\" title=\"\">2025</a>)</cite>. Together, these reasons motivate using synthetic data to augment existing data from web-crawls. Moreover, in our web-crawled audio data, we find\nnoisy text-annotations (due to hallucinations from transcription models) and\nartifacts like background noise and speaker overlap.\nThereby, we explore synthesizing clean interleaved speech-text datasets from existing text-only corpora. We build two synthetic datasets (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-B) to augment our web-crawled data&#8212;<span class=\"ltx_text ltx_framed ltx_framed_underline\">K</span>nowledge-<span class=\"ltx_text ltx_framed ltx_framed_underline\">R</span>ich <span class=\"ltx_text ltx_framed ltx_framed_underline\">I</span>nterleaved <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Krist</span>) and <span class=\"ltx_text ltx_framed ltx_framed_underline\">Que</span>stion-Answering <span class=\"ltx_text ltx_framed ltx_framed_underline\">S</span>peech-<span class=\"ltx_text ltx_framed ltx_framed_underline\">T</span>ext (<span class=\"ltx_text ltx_font_italic\">Quest</span>).</p>\n\n",
                "matched_terms": [
                    "krist",
                    "clean",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge-Rich Interleaved Speech-Text (Krist).</span> We start from lightly-filtered web-crawled documents (similar to WARC files from <cite class=\"ltx_cite ltx_citemacro_citet\">CommonCrawl (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib27\" title=\"\">2007</a>)</cite>). We then apply URL-filtering to preserve documents from <span class=\"ltx_text ltx_font_italic\">knowledge-rich domains</span> (list of domains is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS1\" title=\"B.1 Knowledge-rich domains used for synthetic datasets &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.1</span></a>). This is motivated by recent efforts advocating high-quality educational data for accelerating model training <cite class=\"ltx_cite ltx_citemacro_citep\">(Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib109\" title=\"\">2024</a>; Abdin et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib1\" title=\"\">2024</a>; Gunasekar et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib56\" title=\"\">2023</a>)</cite>. Next, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini</span> to extract and lightly rewrite the text-content from raw HTML, following <cite class=\"ltx_cite ltx_citemacro_citet\">Maini et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib87\" title=\"\">2024</a>)</cite> (prompt used in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). We then segment the texts based on sentence-level splitting, to produce different text chunks. Finally, we synthesize audio for each chunk using <span class=\"ltx_text ltx_font_typewriter\">melo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib185\" title=\"\">2023</a>)</cite>. To improve speaker diversity in the synthesized data, we randomly sample voices from <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> different accents. This pipeline yields <math alttext=\"{\\sim}{4.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>4.6</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}{4.6}</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question-Answering Speech-Text (Quest).</span> Since <span class=\"ltx_text ltx_font_italic\">Krist</span> is synthesized from HTML-extracted text, its constituent samples do not sound like natural conversations.\nWe therefore build <span class=\"ltx_text ltx_font_italic\">Quest</span>, explicitly organized in a question-answering format to mimic real world audio.\nStarting from the same high-quality HTML pool as <span class=\"ltx_text ltx_font_italic\">Krist</span>, we first mine all possible question texts using regex-parsing. We then use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to filter out invalid questions (some examples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS3\" title=\"B.3 Examples of invalid questions in Quest &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.3</span></a>).\nFinally, we use <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> to generate responses along with a chain-of-thought <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib153\" title=\"\">2022</a>)</cite> trace (generation prompts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A2.SS2\" title=\"B.2 Prompt &#8227; Appendix B Details of Synthetic Datasets &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B.2</span></a>).\nWe use the same sentence-level chunking strategy as <span class=\"ltx_text ltx_font_italic\">Krist</span>.\nThis pipeline produces <math alttext=\"{\\sim}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}0.9</annotation></semantics></math>M hours of interleaved speech-text data.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe study the impact of independently mixing <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawled data (mixed proportional to their approximate token counts, for details see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>) in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. We find mixing in <span class=\"ltx_text ltx_font_italic\">Krist</span> brings a <math alttext=\"0.8\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mn>0.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.8\\%</annotation></semantics></math> lift in SQA performance while also moderately benefitting text-only benchmarks, compared to training on web-crawl alone.\nFurther, mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> with web-crawl improves both MMLU and SQA performance by large margins of <math alttext=\"2.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mn>2.1</mn><annotation encoding=\"application/x-tex\">2.1</annotation></semantics></math>% and <math alttext=\"7.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m3\" intent=\":literal\"><semantics><mn>7.2</mn><annotation encoding=\"application/x-tex\">7.2</annotation></semantics></math>%. We hypothesize that the QA format in interleaved training with <span class=\"ltx_text ltx_font_italic\">Quest</span> helps to efficiently adapt to downstream SQA capabilities.\nWe additionally explore two ratios for mixing <span class=\"ltx_text ltx_font_italic\">Quest</span> and <span class=\"ltx_text ltx_font_italic\">Krist</span> with the web-crawled data&#8212;one\nwhere we sample according to approximate token-counts of each data source (<math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m4\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% web-crawl), and another\nwhere we upsample the synthetic proportion (<math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m5\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% web-crawl).\nBoth settings improve over web-crawl by <math alttext=\"{1.4}{-}{0.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m6\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{0.7}</annotation></semantics></math>% SQA. However, due to complex interactions between mixing ratios and data repeats <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib93\" title=\"\">2023</a>; Xue et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib165\" title=\"\">2023</a>)</cite>, it is unclear how to construct an optimal mixture extracting the best of each data source <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib168\" title=\"\">2024a</a>)</cite> (details on exact token counts in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3\" title=\"Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>).\nWe leave such a data-mixing exploration for future work.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">So far, we have discussed interleaved speech-text data <span class=\"ltx_text ltx_font_italic\">processing</span> and <span class=\"ltx_text ltx_font_italic\">curation</span> for improving SQA performance. However, we did not describe <span class=\"ltx_text ltx_font_italic\">how we sample modality chunks during interleaved training</span>. Here, we study two different sampling schemes as shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.F2\" title=\"In 3.1 Evaluation Benchmarks &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>-C. Recollect that each interleaved speech-text training sample is of the form <math alttext=\"{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>,</mo><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>A</mi><mi>n</mi></msub><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{X_{i}}{=}{\\{{(}{A_{1}}{,}{T_{1}}{)}{,}{(}{A_{2}}{,}{T_{2}}{)}{\\cdots}{(}{A_{n}}{,}{T_{n}}{)}{\\}}}</annotation></semantics></math>. We now test two variants:</p>\n\n",
                "matched_terms": [
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic Sampling.</span>\nWhile the stochastic variant allows flexibility and potentially offers better generalization,\nit can restrict the number of <span class=\"ltx_text ltx_font_italic\">modality switches</span> during training.\nHence, we test a deterministic approach, where we alternate between audio and text modalities at each chunk, i.e. we formulate the training sequence as <math alttext=\"{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><mrow><msub><mi>A</mi><mn>3</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>A</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><mo>,</mo><msub><mi>T</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">{\\{{A_{1}}{,}{T_{2}}{,}{A_{3}}{\\cdots}{A_{n-1}}{,}{T_{n}}{\\}}}</annotation></semantics></math>.\nThis <span class=\"ltx_text ltx_font_italic\">maximizes the number of modality switches</span> for a given sample.\nHere too, we always start with <math alttext=\"{A}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">{A}_{1}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p5.m1\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math> pretrained checkpoints from our previous experiments to conduct SFT training using the exact same SFT data. The first checkpoint, denoted as <span class=\"ltx_text ltx_font_typewriter\">coarse</span>, is the one trained on web-crawl only data with coarse interleaving (Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>); the second one, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine</span> is obtained by training on web-crawl data with fine interleaving (Row 2 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T1\" title=\"In 3.3 Processing pretraining data via fine-grained interleaving &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> and Row 1 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>); the third one is the best model in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, denoted as <span class=\"ltx_text ltx_font_typewriter\">fine + syn</span>, obtained by training on web-crawl and <span class=\"ltx_text ltx_font_italic\">Quest</span> (Row 3 in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFT training, we used a constant learning rate of <math alttext=\"{5}{e}{-}{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">{5}{e}{-}{5}</annotation></semantics></math> with <math alttext=\"{0.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m2\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">{0.1}</annotation></semantics></math> dropout. We train for <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math>k steps using a batch size of <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m4\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m5\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math>.\nTo prevent regression on text-related metrics, we mix in a text pre-training dataset with a <math alttext=\"0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m6\" intent=\":literal\"><semantics><mn>0.6</mn><annotation encoding=\"application/x-tex\">0.6</annotation></semantics></math> sampling weight, i.e., <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS7.p6.m7\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% of the joint SFT mix is audio SFT data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "mix"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> From <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T5\" title=\"In 3.7 Our data-centric lessons transfer after post-training &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the gains obtained from our pretraining data interventions are largely carried on to the SFT stage, for both text response quality and audio\nresponse quality metrics. This suggests that SQA accuracy can be a good proxy metric for model quality after post-training as well.\nThe results also suggest that fine interleaving is generally better than coarse interleaving while mixing additional synthetic data like <span class=\"ltx_text ltx_font_italic\">Quest</span> still helps after the SFT training, even though a large portion of SFT data is already QA-like data. Similar results suggesting that front-loading high quality data into pretraining can benefit post-trained models have been shown in the text-only domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Akter et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib3\" title=\"\">2025</a>; Shah et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib125\" title=\"\">2025</a>)</cite>.\nTaken together, our results demonstrate the effectiveness of our proposed data-centric methods on downstream SFT tasks.</p>\n\n",
                "matched_terms": [
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup.</span> We start with the Spoken-LLaMA-Questions test set. For each test sample, we independently compute the token-wise teacher-forced probability distributions based on conditioning on audio and text questions separately. We then compute the mean token-wise reverse-KL-divergence values between the two probability distributions. For details, definitions and other metrics, please refer to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span> In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F5\" title=\"In 4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, we plot the distribution of mean reverse-KL-divergence values between text-conditioned and audio-conditioned output distributions on the full Spoken-LLaMA-Questions test set (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a> for definition of reverse-KLD).\nWe find that fine interleaving induces lower KL-divergence values (mean=<math alttext=\"2.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mn>2.21</mn><annotation encoding=\"application/x-tex\">2.21</annotation></semantics></math>) compared to coarse interleaving (mean=<math alttext=\"3.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mn>3.20</mn><annotation encoding=\"application/x-tex\">3.20</annotation></semantics></math>). Moreover, a model trained with both fine interleaving and synthetic data further closes the modality distribution gap (mean KLD=<math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mn>1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math>).\nThis trend also holds across other metrics and test sets too (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9\" title=\"Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">I</span></a>).\nThis result suggests that our data interventions indeed help to close the gap between text-conditioned and audio-conditioned probability distributions, thereby better aligning the two modalities, leading to stronger downstream SQA performance.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we observed that our synthetic speech-text datasets improve both text and SQA performance significantly.\nOur central hypothesis for <span class=\"ltx_text ltx_font_italic\">why</span> is&#8212;<span class=\"ltx_text ltx_font_italic\">web-crawled data has a very skewed topic distribution and our synthetic data improves the domain coverage</span>.\nTo help understand the composition of our web-crawled and synthetic datasets from a <span class=\"ltx_text ltx_font_italic\">topic</span> perspective, we leveraged the topic-domain classifier from <cite class=\"ltx_cite ltx_citemacro_citep\">(Wettig et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib155\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/WebOrganizer/TopicClassifier-NoURL\" title=\"\">https://huggingface.co/WebOrganizer/TopicClassifier-NoURL</a></span></span></span>,\nwhich can categorize texts in 24 different topic domains (an analysis with more fine-grained classifiers is in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10.SS3\" title=\"J.3 A more fine-grained topic distribution analysis &#8227; Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J.3</span></a>). We run the classifier on <math alttext=\"5000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mn>5000</mn><annotation encoding=\"application/x-tex\">5000</annotation></semantics></math> random samples from each of our training datasets (<span class=\"ltx_text ltx_font_italic\">Web-crawl</span>, <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span>). We also annotate topics covered in evaluation datasets. From our results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F6\" title=\"In 4.2 Synthetic data improves domain coverage &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a> (more results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A10\" title=\"Appendix J Topic domain analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">J</span></a>), we make two key observations:</p>\n\n",
                "matched_terms": [
                    "random",
                    "training",
                    "webcrawl",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic data improves topic coverage.</span> It is evident that both the <span class=\"ltx_text ltx_font_italic\">Krist</span> and <span class=\"ltx_text ltx_font_italic\">Quest</span> datasets oversample data from the domains of <span class=\"ltx_text ltx_font_italic\">science and tech</span>, <span class=\"ltx_text ltx_font_italic\">health</span>, <span class=\"ltx_text ltx_font_italic\">education and jobs</span>, and <span class=\"ltx_text ltx_font_italic\">finance</span>, all of which are extremely under-represented in the web-crawled data.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the significant boosts induced by our synthetic datasets, a natural question arises&#8212;<span class=\"ltx_text ltx_font_italic\">Is there test-set leakage, and if so, how does it impact SQA performance?</span>\nTo address this, we conduct a contamination analysis with two goals in mind: (1) identify the proportion of test samples that are likely contaminated in our training data, and (2) understand the downstream performance impact of this leakage.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contamination detection.</span>\nTo find the extent of contamination in our synthetic datasets, we follow recent works <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib128\" title=\"\">2024</a>; Sainz et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib121\" title=\"\">2024</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>)</cite> and use <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram token overlaps. While prior works used <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math>, we opt for a window from <math alttext=\"{n}{=}{6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{6}</annotation></semantics></math> to <math alttext=\"{n}{=}{13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">{n}{=}{13}</annotation></semantics></math> to improve recall, at the expense of more false-positives. We use the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o</span> tokenizer and apply lower-case normalization pre-tokenizing. We mark a test sample as contaminated if we find a matching <math alttext=\"{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">{n}</annotation></semantics></math>-gram in any equivalent <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-token span of a synthetic dataset (pseudo-code in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#alg1\" title=\"In K.5 Code for identifying matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">algorithm</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). We consider all three SQA test sets for analysis, and concatenate the question and answer of each sample for matching. For train sets, we take samples from the original seed text-datasets (from which we synthesize audio) for detecting matches.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Proportion of contamination.</span> We report the proportion of contaminated test samples in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.T12\" title=\"In K.2 Proportion of contamination in eval datasets &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>. We find that the <span class=\"ltx_text ltx_font_italic\">Quest</span> dataset has almost no contamination, while <span class=\"ltx_text ltx_font_italic\">Krist</span> has a small, yet non-negligible amount of contamination. Overall, the <span class=\"ltx_text ltx_font_italic\">SWQ</span> eval dataset is barely contaminated (<math alttext=\"0.4\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.4</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">0.4\\%</annotation></semantics></math>) while <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SLQ</span> evals have <math alttext=\"{2.5}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{2.5}{\\%}</annotation></semantics></math> and <math alttext=\"{7.7}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mn>7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">{7.7}{\\%}</annotation></semantics></math> contaminated samples respectively.\nImportantly, note that due to our windowed <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram approach, we have many false-positive matches (examples of matches are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS1\" title=\"K.1 Examples of contaminated matches &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.1</span></a>). However, we keep all matches to be as conservative as possible while analysing effect of contamination.\nTo understand the impact of test-set contamination on downstream SQA model performance, we consider the tests sets with these contaminated samples removed as <span class=\"ltx_text ltx_font_italic\">clean</span> sets&#8212;<span class=\"ltx_text ltx_font_italic\">SWQ-clean</span> has 996 samples, <span class=\"ltx_text ltx_font_italic\">STQ-clean</span> has 975 samples, and <span class=\"ltx_text ltx_font_italic\">SLQ-clean</span> has 277 samples.</p>\n\n",
                "matched_terms": [
                    "test",
                    "swq",
                    "contamination",
                    "clean",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Significance testing setup.</span>\nWe conduct a one-sided significance test on the differences between performance on the <span class=\"ltx_text ltx_font_italic\">full</span> test set (including all contaminated samples) and performance on the <span class=\"ltx_text ltx_font_italic\">clean</span> set (removing all contaminated samples). To control for the accuracy difference induced by reducing test set size for the clean sets, we compute the <span class=\"ltx_text ltx_font_italic\">random removal baseline accuracy</span>&#8212;model performance after removing the same number of randomly selected test samples, averaged across <math alttext=\"100\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m1\" intent=\":literal\"><semantics><mn>100</mn><annotation encoding=\"application/x-tex\">100</annotation></semantics></math> bootstrap replicates with different random seeds. We compute empirical <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p4.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values by comparing the clean test accuracy against the bootstrapped random removal distribution. Under this setting, our null hypothesis is: <span class=\"ltx_text ltx_font_italic\">observed model accuracy on the full test set is not artificially inflated by contamination</span>. For more details on the significance testing setup, refer <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "random",
                    "test",
                    "full",
                    "onesided",
                    "contamination",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results.</span>\nWe apply the previously described significance testing procedure for all <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m1\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math> models trained in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> that use synthetic data in their data mixture. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.F7\" title=\"In 4.3 Analysing train-test contamination &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we plot the absolute differences between the <span class=\"ltx_text ltx_font_italic\">clean</span> and <span class=\"ltx_text ltx_font_italic\">random removal mean</span> accuracy for all eval-model pairs. We find that contamination does not seem to significantly improve performance for <span class=\"ltx_text ltx_font_italic\">Spoken-TriviaQA</span> and <span class=\"ltx_text ltx_font_italic\">Spoken-Web-Questions</span> evaluations. For <span class=\"ltx_text ltx_font_italic\">Spoken-LLaMA-Questions</span>, contamination seems to be having a minor effect (<math alttext=\"{1.4}{-}{2.1}{\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo>&#8722;</mo><mrow><mn>2.1</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{1.4}{-}{2.1}{\\%}</annotation></semantics></math>) when <span class=\"ltx_text ltx_font_italic\">Krist</span> is in the data mixture. However, we find that the effect of contamination is not statistically significant in almost all the settings (at a significance level of <math alttext=\"{\\alpha}{=}{0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{\\alpha}{=}{0.01}</annotation></semantics></math>). We provide an in-depth analysis with confidence intervals (CIs) and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A11.SS3\" title=\"K.3 Expanded description of significance testing setup and results &#8227; Appendix K Details about contamination analysis &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">K.3</span></a>.\nAdditionally, we note that the performance boosts on Spoken-LLaMA-Questions due to synthetic data observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a> (<math alttext=\"{3.7}{\\%}{-}{19}\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3.7</mn><mo>%</mo></mrow><mo>&#8722;</mo><mrow><mn>19</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{3.7}{\\%}{-}{19}\\%</annotation></semantics></math>) far exceed the clean vs random-removal-mean accuracy differences observed (upto <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math>).\nTaken together, these results suggest that test-set contamination does not play a major role in explaining the accuracy boosts observed in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "random",
                    "mean",
                    "contamination",
                    "clean",
                    "krist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-text datasets.</span> Here, we provide the exact details of all our speech-text training data sources. Note that since our tokenizer processes audio at <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>Hz, our token yield per second is <math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math> speech tokens. Hence, an hour of audio (<math alttext=\"3600\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><mn>3600</mn><annotation encoding=\"application/x-tex\">3600</annotation></semantics></math>s) corresponds to <math alttext=\"45\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mn>45</mn><annotation encoding=\"application/x-tex\">45</annotation></semantics></math>k speech tokens.\nIn <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T8\" title=\"In Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, for each dataset, we report the number of raw hours of speech content along with the total number of speech tokens. As is evident, web-crawl data contains the most number of unique tokens followed by Krist and Quest.</p>\n\n",
                "matched_terms": [
                    "krist",
                    "webcrawl",
                    "training",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we break down the exact token counts used for each data mixture in the experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nRemember that we train for a total of <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>200</mn><annotation encoding=\"application/x-tex\">200</annotation></semantics></math>k steps with a batch-size of <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> and sequence-length of <math alttext=\"16,384\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>16</mn><mo>,</mo><mn>384</mn></mrow><annotation encoding=\"application/x-tex\">16,384</annotation></semantics></math> yielding <math alttext=\"1.67\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m4\" intent=\":literal\"><semantics><mn>1.67</mn><annotation encoding=\"application/x-tex\">1.67</annotation></semantics></math>T multimodal tokens for the full training run. For each experiment, we use <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m5\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text-only and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m6\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text mixing ratio. Hence, the text-only ratio corresponds to <math alttext=\"{\\sim}1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}1</annotation></semantics></math>T tokens. The speech-text ratio corresponds to the remaining <math alttext=\"{\\sim}670\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>670</mn></mrow><annotation encoding=\"application/x-tex\">{\\sim}670</annotation></semantics></math>B tokens. Now, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A3.T9\" title=\"In C.1 Details of data mixtures for synthetic data experiments &#8227; Appendix C Training data statistics &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report for each data source (text-only, web-crawl, Krist and Quest), the exact mixing proportion in the training mixture (%mix), total number of tokens in the training mixture (#toks) and the number of repeats (epochs) of the original data source (#repeats) used across all our experiments in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.T2\" title=\"In 3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>. As is evident from the table, due to the heterogenity of data sources and their corresponding token-sizes, it is quite complex to determine an optimal mixing proportion.\nOur results also corroborate existing results in language <cite class=\"ltx_cite ltx_citemacro_citep\">(Guha et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib54\" title=\"\">2025</a>)</cite> and vision-language <cite class=\"ltx_cite ltx_citemacro_citep\">(Bansal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib10\" title=\"\">2025</a>)</cite> reasoning domains, finding that mixing several data sources to improve performance is non-trivial.</p>\n\n",
                "matched_terms": [
                    "training",
                    "full",
                    "mix",
                    "webcrawl",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the main paper <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S4.SS1\" title=\"4.1 Improved alignment between modality distributions &#8227; 4 Understanding Why Our Data Interventions Help &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we showcased the divergence plots between the conditional next-token distributions, on the Spoken-LLaMA-Questions test with the reverse KL-divergence metric only. Here, we showcase the divergence distributions across all three of our test sets&#8212;Spoken-LLaMA-Questions, Spoken-Web-Questions and Spoken-TriviaQA&#8212;across three divergence metrics&#8212;Forward KL Divergence, Reverse KL Divergence and Jensen Shannon Divergence. The plots for Spoken-LLaMA-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F9\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, for Spoken-Web-Questions are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F10\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>, and for Spoken-TriviaQA are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.F11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>. Furthermore, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#A9.T11\" title=\"In I.1 More results across different metrics and test sets &#8227; Appendix I Divergence analysis between modality distributions &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we report the mean values of the divergence distributions obtained. Across all plots and the table, we observe that our data interventions consistently close the distribution mismatch between the conditional probability distributions of audio and text modalities. This suggests that our data intervention implicitly induce a self-distillation behaviour <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib180\" title=\"\">2021a</a>; Mobahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib92\" title=\"\">2020</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib179\" title=\"\">2019</a>)</cite> in our trained SpeechLMs. Such an implicit &#8220;distillation through data&#8221; property has also been observed in prior works in the multimodal and language domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Udandarao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib145\" title=\"\">2025</a>; Rawat et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib116\" title=\"\">2024</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib147\" title=\"\">2024</a>; Sachdeva &amp; McAuley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib120\" title=\"\">2023</a>; Wang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib149\" title=\"\">2018</a>)</cite>. Further, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib148\" title=\"\">2025a</a>)</cite> showed that explicitly applying a cross-modal distillation objective further helps to reduce the modality distribution gap, and our results further implicitly confirm this. In the future, further methods that have been proposed to reduce the modality gap in vision-language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Schrodi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib124\" title=\"\">2024</a>; Udandarao, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib142\" title=\"\">2022</a>; Liang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib82\" title=\"\">2022</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib76\" title=\"\">2025a</a>)</cite> can also be experimented with in the speech-language domain.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show some examples of the matches we get from our contamination identification procedure. For each match, we show the training dataset, the training sample, the contaminated test sample, the test dataset it belongs to, and the contaminated <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram span.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We start from the full test set (containing contaminated samples).\nIn our significance test, we test<span class=\"ltx_text ltx_font_italic\"> whether removing contaminated test items reduces accuracy beyond what would be expected under random removal of an equal number of items.</span>\nFormally, for accuracy <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math>, the null is:</p>\n\n",
                "matched_terms": [
                    "random",
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">i.e., the clean accuracy is not lower than the random-removal distribution. Because the contamination claim is directional (contamination would inflate accuracy), we use a <em class=\"ltx_emph ltx_font_italic\">one-sided</em> test.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "clean",
                    "test",
                    "onesided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each training mix and dataset from <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#S3.SS4\" title=\"3.4 Constructing effective synthetic datasets &#8227; 3 Controlled Data-Centric Experiments &#8227; Data-Centric Lessons To Improve Speech-Language Pretraining\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we compute:\n(i) <span class=\"ltx_text ltx_font_italic\">Full</span> accuracy on the full test set;\n(ii) <span class=\"ltx_text ltx_font_italic\">Clean</span> accuracy after removing all known contaminated items;\n(iii) a <span class=\"ltx_text ltx_font_italic\">random-removal baseline</span> by drawing 100 random subsets (without replacement) of the same size as the contaminated set, recomputing accuracy on the remaining items each time.\nAccuracies for (ii) and (iii) are computed over the reduced denominators (remaining items).\nFrom the bootstrap distribution we report the mean and 95% percentile CI and compute the empirical one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value as:</p>\n\n",
                "matched_terms": [
                    "random",
                    "training",
                    "test",
                    "mix",
                    "full",
                    "onesided",
                    "mean",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value is appropriate for the hypothesis that contamination inflates accuracy (so clean should be lower if inflation is present).\nWith 100 replicates, the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value granularity is <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. Hence, we report <math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math> when no replicate from the bootstrap distribution is as low as the clean accuracy.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across <span class=\"ltx_text ltx_font_italic\">STQ</span> and <span class=\"ltx_text ltx_font_italic\">SWQ</span>, clean accuracies consistently fall within the random-removal confidence intervals. Therefore, <span class=\"ltx_text ltx_font_italic\">we find no significant contamination-driven inflation.</span>\nFor <span class=\"ltx_text ltx_font_italic\">SLQ</span>, the Web-crawl <math alttext=\"59\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mn>59</mn><annotation encoding=\"application/x-tex\">59</annotation></semantics></math>% + Quest <math alttext=\"6\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mn>6</mn><annotation encoding=\"application/x-tex\">6</annotation></semantics></math>% + Krist <math alttext=\"35\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mn>35</mn><annotation encoding=\"application/x-tex\">35</annotation></semantics></math>% mix shows a drop in clean accuracy relative to the random baseline that is statistically significant under our one-sided <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-test (<math alttext=\"p{&lt;}0.01\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS3.SSS0.Px4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">p{&lt;}0.01</annotation></semantics></math>), consistent with contamination inflating test performance. However, for the other three data mixes we again see no significant evidence of inflation, under our testing setup. Hence, overall we conclude that <span class=\"ltx_text ltx_font_italic\">contamination does not have a major effect</span> on inflating model performance.</p>\n\n",
                "matched_terms": [
                    "random",
                    "mix",
                    "test",
                    "swq",
                    "webcrawl",
                    "onesided",
                    "contamination",
                    "clean",
                    "krist",
                    "quest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination analysis is entirely post-hoc, after training of a model is complete. In the ideal case, one would decontaminate the training sets with respect to the test sets a-priori <cite class=\"ltx_cite ltx_citemacro_citep\">(Beyer et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib12\" title=\"\">2024</a>; Zhai et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib175\" title=\"\">2022</a>; Oquab et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib103\" title=\"\">2023</a>; Trinh &amp; Le, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib140\" title=\"\">2018</a>; Gao et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib44\" title=\"\">2020</a>; Mizrahi et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib91\" title=\"\">2025</a>; Allal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib6\" title=\"\">2025</a>; OLMo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib100\" title=\"\">2024</a>)</cite>. In practice, however, this is unrealistic, since this assumes prior knowledge of all possible test sets that the model may encounter in the wild. Infact, several popular language model trainers do not decontaminate their training sets precisely for this reason <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib131\" title=\"\">2024</a>; Weber et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib152\" title=\"\">2024</a>; Maini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib88\" title=\"\">2025</a>; Rae et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib113\" title=\"\">2021</a>; Penedo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib108\" title=\"\">2023</a>; Kandpal et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib68\" title=\"\">2025</a>)</cite>.\nFurther, while we acknowledge that our post-hoc contamination analysis can be limiting and would benefit from a more causal treatment such as in works like <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib78\" title=\"\">2024</a>; Soldaini et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib129\" title=\"\">2024</a>; Bordt et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib15\" title=\"\">2024</a>; Jiang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib65\" title=\"\">2024</a>)</cite>, we however note that the downside of such a causal analysis is the significant overhead of re-training our models. Hence, we also note that many works in the literature refrain from a fully causal treatment of contamination <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib111\" title=\"\">2019</a>; Brown et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib18\" title=\"\">2020</a>; Dubey et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib34\" title=\"\">2024</a>; Achiam et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib2\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contamination detection only operates on the seed text-datasets that we generate our synthetic datasets from. We have not done any contamination analysis between the spoken question audio in our test sets with the audio in our training sets (we note that prior works in speech-language processing also mainly do contamination analysis at the text-level <cite class=\"ltx_cite ltx_citemacro_citep\">(Ngo et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib97\" title=\"\">2025</a>; Tseng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib141\" title=\"\">2025</a>)</cite>).\nWhile this is a reasonable proxy for our synthetic datasets, such a method might not transfer well for decontamination analyses of web-crawled datasets. This is because many of the speech transcriptions of the web-crawled speech might be noisy, incorrect or contain hallucinations induced by the transcription model. Hence, measuring, detecting and quantifying contamination on the audio modality is an important research problem that warrants futher research attention.</p>\n\n",
                "matched_terms": [
                    "contamination",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, our evaluations involve testing on text-only benchmarks (text-in text-out) and spoken question-answering benchmarks (audio-in text-out). However, end-to-end spoken question-answering, where both the input and output is in audio (audio-in audio-out) is an important capability that remains untested. While there have been some prior works testing explicitly for the full end-to-end capability <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Hassid et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib58\" title=\"\">2023</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite>, we note that reliable evaluation for this task is still quite challenging&#8212;there is a lack of standardization in the evaluation procedures used across the different model releases. For example Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>)</cite> uses a human judgement rating for comparing model outputs, while GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>, MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite>, Spectron-LM <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib95\" title=\"\">2023</a>)</cite> and Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>)</cite> use automated methods with ASR transcription models and LLM-as-judges. However, the ASR and judge-models used can be biased and impact results quite a lot <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib169\" title=\"\">2024b</a>; Panickssery et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib105\" title=\"\">2024</a>)</cite>, which has not been discussed in these prior works.\nMore importantly, previous works in image omni-models have demonstrated that the data curation procedures for targeting understanding and generation capabilities might differ significantly <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib138\" title=\"\">2024b</a>; Chen et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib20\" title=\"\">2025</a>; Deng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib29\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib161\" title=\"\">2025b</a>; Zhang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib182\" title=\"\">2025</a>)</cite>. Hence, we posit that similar takeaways might also hold for the speech-language pretraining task, where the data processing and curation strategies for understanding only tasks (audio-in text-out) are potentially different from generation tasks (audio-in audio-out). However, it is an interesting and important direction to test if our approaches transfer to the full end-to-end evaluation setting as well.</p>\n\n",
                "matched_terms": [
                    "full",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All our training runs initialize the language model backbone for our SpeechLM using a pretrained base-LM. This is the standard recipe used by almost all the existing foundation SpeechLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; D&#233;fossez et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib28\" title=\"\">2024</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib83\" title=\"\">2025</a>; Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Chu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib23\" title=\"\">2024</a>; Ding et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib30\" title=\"\">2025</a>; Zeng et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib172\" title=\"\">2024a</a>)</cite>. However, recent work in the vision-language literature has advocated for full native multimodal pretraining from scratch <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>)</cite>, where both the language model and the modality-specific encoder/tokenizer are trained from scratch. It would be interesting to explore if our data-centric methods also enable more efficient SpeechLM pretraining from scratch in the future.</p>\n\n",
                "matched_terms": [
                    "training",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all our experiments, we freeze the speech tokenizer while only training the language model. In the SpeechLM literature, there is no strong consensus regarding freezing or unfreezing the speech tokenizer. A potential next step could be to unfreeze the tokenizer and study the transferability of our data-centric recipes.\nAdditionally, we conduct only one continued-pretraining stage&#8212;however, recent SpeechLM works have explored more sophisticated multi-stage pipelines involving pretraining and mid-training <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib160\" title=\"\">2025a</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>; Li et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib79\" title=\"\">2025c</a>; Goel et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib50\" title=\"\">2025</a>)</cite>. It would again be interesting to test our methods in a multi-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we always used a mixture ratio of <math alttext=\"60\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m1\" intent=\":literal\"><semantics><mn>60</mn><annotation encoding=\"application/x-tex\">60</annotation></semantics></math>% text and <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m2\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math>% speech-text tokens. While we followed existing multimodal literature for these ratios <cite class=\"ltx_cite ltx_citemacro_citep\">(Shukor et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib126\" title=\"\">2025</a>; McKinzie et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib89\" title=\"\">2024</a>; Tong et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib137\" title=\"\">2024a</a>)</cite>, it is likely that this mixture ratio could be further tuned.\nA key reason for having such a large text-only proportion was to ensure the model does not lose its language-only base capabilities.\nHowever, for larger models (<math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS0.SSS0.Px7.p1.m3\" intent=\":literal\"><semantics><mn>7</mn><annotation encoding=\"application/x-tex\">7</annotation></semantics></math>B-parameter scales and beyond), a smaller text-proportion might be viable since larger models generally are prone to lesser catastrophic forgetting <cite class=\"ltx_cite ltx_citemacro_citep\">(Y&#305;ld&#305;z et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib170\" title=\"\">2024</a>; Roth et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib118\" title=\"\">2024</a>; Dziadzio et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib36\" title=\"\">2025</a>; Ramasesh et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib115\" title=\"\">2021</a>; Ibrahim et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib62\" title=\"\">2024</a>)</cite>.\nIndeed, recent SpeechLMs like MiMo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib162\" title=\"\">2025</a>)</cite> and StepAudio-AQAA <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.20860v1#bib.bib61\" title=\"\">2025</a>)</cite> use much smaller text-proportions in their training mix, suggesting that this is a valid strategy to improve speech-language pretraining.</p>\n\n",
                "matched_terms": [
                    "training",
                    "mix"
                ]
            }
        ]
    }
}