{
    "S4.T1": {
        "caption": "Table 1: \nDirect Whisper fine-tuning results on the Fisher-Spanish and BBN-Mandarin datasets. The Objective column specifies under which training objective the model system is fine-tuned. None refers to the original model. Underline highlights the cross-task synergy.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">E2E-ST</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Fisher</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"26.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mn>26.7</mn><annotation encoding=\"application/x-tex\">26.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"51.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mn>51.6</mn><annotation encoding=\"application/x-tex\">51.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mn>19.1</mn><annotation encoding=\"application/x-tex\">19.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"54.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mn>54.9</mn><annotation encoding=\"application/x-tex\">54.9</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ST</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"20.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mn>20.3</mn><annotation encoding=\"application/x-tex\">20.3</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"61.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mn>61.2</mn><annotation encoding=\"application/x-tex\">61.2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">BBN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"32.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mn>32.2</mn><annotation encoding=\"application/x-tex\">32.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"13.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mn>13.0</mn><annotation encoding=\"application/x-tex\">13.0</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{18.9}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mn>18.9</mn><annotation encoding=\"application/x-tex\">{18.9}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"16.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mn>16.2</mn><annotation encoding=\"application/x-tex\">16.2</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"23.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m13\" intent=\":literal\"><semantics><mn>23.1</mn><annotation encoding=\"application/x-tex\">23.1</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{16.8}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m14\" intent=\":literal\"><semantics><mn>16.8</mn><annotation encoding=\"application/x-tex\">{16.8}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer↓downarrow",
            "original",
            "column",
            "finetuned",
            "synergy",
            "which",
            "e2est",
            "crosstask",
            "underline",
            "bbn",
            "objective",
            "training",
            "system",
            "highlights",
            "bleu↑uparrow",
            "finetuning",
            "bbnmandarin",
            "fisherspanish",
            "under",
            "results",
            "asr",
            "datasets",
            "direct",
            "fisher",
            "task",
            "none",
            "model",
            "whisper",
            "specifies",
            "refers",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents results from directly fine-tuning Whisper, which reveals a cross-task synergy phenomenon: optimizing for one task (e.g., ASR) not only preserves but often enhances performance on another (e.g., ST), as indicated by underlined improvements across both datasets.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports Whisper-UT results on three corpora: CoVoST2 (French <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English, German <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English), Fisher&#8209;Spanish, and BBN&#8209;Mandarin. Across all settings, our proposed Whisper&#8209;UT variants demonstrate consistent improvements in transcription accuracy (WER&#8595;) and translation quality (BLEU&#8593;).</p>\n\n",
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveals that fine-tuning on one task does not only improve performance on the target task but also benefits other tasks as well. Notably, ASR fine-tuning enhances ST performance (51.6 to 54.9 on Fisher and 13.0 to 16.2 on BBN), and ST fine-tuning reciprocally benefits ASR (26.7 to 20.3 on Fisher and 32.2 to 23.1 on BBN).\nThis suggests that cross-task fine-tuning may mutually reinforce capabilities without architectural changes, inspiring Whisper-UT&#8217;s unified speech-text framework.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "task",
                    "model",
                    "which",
                    "crosstask",
                    "whisper",
                    "results",
                    "system",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The task of speech-to-text translation (ST) encompasses converting spoken content from one language to another, aiming to overcome language barriers to communication. Traditionally, the task involves an automatic speech recognition (ASR) module to transcribe spoken words, followed by a machine translation (MT) module to convert the transcribed text into the target language in a cascaded manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ney (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib21\" title=\"\">1999</a>)</cite>. The recent development of end-to-end neural architectures and large pre-trained models have substantially propelled advancements in downstream speech tasks, via either self-supervised learning (SSL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib1\" title=\"\">2020</a>); Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib11\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib4\" title=\"\">2022</a>)</cite> or fully supervised learning. Among the pre-trained acoustic models, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib27\" title=\"\">2022</a>)</cite>, a transformer-based encoder-decoder multi-task model trained with large-scale data in a supervised manner, has exhibited good performance on various ST corpora.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we systematically investigate how multi-task encoder-decoder models&#8212;using Whisper as a representative case study&#8212;can be efficiently adapted to these heterogeneous scenarios. First, we examine fine-tuning strategies for conventional ST (using 3-way parallel speech-transcript-translation data), speech-to-text tasks (ASR-only data), and MT, while also methods for multi-modal translation where both speech and transcripts are available. Our analysis reveals two key insights:</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Cross-task training induces synergistic benefits</span>&#8212;fine-tuning on in-domain ASR data improves ST performance, while ST training conversely enhances ASR accuracy, suggesting mutual reinforcement between the ASR and ST tasks even without 3-way parallel data;</p>\n\n",
                "matched_terms": [
                    "crosstask",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on these findings, we propose <span class=\"ltx_text ltx_font_bold\">Whisper</span> for <span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">T</span>ranslation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We open source our code at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BorrisonXiao/Whisper-UT\" title=\"\">https://github.com/BorrisonXiao/Whisper-UT</a>.</span></span></span>, or <span class=\"ltx_text ltx_font_bold\">Whisper-UT</span>, a framework that transforms Whisper&#8217;s decoder into a unified conditional generation model, capable of dynamically conditioning on speech, text, or both modalities. The framework repurposes Whisper&#8217;s encoder-decoder architecture as a versatile multi-modal interface through two innovations:</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A multi-task learning paradigm</span> with a stochastic task-selection mechanism to adapt the system across ASR, MT, ST, and multimodal translation tasks using a single set of LoRA parameters;</p>\n\n",
                "matched_terms": [
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments on CoVoST2&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite> French-English (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>) and German-English (<span class=\"ltx_text ltx_font_typewriter\">de-en</span>) subsets demonstrate strong performance. Extended evaluations on conversational telephone speech (CTS) corpora&#8212;Fisher-CallHome Spanish&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib26\" title=\"\">2013</a>)</cite>, and BBN Mandarin-English&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wotherspoon et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib39\" title=\"\">2024</a>)</cite> further confirm the robustness of our approach across diverse domains. Notably, Whisper-UT outperforms the 1.3B-parameter NLLB model in multi-modal settings (speech + ground-truth text) and achieves superior speech-only translation via hypothesis prompting.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work highlights the untapped potential of multi-task models in adaptive translation systems. By unifying modality handling and enabling efficient task specialization, Whisper-UT bridges the gap between rigid single-modality systems and the dynamic needs of real-world applications.</p>\n\n",
                "matched_terms": [
                    "task",
                    "highlights"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is an end-to-end multi-task speech model that adopts a transformer-like encoder-decoder architecture. Its <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version is pre-trained on 680,000 hours of speech data with multiple supervision.\nAs with the original transformer model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib34\" title=\"\">2023</a>)</cite>, the loss function Whisper used at its pre-training time is the cross-entropy objective for all tasks.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "whisper",
                    "model",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper&#8217;s decoder supports a prompting mechanism, originally designed for better capturing long-range dependencies of the transcripts/translations to resolve local audio ambiguities. Particularly, long utterances are segmented into chunks and the decoder generates its hypothesis for the current segment conditioning on the previous segment&#8217;s transcripts. Inspired by the effectiveness of GPT-like decoder-only models in machine translation, we hypothesize that Whisper&#8217;s decoder, which may be viewed as an audio-conditional language model, is also capable of performing audio-augmented text generation conditioning on <span class=\"ltx_text ltx_font_italic\">both inputs</span>. Our work extends recent work showing that the Whisper can be adapted via fine-tuning to perform a number of novel tasks including, audio-visual speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">Rouditchenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib29\" title=\"\">2024</a>)</cite>, target-speaker ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib10\" title=\"\">2024</a>); Polok et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib25\" title=\"\">2024</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib19\" title=\"\">2024a</a>)</cite>, translation to non-English languages <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib23\" title=\"\">2023</a>)</cite>, by showing that Whisper can be extended to enable multi-modal translation, i.e., using either only text or both text and speech inputs simultaneously.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model",
                    "which",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib8\" title=\"\">2023</a>)</cite> is another innovative model that further refines the integration of multi-modal inputs for speech and text translation tasks. As a single model designed for ASR, T2T translation, T2S translation, S2T translation and S2S translation, it consists of multiple building blocks to leverage uni-modal data, including a w2v-BERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib7\" title=\"\">2021</a>)</cite> as the speech encoder, a 1.3B NLLB model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib33\" title=\"\">2022</a>)</cite> as the text encoder and decoder, a transformer-based text-to-unit encoder-decoder model for speech, with a vocoder for converting the unit-sequences to waveforms.\nThese systems, along with most existing methods, primarily seek to simply align the representations of the text and speech modalities, limiting the model to still accept only one input modality at a time during inference, which prevents exploitation of <span class=\"ltx_text ltx_font_italic\">cross-modal cues</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of related works <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib20\" title=\"\">2024b</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib42\" title=\"\">2023</a>); Liu and Niehues (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib18\" title=\"\">2024</a>); Le et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib17\" title=\"\">2024</a>)</cite> have also demonstrated that multi-task learning can greatly improve speech translation performance.\nHere, we focus on model fine-tuning and demonstrate that training end-to-end models for either ASR or ST alone improves performance on the other task, enabling fine-tuning with data that was not original annotated for the target domain task.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "original",
                    "task",
                    "model",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional translation systems treat ST, MT, and ASR as distinct tasks, each requiring separate models or specialized architectures. In this work, we propose a <span class=\"ltx_text ltx_font_bold\">unified translation</span> framework that unifies these tasks under a single encoder-decoder paradigm, treating all forms of language conversion&#8212;including audio-to-text, text-to-text, and multi-modal translation&#8212;as conditional generation tasks. Our approach enables seamless adaptation to various input modalities and data conditions without requiring fundamental architectural changes.</p>\n\n",
                "matched_terms": [
                    "under",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our method is the insight that ASR can be reformulated as a source-language transcription task, ST as a direct speech-to-text translation task, and MT as a standard text-to-text translation task&#8212;all of which can be expressed as instances of sequence-to-sequence learning. Extending this idea, we introduce a <span class=\"ltx_text ltx_font_bold\">multi-modal translation</span> task, for which the model conditions on both speech and its corresponding transcript (either human-annotated or ASR-generated) to improve translation quality. This formulation generalizes the conventional ST and MT paradigms, leveraging available transcripts to enhance translation in scenarios where speech alone may be ambiguous or error-prone.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "which",
                    "asr",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the remarkable performance observed with decoder-only language models in machine translation, we presume that encoder-decoder models&#8217; audio-conditioned decoder possesses the potential for undertaking the audio-conditioned text translation task. In particular, one may prompt the decoder with source language text, generated either by human annotators or any ASR system, in the translation process, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Consequently, the resulting model is trained to learn the distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">which is practical in that it enables modular training of components, i.e.,</p>\n\n",
                "matched_terms": [
                    "which",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems such as Whisper, however, model the problem without explicitly conditioning on the ASR transcripts, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>. Its single-decoder multi-task paradigm presumably captures a higher-level abstract semantics of the speech signals, such that the ST decoding process is implicitly entangled with the model&#8217;s ASR ability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we carry out a <span class=\"ltx_text ltx_font_bold\">two-stage decoding</span> process. In the first stage, the model is used to produce the ASR hypotheses, and subsequently, in the second stage, the model conditions on them to generate the translations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative perspective on this modeling is that it fully leverages the system&#8217;s source-language modeling capability. In end-to-end multi-task models, the decoder can be viewed as implicitly &#8220;partitioned&#8221; into two roles: source-language modeling and target-language generation. While these functions share parameters and benefit from joint optimization, they may still develop distinct competencies. By conditioning translation on both speech and textual transcripts, this approach explicitly harnesses a well-trained source-language model&#8212;potentially even from an external ASR system&#8212;allowing the decoder to generate more accurate translations. This perspective highlights how multi-modal conditioning can serve as a mechanism to refine and reinforce the system&#8217;s understanding of the source language, ultimately improving translation quality.</p>\n\n",
                "matched_terms": [
                    "highlights",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Integrating MT functionality into a multi-modal encoder-decoder model presents unique challenges. In conventional encoder-decoder MT systems, the source language text is processed through the encoder, which generates contextual representations for the decoder to cross-attend to. However, oftentimes the pre-trained encoder is designed specifically for processing speech features, making direct text encoding potentially ineffective. Training the encoder to handle text inputs would require a significant amount of additional data and could lead to catastrophic forgetting, where the model loses its ability to process speech effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "direct",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of decoder-only MT models such as GPT-like systems, we adopt an alternative strategy: instead of modifying the encoder to accommodate text, we encode the source text directly within the decoder, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a). Specifically, we prepend the source text as a prefix to the decoder input, leveraging the self-attention mechanism to implicitly model source-target dependencies. However, implementing this method within an encoder-decoder framework requires careful handling of the cross-attention mechanism. Since the decoder in our system is designed to attend to encoded speech representations, directly bypassing the encoder would disrupt the model&#8217;s expected structure. To address this, we introduce a single learnable vector in the encoder, serving as an indicator that informs the decoder that text input is being processed. The remaining encoder output is padded with zeros, and we modify the cross-attention mask such that the decoder attends only to this learnable embedding. This design ensures that the model&#8217;s architecture remains structurally intact while effectively repurposing the decoder for text-based translation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To achieve a unified translation framework that encompasses multiple translation paradigms, we propose Whisper-UT, a system designed to handle ASR, ST, MT, and MMT within a single model. Our approach is built on multi-task learning, leveraging 3-way parallel data and text-only MT data to optimize multiple objectives in a stochastic fashion.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Objective.</span> Learning the mapping <math alttext=\"X\\to Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">&#8594;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X\\to Y</annotation></semantics></math>, i.e., predicting the source language transcript from speech.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">E2E-ST Objective.</span> Directly predicting the target language text <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> from speech <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p4.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "e2est",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}^{CE}_{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>asr</mtext><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{CE}_{\\text{asr}}</annotation></semantics></math> is the ASR loss (or SLM loss for text-only samples), and <math alttext=\"\\mathcal{L}^{CE}_{\\text{st}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>st</mtext><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{CE}_{\\text{st}}</annotation></semantics></math> is either the ST loss or the MMT loss, selected via stochastic task selection.</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MMT, we introduce an ASR error simulation mechanism to enhance robustness. With probability <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we perturb a batch by replacing the source language tokens, sampled with probability <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, with a similar alternative sampled randomly from the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors in the embeddings space. To explicitly signal perturbed inputs, we prepend a special token to the modified sequence, allowing for the model to dynamically re-weight its reliance on the noisy text prefix and the corresponding audio input at inference time. This aims to simulate real-world noise in transcripts (e.g., ASR errors, omissions), encouraging the model to rely on both modalities for translation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, our unified training framework integrates ASR, ST, MMT, and MT into a single multi-task learning process. To achieve this, we first concatenate both speech-text and text-only datasets, allowing for random sampling within each batch. For every batch, we compute the ASR loss, which corresponds to the source language modeling loss when dealing with text-only samples. The ASR and ST loss weights are dynamically balanced by sampling a weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from a Beta distribution. Next, we stochastically determine whether the batch follows the ST/TLM objective or the MMT/MT objective. If the batch is selected for MMT training, ASR error simulation is applied with a certain probability to mimic transcription imperfections and enhance robustness. By combining these components, Whisper-UT serves as a unified model for ASR, ST, MT, and MMT, leveraging both textual and speech inputs efficiently.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "objective",
                    "training",
                    "asr",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our approach on CoVoST2, a general-domain speech translation benchmark, using its French-English (180 hours) and German-English (119 hours) subsets for training. To assess performance on challenging conversational telephony speech (CTS), we conduct experiments on the Fisher-CallHome Spanish-to-English corpus (186 hours of spontaneous Spanish dialogues) and the BBN Mandarin-to-English corpus (110 hours of Mandarin-English telephony conversations). This setup tests our method&#8217;s adaptability across both general and domain-specific speech, with CTS posing unique challenges such as disfluencies, code-switching, and informal dialogue structures.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both ASR and ST, we normalize the text by lower-casing all characters and removing all punctuations before computing the metrics. For the Fisher Spanish corpus, the BLEU score is computed with multiple references using the Moses&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib16\" title=\"\">2007</a>)</cite> toolkit as reported in other work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weiss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib37\" title=\"\">2017a</a>)</cite>. The evaluation script used is provided in the code.</p>\n\n",
                "matched_terms": [
                    "fisher",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate our proposed approach, we adopt the <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version of Whisper with 1.6 billion parameters as the base model and fine-tune it for our unified translation modeling.\nTo enable joint training of speech-to-text and text-to-text translation within a single framework, we repurpose the 3-way parallel dataset by strategically replicating its text pairs. Specifically, we create a duplicate of the original dataset where the audio signals are removed, retaining only the source-target text pairs. This allows us to simulate text-only data without introducing external resources, ensuring parity in training scale across objectives.</p>\n\n",
                "matched_terms": [
                    "original",
                    "model",
                    "whisper",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on CoVoST2, Whisper&#8209;UT reduces WER from 13.4/7.0 (Whisper) to 8.3/5.8. Similar gains appear on Fisher (from 18.8 to 16.3) and BBN (from 32.2 to 17.4). These improvements suggest that our stochastic task-interleaving mechanism effectively mitigates catastrophic forgetting, despite the addition of MT and MMT as new tasks. This stability preserves modality-specific expertise while introducing new tasks and enabling cross-task synergy.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "fisher",
                    "synergy",
                    "crosstask",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-only translation, Whisper-UT&#8212;trained without architectural modifications&#8212;narrowly trails the 1.3B-parameter NLLB model on general-domain CoVoST2 (36.5/26.9 vs. 42.3/31.0 BLEU) but surpasses it by +7.6 and +7.0 BLEU on domain-specific Fisher-Spanish (55.9 vs. 48.3) and BBN-Mandarin (15.7 vs. 8.7) benchmarks, despite using fewer parameters and no dedicated MT pretraining. This divergence highlights two key insights: (1) Whisper&#8217;s decoder inherently functions as a multilingual language model, capable of text-to-text translation with light-touch adaptation, and (2) its cross-lingual transfer capabilities, honed during speech-centric pretraining, generalize robustly to textual MT in low-resource, domain-specific scenarios. Critically, these results validate our hypothesis that minimal modifications&#8212;enabling joint training on speech and text&#8212;can unlock Whisper&#8217;s latent capacity for unified cross-modal translation, bridging the gap between speech and text without sacrificing architectural simplicity.</p>\n\n",
                "matched_terms": [
                    "bbnmandarin",
                    "fisherspanish",
                    "model",
                    "training",
                    "results",
                    "highlights"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the ST setting, Whisper-UT achieves competitive performance with single-pass end-to-end decoding: 40.8/37.7 BLEU on CoVoST2 (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>/<span class=\"ltx_text ltx_font_typewriter\">de-en</span>), 62.0 BLEU on Fisher-Spanish, and 19.8 BLEU on BBN-Mandarin, surpassing QWen2-Audio, SeamlessM4T, and STAC-ST by margins of 2&#8211;8 BLEU points. Crucially, the 2-Stage inference variant yields systematic improvements over promptless decoding: +0.6/+0.4 BLEU on CoVoST2 (41.4/38.1 vs. 40.8/37.7), +0.1 BLEU on Fisher-Spanish (62.1 vs. 62.0), and +1.8 BLEU on BBN-Mandarin (21.6 vs. 19.8). These improvements are amplified in error-prone conditions, reflecting successful mitigation of ASR error propagation&#8212;a key challenge in cascaded systems. By prepending the special token during training (with simulated ASR noise) and inference (for 2-Stage decoding), the model learns to conditionally distrust imperfect transcripts while retaining their partial utility, rebalancing reliance on audio signals to correct latent errors. These consistent incremental gains validate the effectiveness of our two-stage modeling, demonstrating that even imperfect intermediate transcripts enhance translation fidelity through explicit cross-modal grounding when combined with learned distrust mechanisms.</p>\n\n",
                "matched_terms": [
                    "bbnmandarin",
                    "fisherspanish",
                    "model",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The unified Whisper-UT framework achieves robust performance across three key tasks: monolingual ASR, text-only machine translation, and speech translation. Improvements are most pronounced in conversational Mandarin and Spanish settings. Moreover, the 2&#8209;Stage decoding strategy provides a reliable way to enhance translation in fully end&#8208;to&#8208;end deployments. Overall, these results highlight Whisper-UT&#8217;s ability to unify cross-modal and cross-lingual speech-text tasks within a single architecture, offering a versatile solution for scenarios requiring joint speech-text modeling.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced Whisper-UT, a unified translation framework that integrates ASR, ST, MT, and MMT within a single multi-task learning paradigm. In addition to this unified framework, we propose an explicit modeling approach for speech translation that conditions on both speech signals and textual prompts, effectively leveraging ASR hypotheses or ground-truth transcripts. Our training strategy, incorporating stochastic task selection and modality-aware error simulation, ensures effective multi-task learning while mitigating catastrophic forgetting. Experimental results show that Whisper-UT achieves strong performance across various translation tasks, demonstrating the benefits of cross-task synergy. Future work will explore scaling to more languages and extending to broader multi-modal scenarios.</p>\n\n",
                "matched_terms": [
                    "task",
                    "synergy",
                    "crosstask",
                    "training",
                    "asr",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our approach demonstrates strong improvements, several limitations remain. To ensure fair comparisons, we kept training steps consistent across models, meaning our best-performing system may not have reached its full potential with extended training.</p>\n\n",
                "matched_terms": [
                    "training",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to resource constraints, we fine-tuned Whisper rather than training from scratch, which might limit the full integration of the objectives. Ideally, to demonstrate cross-task fine-tuning, we would start from a pretrained model that natively support each of our tasks, (MT, MMT, ST, ASR), but building state-of-the-art, or close to state-of-the-art systems requires building from existing models, such as Whisper, and adapting to Whisper to additionally perform these tasks, while a contribution in its own right, ultimately requires a two-stage fine-tuning approach that complicates analysis of the effectiveness of cross-task fine-tuning. Furthermore, while we believe our method to be general, i.e., it could be applied to similar models such as the OWSM model <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib24\" title=\"\">2024</a>)</cite>, we have only demonstrated our results using the Whisper model.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "finetuned",
                    "model",
                    "which",
                    "crosstask",
                    "training",
                    "results",
                    "asr",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training of machine learning models is a costly, energy-intensive process, so our method, which introduces a novel means of efficiently adapting existing large pre-trained models to new tasks, may mitigate the ethical concerns about the costs, financial, environmental, or other, associated with training ML models. Furthermore, the success of our approach, specifically cross-task fine-tuning, implies that speech translation systems can be more easily trained for new domains, including languages with limited training resources.</p>\n\n",
                "matched_terms": [
                    "crosstask",
                    "finetuning",
                    "which",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To efficiently adapt the model to these conversational scenarios without overfitting or incurring excessive computational cost, we leverage several parameter-efficient fine-tuning (PEFT) techniques.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.T3\" title=\"Table 3 &#8227; A.2 Hyperparameter Settings &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the hyperparameter configurations used for training our Whisper-UT model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CTS corpora usually consist of short utterances segmented from a full recording, reflecting the alternating speech of participants during conversations. However, we found empirically that fine-tuning on such segments, presumably due to a mismatch in sample lengths compared to Whisper&#8217;s pre-training data, leads to significant performance degradation. The resulting model tends to repetitively produce frequent filler words in the training corpus at inference time regardless of the input. Therefore, we re-segmented the utterances by merging them chronologically, with durations (in seconds) sampled from a Gaussian distribution, e.g. <math alttext=\"\\mathcal{N}(15,5^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>15</mn><mo>,</mo><msup><mn>5</mn><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(15,5^{2})</annotation></semantics></math>. As Whisper&#8217;s feature extractor automatically pads the features up to 30 seconds, such re-segmentation also significantly reduced the training cost in terms of memory and time.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HKUST Mandarin ASR Dataset</span> (90.1 hours): Mandarin conversational speech from telephony interactions, originally designed for ASR research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib9\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CallHome Mandarin ASR Dataset</span> (20.5 hours): Informal Mandarin dialogues curated for ASR study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Canavan and Zipperlen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib3\" title=\"\">1996</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Preservation of Linguistic Salience:</span> The 2-Stage decoding system successfully retains the code-switched terms &#8220;master&#8221; and &#8220;popular&#8221; (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0% for these tokens), while E2E-ST completely omits &#8220;master&#8221;. This suggests that: 1) direct audio-to-translation mapping struggles with lexical disambiguation of homophones (&#8220;master&#8221; vs. contextually expected &#8220;computer&#8221;), and 2) explicit intermediate ASR provides discrete textual anchors that guide translation decisions.</p>\n\n",
                "matched_terms": [
                    "e2est",
                    "direct",
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Modal Faithfulness:</span> While the reference MT (REF-MT) omits the final &#8220;&#24456;&#8221; (translated as \"very\") from the source utterance &#8220;&#24456;&#24212;&#35813;&#24456;&#8221;, our ASR transcript preserves all repetitions. This discrepancy highlights how audio-derived prosodic cues (e.g., emphatic stress on the final &#8220;&#24456;&#8221;) enable 2Stage-ST and MMT to retain pragmatic emphasis (&#8220;&#8230;that&#8217;s right very should be very&#8221;) where text-only MT truncates for conciseness. By aligning acoustic signals (stress patterns) with textual redundancy, our framework distinguishes intentional repetition&#8212;a discourse marker of conviction in Mandarin&#8212;from superficial noise, demonstrating superior faithfulness to both linguistic content and pragmatic intent compared to E2E ST pipelines.</p>\n\n",
                "matched_terms": [
                    "highlights",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The example validates our hypothesis that two-stage processing particularly benefits scenarios where: 1) ASR can reliably capture linguistically salient content (code-switches, proper nouns), and 2) Audio signals contain complementary paralinguistic information (prosodic boundaries, emphasis) that each modality alone cannot convey. This dual-modality advantage explains 2-Stage-ST&#8217;s performance gain over E2E-ST on BBN-Mandarin despite identical model parameters.</p>\n\n",
                "matched_terms": [
                    "e2est",
                    "model",
                    "asr",
                    "bbnmandarin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation experiments presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T5\" title=\"Table 5 &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> on the two CTS datasets (Fisher-Spanish and BBN-Mandarin), as their domain-specific challenges&#8212;disfluencies, code-switching, and spontaneous dialogue&#8212;diverged significantly from Whisper&#8217;s pretraining data. This allows us to isolate our framework&#8217;s adaptability beyond pretraining biases and quantify its efficacy in resource-constrained, real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "bbnmandarin",
                    "fisherspanish",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 7 and 17 show the results of the MT-only fine-tuning experiment, demonstrating that the model achieves strong text translation performance even with limited in-domain data&#8212;BLEU 63.4 on Fisher-Spanish and 16.0 on BBN-Mandarin. This outperforms the original NLLB-1.3B model, though it remains modestly behind its fine-tuned counterpart. This suggests that Whisper&#8217;s decoder inherently possesses some text translation capabilities or at least has sufficiently strong source and target language modeling abilities such that minimal adaptation enables it to perform the MT task. Interestingly, this MT training also gives the system MMT ability, as suggested by the 61.1/20.4 (Fisher/BBN) BLEU score, despite MMT being a novel objective that the model was not explicitly trained on. In fact, on the BBN corpus, the MT-trained model exhibits MMT capabilities that surpass its original training objective, achieving a BLEU score of 20.4 (MMT) compared to 16.0 (MT). This finding reinforces our earlier observation of cross-task synergy.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "original",
                    "finetuning",
                    "finetuned",
                    "bbnmandarin",
                    "synergy",
                    "fisherspanish",
                    "task",
                    "model",
                    "crosstask",
                    "objective",
                    "training",
                    "results",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In rows 6 and 17, we conduct straightforward multi-task fine-tuning experiments by duplicating the speech dataset with both ASR and ST supervision, concatenating the datasets, and employing random sampling within each batch. These experiments confirm that multi-task training is beneficial, as it enhances BLEU score from 61.2 to 62.2 and WER is reduced from 20.3 to 16.3 on the Fisher-Spanish corpus. A similar trend is observed on the BBN set as well. This suggests that jointly optimizing multiple relevant objectives allows the model to better capture linguistic patterns and improve generalization across tasks.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "finetuning",
                    "fisherspanish",
                    "model",
                    "training",
                    "asr",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 9 and 20 evaluate MMT-multi-task fine-tuned models, that is, the model is trained with <math alttext=\"q=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">q=0</annotation></semantics></math> and <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">b=0</annotation></semantics></math>. Notably, the MMT inference results outperform even the strong fine-tuned NLLB-1.3B baseline in MT performance, 70.4 vs. 67.4 on Fisher and 26.0 vs. 22.7 on BBN&#8212; demonstrating that MMT provides tangible benefits over traditional cascaded MT approaches.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "model",
                    "fisher",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a gap remains between different MMT settings. Specifically, when using the ASR hypothesis as input instead of the ground-truth transcript, i.e., the 2-Stage-ST decoding, performance drops from 67.5 to 58.6 on Fisher and from 25.2 to 20.6 on BBN. While this still exceeds the results from direct ST (52.4 vs. 51.0 on Fisher and 20.6 vs. 19.5 on BBN), the model tends to over-rely on the transcript in the absence of explicit modeling. Specifically, without the special tag to signal potential errors, the model treats the input transcript as fully reliable ground truth&#8212;an assumption that breaks down when using ASR outputs, which may contain recognition errors. These highlight both the effectiveness of explicit modeling and the limitations introduced by ASR errors.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "fisher",
                    "model",
                    "which",
                    "asr",
                    "results",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the UT-trained system (rows 11 and 22) achieves the best MMT and 2-Stage-ST results, with MMT reaching 70.4/26.0 BLEU and 62.1/21.6 BLEU, respectively, on the Fisher-Spanish and BBN-Mandarin corpora, proving the method&#8217;s effectiveness. Applying the error simulation strategy in this training scheme improves the robustness of the two-stage approach, narrowing the performance gap between MMT and 2-Stage-ST decoding. Specifically, on Fisher, the gap decreases from 8.9 to 8.3 BLEU (row 9 vs. 11), and on BBN, from 4.6 to 4.4 BLEU (row 20 vs. 22), indicating more stable performance under ASR-transcript input.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "fisher",
                    "bbnmandarin",
                    "fisherspanish",
                    "under",
                    "training",
                    "results",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Fisher test set, the 2&#8209;Stage&#8209;ST decoding strategy of the Whisper&#8209;UT model actually falls slightly behind the simpler ASR+ST multi&#8209;task E2E&#8209;ST model. Direct multi&#8209;task training of ASR and ST (row 6) achieves a BLEU of 62.2, whereas conditioning on ASR hypotheses under the unified&#8209;translation objective (row 11, 2&#8209;Stage&#8209;ST) yields 62.1&#8212;a 0.1 BLEU drop. Through manual inspection, we found this gap is driven largely by inconsistent translation of filler words: the same Spanish filler (e.g., &#8220;eh,&#8221; &#8220;um&#8221;) in ASR transcripts is rendered inconsistently in output, magnifying ASR transcript &#8220;errors&#8221; during translation. Moreover, because Whisper&#8217;s ASR and ST performance on Fisher Spanish are both strong already (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 16, BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 60), there is little mismatch for transcript conditioning to resolve, so the transcript signal offers marginal benefit.</p>\n\n",
                "matched_terms": [
                    "fisher",
                    "model",
                    "objective",
                    "under",
                    "training",
                    "asr",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, on the BBN corpus, the UT model demonstrates a clear advantage. The ASR+ST multi&#8209;task E2E&#8209;ST model (row 16) scores 20.2 BLEU, while the Whisper&#8209;UT 2&#8209;Stage&#8209;ST decoder (row 22) jumps to 21.6 BLEU&#8212;a significant 1.4&#8209;point gain. This larger benefit arises because BBN combines relatively low WER (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 18) with much lower translation quality (BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 20), indicating that the model&#8217;s ST ability lags behind its ASR competence. In this scenario, explicitly leveraging ASR transcripts helps fill the performance gap, yielding more accurate translations under the unified objective.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "model",
                    "objective",
                    "under",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Injecting out&#8209;of&#8209;domain text under the unified objective appears to have limited benefit and in some cases even disrupted established behaviors. On Fisher, UT&#8209;OOD (row 10) lags behind UT&#8209;CTS across every translation metric&#8212;most notably MT accuracy, which jumps from 44.2 BLEU with OOD data to 55.9 BLEU when text is drawn from the CTS domain. This suggests that the linguistic and stylistic mismatch of web&#8209;mined, TED talk, and parliamentary text fails to reinforce the speech&#8209;to&#8209;text alignment learned on conversational telephone speech, and may inject conflicting patterns that the model struggles to reconcile.</p>\n\n",
                "matched_terms": [
                    "fisher",
                    "model",
                    "which",
                    "objective",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A similar story unfolds on BBN. On BBN, the impact of injecting OOD text is most pronounced in the MT task. Under UT&#8209;OOD (row 21), the model&#8217;s MT performance barely improves over the base unified setting and remains far below the CTS&#8209;matched variant&#8212;rising only to 11.1 BLEU compared with 15.7 BLEU for UT&#8209;CTS (row 22). In contrast, UT&#8209;CTS consistently lifts MT and MMT performance by several BLEU points and slightly improves ASR quality. Together, these findings imply that substituting in&#8209;domain transcripts with heterogeneous text corpora does not generalize well in a cross&#8209;modal training regime and can inadvertently weaken the model&#8217;s ability to leverage the unified translation objective.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "task",
                    "objective",
                    "under",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we also present a rough sketch of the training data amounts for our model and the compared methods, as summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T6\" title=\"Table 6 &#8227; D.5.1 Dataset Setup &#8227; D.5 Impact of Out-of-Domain Text Data &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. However, it is important to note that due to differences in training methodologies, stages, and the unavailability of precise details for some systems, this comparison should be interpreted with caution and may contain ambiguities. We encourage readers to consult the original publications for more accurate and comprehensive descriptions of the training data used in each model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "original",
                    "training"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: \nResults on the test sets.\nMMT refers to the translation process that conditions on both the ground-truth transcript and the speech signals, while 2-Stage-ST refers to the MMT process with ASR hypothesis.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Metrics</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Results</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"12\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CoVoST2</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">fr-en</span> | <span class=\"ltx_text ltx_font_italic\">de-en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Baseline&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"18.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mn>18.3</mn><annotation encoding=\"application/x-tex\">18.3</annotation></semantics></math> | <math alttext=\"21.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mn>21.4</mn><annotation encoding=\"application/x-tex\">21.4</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-Large-V2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"13.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mn>13.4</mn><annotation encoding=\"application/x-tex\">13.4</annotation></semantics></math> | <math alttext=\"7.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mn>7.0</mn><annotation encoding=\"application/x-tex\">7.0</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"\\mathbf{8.3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">8.3</mn><annotation encoding=\"application/x-tex\">\\mathbf{8.3}</annotation></semantics></math> | <math alttext=\"\\mathbf{5.8}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">5.8</mn><annotation encoding=\"application/x-tex\">\\mathbf{5.8}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Fisher-Spanish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SeamlessM4T-Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"76.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mn>76.3</mn><annotation encoding=\"application/x-tex\">76.3</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-Large-V2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"26.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mn>26.7</mn><annotation encoding=\"application/x-tex\">26.7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Seq2seq&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weiss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib38\" title=\"\">2017b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"23.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mn>23.2</mn><annotation encoding=\"application/x-tex\">23.2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Multi-ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib13\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"22.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mn>22.9</mn><annotation encoding=\"application/x-tex\">22.9</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">STAC-ST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zuluaga-Gomez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib43\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"18.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mn>18.8</mn><annotation encoding=\"application/x-tex\">18.8</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{16.3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">16.3</mn><annotation encoding=\"application/x-tex\">\\mathbf{16.3}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">BBN-Mandarin</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SeamlessM4T-Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"52.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mn>52.6</mn><annotation encoding=\"application/x-tex\">52.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-Large-V2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"32.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mn>32.2</mn><annotation encoding=\"application/x-tex\">32.2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{17.4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">17.4</mn><annotation encoding=\"application/x-tex\">\\mathbf{17.4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CoVoST2</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">fr-en</span> | <span class=\"ltx_text ltx_font_italic\">de-en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Baseline&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"37.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mn>37.9</mn><annotation encoding=\"application/x-tex\">37.9</annotation></semantics></math> | <math alttext=\"28.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mn>28.2</mn><annotation encoding=\"application/x-tex\">28.2</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NLLB-1.3B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"\\mathbf{42.3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">42.3</mn><annotation encoding=\"application/x-tex\">\\mathbf{42.3}</annotation></semantics></math> | <math alttext=\"\\mathbf{31.0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m23\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">31.0</mn><annotation encoding=\"application/x-tex\">\\mathbf{31.0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"36.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m24\" intent=\":literal\"><semantics><mn>36.5</mn><annotation encoding=\"application/x-tex\">36.5</annotation></semantics></math> | <math alttext=\"26.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m25\" intent=\":literal\"><semantics><mn>26.9</mn><annotation encoding=\"application/x-tex\">26.9</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fisher-Spanish</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NLLB-1.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m26\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"48.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m27\" intent=\":literal\"><semantics><mn>48.3</mn><annotation encoding=\"application/x-tex\">48.3</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Bi-NMT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib13\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{59.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m28\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">59.6</mn><annotation encoding=\"application/x-tex\">\\mathbf{59.6}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"55.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m29\" intent=\":literal\"><semantics><mn>55.9</mn><annotation encoding=\"application/x-tex\">55.9</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BBN-Mandarin</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NLLB-1.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m30\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m31\" intent=\":literal\"><semantics><mn>8.7</mn><annotation encoding=\"application/x-tex\">8.7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{15.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m32\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">15.7</mn><annotation encoding=\"application/x-tex\">\\mathbf{15.7}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MMT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CoVoST2</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">fr-en</span> | <span class=\"ltx_text ltx_font_italic\">de-en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m33\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"\\mathbf{46.2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m34\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">46.2</mn><annotation encoding=\"application/x-tex\">\\mathbf{46.2}</annotation></semantics></math> | <math alttext=\"\\mathbf{40.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m35\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">40.1</mn><annotation encoding=\"application/x-tex\">\\mathbf{40.1}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Fisher-Spanish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m36\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{70.4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m37\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">70.4</mn><annotation encoding=\"application/x-tex\">\\mathbf{70.4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">BBN-Mandarin</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m38\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{26.0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m39\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">26.0</mn><annotation encoding=\"application/x-tex\">\\mathbf{26.0}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"18\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CoVoST2</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">fr-en</span> | <span class=\"ltx_text ltx_font_italic\">de-en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Baseline&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m40\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"27.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m41\" intent=\":literal\"><semantics><mn>27.6</mn><annotation encoding=\"application/x-tex\">27.6</annotation></semantics></math> | <math alttext=\"21.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m42\" intent=\":literal\"><semantics><mn>21.0</mn><annotation encoding=\"application/x-tex\">21.0</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SeamlessM4T-Large</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"33.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m43\" intent=\":literal\"><semantics><mn>33.1</mn><annotation encoding=\"application/x-tex\">33.1</annotation></semantics></math> | <math alttext=\"35.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m44\" intent=\":literal\"><semantics><mn>35.8</mn><annotation encoding=\"application/x-tex\">35.8</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-Large-V2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"36.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m45\" intent=\":literal\"><semantics><mn>36.7</mn><annotation encoding=\"application/x-tex\">36.7</annotation></semantics></math> | <math alttext=\"36.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m46\" intent=\":literal\"><semantics><mn>36.8</mn><annotation encoding=\"application/x-tex\">36.8</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">QWen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib6\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"38.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m47\" intent=\":literal\"><semantics><mn>38.5</mn><annotation encoding=\"application/x-tex\">38.5</annotation></semantics></math> | <math alttext=\"35.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m48\" intent=\":literal\"><semantics><mn>35.2</mn><annotation encoding=\"application/x-tex\">35.2</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"{40.8}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m49\" intent=\":literal\"><semantics><mn>40.8</mn><annotation encoding=\"application/x-tex\">{40.8}</annotation></semantics></math> | <math alttext=\"{37.7}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m50\" intent=\":literal\"><semantics><mn>37.7</mn><annotation encoding=\"application/x-tex\">{37.7}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT-<span class=\"ltx_text ltx_font_italic\">2-Stage</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"\\mathbf{41.4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m51\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">41.4</mn><annotation encoding=\"application/x-tex\">\\mathbf{41.4}</annotation></semantics></math> | <math alttext=\"\\mathbf{38.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m52\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">38.1</mn><annotation encoding=\"application/x-tex\">\\mathbf{38.1}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Fisher-Spanish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SeamlessM4T-Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"7\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m53\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"14.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m54\" intent=\":literal\"><semantics><mn>14.7</mn><annotation encoding=\"application/x-tex\">14.7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Multi-ST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib13\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"45.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m55\" intent=\":literal\"><semantics><mn>45.2</mn><annotation encoding=\"application/x-tex\">45.2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Multi-task ST/ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weiss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib38\" title=\"\">2017b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"48.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m56\" intent=\":literal\"><semantics><mn>48.7</mn><annotation encoding=\"application/x-tex\">48.7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-Large-V2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"51.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m57\" intent=\":literal\"><semantics><mn>51.6</mn><annotation encoding=\"application/x-tex\">51.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">STAC-ST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zuluaga-Gomez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib43\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"52.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m58\" intent=\":literal\"><semantics><mn>52.6</mn><annotation encoding=\"application/x-tex\">52.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{62.0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m59\" intent=\":literal\"><semantics><mn>62.0</mn><annotation encoding=\"application/x-tex\">{62.0}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT-<span class=\"ltx_text ltx_font_italic\">2-Stage</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{62.1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m60\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">62.1</mn><annotation encoding=\"application/x-tex\">\\mathbf{62.1}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">BBN-Mandarin</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SeamlessM4T-Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m61\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"7.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m62\" intent=\":literal\"><semantics><mn>7.0</mn><annotation encoding=\"application/x-tex\">7.0</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-Large-V2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"13.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m63\" intent=\":literal\"><semantics><mn>13.0</mn><annotation encoding=\"application/x-tex\">13.0</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m64\" intent=\":literal\"><semantics><mn>19.8</mn><annotation encoding=\"application/x-tex\">19.8</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper-UT-<span class=\"ltx_text ltx_font_italic\">2-Stage</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{21.6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m65\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">21.6</mn><annotation encoding=\"application/x-tex\">\\mathbf{21.6}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mmt",
            "wer↓downarrow",
            "462mathbf462",
            "while",
            "chu",
            "hypothesis",
            "260mathbf260",
            "704mathbf704",
            "signals",
            "seamlessm4tlarge",
            "381mathbf381",
            "2017b",
            "2020b",
            "nllb13b",
            "qwen2audio",
            "multist",
            "stacst",
            "translation",
            "covost2",
            "groundtruth",
            "596mathbf596",
            "weiss",
            "deen",
            "test",
            "conditions",
            "inaguma",
            "157mathbf157",
            "bleu↑uparrow",
            "stasr",
            "bbnmandarin",
            "sets",
            "fisherspanish",
            "414mathbf414",
            "metrics",
            "whisperut",
            "174mathbf174",
            "423mathbf423",
            "83mathbf83",
            "401mathbf401",
            "both",
            "results",
            "transcript",
            "speech",
            "asr",
            "multiasr",
            "binmt",
            "216mathbf216",
            "fren",
            "process",
            "task",
            "multitask",
            "2stagest",
            "310mathbf310",
            "model",
            "zuluagagomez",
            "whisperut2stage",
            "621mathbf621",
            "whisperlargev2",
            "163mathbf163",
            "58mathbf58",
            "refers",
            "dataset",
            "baseline",
            "wang",
            "seq2seq"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents results from directly fine-tuning Whisper, which reveals a cross-task synergy phenomenon: optimizing for one task (e.g., ASR) not only preserves but often enhances performance on another (e.g., ST), as indicated by underlined improvements across both datasets.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports Whisper-UT results on three corpora: CoVoST2 (French <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English, German <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English), Fisher&#8209;Spanish, and BBN&#8209;Mandarin. Across all settings, our proposed Whisper&#8209;UT variants demonstrate consistent improvements in transcription accuracy (WER&#8595;) and translation quality (BLEU&#8593;).</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on CoVoST2, Whisper&#8209;UT reduces WER from 13.4/7.0 (Whisper) to 8.3/5.8. Similar gains appear on Fisher (from 18.8 to 16.3) and BBN (from 32.2 to 17.4). These improvements suggest that our stochastic task-interleaving mechanism effectively mitigates catastrophic forgetting, despite the addition of MT and MMT as new tasks. This stability preserves modality-specific expertise while introducing new tasks and enabling cross-task synergy.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "translation",
                    "process",
                    "task",
                    "multitask",
                    "groundtruth",
                    "whisperut",
                    "model",
                    "speech",
                    "both",
                    "results",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Whisper-UT: A Unified Translation Framework for Speech and Text</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whisperut",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The task of speech-to-text translation (ST) encompasses converting spoken content from one language to another, aiming to overcome language barriers to communication. Traditionally, the task involves an automatic speech recognition (ASR) module to transcribe spoken words, followed by a machine translation (MT) module to convert the transcribed text into the target language in a cascaded manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ney (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib21\" title=\"\">1999</a>)</cite>. The recent development of end-to-end neural architectures and large pre-trained models have substantially propelled advancements in downstream speech tasks, via either self-supervised learning (SSL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib1\" title=\"\">2020</a>); Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib11\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib4\" title=\"\">2022</a>)</cite> or fully supervised learning. Among the pre-trained acoustic models, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib27\" title=\"\">2022</a>)</cite>, a transformer-based encoder-decoder multi-task model trained with large-scale data in a supervised manner, has exhibited good performance on various ST corpora.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "multitask",
                    "model",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, in real-world scenarios, input modalities and data conditions vary widely. In offline settings, for instance, translating conversational or dialectal speech&#8212;characterized by disfluencies, code-switching, and noisy acoustic environments&#8212;poses significant challenges to end-to-end models, often resulting in degraded performance. Conversely, scenarios like business meetings or translated media archives frequently provide both source-language speech and (manual or ASR-generated) transcripts. Yet existing systems fail to exploit this multi-modal synergy.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "speech",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we systematically investigate how multi-task encoder-decoder models&#8212;using Whisper as a representative case study&#8212;can be efficiently adapted to these heterogeneous scenarios. First, we examine fine-tuning strategies for conventional ST (using 3-way parallel speech-transcript-translation data), speech-to-text tasks (ASR-only data), and MT, while also methods for multi-modal translation where both speech and transcripts are available. Our analysis reveals two key insights:</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation",
                    "multitask",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Cross-task training induces synergistic benefits</span>&#8212;fine-tuning on in-domain ASR data improves ST performance, while ST training conversely enhances ASR accuracy, suggesting mutual reinforcement between the ASR and ST tasks even without 3-way parallel data;</p>\n\n",
                "matched_terms": [
                    "while",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-modal inputs (speech + text) consistently enhance translation quality when fused</span>, even with imperfect ASR transcripts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on these findings, we propose <span class=\"ltx_text ltx_font_bold\">Whisper</span> for <span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">T</span>ranslation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We open source our code at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BorrisonXiao/Whisper-UT\" title=\"\">https://github.com/BorrisonXiao/Whisper-UT</a>.</span></span></span>, or <span class=\"ltx_text ltx_font_bold\">Whisper-UT</span>, a framework that transforms Whisper&#8217;s decoder into a unified conditional generation model, capable of dynamically conditioning on speech, text, or both modalities. The framework repurposes Whisper&#8217;s encoder-decoder architecture as a versatile multi-modal interface through two innovations:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "whisperut",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A multi-task learning paradigm</span> with a stochastic task-selection mechanism to adapt the system across ASR, MT, ST, and multimodal translation tasks using a single set of LoRA parameters;</p>\n\n",
                "matched_terms": [
                    "multitask",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A two-stage decoding strategy</span>, where the decoder first generates an ASR transcript from speech, then reuses it as context for translation, perhaps emulating human thought processes, even when a transcript is not provided.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcript",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, Whisper-UT requires no architectural modifications&#8212;only fine-tuning&#8212;ensuring compatibility with any encoder-decoder model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisperut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments on CoVoST2&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite> French-English (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>) and German-English (<span class=\"ltx_text ltx_font_typewriter\">de-en</span>) subsets demonstrate strong performance. Extended evaluations on conversational telephone speech (CTS) corpora&#8212;Fisher-CallHome Spanish&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib26\" title=\"\">2013</a>)</cite>, and BBN Mandarin-English&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wotherspoon et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib39\" title=\"\">2024</a>)</cite> further confirm the robustness of our approach across diverse domains. Notably, Whisper-UT outperforms the 1.3B-parameter NLLB model in multi-modal settings (speech + ground-truth text) and achieves superior speech-only translation via hypothesis prompting.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "groundtruth",
                    "hypothesis",
                    "whisperut",
                    "model",
                    "deen",
                    "speech",
                    "wang",
                    "fren",
                    "2020b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work highlights the untapped potential of multi-task models in adaptive translation systems. By unifying modality handling and enabling efficient task specialization, Whisper-UT bridges the gap between rigid single-modality systems and the dynamic needs of real-world applications.</p>\n\n",
                "matched_terms": [
                    "task",
                    "multitask",
                    "whisperut",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is an end-to-end multi-task speech model that adopts a transformer-like encoder-decoder architecture. Its <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version is pre-trained on 680,000 hours of speech data with multiple supervision.\nAs with the original transformer model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib34\" title=\"\">2023</a>)</cite>, the loss function Whisper used at its pre-training time is the cross-entropy objective for all tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "multitask",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper&#8217;s decoder supports a prompting mechanism, originally designed for better capturing long-range dependencies of the transcripts/translations to resolve local audio ambiguities. Particularly, long utterances are segmented into chunks and the decoder generates its hypothesis for the current segment conditioning on the previous segment&#8217;s transcripts. Inspired by the effectiveness of GPT-like decoder-only models in machine translation, we hypothesize that Whisper&#8217;s decoder, which may be viewed as an audio-conditional language model, is also capable of performing audio-augmented text generation conditioning on <span class=\"ltx_text ltx_font_italic\">both inputs</span>. Our work extends recent work showing that the Whisper can be adapted via fine-tuning to perform a number of novel tasks including, audio-visual speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">Rouditchenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib29\" title=\"\">2024</a>)</cite>, target-speaker ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib10\" title=\"\">2024</a>); Polok et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib25\" title=\"\">2024</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib19\" title=\"\">2024a</a>)</cite>, translation to non-English languages <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib23\" title=\"\">2023</a>)</cite>, by showing that Whisper can be extended to enable multi-modal translation, i.e., using either only text or both text and speech inputs simultaneously.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "hypothesis",
                    "model",
                    "both",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent developments in multi-modal and multi-task systems, e.g.,&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib32\" title=\"\">2021</a>)</cite>, are exploring new ways to combine audio and text to improve various language-related tasks. mSLAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bapna et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib2\" title=\"\">2022</a>)</cite>, a multilingual speech and language model, has emerged as a pioneering approach. It aims to construct a shared representation space for both speech and text through joint pre-training on both self-supervised and supervised tasks with various loss objectives, including translation language modeling (TLM) loss for ST.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "multitask",
                    "model",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib8\" title=\"\">2023</a>)</cite> is another innovative model that further refines the integration of multi-modal inputs for speech and text translation tasks. As a single model designed for ASR, T2T translation, T2S translation, S2T translation and S2S translation, it consists of multiple building blocks to leverage uni-modal data, including a w2v-BERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib7\" title=\"\">2021</a>)</cite> as the speech encoder, a 1.3B NLLB model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib33\" title=\"\">2022</a>)</cite> as the text encoder and decoder, a transformer-based text-to-unit encoder-decoder model for speech, with a vocoder for converting the unit-sequences to waveforms.\nThese systems, along with most existing methods, primarily seek to simply align the representations of the text and speech modalities, limiting the model to still accept only one input modality at a time during inference, which prevents exploitation of <span class=\"ltx_text ltx_font_italic\">cross-modal cues</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recently, speech&#8209;centric large language models such as QWen&#8209;Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib6\" title=\"\">2024</a>)</cite> have shown that a unified decoder can be fine&#8209;tuned for a broad spectrum of text&#8209;conditioned speech tasks&#8212;including contextual ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib40\" title=\"\">2025</a>)</cite>&#8212;but these approaches rely on massive pretrained text LLMs and demand extensive data and compute during fine&#8209;tuning. This is a gap we aim to fill.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chu",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of related works <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib20\" title=\"\">2024b</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib42\" title=\"\">2023</a>); Liu and Niehues (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib18\" title=\"\">2024</a>); Le et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib17\" title=\"\">2024</a>)</cite> have also demonstrated that multi-task learning can greatly improve speech translation performance.\nHere, we focus on model fine-tuning and demonstrate that training end-to-end models for either ASR or ST alone improves performance on the other task, enabling fine-tuning with data that was not original annotated for the target domain task.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "multitask",
                    "model",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional translation systems treat ST, MT, and ASR as distinct tasks, each requiring separate models or specialized architectures. In this work, we propose a <span class=\"ltx_text ltx_font_bold\">unified translation</span> framework that unifies these tasks under a single encoder-decoder paradigm, treating all forms of language conversion&#8212;including audio-to-text, text-to-text, and multi-modal translation&#8212;as conditional generation tasks. Our approach enables seamless adaptation to various input modalities and data conditions without requiring fundamental architectural changes.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our method is the insight that ASR can be reformulated as a source-language transcription task, ST as a direct speech-to-text translation task, and MT as a standard text-to-text translation task&#8212;all of which can be expressed as instances of sequence-to-sequence learning. Extending this idea, we introduce a <span class=\"ltx_text ltx_font_bold\">multi-modal translation</span> task, for which the model conditions on both speech and its corresponding transcript (either human-annotated or ASR-generated) to improve translation quality. This formulation generalizes the conventional ST and MT paradigms, leveraging available transcripts to enhance translation in scenarios where speech alone may be ambiguous or error-prone.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "model",
                    "speech",
                    "transcript",
                    "both",
                    "asr",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first provide a formal definition of the multi-modal translation\n(MMT) task, or more precisely, the task of speech-and-text-conditioned translation. Let <math alttext=\"X=(x_{1},x_{2},\\cdots,x_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=(x_{1},x_{2},\\cdots,x_{T})</annotation></semantics></math> denote the speech signal of an utterance, <math alttext=\"Y=(y_{1},y_{2},\\cdots,y_{M})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>y</mi><mi>M</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{1},y_{2},\\cdots,y_{M})</annotation></semantics></math> denote the ground-truth transcript of the utterance, and <math alttext=\"Z=(z_{1},z_{2},\\cdots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\cdots,z_{N})</annotation></semantics></math> denote its corresponding text translation. The goal of the task is then to find the conditional distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>. We hypothesize that often <math alttext=\"H(Z|X,Y)&lt;H(Z|Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H(Z|X,Y)&lt;H(Z|Y)</annotation></semantics></math> in practice, where <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> denotes the information entropy. In other words, the speech signal may contain additional information for a more accurate translation of the utterance, as it may be able to aid resolving ambiguities such as homographs, tonal variations, and omitted content&#8212;such as repetitions and filler words&#8212;that may be present in human-annotated transcripts.</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "translation",
                    "task",
                    "groundtruth",
                    "transcript",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the remarkable performance observed with decoder-only language models in machine translation, we presume that encoder-decoder models&#8217; audio-conditioned decoder possesses the potential for undertaking the audio-conditioned text translation task. In particular, one may prompt the decoder with source language text, generated either by human annotators or any ASR system, in the translation process, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Consequently, the resulting model is trained to learn the distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "process",
                    "task",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem of speech translation can be directly modeled as <math alttext=\"P\\left(Z|X\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P\\left(Z|X\\right)</annotation></semantics></math> or modeled by marginalizing over an underlying latent variable, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>, representing valid transcripts of the audio <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the summation over <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math> is generally intractable. One common solution, also adopted by cascaded approaches to speech translation, is to approximate the summation with the single highest weight term in the summation, i.e.,</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, cascaded speech translation further assumes that the translation is conditionally independent of the audio given the transcript,</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcript",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems such as Whisper, however, model the problem without explicitly conditioning on the ASR transcripts, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>. Its single-decoder multi-task paradigm presumably captures a higher-level abstract semantics of the speech signals, such that the ST decoding process is implicitly entangled with the model&#8217;s ASR ability.</p>\n\n",
                "matched_terms": [
                    "process",
                    "multitask",
                    "model",
                    "signals",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We seek to combine the modeling advantages of the cascaded and end-to-end systems and generalize the multi-modal translation setting to re-formulate the system&#8217;s speech-only translation process for approximating Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S3.E1\" title=\"In 3.2 Translation with Speech-only Inputs &#8227; 3 Methodology &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Specifically, we relax the conditional independence assumption of cascade approaches, by endowing end-to-end speech translation models with the capacity to also condition on either a ground-truth or hypothesized transcript defined by Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S3.E2\" title=\"In 3.2 Translation with Speech-only Inputs &#8227; 3 Methodology &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, i.e.:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "process",
                    "groundtruth",
                    "transcript",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we carry out a <span class=\"ltx_text ltx_font_bold\">two-stage decoding</span> process. In the first stage, the model is used to produce the ASR hypotheses, and subsequently, in the second stage, the model conditions on them to generate the translations.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "model",
                    "asr",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative perspective on this modeling is that it fully leverages the system&#8217;s source-language modeling capability. In end-to-end multi-task models, the decoder can be viewed as implicitly &#8220;partitioned&#8221; into two roles: source-language modeling and target-language generation. While these functions share parameters and benefit from joint optimization, they may still develop distinct competencies. By conditioning translation on both speech and textual transcripts, this approach explicitly harnesses a well-trained source-language model&#8212;potentially even from an external ASR system&#8212;allowing the decoder to generate more accurate translations. This perspective highlights how multi-modal conditioning can serve as a mechanism to refine and reinforce the system&#8217;s understanding of the source language, ultimately improving translation quality.</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation",
                    "multitask",
                    "both",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Integrating MT functionality into a multi-modal encoder-decoder model presents unique challenges. In conventional encoder-decoder MT systems, the source language text is processed through the encoder, which generates contextual representations for the decoder to cross-attend to. However, oftentimes the pre-trained encoder is designed specifically for processing speech features, making direct text encoding potentially ineffective. Training the encoder to handle text inputs would require a significant amount of additional data and could lead to catastrophic forgetting, where the model loses its ability to process speech effectively.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of decoder-only MT models such as GPT-like systems, we adopt an alternative strategy: instead of modifying the encoder to accommodate text, we encode the source text directly within the decoder, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a). Specifically, we prepend the source text as a prefix to the decoder input, leveraging the self-attention mechanism to implicitly model source-target dependencies. However, implementing this method within an encoder-decoder framework requires careful handling of the cross-attention mechanism. Since the decoder in our system is designed to attend to encoded speech representations, directly bypassing the encoder would disrupt the model&#8217;s expected structure. To address this, we introduce a single learnable vector in the encoder, serving as an indicator that informs the decoder that text input is being processed. The remaining encoder output is padded with zeros, and we modify the cross-attention mask such that the decoder attends only to this learnable embedding. This design ensures that the model&#8217;s architecture remains structurally intact while effectively repurposing the decoder for text-based translation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "while",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To achieve a unified translation framework that encompasses multiple translation paradigms, we propose Whisper-UT, a system designed to handle ASR, ST, MT, and MMT within a single model. Our approach is built on multi-task learning, leveraging 3-way parallel data and text-only MT data to optimize multiple objectives in a stochastic fashion.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "multitask",
                    "model",
                    "whisperut",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-way dataset that provide speech, transcripts, and translations <math alttext=\"\\{X,Y,Z\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{X,Y,Z\\}</annotation></semantics></math>, we define three primary objectives:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Objective.</span> Learning the mapping <math alttext=\"X\\to Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">&#8594;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X\\to Y</annotation></semantics></math>, i.e., predicting the source language transcript from speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcript",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMT Objective.</span> Predicting <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p5.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> while attending to both <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p5.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> (speech) and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p5.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> (source transcript).</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "while",
                    "transcript",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}^{CE}_{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>asr</mtext><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{CE}_{\\text{asr}}</annotation></semantics></math> is the ASR loss (or SLM loss for text-only samples), and <math alttext=\"\\mathcal{L}^{CE}_{\\text{st}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>st</mtext><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{CE}_{\\text{st}}</annotation></semantics></math> is either the ST loss or the MMT loss, selected via stochastic task selection.</p>\n\n",
                "matched_terms": [
                    "task",
                    "mmt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ST vs. MMT Objective:</span>\nWith probability <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>, apply standard ST loss; for text-only data, this is equivalent to the TLM loss.\nWith probability <math alttext=\"(1-q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1-q)</annotation></semantics></math>, apply MMT loss, where the decoder cross-attends to both speech features and source text tokens; for text-only data, this becomes the conventional MT loss.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mmt",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MMT, we introduce an ASR error simulation mechanism to enhance robustness. With probability <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we perturb a batch by replacing the source language tokens, sampled with probability <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, with a similar alternative sampled randomly from the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors in the embeddings space. To explicitly signal perturbed inputs, we prepend a special token to the modified sequence, allowing for the model to dynamically re-weight its reliance on the noisy text prefix and the corresponding audio input at inference time. This aims to simulate real-world noise in transcripts (e.g., ASR errors, omissions), encouraging the model to rely on both modalities for translation.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "both",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, our unified training framework integrates ASR, ST, MMT, and MT into a single multi-task learning process. To achieve this, we first concatenate both speech-text and text-only datasets, allowing for random sampling within each batch. For every batch, we compute the ASR loss, which corresponds to the source language modeling loss when dealing with text-only samples. The ASR and ST loss weights are dynamically balanced by sampling a weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from a Beta distribution. Next, we stochastically determine whether the batch follows the ST/TLM objective or the MMT/MT objective. If the batch is selected for MMT training, ASR error simulation is applied with a certain probability to mimic transcription imperfections and enhance robustness. By combining these components, Whisper-UT serves as a unified model for ASR, ST, MT, and MMT, leveraging both textual and speech inputs efficiently.</p>\n\n",
                "matched_terms": [
                    "process",
                    "multitask",
                    "whisperut",
                    "model",
                    "speech",
                    "both",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our approach on CoVoST2, a general-domain speech translation benchmark, using its French-English (180 hours) and German-English (119 hours) subsets for training. To assess performance on challenging conversational telephony speech (CTS), we conduct experiments on the Fisher-CallHome Spanish-to-English corpus (186 hours of spontaneous Spanish dialogues) and the BBN Mandarin-to-English corpus (110 hours of Mandarin-English telephony conversations). This setup tests our method&#8217;s adaptability across both general and domain-specific speech, with CTS posing unique challenges such as disfluencies, code-switching, and informal dialogue structures.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "covost2",
                    "both",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both ASR and ST, we normalize the text by lower-casing all characters and removing all punctuations before computing the metrics. For the Fisher Spanish corpus, the BLEU score is computed with multiple references using the Moses&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib16\" title=\"\">2007</a>)</cite> toolkit as reported in other work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weiss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib37\" title=\"\">2017a</a>)</cite>. The evaluation script used is provided in the code.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "both",
                    "asr",
                    "weiss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate our proposed approach, we adopt the <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version of Whisper with 1.6 billion parameters as the base model and fine-tune it for our unified translation modeling.\nTo enable joint training of speech-to-text and text-to-text translation within a single framework, we repurpose the 3-way parallel dataset by strategically replicating its text pairs. Specifically, we create a duplicate of the original dataset where the audio signals are removed, retaining only the source-target text pairs. This allows us to simulate text-only data without introducing external resources, ensuring parity in training scale across objectives.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "signals",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveals that fine-tuning on one task does not only improve performance on the target task but also benefits other tasks as well. Notably, ASR fine-tuning enhances ST performance (51.6 to 54.9 on Fisher and 13.0 to 16.2 on BBN), and ST fine-tuning reciprocally benefits ASR (26.7 to 20.3 on Fisher and 32.2 to 23.1 on BBN).\nThis suggests that cross-task fine-tuning may mutually reinforce capabilities without architectural changes, inspiring Whisper-UT&#8217;s unified speech-text framework.</p>\n\n",
                "matched_terms": [
                    "task",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-only translation, Whisper-UT&#8212;trained without architectural modifications&#8212;narrowly trails the 1.3B-parameter NLLB model on general-domain CoVoST2 (36.5/26.9 vs. 42.3/31.0 BLEU) but surpasses it by +7.6 and +7.0 BLEU on domain-specific Fisher-Spanish (55.9 vs. 48.3) and BBN-Mandarin (15.7 vs. 8.7) benchmarks, despite using fewer parameters and no dedicated MT pretraining. This divergence highlights two key insights: (1) Whisper&#8217;s decoder inherently functions as a multilingual language model, capable of text-to-text translation with light-touch adaptation, and (2) its cross-lingual transfer capabilities, honed during speech-centric pretraining, generalize robustly to textual MT in low-resource, domain-specific scenarios. Critically, these results validate our hypothesis that minimal modifications&#8212;enabling joint training on speech and text&#8212;can unlock Whisper&#8217;s latent capacity for unified cross-modal translation, bridging the gap between speech and text without sacrificing architectural simplicity.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "bbnmandarin",
                    "covost2",
                    "fisherspanish",
                    "hypothesis",
                    "model",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When translating with access to both speech and ground&#8208;truth transcripts, Whisper&#8209;UT achieves 46.2/40.1 BLEU on CoVoST2, 70.4 BLEU on Fisher&#8209;Spanish, and 26.0 BLEU on BBN&#8209;Mandarin&#8212;surpassing all MT baselines. This substantial improvement underscores the complementary nature of audio and text modalities: acoustic cues (e.g., prosody, emotion, pauses, repetitions) resolve ambiguities in noisy transcripts, while lexical context sharpens alignment of speech-derived semantics. By explicitly modeling these mutually compensatory signals, our unified architecture fuses audio and text modalities, yielding more robust translations when multi-modal information is available.</p>\n\n",
                "matched_terms": [
                    "while",
                    "covost2",
                    "signals",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the ST setting, Whisper-UT achieves competitive performance with single-pass end-to-end decoding: 40.8/37.7 BLEU on CoVoST2 (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>/<span class=\"ltx_text ltx_font_typewriter\">de-en</span>), 62.0 BLEU on Fisher-Spanish, and 19.8 BLEU on BBN-Mandarin, surpassing QWen2-Audio, SeamlessM4T, and STAC-ST by margins of 2&#8211;8 BLEU points. Crucially, the 2-Stage inference variant yields systematic improvements over promptless decoding: +0.6/+0.4 BLEU on CoVoST2 (41.4/38.1 vs. 40.8/37.7), +0.1 BLEU on Fisher-Spanish (62.1 vs. 62.0), and +1.8 BLEU on BBN-Mandarin (21.6 vs. 19.8). These improvements are amplified in error-prone conditions, reflecting successful mitigation of ASR error propagation&#8212;a key challenge in cascaded systems. By prepending the special token during training (with simulated ASR noise) and inference (for 2-Stage decoding), the model learns to conditionally distrust imperfect transcripts while retaining their partial utility, rebalancing reliance on audio signals to correct latent errors. These consistent incremental gains validate the effectiveness of our two-stage modeling, demonstrating that even imperfect intermediate transcripts enhance translation fidelity through explicit cross-modal grounding when combined with learned distrust mechanisms.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "stacst",
                    "while",
                    "translation",
                    "bbnmandarin",
                    "covost2",
                    "fisherspanish",
                    "whisperut",
                    "model",
                    "signals",
                    "asr",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The unified Whisper-UT framework achieves robust performance across three key tasks: monolingual ASR, text-only machine translation, and speech translation. Improvements are most pronounced in conversational Mandarin and Spanish settings. Moreover, the 2&#8209;Stage decoding strategy provides a reliable way to enhance translation in fully end&#8208;to&#8208;end deployments. Overall, these results highlight Whisper-UT&#8217;s ability to unify cross-modal and cross-lingual speech-text tasks within a single architecture, offering a versatile solution for scenarios requiring joint speech-text modeling.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "whisperut",
                    "results",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced Whisper-UT, a unified translation framework that integrates ASR, ST, MT, and MMT within a single multi-task learning paradigm. In addition to this unified framework, we propose an explicit modeling approach for speech translation that conditions on both speech signals and textual prompts, effectively leveraging ASR hypotheses or ground-truth transcripts. Our training strategy, incorporating stochastic task selection and modality-aware error simulation, ensures effective multi-task learning while mitigating catastrophic forgetting. Experimental results show that Whisper-UT achieves strong performance across various translation tasks, demonstrating the benefits of cross-task synergy. Future work will explore scaling to more languages and extending to broader multi-modal scenarios.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "while",
                    "translation",
                    "task",
                    "multitask",
                    "groundtruth",
                    "whisperut",
                    "speech",
                    "signals",
                    "both",
                    "results",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to resource constraints, we fine-tuned Whisper rather than training from scratch, which might limit the full integration of the objectives. Ideally, to demonstrate cross-task fine-tuning, we would start from a pretrained model that natively support each of our tasks, (MT, MMT, ST, ASR), but building state-of-the-art, or close to state-of-the-art systems requires building from existing models, such as Whisper, and adapting to Whisper to additionally perform these tasks, while a contribution in its own right, ultimately requires a two-stage fine-tuning approach that complicates analysis of the effectiveness of cross-task fine-tuning. Furthermore, while we believe our method to be general, i.e., it could be applied to similar models such as the OWSM model <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib24\" title=\"\">2024</a>)</cite>, we have only demonstrated our results using the Whisper model.</p>\n\n",
                "matched_terms": [
                    "while",
                    "model",
                    "asr",
                    "results",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training of machine learning models is a costly, energy-intensive process, so our method, which introduces a novel means of efficiently adapting existing large pre-trained models to new tasks, may mitigate the ethical concerns about the costs, financial, environmental, or other, associated with training ML models. Furthermore, the success of our approach, specifically cross-task fine-tuning, implies that speech translation systems can be more easily trained for new domains, including languages with limited training resources.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.T3\" title=\"Table 3 &#8227; A.2 Hyperparameter Settings &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the hyperparameter configurations used for training our Whisper-UT model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisperut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CTS corpora usually consist of short utterances segmented from a full recording, reflecting the alternating speech of participants during conversations. However, we found empirically that fine-tuning on such segments, presumably due to a mismatch in sample lengths compared to Whisper&#8217;s pre-training data, leads to significant performance degradation. The resulting model tends to repetitively produce frequent filler words in the training corpus at inference time regardless of the input. Therefore, we re-segmented the utterances by merging them chronologically, with durations (in seconds) sampled from a Gaussian distribution, e.g. <math alttext=\"\\mathcal{N}(15,5^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>15</mn><mo>,</mo><msup><mn>5</mn><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(15,5^{2})</annotation></semantics></math>. As Whisper&#8217;s feature extractor automatically pads the features up to 30 seconds, such re-segmentation also significantly reduced the training cost in terms of memory and time.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HKUST Mandarin ASR Dataset</span> (90.1 hours): Mandarin conversational speech from telephony interactions, originally designed for ASR research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib9\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CallHome Mandarin ASR Dataset</span> (20.5 hours): Informal Mandarin dialogues curated for ASR study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Canavan and Zipperlen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib3\" title=\"\">1996</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The BBN team&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wotherspoon et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib39\" title=\"\">2024</a>)</cite> translated these into English to create parallel speech-to-text translation pairs. While our experiments utilized a pre-publication version provided directly by the BBN authors, minor discrepancies (e.g., data splits, preprocessing, or translation refinements) may exist compared to the final published version. Nevertheless, the corpus retains its core characteristics: conversational telephony domain focus, code-switching prevalence, and disfluency patterns.</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Preservation of Linguistic Salience:</span> The 2-Stage decoding system successfully retains the code-switched terms &#8220;master&#8221; and &#8220;popular&#8221; (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0% for these tokens), while E2E-ST completely omits &#8220;master&#8221;. This suggests that: 1) direct audio-to-translation mapping struggles with lexical disambiguation of homophones (&#8220;master&#8221; vs. contextually expected &#8220;computer&#8221;), and 2) explicit intermediate ASR provides discrete textual anchors that guide translation decisions.</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Modal Faithfulness:</span> While the reference MT (REF-MT) omits the final &#8220;&#24456;&#8221; (translated as \"very\") from the source utterance &#8220;&#24456;&#24212;&#35813;&#24456;&#8221;, our ASR transcript preserves all repetitions. This discrepancy highlights how audio-derived prosodic cues (e.g., emphatic stress on the final &#8220;&#24456;&#8221;) enable 2Stage-ST and MMT to retain pragmatic emphasis (&#8220;&#8230;that&#8217;s right very should be very&#8221;) where text-only MT truncates for conciseness. By aligning acoustic signals (stress patterns) with textual redundancy, our framework distinguishes intentional repetition&#8212;a discourse marker of conviction in Mandarin&#8212;from superficial noise, demonstrating superior faithfulness to both linguistic content and pragmatic intent compared to E2E ST pipelines.</p>\n\n",
                "matched_terms": [
                    "while",
                    "2stagest",
                    "signals",
                    "both",
                    "transcript",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The example validates our hypothesis that two-stage processing particularly benefits scenarios where: 1) ASR can reliably capture linguistically salient content (code-switches, proper nouns), and 2) Audio signals contain complementary paralinguistic information (prosodic boundaries, emphasis) that each modality alone cannot convey. This dual-modality advantage explains 2-Stage-ST&#8217;s performance gain over E2E-ST on BBN-Mandarin despite identical model parameters.</p>\n\n",
                "matched_terms": [
                    "bbnmandarin",
                    "model",
                    "hypothesis",
                    "signals",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation experiments presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T5\" title=\"Table 5 &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> on the two CTS datasets (Fisher-Spanish and BBN-Mandarin), as their domain-specific challenges&#8212;disfluencies, code-switching, and spontaneous dialogue&#8212;diverged significantly from Whisper&#8217;s pretraining data. This allows us to isolate our framework&#8217;s adaptability beyond pretraining biases and quantify its efficacy in resource-constrained, real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "bbnmandarin",
                    "fisherspanish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 7 and 17 show the results of the MT-only fine-tuning experiment, demonstrating that the model achieves strong text translation performance even with limited in-domain data&#8212;BLEU 63.4 on Fisher-Spanish and 16.0 on BBN-Mandarin. This outperforms the original NLLB-1.3B model, though it remains modestly behind its fine-tuned counterpart. This suggests that Whisper&#8217;s decoder inherently possesses some text translation capabilities or at least has sufficiently strong source and target language modeling abilities such that minimal adaptation enables it to perform the MT task. Interestingly, this MT training also gives the system MMT ability, as suggested by the 61.1/20.4 (Fisher/BBN) BLEU score, despite MMT being a novel objective that the model was not explicitly trained on. In fact, on the BBN corpus, the MT-trained model exhibits MMT capabilities that surpass its original training objective, achieving a BLEU score of 20.4 (MMT) compared to 16.0 (MT). This finding reinforces our earlier observation of cross-task synergy.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "bbnmandarin",
                    "task",
                    "fisherspanish",
                    "model",
                    "results",
                    "mmt",
                    "nllb13b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In rows 6 and 17, we conduct straightforward multi-task fine-tuning experiments by duplicating the speech dataset with both ASR and ST supervision, concatenating the datasets, and employing random sampling within each batch. These experiments confirm that multi-task training is beneficial, as it enhances BLEU score from 61.2 to 62.2 and WER is reduced from 20.3 to 16.3 on the Fisher-Spanish corpus. A similar trend is observed on the BBN set as well. This suggests that jointly optimizing multiple relevant objectives allows the model to better capture linguistic patterns and improve generalization across tasks.</p>\n\n",
                "matched_terms": [
                    "multitask",
                    "fisherspanish",
                    "model",
                    "both",
                    "asr",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 9 and 20 evaluate MMT-multi-task fine-tuned models, that is, the model is trained with <math alttext=\"q=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">q=0</annotation></semantics></math> and <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">b=0</annotation></semantics></math>. Notably, the MMT inference results outperform even the strong fine-tuned NLLB-1.3B baseline in MT performance, 70.4 vs. 67.4 on Fisher and 26.0 vs. 22.7 on BBN&#8212; demonstrating that MMT provides tangible benefits over traditional cascaded MT approaches.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "mmt",
                    "baseline",
                    "nllb13b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a gap remains between different MMT settings. Specifically, when using the ASR hypothesis as input instead of the ground-truth transcript, i.e., the 2-Stage-ST decoding, performance drops from 67.5 to 58.6 on Fisher and from 25.2 to 20.6 on BBN. While this still exceeds the results from direct ST (52.4 vs. 51.0 on Fisher and 20.6 vs. 19.5 on BBN), the model tends to over-rely on the transcript in the absence of explicit modeling. Specifically, without the special tag to signal potential errors, the model treats the input transcript as fully reliable ground truth&#8212;an assumption that breaks down when using ASR outputs, which may contain recognition errors. These highlight both the effectiveness of explicit modeling and the limitations introduced by ASR errors.</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "while",
                    "groundtruth",
                    "2stagest",
                    "hypothesis",
                    "model",
                    "transcript",
                    "asr",
                    "results",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the UT-trained system (rows 11 and 22) achieves the best MMT and 2-Stage-ST results, with MMT reaching 70.4/26.0 BLEU and 62.1/21.6 BLEU, respectively, on the Fisher-Spanish and BBN-Mandarin corpora, proving the method&#8217;s effectiveness. Applying the error simulation strategy in this training scheme improves the robustness of the two-stage approach, narrowing the performance gap between MMT and 2-Stage-ST decoding. Specifically, on Fisher, the gap decreases from 8.9 to 8.3 BLEU (row 9 vs. 11), and on BBN, from 4.6 to 4.4 BLEU (row 20 vs. 22), indicating more stable performance under ASR-transcript input.</p>\n\n",
                "matched_terms": [
                    "bbnmandarin",
                    "2stagest",
                    "fisherspanish",
                    "results",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Fisher test set, the 2&#8209;Stage&#8209;ST decoding strategy of the Whisper&#8209;UT model actually falls slightly behind the simpler ASR+ST multi&#8209;task E2E&#8209;ST model. Direct multi&#8209;task training of ASR and ST (row 6) achieves a BLEU of 62.2, whereas conditioning on ASR hypotheses under the unified&#8209;translation objective (row 11, 2&#8209;Stage&#8209;ST) yields 62.1&#8212;a 0.1 BLEU drop. Through manual inspection, we found this gap is driven largely by inconsistent translation of filler words: the same Spanish filler (e.g., &#8220;eh,&#8221; &#8220;um&#8221;) in ASR transcripts is rendered inconsistently in output, magnifying ASR transcript &#8220;errors&#8221; during translation. Moreover, because Whisper&#8217;s ASR and ST performance on Fisher Spanish are both strong already (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 16, BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 60), there is little mismatch for transcript conditioning to resolve, so the transcript signal offers marginal benefit.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "test",
                    "transcript",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, on the BBN corpus, the UT model demonstrates a clear advantage. The ASR+ST multi&#8209;task E2E&#8209;ST model (row 16) scores 20.2 BLEU, while the Whisper&#8209;UT 2&#8209;Stage&#8209;ST decoder (row 22) jumps to 21.6 BLEU&#8212;a significant 1.4&#8209;point gain. This larger benefit arises because BBN combines relatively low WER (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 18) with much lower translation quality (BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 20), indicating that the model&#8217;s ST ability lags behind its ASR competence. In this scenario, explicitly leveraging ASR transcripts helps fill the performance gap, yielding more accurate translations under the unified objective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "while",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoVoST 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite> (diverse web-mined speech),</p>\n\n",
                "matched_terms": [
                    "speech",
                    "wang",
                    "2020b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The OOD sets contrast sharply with CTS data in domain (e.g., formal talks vs. casual dialogues) and lexical style. To isolate the effect of data domain (not scale), we match the total training steps to our baseline CTS experiments, ensuring comparable optimization cycles. This setup tests whether cross-modal alignment generalizes to heterogeneous text distributions.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Injecting out&#8209;of&#8209;domain text under the unified objective appears to have limited benefit and in some cases even disrupted established behaviors. On Fisher, UT&#8209;OOD (row 10) lags behind UT&#8209;CTS across every translation metric&#8212;most notably MT accuracy, which jumps from 44.2 BLEU with OOD data to 55.9 BLEU when text is drawn from the CTS domain. This suggests that the linguistic and stylistic mismatch of web&#8209;mined, TED talk, and parliamentary text fails to reinforce the speech&#8209;to&#8209;text alignment learned on conversational telephone speech, and may inject conflicting patterns that the model struggles to reconcile.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A similar story unfolds on BBN. On BBN, the impact of injecting OOD text is most pronounced in the MT task. Under UT&#8209;OOD (row 21), the model&#8217;s MT performance barely improves over the base unified setting and remains far below the CTS&#8209;matched variant&#8212;rising only to 11.1 BLEU compared with 15.7 BLEU for UT&#8209;CTS (row 22). In contrast, UT&#8209;CTS consistently lifts MT and MMT performance by several BLEU points and slightly improves ASR quality. Together, these findings imply that substituting in&#8209;domain transcripts with heterogeneous text corpora does not generalize well in a cross&#8209;modal training regime and can inadvertently weaken the model&#8217;s ability to leverage the unified translation objective.</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "task",
                    "translation",
                    "asr"
                ]
            }
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Hyperparameter configurations used for training.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LoRA Rank</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">200</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LoRA Alpha</td>\n<td class=\"ltx_td ltx_align_center\">400</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LoRA Dropout</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Max Training Steps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td class=\"ltx_td ltx_align_center\">64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gradient Accumulation Steps</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Warmup Steps</td>\n<td class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Learning Rate</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1\\text{e}^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mtext>e</mtext><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\text{e}^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Weight Decay</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"5\\text{e}^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T3.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mtext>e</mtext><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\text{e}^{-4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SpecAug Mask Feature Probability</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">SpecAug Mask Time Probability</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.05</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mask",
            "1​e−51texte5",
            "configurations",
            "size",
            "accumulation",
            "feature",
            "time",
            "rate",
            "probability",
            "max",
            "batch",
            "learning",
            "decay",
            "training",
            "5​e−45texte4",
            "used",
            "value",
            "lora",
            "specaug",
            "alpha",
            "hyperparameter",
            "gradient",
            "warmup",
            "weight",
            "rank",
            "steps",
            "dropout"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.T3\" title=\"Table 3 &#8227; A.2 Hyperparameter Settings &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the hyperparameter configurations used for training our Whisper-UT model.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A multi-task learning paradigm</span> with a stochastic task-selection mechanism to adapt the system across ASR, MT, ST, and multimodal translation tasks using a single set of LoRA parameters;</p>\n\n",
                "matched_terms": [
                    "learning",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is an end-to-end multi-task speech model that adopts a transformer-like encoder-decoder architecture. Its <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version is pre-trained on 680,000 hours of speech data with multiple supervision.\nAs with the original transformer model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib34\" title=\"\">2023</a>)</cite>, the loss function Whisper used at its pre-training time is the cross-entropy objective for all tasks.</p>\n\n",
                "matched_terms": [
                    "time",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of related works <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib20\" title=\"\">2024b</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib42\" title=\"\">2023</a>); Liu and Niehues (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib18\" title=\"\">2024</a>); Le et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib17\" title=\"\">2024</a>)</cite> have also demonstrated that multi-task learning can greatly improve speech translation performance.\nHere, we focus on model fine-tuning and demonstrate that training end-to-end models for either ASR or ST alone improves performance on the other task, enabling fine-tuning with data that was not original annotated for the target domain task.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formulate the learning process with six distinct training objectives, categorized based on the availability of parallel data.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MMT, we introduce an ASR error simulation mechanism to enhance robustness. With probability <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we perturb a batch by replacing the source language tokens, sampled with probability <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, with a similar alternative sampled randomly from the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors in the embeddings space. To explicitly signal perturbed inputs, we prepend a special token to the modified sequence, allowing for the model to dynamically re-weight its reliance on the noisy text prefix and the corresponding audio input at inference time. This aims to simulate real-world noise in transcripts (e.g., ASR errors, omissions), encouraging the model to rely on both modalities for translation.</p>\n\n",
                "matched_terms": [
                    "time",
                    "batch",
                    "probability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, our unified training framework integrates ASR, ST, MMT, and MT into a single multi-task learning process. To achieve this, we first concatenate both speech-text and text-only datasets, allowing for random sampling within each batch. For every batch, we compute the ASR loss, which corresponds to the source language modeling loss when dealing with text-only samples. The ASR and ST loss weights are dynamically balanced by sampling a weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from a Beta distribution. Next, we stochastically determine whether the batch follows the ST/TLM objective or the MMT/MT objective. If the batch is selected for MMT training, ASR error simulation is applied with a certain probability to mimic transcription imperfections and enhance robustness. By combining these components, Whisper-UT serves as a unified model for ASR, ST, MT, and MMT, leveraging both textual and speech inputs efficiently.</p>\n\n",
                "matched_terms": [
                    "probability",
                    "batch",
                    "learning",
                    "training",
                    "weight"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced Whisper-UT, a unified translation framework that integrates ASR, ST, MT, and MMT within a single multi-task learning paradigm. In addition to this unified framework, we propose an explicit modeling approach for speech translation that conditions on both speech signals and textual prompts, effectively leveraging ASR hypotheses or ground-truth transcripts. Our training strategy, incorporating stochastic task selection and modality-aware error simulation, ensures effective multi-task learning while mitigating catastrophic forgetting. Experimental results show that Whisper-UT achieves strong performance across various translation tasks, demonstrating the benefits of cross-task synergy. Future work will explore scaling to more languages and extending to broader multi-modal scenarios.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our approach demonstrates strong improvements, several limitations remain. To ensure fair comparisons, we kept training steps consistent across models, meaning our best-performing system may not have reached its full potential with extended training.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training of machine learning models is a costly, energy-intensive process, so our method, which introduces a novel means of efficiently adapting existing large pre-trained models to new tasks, may mitigate the ethical concerns about the costs, financial, environmental, or other, associated with training ML models. Furthermore, the success of our approach, specifically cross-task fine-tuning, implies that speech translation systems can be more easily trained for new domains, including languages with limited training resources.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Rank Adaptation (LoRA).</span> LoRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib12\" title=\"\">2021</a>)</cite> introduces a trainable adapter comprised of rank decomposition matrices on top of the fixed pre-trained model&#8217;s weight matrices in specified layers so that the number of trainable parameters can be considerably reduced.</p>\n\n",
                "matched_terms": [
                    "rank",
                    "lora",
                    "weight"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the conventional speed perturbation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib14\" title=\"\">2015</a>)</cite> with parameters 0.9, 1.0, 1.1 to the speech prior to the training stage. Additionally, we adopt SpecAug&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Park et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib22\" title=\"\">2019</a>)</cite> to randomly mask extracted speech features during training.</p>\n\n",
                "matched_terms": [
                    "mask",
                    "training",
                    "specaug"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CTS corpora usually consist of short utterances segmented from a full recording, reflecting the alternating speech of participants during conversations. However, we found empirically that fine-tuning on such segments, presumably due to a mismatch in sample lengths compared to Whisper&#8217;s pre-training data, leads to significant performance degradation. The resulting model tends to repetitively produce frequent filler words in the training corpus at inference time regardless of the input. Therefore, we re-segmented the utterances by merging them chronologically, with durations (in seconds) sampled from a Gaussian distribution, e.g. <math alttext=\"\\mathcal{N}(15,5^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>15</mn><mo>,</mo><msup><mn>5</mn><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(15,5^{2})</annotation></semantics></math>. As Whisper&#8217;s feature extractor automatically pads the features up to 30 seconds, such re-segmentation also significantly reduced the training cost in terms of memory and time.</p>\n\n",
                "matched_terms": [
                    "time",
                    "feature",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In rows 6 and 17, we conduct straightforward multi-task fine-tuning experiments by duplicating the speech dataset with both ASR and ST supervision, concatenating the datasets, and employing random sampling within each batch. These experiments confirm that multi-task training is beneficial, as it enhances BLEU score from 61.2 to 62.2 and WER is reduced from 20.3 to 16.3 on the Fisher-Spanish corpus. A similar trend is observed on the BBN set as well. This suggests that jointly optimizing multiple relevant objectives allows the model to better capture linguistic patterns and improve generalization across tasks.</p>\n\n",
                "matched_terms": [
                    "batch",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The OOD sets contrast sharply with CTS data in domain (e.g., formal talks vs. casual dialogues) and lexical style. To isolate the effect of data domain (not scale), we match the total training steps to our baseline CTS experiments, ensuring comparable optimization cycles. This setup tests whether cross-modal alignment generalizes to heterogeneous text distributions.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we also present a rough sketch of the training data amounts for our model and the compared methods, as summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T6\" title=\"Table 6 &#8227; D.5.1 Dataset Setup &#8227; D.5 Impact of Out-of-Domain Text Data &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. However, it is important to note that due to differences in training methodologies, stages, and the unavailability of precise details for some systems, this comparison should be interpreted with caution and may contain ambiguities. We encourage readers to consult the original publications for more accurate and comprehensive descriptions of the training data used in each model.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A2.T4": {
        "caption": "Table 4: Code-switching example with system outputs.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">REF-ASR:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">&#30005;&#33041;&#30340; MASTER &#24212;&#35813;&#26159;&#24456; POPULAR &#23601;&#23545;&#20102;&#24456;&#24212;&#35813;&#24456;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">HYP-ASR:</span></td>\n<td class=\"ltx_td ltx_align_left\">&#30005;&#33041;&#30340; master &#24212;&#35813;&#26159;&#24456; popular &#23601;&#23545;&#20102;&#24456;&#24212;&#35813;&#24456;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">REF-MT:</span></td>\n<td class=\"ltx_td ltx_align_left\">MASTER degree of computer science it should be very POPULAR it should be</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">HYP-E2E-ST:</span></td>\n<td class=\"ltx_td ltx_align_left\">The computer should be very popular, should be very</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">HYP-2-Stage-ST:</span></td>\n<td class=\"ltx_td ltx_align_left\">The computer&#8217;s master should be very popular that&#8217;s right very should be very</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">HYP-MMT:</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">The computer&#8217;s MASTER should be very popular that&#8217;s right very should be very</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "hype2est",
            "popular",
            "refmt",
            "example",
            "very",
            "master",
            "电脑的",
            "refasr",
            "science",
            "system",
            "hypasr",
            "that’s",
            "computer’s",
            "就对了很应该很",
            "degree",
            "应该是很",
            "hypmmt",
            "hyp2stagest",
            "codeswitching",
            "right",
            "outputs",
            "computer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The code-switching example presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A2.T4\" title=\"Table 4 &#8227; B.1 Pre-processing &#8227; Appendix B CTS Data Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> demonstrates two critical insights:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Modal Faithfulness:</span> While the reference MT (REF-MT) omits the final &#8220;&#24456;&#8221; (translated as \"very\") from the source utterance &#8220;&#24456;&#24212;&#35813;&#24456;&#8221;, our ASR transcript preserves all repetitions. This discrepancy highlights how audio-derived prosodic cues (e.g., emphatic stress on the final &#8220;&#24456;&#8221;) enable 2Stage-ST and MMT to retain pragmatic emphasis (&#8220;&#8230;that&#8217;s right very should be very&#8221;) where text-only MT truncates for conciseness. By aligning acoustic signals (stress patterns) with textual redundancy, our framework distinguishes intentional repetition&#8212;a discourse marker of conviction in Mandarin&#8212;from superficial noise, demonstrating superior faithfulness to both linguistic content and pragmatic intent compared to E2E ST pipelines.</p>\n\n",
                "matched_terms": [
                    "very",
                    "refmt",
                    "right"
                ]
            }
        ]
    },
    "A4.T5": {
        "caption": "Table 5: \nAblation studies on the CTS test sets. The Objective column specifies under which training objective the model system is fine-tuned. The UT objective refers to the unified-translation objective described in section 3.5. The Task column specifies the target inference task. E2E-ST refers to the promptless E2E speech translation setting, MMT refers to the translation process that conditions on both the ground-truth transcript and the speech signals, while 2-Stage-ST refers to the MMT process which conditions on the model’s own ASR hypotheses.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">Task</span> (num_beams = 1)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">E2E-ST</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MMT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2-Stage-ST</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">(BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Fisher</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NLLB-1.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"48.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m6\" intent=\":literal\"><semantics><mn>48.3</mn><annotation encoding=\"application/x-tex\">48.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{67.3}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m7\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">67.3</mn><annotation encoding=\"application/x-tex\">\\mathbf{67.3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"26.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m8\" intent=\":literal\"><semantics><mn>26.7</mn><annotation encoding=\"application/x-tex\">26.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"51.6\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m9\" intent=\":literal\"><semantics><mn>51.6</mn><annotation encoding=\"application/x-tex\">51.6</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m10\" intent=\":literal\"><semantics><mn>19.1</mn><annotation encoding=\"application/x-tex\">19.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"54.9\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m11\" intent=\":literal\"><semantics><mn>54.9</mn><annotation encoding=\"application/x-tex\">54.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ST</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"20.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m12\" intent=\":literal\"><semantics><mn>20.3</mn><annotation encoding=\"application/x-tex\">20.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"61.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m13\" intent=\":literal\"><semantics><mn>61.2</mn><annotation encoding=\"application/x-tex\">61.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR + ST</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m14\" intent=\":literal\"><semantics><mn>16.3</mn><annotation encoding=\"application/x-tex\">16.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{62.2}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m15\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">62.2</mn><annotation encoding=\"application/x-tex\">\\mathbf{62.2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"60.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m16\" intent=\":literal\"><semantics><mn>60.3</mn><annotation encoding=\"application/x-tex\">60.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"51.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m17\" intent=\":literal\"><semantics><mn>51.0</mn><annotation encoding=\"application/x-tex\">51.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"63.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m18\" intent=\":literal\"><semantics><mn>63.4</mn><annotation encoding=\"application/x-tex\">63.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"61.1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m19\" intent=\":literal\"><semantics><mn>61.1</mn><annotation encoding=\"application/x-tex\">61.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"52.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m20\" intent=\":literal\"><semantics><mn>52.4</mn><annotation encoding=\"application/x-tex\">52.4</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR + ST + MT + LM</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{16.0}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m21\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">16.0</mn><annotation encoding=\"application/x-tex\">\\mathbf{16.0}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"61.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m22\" intent=\":literal\"><semantics><mn>61.7</mn><annotation encoding=\"application/x-tex\">61.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"55.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m23\" intent=\":literal\"><semantics><mn>55.2</mn><annotation encoding=\"application/x-tex\">55.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MMT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m24\" intent=\":literal\"><semantics><mn>16.4</mn><annotation encoding=\"application/x-tex\">16.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"57.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m25\" intent=\":literal\"><semantics><mn>57.4</mn><annotation encoding=\"application/x-tex\">57.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"1.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m26\" intent=\":literal\"><semantics><mn>1.4</mn><annotation encoding=\"application/x-tex\">1.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"67.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m27\" intent=\":literal\"><semantics><mn>67.5</mn><annotation encoding=\"application/x-tex\">67.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"58.6\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m28\" intent=\":literal\"><semantics><mn>58.6</mn><annotation encoding=\"application/x-tex\">58.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UT-OOD</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{16.0}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m29\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">16.0</mn><annotation encoding=\"application/x-tex\">\\mathbf{16.0}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{61.5}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m30\" intent=\":literal\"><semantics><mn>61.5</mn><annotation encoding=\"application/x-tex\">{61.5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"44.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m31\" intent=\":literal\"><semantics><mn>44.2</mn><annotation encoding=\"application/x-tex\">44.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"70.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m32\" intent=\":literal\"><semantics><mn>70.0</mn><annotation encoding=\"application/x-tex\">70.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"61.6\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m33\" intent=\":literal\"><semantics><mn>61.6</mn><annotation encoding=\"application/x-tex\">61.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">11</td>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UT-CTS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m34\" intent=\":literal\"><semantics><mn>16.3</mn><annotation encoding=\"application/x-tex\">16.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"62.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m35\" intent=\":literal\"><semantics><mn>62.0</mn><annotation encoding=\"application/x-tex\">62.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"55.9\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m36\" intent=\":literal\"><semantics><mn>55.9</mn><annotation encoding=\"application/x-tex\">55.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{70.4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m37\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">70.4</mn><annotation encoding=\"application/x-tex\">\\mathbf{70.4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{62.1}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m38\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">62.1</mn><annotation encoding=\"application/x-tex\">\\mathbf{62.1}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"11\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">BBN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">NLLB-1.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m39\" intent=\":literal\"><semantics><mn>8.7</mn><annotation encoding=\"application/x-tex\">8.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{22.7}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m40\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">22.7</mn><annotation encoding=\"application/x-tex\">\\mathbf{22.7}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"8\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"32.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m41\" intent=\":literal\"><semantics><mn>32.2</mn><annotation encoding=\"application/x-tex\">32.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"13.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m42\" intent=\":literal\"><semantics><mn>13.0</mn><annotation encoding=\"application/x-tex\">13.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"18.9\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m43\" intent=\":literal\"><semantics><mn>18.9</mn><annotation encoding=\"application/x-tex\">18.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m44\" intent=\":literal\"><semantics><mn>16.2</mn><annotation encoding=\"application/x-tex\">16.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ST</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"23.1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m45\" intent=\":literal\"><semantics><mn>23.1</mn><annotation encoding=\"application/x-tex\">23.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m46\" intent=\":literal\"><semantics><mn>16.8</mn><annotation encoding=\"application/x-tex\">16.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR + ST</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"18.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m47\" intent=\":literal\"><semantics><mn>18.5</mn><annotation encoding=\"application/x-tex\">18.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"20.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m48\" intent=\":literal\"><semantics><mn>20.2</mn><annotation encoding=\"application/x-tex\">20.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"37.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m49\" intent=\":literal\"><semantics><mn>37.7</mn><annotation encoding=\"application/x-tex\">37.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"12.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m50\" intent=\":literal\"><semantics><mn>12.7</mn><annotation encoding=\"application/x-tex\">12.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m51\" intent=\":literal\"><semantics><mn>16.0</mn><annotation encoding=\"application/x-tex\">16.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"20.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m52\" intent=\":literal\"><semantics><mn>20.4</mn><annotation encoding=\"application/x-tex\">20.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"15.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m53\" intent=\":literal\"><semantics><mn>15.5</mn><annotation encoding=\"application/x-tex\">15.5</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ASR + ST + MT + LM</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"17.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m54\" intent=\":literal\"><semantics><mn>17.7</mn><annotation encoding=\"application/x-tex\">17.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"20.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m55\" intent=\":literal\"><semantics><mn>20.4</mn><annotation encoding=\"application/x-tex\">20.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"14.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m56\" intent=\":literal\"><semantics><mn>14.8</mn><annotation encoding=\"application/x-tex\">14.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MMT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"17.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m57\" intent=\":literal\"><semantics><mn>17.5</mn><annotation encoding=\"application/x-tex\">17.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m58\" intent=\":literal\"><semantics><mn>19.5</mn><annotation encoding=\"application/x-tex\">19.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m59\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"25.2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m60\" intent=\":literal\"><semantics><mn>25.2</mn><annotation encoding=\"application/x-tex\">25.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"20.6\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m61\" intent=\":literal\"><semantics><mn>20.6</mn><annotation encoding=\"application/x-tex\">20.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UT-OOD</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"17.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m62\" intent=\":literal\"><semantics><mn>17.5</mn><annotation encoding=\"application/x-tex\">17.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{20.6}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m63\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">20.6</mn><annotation encoding=\"application/x-tex\">\\mathbf{20.6}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m64\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"25.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m65\" intent=\":literal\"><semantics><mn>25.3</mn><annotation encoding=\"application/x-tex\">25.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"21.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m66\" intent=\":literal\"><semantics><mn>21.5</mn><annotation encoding=\"application/x-tex\">21.5</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">22</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UT-CTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{17.4}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m67\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">17.4</mn><annotation encoding=\"application/x-tex\">\\mathbf{17.4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m68\" intent=\":literal\"><semantics><mn>19.8</mn><annotation encoding=\"application/x-tex\">19.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"15.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m69\" intent=\":literal\"><semantics><mn>15.7</mn><annotation encoding=\"application/x-tex\">15.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{26.0}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m70\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">26.0</mn><annotation encoding=\"application/x-tex\">\\mathbf{26.0}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{21.6}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T5.m71\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">21.6</mn><annotation encoding=\"application/x-tex\">\\mathbf{21.6}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mmt",
            "wer↓downarrow",
            "model’s",
            "while",
            "column",
            "finetuned",
            "160mathbf160",
            "which",
            "setting",
            "utcts",
            "e2est",
            "260mathbf260",
            "704mathbf704",
            "inference",
            "signals",
            "e2e",
            "hypotheses",
            "nllb13b",
            "promptless",
            "bbn",
            "translation",
            "studies",
            "groundtruth",
            "ablation",
            "objective",
            "numbeams",
            "target",
            "training",
            "test",
            "system",
            "unifiedtranslation",
            "described",
            "conditions",
            "bleu↑uparrow",
            "utood",
            "622mathbf622",
            "673mathbf673",
            "sets",
            "174mathbf174",
            "227mathbf227",
            "under",
            "cts",
            "both",
            "transcript",
            "speech",
            "asr",
            "216mathbf216",
            "206mathbf206",
            "own",
            "fisher",
            "process",
            "task",
            "2stagest",
            "none",
            "model",
            "621mathbf621",
            "whisper",
            "specifies",
            "refers",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct ablation experiments presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T5\" title=\"Table 5 &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> on the two CTS datasets (Fisher-Spanish and BBN-Mandarin), as their domain-specific challenges&#8212;disfluencies, code-switching, and spontaneous dialogue&#8212;diverged significantly from Whisper&#8217;s pretraining data. This allows us to isolate our framework&#8217;s adaptability beyond pretraining biases and quantify its efficacy in resource-constrained, real-world scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "translation",
                    "process",
                    "task",
                    "groundtruth",
                    "model",
                    "which",
                    "speech",
                    "both",
                    "asr",
                    "system",
                    "mmt",
                    "whisper",
                    "hypotheses"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Whisper-UT: A Unified Translation Framework for Speech and Text</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The task of speech-to-text translation (ST) encompasses converting spoken content from one language to another, aiming to overcome language barriers to communication. Traditionally, the task involves an automatic speech recognition (ASR) module to transcribe spoken words, followed by a machine translation (MT) module to convert the transcribed text into the target language in a cascaded manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ney (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib21\" title=\"\">1999</a>)</cite>. The recent development of end-to-end neural architectures and large pre-trained models have substantially propelled advancements in downstream speech tasks, via either self-supervised learning (SSL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib1\" title=\"\">2020</a>); Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib11\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib4\" title=\"\">2022</a>)</cite> or fully supervised learning. Among the pre-trained acoustic models, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib27\" title=\"\">2022</a>)</cite>, a transformer-based encoder-decoder multi-task model trained with large-scale data in a supervised manner, has exhibited good performance on various ST corpora.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "model",
                    "target",
                    "whisper",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, in real-world scenarios, input modalities and data conditions vary widely. In offline settings, for instance, translating conversational or dialectal speech&#8212;characterized by disfluencies, code-switching, and noisy acoustic environments&#8212;poses significant challenges to end-to-end models, often resulting in degraded performance. Conversely, scenarios like business meetings or translated media archives frequently provide both source-language speech and (manual or ASR-generated) transcripts. Yet existing systems fail to exploit this multi-modal synergy.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "speech",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we systematically investigate how multi-task encoder-decoder models&#8212;using Whisper as a representative case study&#8212;can be efficiently adapted to these heterogeneous scenarios. First, we examine fine-tuning strategies for conventional ST (using 3-way parallel speech-transcript-translation data), speech-to-text tasks (ASR-only data), and MT, while also methods for multi-modal translation where both speech and transcripts are available. Our analysis reveals two key insights:</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation",
                    "whisper",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Cross-task training induces synergistic benefits</span>&#8212;fine-tuning on in-domain ASR data improves ST performance, while ST training conversely enhances ASR accuracy, suggesting mutual reinforcement between the ASR and ST tasks even without 3-way parallel data;</p>\n\n",
                "matched_terms": [
                    "while",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-modal inputs (speech + text) consistently enhance translation quality when fused</span>, even with imperfect ASR transcripts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on these findings, we propose <span class=\"ltx_text ltx_font_bold\">Whisper</span> for <span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">T</span>ranslation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We open source our code at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BorrisonXiao/Whisper-UT\" title=\"\">https://github.com/BorrisonXiao/Whisper-UT</a>.</span></span></span>, or <span class=\"ltx_text ltx_font_bold\">Whisper-UT</span>, a framework that transforms Whisper&#8217;s decoder into a unified conditional generation model, capable of dynamically conditioning on speech, text, or both modalities. The framework repurposes Whisper&#8217;s encoder-decoder architecture as a versatile multi-modal interface through two innovations:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whisper",
                    "model",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A multi-task learning paradigm</span> with a stochastic task-selection mechanism to adapt the system across ASR, MT, ST, and multimodal translation tasks using a single set of LoRA parameters;</p>\n\n",
                "matched_terms": [
                    "translation",
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A two-stage decoding strategy</span>, where the decoder first generates an ASR transcript from speech, then reuses it as context for translation, perhaps emulating human thought processes, even when a transcript is not provided.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcript",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments on CoVoST2&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite> French-English (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>) and German-English (<span class=\"ltx_text ltx_font_typewriter\">de-en</span>) subsets demonstrate strong performance. Extended evaluations on conversational telephone speech (CTS) corpora&#8212;Fisher-CallHome Spanish&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib26\" title=\"\">2013</a>)</cite>, and BBN Mandarin-English&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wotherspoon et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib39\" title=\"\">2024</a>)</cite> further confirm the robustness of our approach across diverse domains. Notably, Whisper-UT outperforms the 1.3B-parameter NLLB model in multi-modal settings (speech + ground-truth text) and achieves superior speech-only translation via hypothesis prompting.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "translation",
                    "groundtruth",
                    "model",
                    "cts",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work highlights the untapped potential of multi-task models in adaptive translation systems. By unifying modality handling and enabling efficient task specialization, Whisper-UT bridges the gap between rigid single-modality systems and the dynamic needs of real-world applications.</p>\n\n",
                "matched_terms": [
                    "task",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is an end-to-end multi-task speech model that adopts a transformer-like encoder-decoder architecture. Its <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version is pre-trained on 680,000 hours of speech data with multiple supervision.\nAs with the original transformer model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib34\" title=\"\">2023</a>)</cite>, the loss function Whisper used at its pre-training time is the cross-entropy objective for all tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective",
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper&#8217;s decoder supports a prompting mechanism, originally designed for better capturing long-range dependencies of the transcripts/translations to resolve local audio ambiguities. Particularly, long utterances are segmented into chunks and the decoder generates its hypothesis for the current segment conditioning on the previous segment&#8217;s transcripts. Inspired by the effectiveness of GPT-like decoder-only models in machine translation, we hypothesize that Whisper&#8217;s decoder, which may be viewed as an audio-conditional language model, is also capable of performing audio-augmented text generation conditioning on <span class=\"ltx_text ltx_font_italic\">both inputs</span>. Our work extends recent work showing that the Whisper can be adapted via fine-tuning to perform a number of novel tasks including, audio-visual speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">Rouditchenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib29\" title=\"\">2024</a>)</cite>, target-speaker ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib10\" title=\"\">2024</a>); Polok et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib25\" title=\"\">2024</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib19\" title=\"\">2024a</a>)</cite>, translation to non-English languages <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib23\" title=\"\">2023</a>)</cite>, by showing that Whisper can be extended to enable multi-modal translation, i.e., using either only text or both text and speech inputs simultaneously.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "which",
                    "both",
                    "asr",
                    "whisper",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent developments in multi-modal and multi-task systems, e.g.,&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib32\" title=\"\">2021</a>)</cite>, are exploring new ways to combine audio and text to improve various language-related tasks. mSLAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bapna et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib2\" title=\"\">2022</a>)</cite>, a multilingual speech and language model, has emerged as a pioneering approach. It aims to construct a shared representation space for both speech and text through joint pre-training on both self-supervised and supervised tasks with various loss objectives, including translation language modeling (TLM) loss for ST.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "both",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib8\" title=\"\">2023</a>)</cite> is another innovative model that further refines the integration of multi-modal inputs for speech and text translation tasks. As a single model designed for ASR, T2T translation, T2S translation, S2T translation and S2S translation, it consists of multiple building blocks to leverage uni-modal data, including a w2v-BERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib7\" title=\"\">2021</a>)</cite> as the speech encoder, a 1.3B NLLB model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib33\" title=\"\">2022</a>)</cite> as the text encoder and decoder, a transformer-based text-to-unit encoder-decoder model for speech, with a vocoder for converting the unit-sequences to waveforms.\nThese systems, along with most existing methods, primarily seek to simply align the representations of the text and speech modalities, limiting the model to still accept only one input modality at a time during inference, which prevents exploitation of <span class=\"ltx_text ltx_font_italic\">cross-modal cues</span>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "which",
                    "inference",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recently, speech&#8209;centric large language models such as QWen&#8209;Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib6\" title=\"\">2024</a>)</cite> have shown that a unified decoder can be fine&#8209;tuned for a broad spectrum of text&#8209;conditioned speech tasks&#8212;including contextual ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib40\" title=\"\">2025</a>)</cite>&#8212;but these approaches rely on massive pretrained text LLMs and demand extensive data and compute during fine&#8209;tuning. This is a gap we aim to fill.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of related works <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib20\" title=\"\">2024b</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib42\" title=\"\">2023</a>); Liu and Niehues (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib18\" title=\"\">2024</a>); Le et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib17\" title=\"\">2024</a>)</cite> have also demonstrated that multi-task learning can greatly improve speech translation performance.\nHere, we focus on model fine-tuning and demonstrate that training end-to-end models for either ASR or ST alone improves performance on the other task, enabling fine-tuning with data that was not original annotated for the target domain task.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "model",
                    "target",
                    "training",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional translation systems treat ST, MT, and ASR as distinct tasks, each requiring separate models or specialized architectures. In this work, we propose a <span class=\"ltx_text ltx_font_bold\">unified translation</span> framework that unifies these tasks under a single encoder-decoder paradigm, treating all forms of language conversion&#8212;including audio-to-text, text-to-text, and multi-modal translation&#8212;as conditional generation tasks. Our approach enables seamless adaptation to various input modalities and data conditions without requiring fundamental architectural changes.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "under",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our method is the insight that ASR can be reformulated as a source-language transcription task, ST as a direct speech-to-text translation task, and MT as a standard text-to-text translation task&#8212;all of which can be expressed as instances of sequence-to-sequence learning. Extending this idea, we introduce a <span class=\"ltx_text ltx_font_bold\">multi-modal translation</span> task, for which the model conditions on both speech and its corresponding transcript (either human-annotated or ASR-generated) to improve translation quality. This formulation generalizes the conventional ST and MT paradigms, leveraging available transcripts to enhance translation in scenarios where speech alone may be ambiguous or error-prone.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "model",
                    "which",
                    "speech",
                    "both",
                    "asr",
                    "transcript",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first provide a formal definition of the multi-modal translation\n(MMT) task, or more precisely, the task of speech-and-text-conditioned translation. Let <math alttext=\"X=(x_{1},x_{2},\\cdots,x_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=(x_{1},x_{2},\\cdots,x_{T})</annotation></semantics></math> denote the speech signal of an utterance, <math alttext=\"Y=(y_{1},y_{2},\\cdots,y_{M})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>y</mi><mi>M</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{1},y_{2},\\cdots,y_{M})</annotation></semantics></math> denote the ground-truth transcript of the utterance, and <math alttext=\"Z=(z_{1},z_{2},\\cdots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\cdots,z_{N})</annotation></semantics></math> denote its corresponding text translation. The goal of the task is then to find the conditional distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>. We hypothesize that often <math alttext=\"H(Z|X,Y)&lt;H(Z|Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H(Z|X,Y)&lt;H(Z|Y)</annotation></semantics></math> in practice, where <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> denotes the information entropy. In other words, the speech signal may contain additional information for a more accurate translation of the utterance, as it may be able to aid resolving ambiguities such as homographs, tonal variations, and omitted content&#8212;such as repetitions and filler words&#8212;that may be present in human-annotated transcripts.</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "translation",
                    "task",
                    "groundtruth",
                    "transcript",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the remarkable performance observed with decoder-only language models in machine translation, we presume that encoder-decoder models&#8217; audio-conditioned decoder possesses the potential for undertaking the audio-conditioned text translation task. In particular, one may prompt the decoder with source language text, generated either by human annotators or any ASR system, in the translation process, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Consequently, the resulting model is trained to learn the distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "process",
                    "task",
                    "model",
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem of speech translation can be directly modeled as <math alttext=\"P\\left(Z|X\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P\\left(Z|X\\right)</annotation></semantics></math> or modeled by marginalizing over an underlying latent variable, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>, representing valid transcripts of the audio <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the summation over <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math> is generally intractable. One common solution, also adopted by cascaded approaches to speech translation, is to approximate the summation with the single highest weight term in the summation, i.e.,</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, cascaded speech translation further assumes that the translation is conditionally independent of the audio given the transcript,</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcript",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">which is practical in that it enables modular training of components, i.e.,</p>\n\n",
                "matched_terms": [
                    "which",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems such as Whisper, however, model the problem without explicitly conditioning on the ASR transcripts, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>. Its single-decoder multi-task paradigm presumably captures a higher-level abstract semantics of the speech signals, such that the ST decoding process is implicitly entangled with the model&#8217;s ASR ability.</p>\n\n",
                "matched_terms": [
                    "model’s",
                    "process",
                    "model",
                    "signals",
                    "whisper",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We seek to combine the modeling advantages of the cascaded and end-to-end systems and generalize the multi-modal translation setting to re-formulate the system&#8217;s speech-only translation process for approximating Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S3.E1\" title=\"In 3.2 Translation with Speech-only Inputs &#8227; 3 Methodology &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Specifically, we relax the conditional independence assumption of cascade approaches, by endowing end-to-end speech translation models with the capacity to also condition on either a ground-truth or hypothesized transcript defined by Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S3.E2\" title=\"In 3.2 Translation with Speech-only Inputs &#8227; 3 Methodology &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, i.e.:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "process",
                    "groundtruth",
                    "setting",
                    "transcript",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we carry out a <span class=\"ltx_text ltx_font_bold\">two-stage decoding</span> process. In the first stage, the model is used to produce the ASR hypotheses, and subsequently, in the second stage, the model conditions on them to generate the translations.</p>\n\n",
                "matched_terms": [
                    "process",
                    "model",
                    "asr",
                    "conditions",
                    "hypotheses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative perspective on this modeling is that it fully leverages the system&#8217;s source-language modeling capability. In end-to-end multi-task models, the decoder can be viewed as implicitly &#8220;partitioned&#8221; into two roles: source-language modeling and target-language generation. While these functions share parameters and benefit from joint optimization, they may still develop distinct competencies. By conditioning translation on both speech and textual transcripts, this approach explicitly harnesses a well-trained source-language model&#8212;potentially even from an external ASR system&#8212;allowing the decoder to generate more accurate translations. This perspective highlights how multi-modal conditioning can serve as a mechanism to refine and reinforce the system&#8217;s understanding of the source language, ultimately improving translation quality.</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation",
                    "both",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Integrating MT functionality into a multi-modal encoder-decoder model presents unique challenges. In conventional encoder-decoder MT systems, the source language text is processed through the encoder, which generates contextual representations for the decoder to cross-attend to. However, oftentimes the pre-trained encoder is designed specifically for processing speech features, making direct text encoding potentially ineffective. Training the encoder to handle text inputs would require a significant amount of additional data and could lead to catastrophic forgetting, where the model loses its ability to process speech effectively.</p>\n\n",
                "matched_terms": [
                    "process",
                    "model",
                    "which",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of decoder-only MT models such as GPT-like systems, we adopt an alternative strategy: instead of modifying the encoder to accommodate text, we encode the source text directly within the decoder, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a). Specifically, we prepend the source text as a prefix to the decoder input, leveraging the self-attention mechanism to implicitly model source-target dependencies. However, implementing this method within an encoder-decoder framework requires careful handling of the cross-attention mechanism. Since the decoder in our system is designed to attend to encoded speech representations, directly bypassing the encoder would disrupt the model&#8217;s expected structure. To address this, we introduce a single learnable vector in the encoder, serving as an indicator that informs the decoder that text input is being processed. The remaining encoder output is padded with zeros, and we modify the cross-attention mask such that the decoder attends only to this learnable embedding. This design ensures that the model&#8217;s architecture remains structurally intact while effectively repurposing the decoder for text-based translation.</p>\n\n",
                "matched_terms": [
                    "model’s",
                    "while",
                    "translation",
                    "model",
                    "system",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To achieve a unified translation framework that encompasses multiple translation paradigms, we propose Whisper-UT, a system designed to handle ASR, ST, MT, and MMT within a single model. Our approach is built on multi-task learning, leveraging 3-way parallel data and text-only MT data to optimize multiple objectives in a stochastic fashion.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "asr",
                    "system",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formulate the learning process with six distinct training objectives, categorized based on the availability of parallel data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-way dataset that provide speech, transcripts, and translations <math alttext=\"\\{X,Y,Z\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{X,Y,Z\\}</annotation></semantics></math>, we define three primary objectives:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Objective.</span> Learning the mapping <math alttext=\"X\\to Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">&#8594;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X\\to Y</annotation></semantics></math>, i.e., predicting the source language transcript from speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective",
                    "transcript",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">E2E-ST Objective.</span> Directly predicting the target language text <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> from speech <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p4.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "e2est",
                    "objective",
                    "target",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MMT Objective.</span> Predicting <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p5.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> while attending to both <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p5.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> (speech) and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p5.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> (source transcript).</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "while",
                    "objective",
                    "transcript",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MMT and MT objectives, we allow gradients to propagate back through the source language tokens, implicitly enhancing the model&#8217;s source language modeling ability.</p>\n\n",
                "matched_terms": [
                    "mmt",
                    "model’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{L}^{CE}_{\\text{asr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>asr</mtext><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{CE}_{\\text{asr}}</annotation></semantics></math> is the ASR loss (or SLM loss for text-only samples), and <math alttext=\"\\mathcal{L}^{CE}_{\\text{st}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>st</mtext><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{CE}_{\\text{st}}</annotation></semantics></math> is either the ST loss or the MMT loss, selected via stochastic task selection.</p>\n\n",
                "matched_terms": [
                    "task",
                    "mmt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ST vs. MMT Objective:</span>\nWith probability <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>, apply standard ST loss; for text-only data, this is equivalent to the TLM loss.\nWith probability <math alttext=\"(1-q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1-q)</annotation></semantics></math>, apply MMT loss, where the decoder cross-attends to both speech features and source text tokens; for text-only data, this becomes the conventional MT loss.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "objective",
                    "mmt",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MMT, we introduce an ASR error simulation mechanism to enhance robustness. With probability <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we perturb a batch by replacing the source language tokens, sampled with probability <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, with a similar alternative sampled randomly from the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors in the embeddings space. To explicitly signal perturbed inputs, we prepend a special token to the modified sequence, allowing for the model to dynamically re-weight its reliance on the noisy text prefix and the corresponding audio input at inference time. This aims to simulate real-world noise in transcripts (e.g., ASR errors, omissions), encouraging the model to rely on both modalities for translation.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "inference",
                    "both",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, our unified training framework integrates ASR, ST, MMT, and MT into a single multi-task learning process. To achieve this, we first concatenate both speech-text and text-only datasets, allowing for random sampling within each batch. For every batch, we compute the ASR loss, which corresponds to the source language modeling loss when dealing with text-only samples. The ASR and ST loss weights are dynamically balanced by sampling a weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from a Beta distribution. Next, we stochastically determine whether the batch follows the ST/TLM objective or the MMT/MT objective. If the batch is selected for MMT training, ASR error simulation is applied with a certain probability to mimic transcription imperfections and enhance robustness. By combining these components, Whisper-UT serves as a unified model for ASR, ST, MT, and MMT, leveraging both textual and speech inputs efficiently.</p>\n\n",
                "matched_terms": [
                    "process",
                    "model",
                    "which",
                    "objective",
                    "speech",
                    "both",
                    "training",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our approach on CoVoST2, a general-domain speech translation benchmark, using its French-English (180 hours) and German-English (119 hours) subsets for training. To assess performance on challenging conversational telephony speech (CTS), we conduct experiments on the Fisher-CallHome Spanish-to-English corpus (186 hours of spontaneous Spanish dialogues) and the BBN Mandarin-to-English corpus (110 hours of Mandarin-English telephony conversations). This setup tests our method&#8217;s adaptability across both general and domain-specific speech, with CTS posing unique challenges such as disfluencies, code-switching, and informal dialogue structures.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "translation",
                    "speech",
                    "cts",
                    "training",
                    "test",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both ASR and ST, we normalize the text by lower-casing all characters and removing all punctuations before computing the metrics. For the Fisher Spanish corpus, the BLEU score is computed with multiple references using the Moses&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib16\" title=\"\">2007</a>)</cite> toolkit as reported in other work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weiss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib37\" title=\"\">2017a</a>)</cite>. The evaluation script used is provided in the code.</p>\n\n",
                "matched_terms": [
                    "fisher",
                    "both",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate our proposed approach, we adopt the <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version of Whisper with 1.6 billion parameters as the base model and fine-tune it for our unified translation modeling.\nTo enable joint training of speech-to-text and text-to-text translation within a single framework, we repurpose the 3-way parallel dataset by strategically replicating its text pairs. Specifically, we create a duplicate of the original dataset where the audio signals are removed, retaining only the source-target text pairs. This allows us to simulate text-only data without introducing external resources, ensuring parity in training scale across objectives.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "model",
                    "signals",
                    "whisper",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents results from directly fine-tuning Whisper, which reveals a cross-task synergy phenomenon: optimizing for one task (e.g., ASR) not only preserves but often enhances performance on another (e.g., ST), as indicated by underlined improvements across both datasets.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports Whisper-UT results on three corpora: CoVoST2 (French <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English, German <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English), Fisher&#8209;Spanish, and BBN&#8209;Mandarin. Across all settings, our proposed Whisper&#8209;UT variants demonstrate consistent improvements in transcription accuracy (WER&#8595;) and translation quality (BLEU&#8593;).</p>\n\n",
                "matched_terms": [
                    "translation",
                    "task",
                    "which",
                    "both",
                    "asr",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveals that fine-tuning on one task does not only improve performance on the target task but also benefits other tasks as well. Notably, ASR fine-tuning enhances ST performance (51.6 to 54.9 on Fisher and 13.0 to 16.2 on BBN), and ST fine-tuning reciprocally benefits ASR (26.7 to 20.3 on Fisher and 32.2 to 23.1 on BBN).\nThis suggests that cross-task fine-tuning may mutually reinforce capabilities without architectural changes, inspiring Whisper-UT&#8217;s unified speech-text framework.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "fisher",
                    "task",
                    "target",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, on CoVoST2, Whisper&#8209;UT reduces WER from 13.4/7.0 (Whisper) to 8.3/5.8. Similar gains appear on Fisher (from 18.8 to 16.3) and BBN (from 32.2 to 17.4). These improvements suggest that our stochastic task-interleaving mechanism effectively mitigates catastrophic forgetting, despite the addition of MT and MMT as new tasks. This stability preserves modality-specific expertise while introducing new tasks and enabling cross-task synergy.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "while",
                    "fisher",
                    "whisper",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-only translation, Whisper-UT&#8212;trained without architectural modifications&#8212;narrowly trails the 1.3B-parameter NLLB model on general-domain CoVoST2 (36.5/26.9 vs. 42.3/31.0 BLEU) but surpasses it by +7.6 and +7.0 BLEU on domain-specific Fisher-Spanish (55.9 vs. 48.3) and BBN-Mandarin (15.7 vs. 8.7) benchmarks, despite using fewer parameters and no dedicated MT pretraining. This divergence highlights two key insights: (1) Whisper&#8217;s decoder inherently functions as a multilingual language model, capable of text-to-text translation with light-touch adaptation, and (2) its cross-lingual transfer capabilities, honed during speech-centric pretraining, generalize robustly to textual MT in low-resource, domain-specific scenarios. Critically, these results validate our hypothesis that minimal modifications&#8212;enabling joint training on speech and text&#8212;can unlock Whisper&#8217;s latent capacity for unified cross-modal translation, bridging the gap between speech and text without sacrificing architectural simplicity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "translation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When translating with access to both speech and ground&#8208;truth transcripts, Whisper&#8209;UT achieves 46.2/40.1 BLEU on CoVoST2, 70.4 BLEU on Fisher&#8209;Spanish, and 26.0 BLEU on BBN&#8209;Mandarin&#8212;surpassing all MT baselines. This substantial improvement underscores the complementary nature of audio and text modalities: acoustic cues (e.g., prosody, emotion, pauses, repetitions) resolve ambiguities in noisy transcripts, while lexical context sharpens alignment of speech-derived semantics. By explicitly modeling these mutually compensatory signals, our unified architecture fuses audio and text modalities, yielding more robust translations when multi-modal information is available.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "both",
                    "while",
                    "signals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the ST setting, Whisper-UT achieves competitive performance with single-pass end-to-end decoding: 40.8/37.7 BLEU on CoVoST2 (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>/<span class=\"ltx_text ltx_font_typewriter\">de-en</span>), 62.0 BLEU on Fisher-Spanish, and 19.8 BLEU on BBN-Mandarin, surpassing QWen2-Audio, SeamlessM4T, and STAC-ST by margins of 2&#8211;8 BLEU points. Crucially, the 2-Stage inference variant yields systematic improvements over promptless decoding: +0.6/+0.4 BLEU on CoVoST2 (41.4/38.1 vs. 40.8/37.7), +0.1 BLEU on Fisher-Spanish (62.1 vs. 62.0), and +1.8 BLEU on BBN-Mandarin (21.6 vs. 19.8). These improvements are amplified in error-prone conditions, reflecting successful mitigation of ASR error propagation&#8212;a key challenge in cascaded systems. By prepending the special token during training (with simulated ASR noise) and inference (for 2-Stage decoding), the model learns to conditionally distrust imperfect transcripts while retaining their partial utility, rebalancing reliance on audio signals to correct latent errors. These consistent incremental gains validate the effectiveness of our two-stage modeling, demonstrating that even imperfect intermediate transcripts enhance translation fidelity through explicit cross-modal grounding when combined with learned distrust mechanisms.</p>\n\n",
                "matched_terms": [
                    "promptless",
                    "while",
                    "translation",
                    "model",
                    "setting",
                    "inference",
                    "signals",
                    "training",
                    "asr",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The unified Whisper-UT framework achieves robust performance across three key tasks: monolingual ASR, text-only machine translation, and speech translation. Improvements are most pronounced in conversational Mandarin and Spanish settings. Moreover, the 2&#8209;Stage decoding strategy provides a reliable way to enhance translation in fully end&#8208;to&#8208;end deployments. Overall, these results highlight Whisper-UT&#8217;s ability to unify cross-modal and cross-lingual speech-text tasks within a single architecture, offering a versatile solution for scenarios requiring joint speech-text modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced Whisper-UT, a unified translation framework that integrates ASR, ST, MT, and MMT within a single multi-task learning paradigm. In addition to this unified framework, we propose an explicit modeling approach for speech translation that conditions on both speech signals and textual prompts, effectively leveraging ASR hypotheses or ground-truth transcripts. Our training strategy, incorporating stochastic task selection and modality-aware error simulation, ensures effective multi-task learning while mitigating catastrophic forgetting. Experimental results show that Whisper-UT achieves strong performance across various translation tasks, demonstrating the benefits of cross-task synergy. Future work will explore scaling to more languages and extending to broader multi-modal scenarios.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "while",
                    "translation",
                    "task",
                    "groundtruth",
                    "speech",
                    "signals",
                    "training",
                    "both",
                    "asr",
                    "mmt",
                    "hypotheses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our approach demonstrates strong improvements, several limitations remain. To ensure fair comparisons, we kept training steps consistent across models, meaning our best-performing system may not have reached its full potential with extended training.</p>\n\n",
                "matched_terms": [
                    "while",
                    "training",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to resource constraints, we fine-tuned Whisper rather than training from scratch, which might limit the full integration of the objectives. Ideally, to demonstrate cross-task fine-tuning, we would start from a pretrained model that natively support each of our tasks, (MT, MMT, ST, ASR), but building state-of-the-art, or close to state-of-the-art systems requires building from existing models, such as Whisper, and adapting to Whisper to additionally perform these tasks, while a contribution in its own right, ultimately requires a two-stage fine-tuning approach that complicates analysis of the effectiveness of cross-task fine-tuning. Furthermore, while we believe our method to be general, i.e., it could be applied to similar models such as the OWSM model <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib24\" title=\"\">2024</a>)</cite>, we have only demonstrated our results using the Whisper model.</p>\n\n",
                "matched_terms": [
                    "while",
                    "own",
                    "finetuned",
                    "model",
                    "which",
                    "training",
                    "asr",
                    "whisper",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training of machine learning models is a costly, energy-intensive process, so our method, which introduces a novel means of efficiently adapting existing large pre-trained models to new tasks, may mitigate the ethical concerns about the costs, financial, environmental, or other, associated with training ML models. Furthermore, the success of our approach, specifically cross-task fine-tuning, implies that speech translation systems can be more easily trained for new domains, including languages with limited training resources.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "process",
                    "which",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.T3\" title=\"Table 3 &#8227; A.2 Hyperparameter Settings &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the hyperparameter configurations used for training our Whisper-UT model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments in this work are conducted with 8 V100-32GB GPUs. However, PEFT methods outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.SS1\" title=\"A.1 Parameter Efficient Fine-tuning &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> render the use of 8 GPUs redundant, yet they are deployed to accelerate the training process.</p>\n\n",
                "matched_terms": [
                    "training",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the conventional speed perturbation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib14\" title=\"\">2015</a>)</cite> with parameters 0.9, 1.0, 1.1 to the speech prior to the training stage. Additionally, we adopt SpecAug&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Park et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib22\" title=\"\">2019</a>)</cite> to randomly mask extracted speech features during training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CTS corpora usually consist of short utterances segmented from a full recording, reflecting the alternating speech of participants during conversations. However, we found empirically that fine-tuning on such segments, presumably due to a mismatch in sample lengths compared to Whisper&#8217;s pre-training data, leads to significant performance degradation. The resulting model tends to repetitively produce frequent filler words in the training corpus at inference time regardless of the input. Therefore, we re-segmented the utterances by merging them chronologically, with durations (in seconds) sampled from a Gaussian distribution, e.g. <math alttext=\"\\mathcal{N}(15,5^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>15</mn><mo>,</mo><msup><mn>5</mn><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(15,5^{2})</annotation></semantics></math>. As Whisper&#8217;s feature extractor automatically pads the features up to 30 seconds, such re-segmentation also significantly reduced the training cost in terms of memory and time.</p>\n\n",
                "matched_terms": [
                    "model",
                    "inference",
                    "cts",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The BBN Mandarin-English conversational telephony speech (CTS) corpus used in our experiments comprises two primary components:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "bbn",
                    "cts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HKUST Mandarin ASR Dataset</span> (90.1 hours): Mandarin conversational speech from telephony interactions, originally designed for ASR research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib9\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CallHome Mandarin ASR Dataset</span> (20.5 hours): Informal Mandarin dialogues curated for ASR study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Canavan and Zipperlen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib3\" title=\"\">1996</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The BBN team&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wotherspoon et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib39\" title=\"\">2024</a>)</cite> translated these into English to create parallel speech-to-text translation pairs. While our experiments utilized a pre-publication version provided directly by the BBN authors, minor discrepancies (e.g., data splits, preprocessing, or translation refinements) may exist compared to the final published version. Nevertheless, the corpus retains its core characteristics: conversational telephony domain focus, code-switching prevalence, and disfluency patterns.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "while",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Preservation of Linguistic Salience:</span> The 2-Stage decoding system successfully retains the code-switched terms &#8220;master&#8221; and &#8220;popular&#8221; (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0% for these tokens), while E2E-ST completely omits &#8220;master&#8221;. This suggests that: 1) direct audio-to-translation mapping struggles with lexical disambiguation of homophones (&#8220;master&#8221; vs. contextually expected &#8220;computer&#8221;), and 2) explicit intermediate ASR provides discrete textual anchors that guide translation decisions.</p>\n\n",
                "matched_terms": [
                    "while",
                    "translation",
                    "e2est",
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Modal Faithfulness:</span> While the reference MT (REF-MT) omits the final &#8220;&#24456;&#8221; (translated as \"very\") from the source utterance &#8220;&#24456;&#24212;&#35813;&#24456;&#8221;, our ASR transcript preserves all repetitions. This discrepancy highlights how audio-derived prosodic cues (e.g., emphatic stress on the final &#8220;&#24456;&#8221;) enable 2Stage-ST and MMT to retain pragmatic emphasis (&#8220;&#8230;that&#8217;s right very should be very&#8221;) where text-only MT truncates for conciseness. By aligning acoustic signals (stress patterns) with textual redundancy, our framework distinguishes intentional repetition&#8212;a discourse marker of conviction in Mandarin&#8212;from superficial noise, demonstrating superior faithfulness to both linguistic content and pragmatic intent compared to E2E ST pipelines.</p>\n\n",
                "matched_terms": [
                    "while",
                    "2stagest",
                    "signals",
                    "both",
                    "transcript",
                    "asr",
                    "mmt",
                    "e2e"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The example validates our hypothesis that two-stage processing particularly benefits scenarios where: 1) ASR can reliably capture linguistically salient content (code-switches, proper nouns), and 2) Audio signals contain complementary paralinguistic information (prosodic boundaries, emphasis) that each modality alone cannot convey. This dual-modality advantage explains 2-Stage-ST&#8217;s performance gain over E2E-ST on BBN-Mandarin despite identical model parameters.</p>\n\n",
                "matched_terms": [
                    "e2est",
                    "model",
                    "signals",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 7 and 17 show the results of the MT-only fine-tuning experiment, demonstrating that the model achieves strong text translation performance even with limited in-domain data&#8212;BLEU 63.4 on Fisher-Spanish and 16.0 on BBN-Mandarin. This outperforms the original NLLB-1.3B model, though it remains modestly behind its fine-tuned counterpart. This suggests that Whisper&#8217;s decoder inherently possesses some text translation capabilities or at least has sufficiently strong source and target language modeling abilities such that minimal adaptation enables it to perform the MT task. Interestingly, this MT training also gives the system MMT ability, as suggested by the 61.1/20.4 (Fisher/BBN) BLEU score, despite MMT being a novel objective that the model was not explicitly trained on. In fact, on the BBN corpus, the MT-trained model exhibits MMT capabilities that surpass its original training objective, achieving a BLEU score of 20.4 (MMT) compared to 16.0 (MT). This finding reinforces our earlier observation of cross-task synergy.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "translation",
                    "finetuned",
                    "task",
                    "model",
                    "objective",
                    "target",
                    "training",
                    "system",
                    "mmt",
                    "nllb13b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In rows 6 and 17, we conduct straightforward multi-task fine-tuning experiments by duplicating the speech dataset with both ASR and ST supervision, concatenating the datasets, and employing random sampling within each batch. These experiments confirm that multi-task training is beneficial, as it enhances BLEU score from 61.2 to 62.2 and WER is reduced from 20.3 to 16.3 on the Fisher-Spanish corpus. A similar trend is observed on the BBN set as well. This suggests that jointly optimizing multiple relevant objectives allows the model to better capture linguistic patterns and improve generalization across tasks.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "model",
                    "both",
                    "training",
                    "asr",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 9 and 20 evaluate MMT-multi-task fine-tuned models, that is, the model is trained with <math alttext=\"q=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">q=0</annotation></semantics></math> and <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">b=0</annotation></semantics></math>. Notably, the MMT inference results outperform even the strong fine-tuned NLLB-1.3B baseline in MT performance, 70.4 vs. 67.4 on Fisher and 26.0 vs. 22.7 on BBN&#8212; demonstrating that MMT provides tangible benefits over traditional cascaded MT approaches.</p>\n\n",
                "matched_terms": [
                    "fisher",
                    "finetuned",
                    "model",
                    "inference",
                    "mmt",
                    "nllb13b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a gap remains between different MMT settings. Specifically, when using the ASR hypothesis as input instead of the ground-truth transcript, i.e., the 2-Stage-ST decoding, performance drops from 67.5 to 58.6 on Fisher and from 25.2 to 20.6 on BBN. While this still exceeds the results from direct ST (52.4 vs. 51.0 on Fisher and 20.6 vs. 19.5 on BBN), the model tends to over-rely on the transcript in the absence of explicit modeling. Specifically, without the special tag to signal potential errors, the model treats the input transcript as fully reliable ground truth&#8212;an assumption that breaks down when using ASR outputs, which may contain recognition errors. These highlight both the effectiveness of explicit modeling and the limitations introduced by ASR errors.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "while",
                    "fisher",
                    "groundtruth",
                    "2stagest",
                    "model",
                    "which",
                    "transcript",
                    "asr",
                    "both",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the UT-trained system (rows 11 and 22) achieves the best MMT and 2-Stage-ST results, with MMT reaching 70.4/26.0 BLEU and 62.1/21.6 BLEU, respectively, on the Fisher-Spanish and BBN-Mandarin corpora, proving the method&#8217;s effectiveness. Applying the error simulation strategy in this training scheme improves the robustness of the two-stage approach, narrowing the performance gap between MMT and 2-Stage-ST decoding. Specifically, on Fisher, the gap decreases from 8.9 to 8.3 BLEU (row 9 vs. 11), and on BBN, from 4.6 to 4.4 BLEU (row 20 vs. 22), indicating more stable performance under ASR-transcript input.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "fisher",
                    "2stagest",
                    "under",
                    "training",
                    "system",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Fisher test set, the 2&#8209;Stage&#8209;ST decoding strategy of the Whisper&#8209;UT model actually falls slightly behind the simpler ASR+ST multi&#8209;task E2E&#8209;ST model. Direct multi&#8209;task training of ASR and ST (row 6) achieves a BLEU of 62.2, whereas conditioning on ASR hypotheses under the unified&#8209;translation objective (row 11, 2&#8209;Stage&#8209;ST) yields 62.1&#8212;a 0.1 BLEU drop. Through manual inspection, we found this gap is driven largely by inconsistent translation of filler words: the same Spanish filler (e.g., &#8220;eh,&#8221; &#8220;um&#8221;) in ASR transcripts is rendered inconsistently in output, magnifying ASR transcript &#8220;errors&#8221; during translation. Moreover, because Whisper&#8217;s ASR and ST performance on Fisher Spanish are both strong already (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 16, BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 60), there is little mismatch for transcript conditioning to resolve, so the transcript signal offers marginal benefit.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "fisher",
                    "model",
                    "objective",
                    "under",
                    "training",
                    "both",
                    "test",
                    "transcript",
                    "asr",
                    "hypotheses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, on the BBN corpus, the UT model demonstrates a clear advantage. The ASR+ST multi&#8209;task E2E&#8209;ST model (row 16) scores 20.2 BLEU, while the Whisper&#8209;UT 2&#8209;Stage&#8209;ST decoder (row 22) jumps to 21.6 BLEU&#8212;a significant 1.4&#8209;point gain. This larger benefit arises because BBN combines relatively low WER (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 18) with much lower translation quality (BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 20), indicating that the model&#8217;s ST ability lags behind its ASR competence. In this scenario, explicitly leveraging ASR transcripts helps fill the performance gap, yielding more accurate translations under the unified objective.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "model’s",
                    "while",
                    "translation",
                    "model",
                    "objective",
                    "under",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the robustness of our unified framework to domain shifts in text data, we replace the in-domain machine translation (MT) pairs (derived from CTS audio transcripts, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.SS3\" title=\"4.3 Training &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) with out-of-domain (OOD) text pairs. Specifically:</p>\n\n",
                "matched_terms": [
                    "described",
                    "cts",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The OOD sets contrast sharply with CTS data in domain (e.g., formal talks vs. casual dialogues) and lexical style. To isolate the effect of data domain (not scale), we match the total training steps to our baseline CTS experiments, ensuring comparable optimization cycles. This setup tests whether cross-modal alignment generalizes to heterogeneous text distributions.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "cts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Injecting out&#8209;of&#8209;domain text under the unified objective appears to have limited benefit and in some cases even disrupted established behaviors. On Fisher, UT&#8209;OOD (row 10) lags behind UT&#8209;CTS across every translation metric&#8212;most notably MT accuracy, which jumps from 44.2 BLEU with OOD data to 55.9 BLEU when text is drawn from the CTS domain. This suggests that the linguistic and stylistic mismatch of web&#8209;mined, TED talk, and parliamentary text fails to reinforce the speech&#8209;to&#8209;text alignment learned on conversational telephone speech, and may inject conflicting patterns that the model struggles to reconcile.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "fisher",
                    "model",
                    "which",
                    "objective",
                    "under",
                    "cts",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A similar story unfolds on BBN. On BBN, the impact of injecting OOD text is most pronounced in the MT task. Under UT&#8209;OOD (row 21), the model&#8217;s MT performance barely improves over the base unified setting and remains far below the CTS&#8209;matched variant&#8212;rising only to 11.1 BLEU compared with 15.7 BLEU for UT&#8209;CTS (row 22). In contrast, UT&#8209;CTS consistently lifts MT and MMT performance by several BLEU points and slightly improves ASR quality. Together, these findings imply that substituting in&#8209;domain transcripts with heterogeneous text corpora does not generalize well in a cross&#8209;modal training regime and can inadvertently weaken the model&#8217;s ability to leverage the unified translation objective.</p>\n\n",
                "matched_terms": [
                    "bbn",
                    "model’s",
                    "translation",
                    "task",
                    "setting",
                    "objective",
                    "under",
                    "training",
                    "asr",
                    "mmt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we also present a rough sketch of the training data amounts for our model and the compared methods, as summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T6\" title=\"Table 6 &#8227; D.5.1 Dataset Setup &#8227; D.5 Impact of Out-of-Domain Text Data &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. However, it is important to note that due to differences in training methodologies, stages, and the unavailability of precise details for some systems, this comparison should be interpreted with caution and may contain ambiguities. We encourage readers to consult the original publications for more accurate and comprehensive descriptions of the training data used in each model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            }
        ]
    },
    "A4.T6": {
        "caption": "Table 6: Comparison of pre-training and fine-tuning data scales for Whisper-UT and baseline models. “3-Way Parallel” refers to datasets with aligned speech, transcripts, and translations. Note that \"N/A\" means some data is used for the specific training, yet the exact amount is not available.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Pre-training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning (hrs)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Speech (hrs)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Text</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ASR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ST</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">(token | sentence)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3-Way</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ASR-only</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ST-only</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Whisper-large-v2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"555\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m1\" intent=\":literal\"><semantics><mn>555</mn><annotation encoding=\"application/x-tex\">555</annotation></semantics></math>k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"126\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m2\" intent=\":literal\"><semantics><mn>126</mn><annotation encoding=\"application/x-tex\">126</annotation></semantics></math>k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m3\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">NLLB-1.3B</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">N/A | <math alttext=\"&gt;40\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>40</mn></mrow><annotation encoding=\"application/x-tex\">&gt;40</annotation></semantics></math>B</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">SeamlessM4T-Large</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"&gt;1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">&gt;1</annotation></semantics></math>M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">N/A | <math alttext=\"&gt;40\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>40</mn></mrow><annotation encoding=\"application/x-tex\">&gt;40</annotation></semantics></math>B</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"&gt;400\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m7\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>400</mn></mrow><annotation encoding=\"application/x-tex\">&gt;400</annotation></semantics></math>k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">STAC-ST</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"206\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m8\" intent=\":literal\"><semantics><mn>206</mn><annotation encoding=\"application/x-tex\">206</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"206\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m9\" intent=\":literal\"><semantics><mn>206</mn><annotation encoding=\"application/x-tex\">206</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Bi-NMT</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"206\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m10\" intent=\":literal\"><semantics><mn>206</mn><annotation encoding=\"application/x-tex\">206</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"206\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m11\" intent=\":literal\"><semantics><mn>206</mn><annotation encoding=\"application/x-tex\">206</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Multi-ST</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"472\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m12\" intent=\":literal\"><semantics><mn>472</mn><annotation encoding=\"application/x-tex\">472</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"472\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m13\" intent=\":literal\"><semantics><mn>472</mn><annotation encoding=\"application/x-tex\">472</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Multi-ASR</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"269\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m14\" intent=\":literal\"><semantics><mn>269</mn><annotation encoding=\"application/x-tex\">269</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"269\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m15\" intent=\":literal\"><semantics><mn>269</mn><annotation encoding=\"application/x-tex\">269</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">QWen2-Audio</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"&gt;5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m16\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">&gt;5</annotation></semantics></math>M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<math alttext=\"2.4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m17\" intent=\":literal\"><semantics><mn>2.4</mn><annotation encoding=\"application/x-tex\">2.4</annotation></semantics></math>T | N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">N/A</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"520\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m18\" intent=\":literal\"><semantics><mn>520</mn><annotation encoding=\"application/x-tex\">520</annotation></semantics></math>k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Whisper-UT (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"555\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m19\" intent=\":literal\"><semantics><mn>555</mn><annotation encoding=\"application/x-tex\">555</annotation></semantics></math>k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"126\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m20\" intent=\":literal\"><semantics><mn>126</mn><annotation encoding=\"application/x-tex\">126</annotation></semantics></math>k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">\n<math alttext=\"680\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m21\" intent=\":literal\"><semantics><mn>680</mn><annotation encoding=\"application/x-tex\">680</annotation></semantics></math>k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"110\\sim 180\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m22\" intent=\":literal\"><semantics><mrow><mn>110</mn><mo>&#8764;</mo><mn>180</mn></mrow><annotation encoding=\"application/x-tex\">110\\sim 180</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"110\\sim 180\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m23\" intent=\":literal\"><semantics><mrow><mn>110</mn><mo>&#8764;</mo><mn>180</mn></mrow><annotation encoding=\"application/x-tex\">110\\sim 180</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "520520k",
            "yet",
            "stonly",
            "seamlessm4tlarge",
            "“3way",
            "55m",
            "amount",
            "not",
            "comparison",
            "nllb13b",
            "11m",
            "400400k",
            "multist",
            "stacst",
            "qwen2audio",
            "680680k",
            "asronly",
            "token",
            "hrs",
            "126126k",
            "pretraining",
            "training",
            "used",
            "exact",
            "text",
            "sentence",
            "110∼180110sim",
            "2424t",
            "4040b",
            "finetuning",
            "parallel”",
            "555555k",
            "whisperut",
            "note",
            "multiasr",
            "asr",
            "speech",
            "available",
            "binmt",
            "datasets",
            "scales",
            "ours",
            "some",
            "means",
            "transcripts",
            "total",
            "model",
            "translations",
            "aligned",
            "whisperlargev2",
            "data",
            "specific",
            "3way",
            "refers",
            "baseline"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Here, we also present a rough sketch of the training data amounts for our model and the compared methods, as summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T6\" title=\"Table 6 &#8227; D.5.1 Dataset Setup &#8227; D.5 Impact of Out-of-Domain Text Data &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. However, it is important to note that due to differences in training methodologies, stages, and the unavailability of precise details for some systems, this comparison should be interpreted with caution and may contain ambiguities. We encourage readers to consult the original publications for more accurate and comprehensive descriptions of the training data used in each model.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "yet",
                    "transcripts",
                    "whisperut",
                    "model",
                    "not",
                    "data",
                    "3way",
                    "asr",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Whisper-UT: A Unified Translation Framework for Speech and Text</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "whisperut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The task of speech-to-text translation (ST) encompasses converting spoken content from one language to another, aiming to overcome language barriers to communication. Traditionally, the task involves an automatic speech recognition (ASR) module to transcribe spoken words, followed by a machine translation (MT) module to convert the transcribed text into the target language in a cascaded manner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ney (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib21\" title=\"\">1999</a>)</cite>. The recent development of end-to-end neural architectures and large pre-trained models have substantially propelled advancements in downstream speech tasks, via either self-supervised learning (SSL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib1\" title=\"\">2020</a>); Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib11\" title=\"\">2021</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib4\" title=\"\">2022</a>)</cite> or fully supervised learning. Among the pre-trained acoustic models, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib27\" title=\"\">2022</a>)</cite>, a transformer-based encoder-decoder multi-task model trained with large-scale data in a supervised manner, has exhibited good performance on various ST corpora.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "data",
                    "asr",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, in real-world scenarios, input modalities and data conditions vary widely. In offline settings, for instance, translating conversational or dialectal speech&#8212;characterized by disfluencies, code-switching, and noisy acoustic environments&#8212;poses significant challenges to end-to-end models, often resulting in degraded performance. Conversely, scenarios like business meetings or translated media archives frequently provide both source-language speech and (manual or ASR-generated) transcripts. Yet existing systems fail to exploit this multi-modal synergy.</p>\n\n",
                "matched_terms": [
                    "models",
                    "yet",
                    "transcripts",
                    "data",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we systematically investigate how multi-task encoder-decoder models&#8212;using Whisper as a representative case study&#8212;can be efficiently adapted to these heterogeneous scenarios. First, we examine fine-tuning strategies for conventional ST (using 3-way parallel speech-transcript-translation data), speech-to-text tasks (ASR-only data), and MT, while also methods for multi-modal translation where both speech and transcripts are available. Our analysis reveals two key insights:</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "transcripts",
                    "asronly",
                    "data",
                    "3way",
                    "speech",
                    "available"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Cross-task training induces synergistic benefits</span>&#8212;fine-tuning on in-domain ASR data improves ST performance, while ST training conversely enhances ASR accuracy, suggesting mutual reinforcement between the ASR and ST tasks even without 3-way parallel data;</p>\n\n",
                "matched_terms": [
                    "data",
                    "3way",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-modal inputs (speech + text) consistently enhance translation quality when fused</span>, even with imperfect ASR transcripts.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "speech",
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on these findings, we propose <span class=\"ltx_text ltx_font_bold\">Whisper</span> for <span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">T</span>ranslation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We open source our code at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BorrisonXiao/Whisper-UT\" title=\"\">https://github.com/BorrisonXiao/Whisper-UT</a>.</span></span></span>, or <span class=\"ltx_text ltx_font_bold\">Whisper-UT</span>, a framework that transforms Whisper&#8217;s decoder into a unified conditional generation model, capable of dynamically conditioning on speech, text, or both modalities. The framework repurposes Whisper&#8217;s encoder-decoder architecture as a versatile multi-modal interface through two innovations:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "whisperut",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">A two-stage decoding strategy</span>, where the decoder first generates an ASR transcript from speech, then reuses it as context for translation, perhaps emulating human thought processes, even when a transcript is not provided.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "not",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Crucially, Whisper-UT requires no architectural modifications&#8212;only fine-tuning&#8212;ensuring compatibility with any encoder-decoder model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisperut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments on CoVoST2&#8217;s&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib36\" title=\"\">2020b</a>)</cite> French-English (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>) and German-English (<span class=\"ltx_text ltx_font_typewriter\">de-en</span>) subsets demonstrate strong performance. Extended evaluations on conversational telephone speech (CTS) corpora&#8212;Fisher-CallHome Spanish&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib26\" title=\"\">2013</a>)</cite>, and BBN Mandarin-English&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wotherspoon et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib39\" title=\"\">2024</a>)</cite> further confirm the robustness of our approach across diverse domains. Notably, Whisper-UT outperforms the 1.3B-parameter NLLB model in multi-modal settings (speech + ground-truth text) and achieves superior speech-only translation via hypothesis prompting.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "whisperut",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work highlights the untapped potential of multi-task models in adaptive translation systems. By unifying modality handling and enabling efficient task specialization, Whisper-UT bridges the gap between rigid single-modality systems and the dynamic needs of real-world applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "whisperut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper is an end-to-end multi-task speech model that adopts a transformer-like encoder-decoder architecture. Its <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version is pre-trained on 680,000 hours of speech data with multiple supervision.\nAs with the original transformer model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib34\" title=\"\">2023</a>)</cite>, the loss function Whisper used at its pre-training time is the cross-entropy objective for all tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining",
                    "data",
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whisper&#8217;s decoder supports a prompting mechanism, originally designed for better capturing long-range dependencies of the transcripts/translations to resolve local audio ambiguities. Particularly, long utterances are segmented into chunks and the decoder generates its hypothesis for the current segment conditioning on the previous segment&#8217;s transcripts. Inspired by the effectiveness of GPT-like decoder-only models in machine translation, we hypothesize that Whisper&#8217;s decoder, which may be viewed as an audio-conditional language model, is also capable of performing audio-augmented text generation conditioning on <span class=\"ltx_text ltx_font_italic\">both inputs</span>. Our work extends recent work showing that the Whisper can be adapted via fine-tuning to perform a number of novel tasks including, audio-visual speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">Rouditchenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib29\" title=\"\">2024</a>)</cite>, target-speaker ASR <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib10\" title=\"\">2024</a>); Polok et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib25\" title=\"\">2024</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib19\" title=\"\">2024a</a>)</cite>, translation to non-English languages <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib23\" title=\"\">2023</a>)</cite>, by showing that Whisper can be extended to enable multi-modal translation, i.e., using either only text or both text and speech inputs simultaneously.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "transcripts",
                    "model",
                    "asr",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent developments in multi-modal and multi-task systems, e.g.,&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Tang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib32\" title=\"\">2021</a>)</cite>, are exploring new ways to combine audio and text to improve various language-related tasks. mSLAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bapna et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib2\" title=\"\">2022</a>)</cite>, a multilingual speech and language model, has emerged as a pioneering approach. It aims to construct a shared representation space for both speech and text through joint pre-training on both self-supervised and supervised tasks with various loss objectives, including translation language modeling (TLM) loss for ST.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "pretraining",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib8\" title=\"\">2023</a>)</cite> is another innovative model that further refines the integration of multi-modal inputs for speech and text translation tasks. As a single model designed for ASR, T2T translation, T2S translation, S2T translation and S2S translation, it consists of multiple building blocks to leverage uni-modal data, including a w2v-BERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib7\" title=\"\">2021</a>)</cite> as the speech encoder, a 1.3B NLLB model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib33\" title=\"\">2022</a>)</cite> as the text encoder and decoder, a transformer-based text-to-unit encoder-decoder model for speech, with a vocoder for converting the unit-sequences to waveforms.\nThese systems, along with most existing methods, primarily seek to simply align the representations of the text and speech modalities, limiting the model to still accept only one input modality at a time during inference, which prevents exploitation of <span class=\"ltx_text ltx_font_italic\">cross-modal cues</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "data",
                    "asr",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More recently, speech&#8209;centric large language models such as QWen&#8209;Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib6\" title=\"\">2024</a>)</cite> have shown that a unified decoder can be fine&#8209;tuned for a broad spectrum of text&#8209;conditioned speech tasks&#8212;including contextual ASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Xiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib40\" title=\"\">2025</a>)</cite>&#8212;but these approaches rely on massive pretrained text LLMs and demand extensive data and compute during fine&#8209;tuning. This is a gap we aim to fill.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "asr",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A number of related works <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib20\" title=\"\">2024b</a>); Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib42\" title=\"\">2023</a>); Liu and Niehues (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib18\" title=\"\">2024</a>); Le et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib17\" title=\"\">2024</a>)</cite> have also demonstrated that multi-task learning can greatly improve speech translation performance.\nHere, we focus on model fine-tuning and demonstrate that training end-to-end models for either ASR or ST alone improves performance on the other task, enabling fine-tuning with data that was not original annotated for the target domain task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "model",
                    "data",
                    "training",
                    "asr",
                    "speech",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional translation systems treat ST, MT, and ASR as distinct tasks, each requiring separate models or specialized architectures. In this work, we propose a <span class=\"ltx_text ltx_font_bold\">unified translation</span> framework that unifies these tasks under a single encoder-decoder paradigm, treating all forms of language conversion&#8212;including audio-to-text, text-to-text, and multi-modal translation&#8212;as conditional generation tasks. Our approach enables seamless adaptation to various input modalities and data conditions without requiring fundamental architectural changes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our method is the insight that ASR can be reformulated as a source-language transcription task, ST as a direct speech-to-text translation task, and MT as a standard text-to-text translation task&#8212;all of which can be expressed as instances of sequence-to-sequence learning. Extending this idea, we introduce a <span class=\"ltx_text ltx_font_bold\">multi-modal translation</span> task, for which the model conditions on both speech and its corresponding transcript (either human-annotated or ASR-generated) to improve translation quality. This formulation generalizes the conventional ST and MT paradigms, leveraging available transcripts to enhance translation in scenarios where speech alone may be ambiguous or error-prone.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "model",
                    "asr",
                    "speech",
                    "available"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first provide a formal definition of the multi-modal translation\n(MMT) task, or more precisely, the task of speech-and-text-conditioned translation. Let <math alttext=\"X=(x_{1},x_{2},\\cdots,x_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=(x_{1},x_{2},\\cdots,x_{T})</annotation></semantics></math> denote the speech signal of an utterance, <math alttext=\"Y=(y_{1},y_{2},\\cdots,y_{M})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>y</mi><mi>M</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=(y_{1},y_{2},\\cdots,y_{M})</annotation></semantics></math> denote the ground-truth transcript of the utterance, and <math alttext=\"Z=(z_{1},z_{2},\\cdots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\cdots,z_{N})</annotation></semantics></math> denote its corresponding text translation. The goal of the task is then to find the conditional distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>. We hypothesize that often <math alttext=\"H(Z|X,Y)&lt;H(Z|Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mi>Y</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H(Z|X,Y)&lt;H(Z|Y)</annotation></semantics></math> in practice, where <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> denotes the information entropy. In other words, the speech signal may contain additional information for a more accurate translation of the utterance, as it may be able to aid resolving ambiguities such as homographs, tonal variations, and omitted content&#8212;such as repetitions and filler words&#8212;that may be present in human-annotated transcripts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcripts",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In light of the remarkable performance observed with decoder-only language models in machine translation, we presume that encoder-decoder models&#8217; audio-conditioned decoder possesses the potential for undertaking the audio-conditioned text translation task. In particular, one may prompt the decoder with source language text, generated either by human annotators or any ASR system, in the translation process, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b). Consequently, the resulting model is trained to learn the distribution <math alttext=\"P(Z|X,Y)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(Z|X,Y)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The problem of speech translation can be directly modeled as <math alttext=\"P\\left(Z|X\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo>(</mo><mrow><mi>Z</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P\\left(Z|X\\right)</annotation></semantics></math> or modeled by marginalizing over an underlying latent variable, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>, representing valid transcripts of the audio <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end systems such as Whisper, however, model the problem without explicitly conditioning on the ASR transcripts, <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>. Its single-decoder multi-task paradigm presumably captures a higher-level abstract semantics of the speech signals, such that the ST decoding process is implicitly entangled with the model&#8217;s ASR ability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcripts",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We seek to combine the modeling advantages of the cascaded and end-to-end systems and generalize the multi-modal translation setting to re-formulate the system&#8217;s speech-only translation process for approximating Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S3.E1\" title=\"In 3.2 Translation with Speech-only Inputs &#8227; 3 Methodology &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Specifically, we relax the conditional independence assumption of cascade approaches, by endowing end-to-end speech translation models with the capacity to also condition on either a ground-truth or hypothesized transcript defined by Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S3.E2\" title=\"In 3.2 Translation with Speech-only Inputs &#8227; 3 Methodology &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, i.e.:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our implementation, we carry out a <span class=\"ltx_text ltx_font_bold\">two-stage decoding</span> process. In the first stage, the model is used to produce the ASR hypotheses, and subsequently, in the second stage, the model conditions on them to generate the translations.</p>\n\n",
                "matched_terms": [
                    "used",
                    "model",
                    "translations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative perspective on this modeling is that it fully leverages the system&#8217;s source-language modeling capability. In end-to-end multi-task models, the decoder can be viewed as implicitly &#8220;partitioned&#8221; into two roles: source-language modeling and target-language generation. While these functions share parameters and benefit from joint optimization, they may still develop distinct competencies. By conditioning translation on both speech and textual transcripts, this approach explicitly harnesses a well-trained source-language model&#8212;potentially even from an external ASR system&#8212;allowing the decoder to generate more accurate translations. This perspective highlights how multi-modal conditioning can serve as a mechanism to refine and reinforce the system&#8217;s understanding of the source language, ultimately improving translation quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "transcripts",
                    "translations",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Integrating MT functionality into a multi-modal encoder-decoder model presents unique challenges. In conventional encoder-decoder MT systems, the source language text is processed through the encoder, which generates contextual representations for the decoder to cross-attend to. However, oftentimes the pre-trained encoder is designed specifically for processing speech features, making direct text encoding potentially ineffective. Training the encoder to handle text inputs would require a significant amount of additional data and could lead to catastrophic forgetting, where the model loses its ability to process speech effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "data",
                    "training",
                    "speech",
                    "amount",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of decoder-only MT models such as GPT-like systems, we adopt an alternative strategy: instead of modifying the encoder to accommodate text, we encode the source text directly within the decoder, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 Multi-modal/-task Speech Systems &#8227; 2 Related Work &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a). Specifically, we prepend the source text as a prefix to the decoder input, leveraging the self-attention mechanism to implicitly model source-target dependencies. However, implementing this method within an encoder-decoder framework requires careful handling of the cross-attention mechanism. Since the decoder in our system is designed to attend to encoded speech representations, directly bypassing the encoder would disrupt the model&#8217;s expected structure. To address this, we introduce a single learnable vector in the encoder, serving as an indicator that informs the decoder that text input is being processed. The remaining encoder output is padded with zeros, and we modify the cross-attention mask such that the decoder attends only to this learnable embedding. This design ensures that the model&#8217;s architecture remains structurally intact while effectively repurposing the decoder for text-based translation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To achieve a unified translation framework that encompasses multiple translation paradigms, we propose Whisper-UT, a system designed to handle ASR, ST, MT, and MMT within a single model. Our approach is built on multi-task learning, leveraging 3-way parallel data and text-only MT data to optimize multiple objectives in a stochastic fashion.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisperut",
                    "data",
                    "3way",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formulate the learning process with six distinct training objectives, categorized based on the availability of parallel data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the 3-way dataset that provide speech, transcripts, and translations <math alttext=\"\\{X,Y,Z\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{X,Y,Z\\}</annotation></semantics></math>, we define three primary objectives:</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "speech",
                    "3way",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ASR Objective.</span> Learning the mapping <math alttext=\"X\\to Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo stretchy=\"false\">&#8594;</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">X\\to Y</annotation></semantics></math>, i.e., predicting the source language transcript from speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">E2E-ST Objective.</span> Directly predicting the target language text <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> from speech <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS1.p4.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since 3-way parallel datasets are scarce in reality, we incorporate text-only MT data <math alttext=\"\\{Y,Z\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{Y,Z\\}</annotation></semantics></math> and define additional objectives:</p>\n\n",
                "matched_terms": [
                    "data",
                    "3way",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source Language Modeling (SLM):</span> Predicting the next source token in <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>, acting as an ASR surrogate for text-only samples.</p>\n\n",
                "matched_terms": [
                    "token",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each batch is sampled from a mixture of the 3-way parallel data and text-only MT data. We define the loss computation as follows:</p>\n\n",
                "matched_terms": [
                    "data",
                    "3way"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ST vs. MMT Objective:</span>\nWith probability <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>, apply standard ST loss; for text-only data, this is equivalent to the TLM loss.\nWith probability <math alttext=\"(1-q)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><mi>q</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1-q)</annotation></semantics></math>, apply MMT loss, where the decoder cross-attends to both speech features and source text tokens; for text-only data, this becomes the conventional MT loss.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For MMT, we introduce an ASR error simulation mechanism to enhance robustness. With probability <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, we perturb a batch by replacing the source language tokens, sampled with probability <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, with a similar alternative sampled randomly from the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors in the embeddings space. To explicitly signal perturbed inputs, we prepend a special token to the modified sequence, allowing for the model to dynamically re-weight its reliance on the noisy text prefix and the corresponding audio input at inference time. This aims to simulate real-world noise in transcripts (e.g., ASR errors, omissions), encouraging the model to rely on both modalities for translation.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "token",
                    "model",
                    "asr",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, our unified training framework integrates ASR, ST, MMT, and MT into a single multi-task learning process. To achieve this, we first concatenate both speech-text and text-only datasets, allowing for random sampling within each batch. For every batch, we compute the ASR loss, which corresponds to the source language modeling loss when dealing with text-only samples. The ASR and ST loss weights are dynamically balanced by sampling a weight <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> from a Beta distribution. Next, we stochastically determine whether the batch follows the ST/TLM objective or the MMT/MT objective. If the batch is selected for MMT training, ASR error simulation is applied with a certain probability to mimic transcription imperfections and enhance robustness. By combining these components, Whisper-UT serves as a unified model for ASR, ST, MT, and MMT, leveraging both textual and speech inputs efficiently.</p>\n\n",
                "matched_terms": [
                    "whisperut",
                    "model",
                    "training",
                    "asr",
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our approach on CoVoST2, a general-domain speech translation benchmark, using its French-English (180 hours) and German-English (119 hours) subsets for training. To assess performance on challenging conversational telephony speech (CTS), we conduct experiments on the Fisher-CallHome Spanish-to-English corpus (186 hours of spontaneous Spanish dialogues) and the BBN Mandarin-to-English corpus (110 hours of Mandarin-English telephony conversations). This setup tests our method&#8217;s adaptability across both general and domain-specific speech, with CTS posing unique challenges such as disfluencies, code-switching, and informal dialogue structures.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both ASR and ST, we normalize the text by lower-casing all characters and removing all punctuations before computing the metrics. For the Fisher Spanish corpus, the BLEU score is computed with multiple references using the Moses&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib16\" title=\"\">2007</a>)</cite> toolkit as reported in other work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Weiss et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib37\" title=\"\">2017a</a>)</cite>. The evaluation script used is provided in the code.</p>\n\n",
                "matched_terms": [
                    "used",
                    "text",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate our proposed approach, we adopt the <span class=\"ltx_text ltx_font_smallcaps\">large-v2</span> version of Whisper with 1.6 billion parameters as the base model and fine-tune it for our unified translation modeling.\nTo enable joint training of speech-to-text and text-to-text translation within a single framework, we repurpose the 3-way parallel dataset by strategically replicating its text pairs. Specifically, we create a duplicate of the original dataset where the audio signals are removed, retaining only the source-target text pairs. This allows us to simulate text-only data without introducing external resources, ensuring parity in training scale across objectives.</p>\n\n",
                "matched_terms": [
                    "model",
                    "data",
                    "3way",
                    "training",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents results from directly fine-tuning Whisper, which reveals a cross-task synergy phenomenon: optimizing for one task (e.g., ASR) not only preserves but often enhances performance on another (e.g., ST), as indicated by underlined improvements across both datasets.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T2\" title=\"Table 2 &#8227; 4.4.1 Overview &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports Whisper-UT results on three corpora: CoVoST2 (French <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English, German <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> English), Fisher&#8209;Spanish, and BBN&#8209;Mandarin. Across all settings, our proposed Whisper&#8209;UT variants demonstrate consistent improvements in transcription accuracy (WER&#8595;) and translation quality (BLEU&#8593;).</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "whisperut",
                    "asr",
                    "not",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Experimental Results &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveals that fine-tuning on one task does not only improve performance on the target task but also benefits other tasks as well. Notably, ASR fine-tuning enhances ST performance (51.6 to 54.9 on Fisher and 13.0 to 16.2 on BBN), and ST fine-tuning reciprocally benefits ASR (26.7 to 20.3 on Fisher and 32.2 to 23.1 on BBN).\nThis suggests that cross-task fine-tuning may mutually reinforce capabilities without architectural changes, inspiring Whisper-UT&#8217;s unified speech-text framework.</p>\n\n",
                "matched_terms": [
                    "not",
                    "finetuning",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-only translation, Whisper-UT&#8212;trained without architectural modifications&#8212;narrowly trails the 1.3B-parameter NLLB model on general-domain CoVoST2 (36.5/26.9 vs. 42.3/31.0 BLEU) but surpasses it by +7.6 and +7.0 BLEU on domain-specific Fisher-Spanish (55.9 vs. 48.3) and BBN-Mandarin (15.7 vs. 8.7) benchmarks, despite using fewer parameters and no dedicated MT pretraining. This divergence highlights two key insights: (1) Whisper&#8217;s decoder inherently functions as a multilingual language model, capable of text-to-text translation with light-touch adaptation, and (2) its cross-lingual transfer capabilities, honed during speech-centric pretraining, generalize robustly to textual MT in low-resource, domain-specific scenarios. Critically, these results validate our hypothesis that minimal modifications&#8212;enabling joint training on speech and text&#8212;can unlock Whisper&#8217;s latent capacity for unified cross-modal translation, bridging the gap between speech and text without sacrificing architectural simplicity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining",
                    "training",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When translating with access to both speech and ground&#8208;truth transcripts, Whisper&#8209;UT achieves 46.2/40.1 BLEU on CoVoST2, 70.4 BLEU on Fisher&#8209;Spanish, and 26.0 BLEU on BBN&#8209;Mandarin&#8212;surpassing all MT baselines. This substantial improvement underscores the complementary nature of audio and text modalities: acoustic cues (e.g., prosody, emotion, pauses, repetitions) resolve ambiguities in noisy transcripts, while lexical context sharpens alignment of speech-derived semantics. By explicitly modeling these mutually compensatory signals, our unified architecture fuses audio and text modalities, yielding more robust translations when multi-modal information is available.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "translations",
                    "speech",
                    "available",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the ST setting, Whisper-UT achieves competitive performance with single-pass end-to-end decoding: 40.8/37.7 BLEU on CoVoST2 (<span class=\"ltx_text ltx_font_typewriter\">fr-en</span>/<span class=\"ltx_text ltx_font_typewriter\">de-en</span>), 62.0 BLEU on Fisher-Spanish, and 19.8 BLEU on BBN-Mandarin, surpassing QWen2-Audio, SeamlessM4T, and STAC-ST by margins of 2&#8211;8 BLEU points. Crucially, the 2-Stage inference variant yields systematic improvements over promptless decoding: +0.6/+0.4 BLEU on CoVoST2 (41.4/38.1 vs. 40.8/37.7), +0.1 BLEU on Fisher-Spanish (62.1 vs. 62.0), and +1.8 BLEU on BBN-Mandarin (21.6 vs. 19.8). These improvements are amplified in error-prone conditions, reflecting successful mitigation of ASR error propagation&#8212;a key challenge in cascaded systems. By prepending the special token during training (with simulated ASR noise) and inference (for 2-Stage decoding), the model learns to conditionally distrust imperfect transcripts while retaining their partial utility, rebalancing reliance on audio signals to correct latent errors. These consistent incremental gains validate the effectiveness of our two-stage modeling, demonstrating that even imperfect intermediate transcripts enhance translation fidelity through explicit cross-modal grounding when combined with learned distrust mechanisms.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "stacst",
                    "transcripts",
                    "token",
                    "whisperut",
                    "model",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The unified Whisper-UT framework achieves robust performance across three key tasks: monolingual ASR, text-only machine translation, and speech translation. Improvements are most pronounced in conversational Mandarin and Spanish settings. Moreover, the 2&#8209;Stage decoding strategy provides a reliable way to enhance translation in fully end&#8208;to&#8208;end deployments. Overall, these results highlight Whisper-UT&#8217;s ability to unify cross-modal and cross-lingual speech-text tasks within a single architecture, offering a versatile solution for scenarios requiring joint speech-text modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whisperut",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced Whisper-UT, a unified translation framework that integrates ASR, ST, MT, and MMT within a single multi-task learning paradigm. In addition to this unified framework, we propose an explicit modeling approach for speech translation that conditions on both speech signals and textual prompts, effectively leveraging ASR hypotheses or ground-truth transcripts. Our training strategy, incorporating stochastic task selection and modality-aware error simulation, ensures effective multi-task learning while mitigating catastrophic forgetting. Experimental results show that Whisper-UT achieves strong performance across various translation tasks, demonstrating the benefits of cross-task synergy. Future work will explore scaling to more languages and extending to broader multi-modal scenarios.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "whisperut",
                    "training",
                    "asr",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our approach demonstrates strong improvements, several limitations remain. To ensure fair comparisons, we kept training steps consistent across models, meaning our best-performing system may not have reached its full potential with extended training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "not",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to resource constraints, we fine-tuned Whisper rather than training from scratch, which might limit the full integration of the objectives. Ideally, to demonstrate cross-task fine-tuning, we would start from a pretrained model that natively support each of our tasks, (MT, MMT, ST, ASR), but building state-of-the-art, or close to state-of-the-art systems requires building from existing models, such as Whisper, and adapting to Whisper to additionally perform these tasks, while a contribution in its own right, ultimately requires a two-stage fine-tuning approach that complicates analysis of the effectiveness of cross-task fine-tuning. Furthermore, while we believe our method to be general, i.e., it could be applied to similar models such as the OWSM model <cite class=\"ltx_cite ltx_citemacro_cite\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib24\" title=\"\">2024</a>)</cite>, we have only demonstrated our results using the Whisper model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "model",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training of machine learning models is a costly, energy-intensive process, so our method, which introduces a novel means of efficiently adapting existing large pre-trained models to new tasks, may mitigate the ethical concerns about the costs, financial, environmental, or other, associated with training ML models. Furthermore, the success of our approach, specifically cross-task fine-tuning, implies that speech translation systems can be more easily trained for new domains, including languages with limited training resources.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuning",
                    "means",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To efficiently adapt the model to these conversational scenarios without overfitting or incurring excessive computational cost, we leverage several parameter-efficient fine-tuning (PEFT) techniques.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero Redundancy Optimizer (ZeRO).</span> ZeRO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Rajbhandari et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib28\" title=\"\">2020</a>)</cite> is an algorithm that partitions data, optimizer states, gradients, and parameters for speeding up the training of large neural models with low communication costs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.T3\" title=\"Table 3 &#8227; A.2 Hyperparameter Settings &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the hyperparameter configurations used for training our Whisper-UT model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "used",
                    "whisperut",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments in this work are conducted with 8 V100-32GB GPUs. However, PEFT methods outlined in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A1.SS1\" title=\"A.1 Parameter Efficient Fine-tuning &#8227; Appendix A Training Detail &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> render the use of 8 GPUs redundant, yet they are deployed to accelerate the training process.</p>\n\n",
                "matched_terms": [
                    "yet",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the conventional speed perturbation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib14\" title=\"\">2015</a>)</cite> with parameters 0.9, 1.0, 1.1 to the speech prior to the training stage. Additionally, we adopt SpecAug&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Park et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib22\" title=\"\">2019</a>)</cite> to randomly mask extracted speech features during training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CTS corpora usually consist of short utterances segmented from a full recording, reflecting the alternating speech of participants during conversations. However, we found empirically that fine-tuning on such segments, presumably due to a mismatch in sample lengths compared to Whisper&#8217;s pre-training data, leads to significant performance degradation. The resulting model tends to repetitively produce frequent filler words in the training corpus at inference time regardless of the input. Therefore, we re-segmented the utterances by merging them chronologically, with durations (in seconds) sampled from a Gaussian distribution, e.g. <math alttext=\"\\mathcal{N}(15,5^{2})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>15</mn><mo>,</mo><msup><mn>5</mn><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(15,5^{2})</annotation></semantics></math>. As Whisper&#8217;s feature extractor automatically pads the features up to 30 seconds, such re-segmentation also significantly reduced the training cost in terms of memory and time.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model",
                    "pretraining",
                    "data",
                    "training",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The BBN Mandarin-English conversational telephony speech (CTS) corpus used in our experiments comprises two primary components:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HKUST Mandarin ASR Dataset</span> (90.1 hours): Mandarin conversational speech from telephony interactions, originally designed for ASR research&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Fung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#bib.bib9\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The example validates our hypothesis that two-stage processing particularly benefits scenarios where: 1) ASR can reliably capture linguistically salient content (code-switches, proper nouns), and 2) Audio signals contain complementary paralinguistic information (prosodic boundaries, emphasis) that each modality alone cannot convey. This dual-modality advantage explains 2-Stage-ST&#8217;s performance gain over E2E-ST on BBN-Mandarin despite identical model parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation experiments presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#A4.T5\" title=\"Table 5 &#8227; Appendix D Ablation Study &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> on the two CTS datasets (Fisher-Spanish and BBN-Mandarin), as their domain-specific challenges&#8212;disfluencies, code-switching, and spontaneous dialogue&#8212;diverged significantly from Whisper&#8217;s pretraining data. This allows us to isolate our framework&#8217;s adaptability beyond pretraining biases and quantify its efficacy in resource-constrained, real-world scenarios.</p>\n\n",
                "matched_terms": [
                    "data",
                    "pretraining",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 7 and 17 show the results of the MT-only fine-tuning experiment, demonstrating that the model achieves strong text translation performance even with limited in-domain data&#8212;BLEU 63.4 on Fisher-Spanish and 16.0 on BBN-Mandarin. This outperforms the original NLLB-1.3B model, though it remains modestly behind its fine-tuned counterpart. This suggests that Whisper&#8217;s decoder inherently possesses some text translation capabilities or at least has sufficiently strong source and target language modeling abilities such that minimal adaptation enables it to perform the MT task. Interestingly, this MT training also gives the system MMT ability, as suggested by the 61.1/20.4 (Fisher/BBN) BLEU score, despite MMT being a novel objective that the model was not explicitly trained on. In fact, on the BBN corpus, the MT-trained model exhibits MMT capabilities that surpass its original training objective, achieving a BLEU score of 20.4 (MMT) compared to 16.0 (MT). This finding reinforces our earlier observation of cross-task synergy.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "some",
                    "model",
                    "not",
                    "training",
                    "text",
                    "nllb13b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In rows 6 and 17, we conduct straightforward multi-task fine-tuning experiments by duplicating the speech dataset with both ASR and ST supervision, concatenating the datasets, and employing random sampling within each batch. These experiments confirm that multi-task training is beneficial, as it enhances BLEU score from 61.2 to 62.2 and WER is reduced from 20.3 to 16.3 on the Fisher-Spanish corpus. A similar trend is observed on the BBN set as well. This suggests that jointly optimizing multiple relevant objectives allows the model to better capture linguistic patterns and improve generalization across tasks.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model",
                    "training",
                    "asr",
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rows 9 and 20 evaluate MMT-multi-task fine-tuned models, that is, the model is trained with <math alttext=\"q=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">q=0</annotation></semantics></math> and <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">b=0</annotation></semantics></math>. Notably, the MMT inference results outperform even the strong fine-tuned NLLB-1.3B baseline in MT performance, 70.4 vs. 67.4 on Fisher and 26.0 vs. 22.7 on BBN&#8212; demonstrating that MMT provides tangible benefits over traditional cascaded MT approaches.</p>\n\n",
                "matched_terms": [
                    "models",
                    "baseline",
                    "model",
                    "nllb13b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a gap remains between different MMT settings. Specifically, when using the ASR hypothesis as input instead of the ground-truth transcript, i.e., the 2-Stage-ST decoding, performance drops from 67.5 to 58.6 on Fisher and from 25.2 to 20.6 on BBN. While this still exceeds the results from direct ST (52.4 vs. 51.0 on Fisher and 20.6 vs. 19.5 on BBN), the model tends to over-rely on the transcript in the absence of explicit modeling. Specifically, without the special tag to signal potential errors, the model treats the input transcript as fully reliable ground truth&#8212;an assumption that breaks down when using ASR outputs, which may contain recognition errors. These highlight both the effectiveness of explicit modeling and the limitations introduced by ASR errors.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the Fisher test set, the 2&#8209;Stage&#8209;ST decoding strategy of the Whisper&#8209;UT model actually falls slightly behind the simpler ASR+ST multi&#8209;task E2E&#8209;ST model. Direct multi&#8209;task training of ASR and ST (row 6) achieves a BLEU of 62.2, whereas conditioning on ASR hypotheses under the unified&#8209;translation objective (row 11, 2&#8209;Stage&#8209;ST) yields 62.1&#8212;a 0.1 BLEU drop. Through manual inspection, we found this gap is driven largely by inconsistent translation of filler words: the same Spanish filler (e.g., &#8220;eh,&#8221; &#8220;um&#8221;) in ASR transcripts is rendered inconsistently in output, magnifying ASR transcript &#8220;errors&#8221; during translation. Moreover, because Whisper&#8217;s ASR and ST performance on Fisher Spanish are both strong already (WER <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 16, BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 60), there is little mismatch for transcript conditioning to resolve, so the transcript signal offers marginal benefit.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "model",
                    "training",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, on the BBN corpus, the UT model demonstrates a clear advantage. The ASR+ST multi&#8209;task E2E&#8209;ST model (row 16) scores 20.2 BLEU, while the Whisper&#8209;UT 2&#8209;Stage&#8209;ST decoder (row 22) jumps to 21.6 BLEU&#8212;a significant 1.4&#8209;point gain. This larger benefit arises because BBN combines relatively low WER (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 18) with much lower translation quality (BLEU <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.SSS2.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 20), indicating that the model&#8217;s ST ability lags behind its ASR competence. In this scenario, explicitly leveraging ASR transcripts helps fill the performance gap, yielding more accurate translations under the unified objective.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "model",
                    "translations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the robustness of our unified framework to domain shifts in text data, we replace the in-domain machine translation (MT) pairs (derived from CTS audio transcripts, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.16375v1#S4.SS3\" title=\"4.3 Training &#8227; 4 Experiments &#8227; Whisper-UT: A Unified Translation Framework for Speech and Text\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>) with out-of-domain (OOD) text pairs. Specifically:</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "data",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The OOD sets contrast sharply with CTS data in domain (e.g., formal talks vs. casual dialogues) and lexical style. To isolate the effect of data domain (not scale), we match the total training steps to our baseline CTS experiments, ensuring comparable optimization cycles. This setup tests whether cross-modal alignment generalizes to heterogeneous text distributions.</p>\n\n",
                "matched_terms": [
                    "total",
                    "not",
                    "data",
                    "training",
                    "baseline",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Injecting out&#8209;of&#8209;domain text under the unified objective appears to have limited benefit and in some cases even disrupted established behaviors. On Fisher, UT&#8209;OOD (row 10) lags behind UT&#8209;CTS across every translation metric&#8212;most notably MT accuracy, which jumps from 44.2 BLEU with OOD data to 55.9 BLEU when text is drawn from the CTS domain. This suggests that the linguistic and stylistic mismatch of web&#8209;mined, TED talk, and parliamentary text fails to reinforce the speech&#8209;to&#8209;text alignment learned on conversational telephone speech, and may inject conflicting patterns that the model struggles to reconcile.</p>\n\n",
                "matched_terms": [
                    "some",
                    "model",
                    "data",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A similar story unfolds on BBN. On BBN, the impact of injecting OOD text is most pronounced in the MT task. Under UT&#8209;OOD (row 21), the model&#8217;s MT performance barely improves over the base unified setting and remains far below the CTS&#8209;matched variant&#8212;rising only to 11.1 BLEU compared with 15.7 BLEU for UT&#8209;CTS (row 22). In contrast, UT&#8209;CTS consistently lifts MT and MMT performance by several BLEU points and slightly improves ASR quality. Together, these findings imply that substituting in&#8209;domain transcripts with heterogeneous text corpora does not generalize well in a cross&#8209;modal training regime and can inadvertently weaken the model&#8217;s ability to leverage the unified translation objective.</p>\n\n",
                "matched_terms": [
                    "transcripts",
                    "not",
                    "training",
                    "asr",
                    "text"
                ]
            }
        ]
    }
}