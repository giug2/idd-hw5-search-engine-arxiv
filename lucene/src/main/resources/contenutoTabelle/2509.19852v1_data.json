{
    "S3.T1": {
        "source_file": "Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance",
        "caption": "Table 1: The results of objective metrics on Seed-TTS-Eval and CV3-EVAL, where “pb” represents “progress bar”.",
        "body": "+ sparse text\n\n\n& pb value",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:0.45pt 5.7pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">+ sparse text</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:0.45pt 5.7pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&amp; pb value</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "bar”",
            "represents",
            "sparse",
            "“progress",
            "cv3eval",
            "value",
            "objective",
            "results",
            "seedttseval",
            "“pb”",
            "where",
            "metrics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we investigated the impact of incorporating </span>\n  <math alttext=\"\\mathcal{L}_{OAS}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{OAS}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into CosyVoice2. The test results of objective metrics, including WER, SIM, and UTMOS, were presented in the upper part of Tab. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The SIM and UTMOS were calculated\nby WavLM </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">4</sup>\n        <span class=\"ltx_tag ltx_tag_note\">4</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-large\" title=\"\">https://huggingface.co/microsoft/wavlm-large</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UTMOS toolkit </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">5</sup>\n        <span class=\"ltx_tag ltx_tag_note\">5</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fakerybakery/utmos\" title=\"\">https://github.com/fakerybakery/utmos</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively.\nFrom the results, it could be observed that manually assigning attention heads to CV_M did not compromise the performance of CosyVoice2.\nFurthermore, for CV2_OAS applying </span>\n  <math alttext=\"\\mathcal{L}_{OAS}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{OAS}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> , the WER of the synthesized speech decreased by 2.1% and 1.6% respectively under the hard text scenarios of Seed-TTS-Eval and CV3-Eval, while under common scenarios, the WER dropped by 0.1% and 0.2% respectively. Meanwhile, the SIM and UTMOS of the speech synthesized by CV2_OAS showed no decrease.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we tested the effectiveness of attention-guided training. As shown in Table.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three models under test were: the full text token guided model, the sparse text token guided model, and the sparse text token &amp; progress bar value guided model.\nIt can be observed that after using the searched attention paths as pseudo-force alignment labels for attention-guided training, the stable hallucination of CosyVoice2 was effectively reduced. Moreover, the sparse text token supervision outperformed the full text token supervision. We calculated the prediction accuracy of text tokens during the training, as shown in Table. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.4 Effectiveness of Attention-Guided Training &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, sparse text tokens can effectively avoid issues such as insufficient accuracy of pseudo-force alignment and ambiguous boundary definition, thereby better facilitating the alignment between text tokens and speech tokens. Additionally, incorporating the supervision of progress bar values into CV_AG can effectively reduce the misalignment between text tokens and speech tokens, thereby further alleviating stability hallucinations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism.\nFirst, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment.\nAdditionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech.\nExperiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsmzzz.github.io/llm_attn\" title=\"\">https://wsmzzz.github.io/llm_attn</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "cv3eval",
                    "value",
                    "seedttseval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we eliminate stability hallucinations in LLM-based TTS models via attention guidance. Notably, our method eliminates the need for forced alignment labels, while effectively improving the stability of LLM-based TTS models on hard text scenarios without compromising speech naturalness.\nThe contributions of this paper are as follows:\n.1) We analyzed the alignment between text tokens and speech tokens in LLMs and proposed the metric termed the Optimal Alignment Score (OAS) to measure alignment quality. Experimental results verify that this proposed metric exhibits a strong correlation with the stability hallucinations of LLM-based TTS models.\n.2) Next, OAS is integrated into the training process of CosyVoice2 to facilitate it in learning continuous, stable alignment between text tokens and speech tokens. Experimental results demonstrate that this approach enables the synthesis of more stable and accurate speech.\n.3) To further mitigate stability hallucinations, we leverage attention values from the pre-trained model as prior knowledge to guide the training of the student CosyVoice2 model via the CoT approach. Specifically, the corresponding text tokens and progress bar values are used as supervision when the student model predicts speech tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Inspired by Glow-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and VITS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Viterbi algorithm is applied to find an optimal alignment path from the aligned regions, maximizing the sum of attention probability along this path.\nThe </span>\n  <math alttext=\"\\boldsymbol{A}\\in\\mathbb{R}^{L_{s}\\times L_{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119912;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{A}\\in\\mathbb{R}^{L_{s}\\times L_{t}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a sub-matrix of the full attention probability matrix, representing the weights assigned by speech tokens to input text tokens, where </span>\n  <math alttext=\"L_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"L_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denote the length of output speech tokens and text tokens to be synthesized, respectively.\nThe calculation of the optimal alignment path </span>\n  <math alttext=\"\\boldsymbol{P}\\in\\mathbb{Z}^{L_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119927;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8484;</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">L</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{P}\\in\\mathbb{Z}^{L_{s}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is shown in Algorithm </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#alg1\" style=\"font-size:90%;\" title=\"Algorithm 1 &#8227; 2.1 Attention analyses based on CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAfter obtaining the optimal alignment path </span>\n  <math alttext=\"\\boldsymbol{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119927;</mi>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{P}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, OAS is calculated as the ratio of the attention probability along the alignment path to the sum of scores within the alignment region, as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Since the OAS calculates the attention probability along the globally optimal alignment path, it can reflect the continuity and monotonicity of the attention between whole text tokens and whole speech tokens. Experimental results in Sec.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Analysis of the OAS metric &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrate a strong correlation between the proposed OAS and word error rate (WER) in synthesized speech (with a correlation coefficient of 0.638).</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To select the alignment heads in CosyVoice2, we calculated the average value of the top 7 OAS for each layer on the Seed-TTS-Eval </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test set, as shown in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.3 Incorporate OAS into the training of CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt can be observed that the average OAS scores of 8-th layer and 9-th layer are significantly higher than those of other layers.\nThus, it can be inferred that alignment heads are primarily distributed in Layers 8 and 9 of CosyVoice2.\nTherefore, we designated half of the attention heads in 8-th layer and 9-th layer of CosyVoice2 as alignment heads.\nTo achieve this, we apply an attention mask that restricts these heads to focus solely on the alignment region between text tokens and speech tokens. The OAS regularization is then applied to these assigned heads.\nSince the attended region is restricted, the denominator in Equation </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.2 Metric for Alignment Quality: OAS &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> becomes a constant. Therefore, the final OAS regularization can be simplified to:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "seedttseval",
                    "value"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The optimal alignment path derived from the head with the highest OAS in teacher CosyVoice2 is employed as the pseudo forced alignment label.\nGiven that pseudo forced alignment labels can only capture the approximate positional correlation between text tokens and speech tokens, this paper proposes two improvements to the text token prediction task under the CoT framework\nFirst, the predicted text tokens are not used as input for the next step, so as to avoid the accumulation of errors caused by inaccurate text token prediction. Second, the text tokens employed as prediction targets are generated in a sparse repetition format, similar to the approach adopted in MegaTTS3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDenote the durations converted from the optimal alignment path and the input text tokens as </span>\n  <math alttext=\"\\boldsymbol{d}=[d_{1},d_{2},...,d_{L_{t}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119941;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{d}=[d_{1},d_{2},...,d_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\boldsymbol{t}=[t_{1},t_{2},...,t_{L_{t}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119957;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{t}=[t_{1},t_{2},...,t_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. Given </span>\n  <math alttext=\"\\boldsymbol{d}=[2,3,3]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119941;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{d}=[2,3,3]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the full repeated target sequence is </span>\n  <math alttext=\"\\boldsymbol{O}_{w}=[t_{1},t_{1},t_{2},t_{2},t_{2},t_{3},t_{3},t_{3}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119926;</mi>\n          <mi mathsize=\"0.900em\">w</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{w}=[t_{1},t_{1},t_{2},t_{2},t_{2},t_{3},t_{3},t_{3}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> while the sparse repeated target sequence is </span>\n  <math alttext=\"\\boldsymbol{O}_{s}=[t_{1},M,M,t_{2},M,M,t_{3},M]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119926;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{s}=[t_{1},M,M,t_{2},M,M,t_{3},M]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, each text token is randomly marked only once in the target sequence, with boundary positions avoided whenever possible.\nIn this way, the issues of pseudo-label inaccuracies and ambiguous boundary classification are addressed, thereby providing more stable supervision for the student CosyVoice2.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "sparse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, hard input texts may contain sentences or phrases with repeated structural patterns. Relying solely on text tokens as the guide when predicting speech tokens is insufficient. Therefore, an additional prediction task for the progress bar value is introduced, which provides absolute positional progress information to help the LLM track how much of the text has been synthesized so far.\nThe progress bar value is denoted as </span>\n  <math alttext=\"\\boldsymbol{p}=[p_{1},p_{2},..,p_{L_{t}}]\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119953;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">p</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">p</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0.0835em\">.</mo>\n          <mo lspace=\"0.0835em\" mathsize=\"0.900em\" rspace=\"0.167em\">.</mo>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">p</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{p}=[p_{1},p_{2},..,p_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "value",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next, </span>\n  <math alttext=\"\\boldsymbol{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119953;</mi>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{p}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is processed with sparse repetition at the valid positions of </span>\n  <math alttext=\"\\boldsymbol{O}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119926;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to generate </span>\n  <math alttext=\"\\boldsymbol{O}_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119926;</mi>\n        <mi mathsize=\"0.900em\">p</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{p}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The loss function for the progress bar value is applied to the unmasked position, incorporating both the L1 loss and the first-order difference loss, which are defined as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "value",
                    "sparse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\hat{p_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{p_{i}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th predicted value of the progress bar value.</span>\n</p>\n\n",
                "matched_terms": [
                    "value",
                    "where",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: WenetSpeech4TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was used for the scratch training of the LLM. WenetSpeech4TTS is an open-source Mandarin dataset spanning multiple domains, containing 12,800 hours of paired text-speech data. Moreover, Seed-TTS-Eval and CV3-Eval </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were used as test sets.\nSpecifically, the hardcase subset of Seed-TTS-Eval and the hard_zh subset of CV3-Eval were used to evaluate CosyVoice2&#8217;s performance in synthesizing speech from hard texts. The meta_zh subset of Seed-TTS-Eval and the zh subset of CV3-Eval, on the other hand, were used to evaluate its performance in synthesizing speech from common texts.</span>\n</p>\n\n",
                "matched_terms": [
                    "cv3eval",
                    "seedttseval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate the soundness and effectiveness of the proposed OAS metric, tests were carried out using the hardcase subset of the Seed-TTS-Eval dataset. A set of 400 hard text sentences was synthesized by CV2, and the WER of the synthesized speech was calculated by Paraformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFollowing this, statistical analysis was performed on the OAS values of all attention heads. To derive the final OAS, the five highest OAS scores across all heads were extracted, and their mean value was computed.\nWe select the top 5 highest-scoring heads because speech-text token alignment depends on the joint participation of multiple heads.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "seedttseval",
                    "value"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The relationship between the computed final OAS values and their corresponding WER values was visualized in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where red dashed line denotes the linear fitting line. Specially, the coefficient between OAS and WER is 0.638. This supports our hypothesis that the alignment between speech and text in LLMs is primarily influenced by several alignment heads with high OAS values.\nThus, the OAS metric can be used to supervise the training of LLMs, thereby enhancing the alignment between text and speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Additionally, the Mean Opinion Score (MOS) test was conducted under the general text input scenario. Specifically, 30 sentences were randomly selected from each dataset for speech synthesis, and 15 native Mandarin speakers were invited to participate in the test. The experimental results are presented in Table.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Analysis of the OAS metric &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. It can be observed that CV_OAS not only reduces stability hallucinations under hard text inputs but also slightly improves subjective speech naturalness. This is attributed to the fact that all improvements to the attention mechanism in CV_OAS are consistent with the task characteristics of TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "results"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance",
        "caption": "Table 2: The mean opinion scores (MOS) with 95% confidence under the common text scenarios",
        "body": "System\n\nMOS\n\n\nSeed-TTS-Eval\nCV3-Eval\n\n\n\n\nGround Truth\n\n4.43 ±\\pm 0.07\n\n\n4.38 ±\\pm 0.07\n\n\n\nCV2\n\n4.13 ±\\pm 0.09\n\n\n4.05 ±\\pm 0.08\n\n\n\nCV2_M\n\n4.17 ±\\pm 0.11\n\n\n3.98 ±\\pm 0.13\n\n\n\nCV2_OAS\n\n4.18 ±\\pm 0.12\n\n\n4.10 ±\\pm 0.10",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">System</span><span class=\"ltx_text\" style=\"font-size:80%;\"/>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">MOS</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Seed-TTS-Eval</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CV3-Eval</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ground Truth</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.43 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.38 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.07</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CV2</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.13 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.05 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CV2_M</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.17 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">3.98 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.13</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CV2_OAS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.18 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.85pt 14.2pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.10 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.10</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "system",
            "cv2m",
            "opinion",
            "confidence",
            "cv2oas",
            "cv3eval",
            "ground",
            "under",
            "cv2",
            "mos",
            "seedttseval",
            "±pm",
            "truth",
            "scenarios",
            "common",
            "scores",
            "mean"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Additionally, the Mean Opinion Score (MOS) test was conducted under the general text input scenario. Specifically, 30 sentences were randomly selected from each dataset for speech synthesis, and 15 native Mandarin speakers were invited to participate in the test. The experimental results are presented in Table.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Analysis of the OAS metric &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. It can be observed that CV_OAS not only reduces stability hallucinations under hard text inputs but also slightly improves subjective speech naturalness. This is attributed to the fact that all improvements to the attention mechanism in CV_OAS are consistent with the task characteristics of TTS.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism.\nFirst, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment.\nAdditionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech.\nExperiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsmzzz.github.io/llm_attn\" title=\"\">https://wsmzzz.github.io/llm_attn</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "cv3eval",
                    "seedttseval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TTS technology aims to generate speech with high expressiveness and naturalness according to text input and other acoustic conditions (e.g., timbre, emotion, style). In recent years,\nwith the rapid progress of speech codec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and generative large model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> technologies, the TTS system has made significant improvements, achieving synthesizing of highly natural, robust and controllable human-like speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In terms of the modeling process from text input to speech representations, TTS large models can be categorized into two types: LLM-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and diffusion-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, LLM-based models typically adopt decoder-only transformer architectures to predict discretized speech tokens via autoregressive or masked language modeling. In contrast, diffusion-based models generate audio features in parallel based on a pre-defined duration.\nIn general, LLM-based TTS models perform better in modeling the temporal sequences and are better suited for multi-modal and multi-task scenarios.\nLLM-based TTS models lack an explicit text-speech alignment process. Instead, they generate speech corresponding to the prompt text via the next-token prediction paradigm. Although large-scale data enable LLM-based TTS models to synthesize stable speech in common text scenarios, they still suffer from stability hallucinations when processing long or hard texts. These issues include generating repetitive, even endless, speech and omitting segments of the intended speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scenarios",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In earlier years, many studies </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on acoustic models attempted to optimize the cross-attention mechanism to improve robustness. They tried to impose forward, single-wise, or monotonic constraints on the text-speech attention learning process to mitigate severe alignment errors. However, these methods are improved based on the cross-attention mechanism between text and speech, making them incompatible with decoder-only LLM-based TTS models.\nRecently, Wang et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Han et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> implemented the hard stepwise monotonic self-attention during LLM-based TTS inference. Yet, the hard attention tends to degrade speech naturalness </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as speech signals are inherently continuous.\nHan et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Lu et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Sheng et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> used the force-alignment labels to help LLM to locate the corresponding text tokens when generating speech tokens.\nAccurate forced alignment labels are difficult to obtain on large-scale datasets, which limits their application scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scenarios"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we eliminate stability hallucinations in LLM-based TTS models via attention guidance. Notably, our method eliminates the need for forced alignment labels, while effectively improving the stability of LLM-based TTS models on hard text scenarios without compromising speech naturalness.\nThe contributions of this paper are as follows:\n.1) We analyzed the alignment between text tokens and speech tokens in LLMs and proposed the metric termed the Optimal Alignment Score (OAS) to measure alignment quality. Experimental results verify that this proposed metric exhibits a strong correlation with the stability hallucinations of LLM-based TTS models.\n.2) Next, OAS is integrated into the training process of CosyVoice2 to facilitate it in learning continuous, stable alignment between text tokens and speech tokens. Experimental results demonstrate that this approach enables the synthesis of more stable and accurate speech.\n.3) To further mitigate stability hallucinations, we leverage attention values from the pre-trained model as prior knowledge to guide the training of the student CosyVoice2 model via the CoT approach. Specifically, the corresponding text tokens and progress bar values are used as supervision when the student model predicts speech tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scenarios"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the alignment between text tokens and speech tokens in LLM-based TTS models, CosyVoice2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used as the backbone to analyze the multi-head self-attention mechanism under the decoder-only architecture. The backbone CosyVoice2 adopts Qwen-0.5B as its initial LLM, which consists of 24 Transformer layers with 14 attention heads per layer.\nWenetSpeech4TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was used for the scratch training of the LLM of CosyVoice2.\nAfter analyzing the attention heads, the following patterns are observed: lower-layer attention heads tend to model global information, higher-layer ones focus on the neighboring regions of output tokens, and middle-layer heads contain specific &#8220;alignment heads&#8221;, as shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In the attention scores of these alignment heads, the weight regions from output speech tokens to input text tokens exhibit global forward alignment paths similar to those in the cross-attention mechanism.\nDetailed visualization analyses are available in the appendix </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsmzzz.github.io/llm_attn/\" title=\"\">https://wsmzzz.github.io/llm_attn/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt is therefore hypothesized that these alignment heads have the most significant impact on the alignment between text tokens and speech tokens, and subsequent improvements and experiments are conducted based on this hypothesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "under",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This section proposes the OAS metric, which is used to evaluate the alignment quality between input text tokens and output speech tokens in attention scores.\nThe OAS metric will serve as a criterion for selecting alignment heads and be used to supervise alignment heads in learning high-quality text-speech alignment.\nDue to the characteristics of TTS tasks, the energy centers of the alignment curves between text tokens and speech tokens are supposed to be continuous and monotonic.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Inspired by Glow-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and VITS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Viterbi algorithm is applied to find an optimal alignment path from the aligned regions, maximizing the sum of attention probability along this path.\nThe </span>\n  <math alttext=\"\\boldsymbol{A}\\in\\mathbb{R}^{L_{s}\\times L_{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119912;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{A}\\in\\mathbb{R}^{L_{s}\\times L_{t}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a sub-matrix of the full attention probability matrix, representing the weights assigned by speech tokens to input text tokens, where </span>\n  <math alttext=\"L_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"L_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denote the length of output speech tokens and text tokens to be synthesized, respectively.\nThe calculation of the optimal alignment path </span>\n  <math alttext=\"\\boldsymbol{P}\\in\\mathbb{Z}^{L_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119927;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8484;</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">L</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{P}\\in\\mathbb{Z}^{L_{s}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is shown in Algorithm </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#alg1\" style=\"font-size:90%;\" title=\"Algorithm 1 &#8227; 2.1 Attention analyses based on CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAfter obtaining the optimal alignment path </span>\n  <math alttext=\"\\boldsymbol{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119927;</mi>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{P}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, OAS is calculated as the ratio of the attention probability along the alignment path to the sum of scores within the alignment region, as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To select the alignment heads in CosyVoice2, we calculated the average value of the top 7 OAS for each layer on the Seed-TTS-Eval </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test set, as shown in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.3 Incorporate OAS into the training of CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt can be observed that the average OAS scores of 8-th layer and 9-th layer are significantly higher than those of other layers.\nThus, it can be inferred that alignment heads are primarily distributed in Layers 8 and 9 of CosyVoice2.\nTherefore, we designated half of the attention heads in 8-th layer and 9-th layer of CosyVoice2 as alignment heads.\nTo achieve this, we apply an attention mask that restricts these heads to focus solely on the alignment region between text tokens and speech tokens. The OAS regularization is then applied to these assigned heads.\nSince the attended region is restricted, the denominator in Equation </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.2 Metric for Alignment Quality: OAS &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> becomes a constant. Therefore, the final OAS regularization can be simplified to:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "seedttseval",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Valle-R </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> incorporates force alignment labels into the prediction targets of LLMs in the form of CoT to strengthen the positional relationship between text tokens and speech tokens.\nHowever, this approach requires forced alignment labels during the alignment process, which is unfavorable for large-scale data training.\nGiven that CosyVoice2 can already synthesize fairly stable speech for common text scenarios, with a WER of less than 4% </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe attempt to use the optimal path extracted from the attention of the pre-trained CosyVoice2 as pseudo forced alignment labels to guide the training of the student CosyVoice2 model, as shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.4 Attention-Guided Training for Student CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scenarios",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The optimal alignment path derived from the head with the highest OAS in teacher CosyVoice2 is employed as the pseudo forced alignment label.\nGiven that pseudo forced alignment labels can only capture the approximate positional correlation between text tokens and speech tokens, this paper proposes two improvements to the text token prediction task under the CoT framework\nFirst, the predicted text tokens are not used as input for the next step, so as to avoid the accumulation of errors caused by inaccurate text token prediction. Second, the text tokens employed as prediction targets are generated in a sparse repetition format, similar to the approach adopted in MegaTTS3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDenote the durations converted from the optimal alignment path and the input text tokens as </span>\n  <math alttext=\"\\boldsymbol{d}=[d_{1},d_{2},...,d_{L_{t}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119941;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{d}=[d_{1},d_{2},...,d_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\boldsymbol{t}=[t_{1},t_{2},...,t_{L_{t}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119957;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{t}=[t_{1},t_{2},...,t_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. Given </span>\n  <math alttext=\"\\boldsymbol{d}=[2,3,3]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119941;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{d}=[2,3,3]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the full repeated target sequence is </span>\n  <math alttext=\"\\boldsymbol{O}_{w}=[t_{1},t_{1},t_{2},t_{2},t_{2},t_{3},t_{3},t_{3}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119926;</mi>\n          <mi mathsize=\"0.900em\">w</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{w}=[t_{1},t_{1},t_{2},t_{2},t_{2},t_{3},t_{3},t_{3}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> while the sparse repeated target sequence is </span>\n  <math alttext=\"\\boldsymbol{O}_{s}=[t_{1},M,M,t_{2},M,M,t_{3},M]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119926;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{s}=[t_{1},M,M,t_{2},M,M,t_{3},M]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, each text token is randomly marked only once in the target sequence, with boundary positions avoided whenever possible.\nIn this way, the issues of pseudo-label inaccuracies and ambiguous boundary classification are addressed, thereby providing more stable supervision for the student CosyVoice2.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: WenetSpeech4TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was used for the scratch training of the LLM. WenetSpeech4TTS is an open-source Mandarin dataset spanning multiple domains, containing 12,800 hours of paired text-speech data. Moreover, Seed-TTS-Eval and CV3-Eval </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were used as test sets.\nSpecifically, the hardcase subset of Seed-TTS-Eval and the hard_zh subset of CV3-Eval were used to evaluate CosyVoice2&#8217;s performance in synthesizing speech from hard texts. The meta_zh subset of Seed-TTS-Eval and the zh subset of CV3-Eval, on the other hand, were used to evaluate its performance in synthesizing speech from common texts.</span>\n</p>\n\n",
                "matched_terms": [
                    "cv3eval",
                    "seedttseval",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Models to be evaluated</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:\n1) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV1/2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The standard CosyVoice1/2 models.\n2) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV2_M</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The CosyVoice2 model with manually designated alignment heads in 8-th layer and 9-th layer.\n3) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV2_OAS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The CosyVoice2 model with </span>\n  <math alttext=\"\\mathcal{L}_{OAS}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{OAS}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applied to the designated attention heads.\n4) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV2_AG</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The student CosyVoice2 model applied the attention-guided training.</span>\n</p>\n\n",
                "matched_terms": [
                    "cv2m",
                    "cv2oas"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate the soundness and effectiveness of the proposed OAS metric, tests were carried out using the hardcase subset of the Seed-TTS-Eval dataset. A set of 400 hard text sentences was synthesized by CV2, and the WER of the synthesized speech was calculated by Paraformer </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFollowing this, statistical analysis was performed on the OAS values of all attention heads. To derive the final OAS, the five highest OAS scores across all heads were extracted, and their mean value was computed.\nWe select the top 5 highest-scoring heads because speech-text token alignment depends on the joint participation of multiple heads.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "cv2",
                    "seedttseval",
                    "scores",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we investigated the impact of incorporating </span>\n  <math alttext=\"\\mathcal{L}_{OAS}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{OAS}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into CosyVoice2. The test results of objective metrics, including WER, SIM, and UTMOS, were presented in the upper part of Tab. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The SIM and UTMOS were calculated\nby WavLM </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">4</sup>\n        <span class=\"ltx_tag ltx_tag_note\">4</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-large\" title=\"\">https://huggingface.co/microsoft/wavlm-large</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UTMOS toolkit </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\">\n    <sup class=\"ltx_note_mark\">5</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">5</sup>\n        <span class=\"ltx_tag ltx_tag_note\">5</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/fakerybakery/utmos\" title=\"\">https://github.com/fakerybakery/utmos</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively.\nFrom the results, it could be observed that manually assigning attention heads to CV_M did not compromise the performance of CosyVoice2.\nFurthermore, for CV2_OAS applying </span>\n  <math alttext=\"\\mathcal{L}_{OAS}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{OAS}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> , the WER of the synthesized speech decreased by 2.1% and 1.6% respectively under the hard text scenarios of Seed-TTS-Eval and CV3-Eval, while under common scenarios, the WER dropped by 0.1% and 0.2% respectively. Meanwhile, the SIM and UTMOS of the speech synthesized by CV2_OAS showed no decrease.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "cv3eval",
                    "under",
                    "seedttseval",
                    "cv2oas",
                    "scenarios",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we tested the effectiveness of attention-guided training. As shown in Table.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three models under test were: the full text token guided model, the sparse text token guided model, and the sparse text token &amp; progress bar value guided model.\nIt can be observed that after using the searched attention paths as pseudo-force alignment labels for attention-guided training, the stable hallucination of CosyVoice2 was effectively reduced. Moreover, the sparse text token supervision outperformed the full text token supervision. We calculated the prediction accuracy of text tokens during the training, as shown in Table. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.4 Effectiveness of Attention-Guided Training &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, sparse text tokens can effectively avoid issues such as insufficient accuracy of pseudo-force alignment and ambiguous boundary definition, thereby better facilitating the alignment between text tokens and speech tokens. Additionally, incorporating the supervision of progress bar values into CV_AG can effectively reduce the misalignment between text tokens and speech tokens, thereby further alleviating stability hallucinations.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper attempts to address stability hallucinations in LLM-based TTS by leveraging the attention mechanism. By introducing the OAS loss and attention-guided training, it effectively reduces stability hallucinations in speech synthesized by CosyVoice2 under hard text scenarios without introducing additional negative effects.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "scenarios",
                    "under"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance",
        "caption": "Table 3: The accuracy of the supervision of text tokens by the CV_AG model on the training and validation sets, where “acc” represents “accuracy”",
        "body": "CV_AG + sparse text",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:0.85pt 14.2pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CV_AG + sparse text</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "validation",
            "sets",
            "represents",
            "cvag",
            "“acc”",
            "sparse",
            "accuracy",
            "supervision",
            "“accuracy”",
            "training",
            "where",
            "model",
            "tokens"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we tested the effectiveness of attention-guided training. As shown in Table.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three models under test were: the full text token guided model, the sparse text token guided model, and the sparse text token &amp; progress bar value guided model.\nIt can be observed that after using the searched attention paths as pseudo-force alignment labels for attention-guided training, the stable hallucination of CosyVoice2 was effectively reduced. Moreover, the sparse text token supervision outperformed the full text token supervision. We calculated the prediction accuracy of text tokens during the training, as shown in Table. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.4 Effectiveness of Attention-Guided Training &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, sparse text tokens can effectively avoid issues such as insufficient accuracy of pseudo-force alignment and ambiguous boundary definition, thereby better facilitating the alignment between text tokens and speech tokens. Additionally, incorporating the supervision of progress bar values into CV_AG can effectively reduce the misalignment between text tokens and speech tokens, thereby further alleviating stability hallucinations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper focuses on resolving stability hallucinations (e.g., repetitive or omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and leveraging the attention mechanism.\nFirst, we analyzed the alignment mechanism between text tokens and speech tokens in LLMs. We then proposed a metric termed the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to evaluate text-speech alignment quality. Subsequently, OAS was integrated into the training of CosyVoice2 to assist LLMs in learning continuous, stable alignment.\nAdditionally, the pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought (CoT), which further reduces stability hallucinations in synthesized speech.\nExperiments on the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods can effectively reduce the stability hallucinations of CosyVoice2 without introducing additional negative effects. The appendix is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsmzzz.github.io/llm_attn\" title=\"\">https://wsmzzz.github.io/llm_attn</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "sets",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TTS technology aims to generate speech with high expressiveness and naturalness according to text input and other acoustic conditions (e.g., timbre, emotion, style). In recent years,\nwith the rapid progress of speech codec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and generative large model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> technologies, the TTS system has made significant improvements, achieving synthesizing of highly natural, robust and controllable human-like speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In terms of the modeling process from text input to speech representations, TTS large models can be categorized into two types: LLM-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and diffusion-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, LLM-based models typically adopt decoder-only transformer architectures to predict discretized speech tokens via autoregressive or masked language modeling. In contrast, diffusion-based models generate audio features in parallel based on a pre-defined duration.\nIn general, LLM-based TTS models perform better in modeling the temporal sequences and are better suited for multi-modal and multi-task scenarios.\nLLM-based TTS models lack an explicit text-speech alignment process. Instead, they generate speech corresponding to the prompt text via the next-token prediction paradigm. Although large-scale data enable LLM-based TTS models to synthesize stable speech in common text scenarios, they still suffer from stability hallucinations when processing long or hard texts. These issues include generating repetitive, even endless, speech and omitting segments of the intended speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In earlier years, many studies </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on acoustic models attempted to optimize the cross-attention mechanism to improve robustness. They tried to impose forward, single-wise, or monotonic constraints on the text-speech attention learning process to mitigate severe alignment errors. However, these methods are improved based on the cross-attention mechanism between text and speech, making them incompatible with decoder-only LLM-based TTS models.\nRecently, Wang et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Han et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> implemented the hard stepwise monotonic self-attention during LLM-based TTS inference. Yet, the hard attention tends to degrade speech naturalness </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as speech signals are inherently continuous.\nHan et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Lu et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Sheng et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> used the force-alignment labels to help LLM to locate the corresponding text tokens when generating speech tokens.\nAccurate forced alignment labels are difficult to obtain on large-scale datasets, which limits their application scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we eliminate stability hallucinations in LLM-based TTS models via attention guidance. Notably, our method eliminates the need for forced alignment labels, while effectively improving the stability of LLM-based TTS models on hard text scenarios without compromising speech naturalness.\nThe contributions of this paper are as follows:\n.1) We analyzed the alignment between text tokens and speech tokens in LLMs and proposed the metric termed the Optimal Alignment Score (OAS) to measure alignment quality. Experimental results verify that this proposed metric exhibits a strong correlation with the stability hallucinations of LLM-based TTS models.\n.2) Next, OAS is integrated into the training process of CosyVoice2 to facilitate it in learning continuous, stable alignment between text tokens and speech tokens. Experimental results demonstrate that this approach enables the synthesis of more stable and accurate speech.\n.3) To further mitigate stability hallucinations, we leverage attention values from the pre-trained model as prior knowledge to guide the training of the student CosyVoice2 model via the CoT approach. Specifically, the corresponding text tokens and progress bar values are used as supervision when the student model predicts speech tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "supervision",
                    "training",
                    "model",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we first conduct experiments to analyze the attention mechanism in LLM-based TTS models, aiming to capture the alignment between text tokens and speech tokens in the decoder-only architecture.\nWe find that there exist some &#8220;alignment heads&#8221; in LLMs, which focus on the alignment between input text tokens and output speech tokens and their functions are similarly to the cross-attention mechanism.\nSubsequently, the OAS metric is proposed, which is used to evaluate the alignment quality between text tokens and speech tokens in LLMs.\nNext, OAS is integrated into the training process of LLMs to facilitate the model&#8217;s learning of robust alignment between text tokens and speech tokens.\nFinally, we demonstrate how to leverage the teacher attention of the pre-trained CosyVoice2 as prior knowledge to guide the student model&#8217;s training, thereby further enhancing the alignment between text tokens and speech tokens to eliminate stability hallucinations.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the alignment between text tokens and speech tokens in LLM-based TTS models, CosyVoice2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used as the backbone to analyze the multi-head self-attention mechanism under the decoder-only architecture. The backbone CosyVoice2 adopts Qwen-0.5B as its initial LLM, which consists of 24 Transformer layers with 14 attention heads per layer.\nWenetSpeech4TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was used for the scratch training of the LLM of CosyVoice2.\nAfter analyzing the attention heads, the following patterns are observed: lower-layer attention heads tend to model global information, higher-layer ones focus on the neighboring regions of output tokens, and middle-layer heads contain specific &#8220;alignment heads&#8221;, as shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In the attention scores of these alignment heads, the weight regions from output speech tokens to input text tokens exhibit global forward alignment paths similar to those in the cross-attention mechanism.\nDetailed visualization analyses are available in the appendix </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://wsmzzz.github.io/llm_attn/\" title=\"\">https://wsmzzz.github.io/llm_attn/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt is therefore hypothesized that these alignment heads have the most significant impact on the alignment between text tokens and speech tokens, and subsequent improvements and experiments are conducted based on this hypothesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This section proposes the OAS metric, which is used to evaluate the alignment quality between input text tokens and output speech tokens in attention scores.\nThe OAS metric will serve as a criterion for selecting alignment heads and be used to supervise alignment heads in learning high-quality text-speech alignment.\nDue to the characteristics of TTS tasks, the energy centers of the alignment curves between text tokens and speech tokens are supposed to be continuous and monotonic.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Inspired by Glow-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and VITS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Viterbi algorithm is applied to find an optimal alignment path from the aligned regions, maximizing the sum of attention probability along this path.\nThe </span>\n  <math alttext=\"\\boldsymbol{A}\\in\\mathbb{R}^{L_{s}\\times L_{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119912;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{A}\\in\\mathbb{R}^{L_{s}\\times L_{t}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a sub-matrix of the full attention probability matrix, representing the weights assigned by speech tokens to input text tokens, where </span>\n  <math alttext=\"L_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"L_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">L</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">L_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denote the length of output speech tokens and text tokens to be synthesized, respectively.\nThe calculation of the optimal alignment path </span>\n  <math alttext=\"\\boldsymbol{P}\\in\\mathbb{Z}^{L_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119927;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8484;</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">L</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{P}\\in\\mathbb{Z}^{L_{s}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is shown in Algorithm </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#alg1\" style=\"font-size:90%;\" title=\"Algorithm 1 &#8227; 2.1 Attention analyses based on CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAfter obtaining the optimal alignment path </span>\n  <math alttext=\"\\boldsymbol{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119927;</mi>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{P}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, OAS is calculated as the ratio of the attention probability along the alignment path to the sum of scores within the alignment region, as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "where",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Since the OAS calculates the attention probability along the globally optimal alignment path, it can reflect the continuity and monotonicity of the attention between whole text tokens and whole speech tokens. Experimental results in Sec.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Analysis of the OAS metric &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrate a strong correlation between the proposed OAS and word error rate (WER) in synthesized speech (with a correlation coefficient of 0.638).</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To select the alignment heads in CosyVoice2, we calculated the average value of the top 7 OAS for each layer on the Seed-TTS-Eval </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test set, as shown in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.3 Incorporate OAS into the training of CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt can be observed that the average OAS scores of 8-th layer and 9-th layer are significantly higher than those of other layers.\nThus, it can be inferred that alignment heads are primarily distributed in Layers 8 and 9 of CosyVoice2.\nTherefore, we designated half of the attention heads in 8-th layer and 9-th layer of CosyVoice2 as alignment heads.\nTo achieve this, we apply an attention mask that restricts these heads to focus solely on the alignment region between text tokens and speech tokens. The OAS regularization is then applied to these assigned heads.\nSince the attended region is restricted, the denominator in Equation </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.2 Metric for Alignment Quality: OAS &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> becomes a constant. Therefore, the final OAS regularization can be simplified to:</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Valle-R </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> incorporates force alignment labels into the prediction targets of LLMs in the form of CoT to strengthen the positional relationship between text tokens and speech tokens.\nHowever, this approach requires forced alignment labels during the alignment process, which is unfavorable for large-scale data training.\nGiven that CosyVoice2 can already synthesize fairly stable speech for common text scenarios, with a WER of less than 4% </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe attempt to use the optimal path extracted from the attention of the pre-trained CosyVoice2 as pseudo forced alignment labels to guide the training of the student CosyVoice2 model, as shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.4 Attention-Guided Training for Student CosyVoice2 &#8227; 2 Proposed methods &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The optimal alignment path derived from the head with the highest OAS in teacher CosyVoice2 is employed as the pseudo forced alignment label.\nGiven that pseudo forced alignment labels can only capture the approximate positional correlation between text tokens and speech tokens, this paper proposes two improvements to the text token prediction task under the CoT framework\nFirst, the predicted text tokens are not used as input for the next step, so as to avoid the accumulation of errors caused by inaccurate text token prediction. Second, the text tokens employed as prediction targets are generated in a sparse repetition format, similar to the approach adopted in MegaTTS3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDenote the durations converted from the optimal alignment path and the input text tokens as </span>\n  <math alttext=\"\\boldsymbol{d}=[d_{1},d_{2},...,d_{L_{t}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119941;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">d</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{d}=[d_{1},d_{2},...,d_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\boldsymbol{t}=[t_{1},t_{2},...,t_{L_{t}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119957;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{t}=[t_{1},t_{2},...,t_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. Given </span>\n  <math alttext=\"\\boldsymbol{d}=[2,3,3]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119941;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{d}=[2,3,3]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the full repeated target sequence is </span>\n  <math alttext=\"\\boldsymbol{O}_{w}=[t_{1},t_{1},t_{2},t_{2},t_{2},t_{3},t_{3},t_{3}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119926;</mi>\n          <mi mathsize=\"0.900em\">w</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{w}=[t_{1},t_{1},t_{2},t_{2},t_{2},t_{3},t_{3},t_{3}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> while the sparse repeated target sequence is </span>\n  <math alttext=\"\\boldsymbol{O}_{s}=[t_{1},M,M,t_{2},M,M,t_{3},M]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119926;</mi>\n          <mi mathsize=\"0.900em\">s</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mn mathsize=\"0.900em\">3</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{O}_{s}=[t_{1},M,M,t_{2},M,M,t_{3},M]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, each text token is randomly marked only once in the target sequence, with boundary positions avoided whenever possible.\nIn this way, the issues of pseudo-label inaccuracies and ambiguous boundary classification are addressed, thereby providing more stable supervision for the student CosyVoice2.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "supervision",
                    "tokens",
                    "sparse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, hard input texts may contain sentences or phrases with repeated structural patterns. Relying solely on text tokens as the guide when predicting speech tokens is insufficient. Therefore, an additional prediction task for the progress bar value is introduced, which provides absolute positional progress information to help the LLM track how much of the text has been synthesized so far.\nThe progress bar value is denoted as </span>\n  <math alttext=\"\\boldsymbol{p}=[p_{1},p_{2},..,p_{L_{t}}]\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS4.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119953;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">p</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">p</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0.0835em\">.</mo>\n          <mo lspace=\"0.0835em\" mathsize=\"0.900em\" rspace=\"0.167em\">.</mo>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">p</mi>\n            <msub>\n              <mi mathsize=\"0.900em\">L</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\boldsymbol{p}=[p_{1},p_{2},..,p_{L_{t}}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "where",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\hat{p_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{p_{i}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th predicted value of the progress bar value.</span>\n</p>\n\n",
                "matched_terms": [
                    "where",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: WenetSpeech4TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was used for the scratch training of the LLM. WenetSpeech4TTS is an open-source Mandarin dataset spanning multiple domains, containing 12,800 hours of paired text-speech data. Moreover, Seed-TTS-Eval and CV3-Eval </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were used as test sets.\nSpecifically, the hardcase subset of Seed-TTS-Eval and the hard_zh subset of CV3-Eval were used to evaluate CosyVoice2&#8217;s performance in synthesizing speech from hard texts. The meta_zh subset of Seed-TTS-Eval and the zh subset of CV3-Eval, on the other hand, were used to evaluate its performance in synthesizing speech from common texts.</span>\n</p>\n\n",
                "matched_terms": [
                    "sets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Models to be evaluated</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:\n1) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV1/2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The standard CosyVoice1/2 models.\n2) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV2_M</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The CosyVoice2 model with manually designated alignment heads in 8-th layer and 9-th layer.\n3) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV2_OAS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The CosyVoice2 model with </span>\n  <math alttext=\"\\mathcal{L}_{OAS}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{OAS}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applied to the designated attention heads.\n4) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CV2_AG</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: The student CosyVoice2 model applied the attention-guided training.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The relationship between the computed final OAS values and their corresponding WER values was visualized in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19852v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.1 Experimental Setup &#8227; 3 Experiment &#8227; Eliminating Stability Hallucinations in LLM-based TTS models via Attention Guidance\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where red dashed line denotes the linear fitting line. Specially, the coefficient between OAS and WER is 0.638. This supports our hypothesis that the alignment between speech and text in LLMs is primarily influenced by several alignment heads with high OAS values.\nThus, the OAS metric can be used to supervise the training of LLMs, thereby enhancing the alignment between text and speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "where",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper attempts to address stability hallucinations in LLM-based TTS by leveraging the attention mechanism. By introducing the OAS loss and attention-guided training, it effectively reduces stability hallucinations in speech synthesized by CosyVoice2 under hard text scenarios without introducing additional negative effects.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "training"
                ]
            }
        ]
    }
}