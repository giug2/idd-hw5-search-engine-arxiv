{
    "S3.T1": {
        "source_file": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency",
        "caption": "Table 1: Evaluation results of zero-shot TTS. VoXtream-NS denotes the non-streaming version. Bold for the best result, underline for the second-best result (per section). Naturalness is measured as MUSHRA scores (0‚Äì100).",
        "body": "Model\nText\n#Data(h)\n#Params\nSEED test-en\n\nLibriSpeech test-clean\nNaturalness ‚Üë\\uparrow\n\n\n\nToken\nWER (%) ‚Üì\\downarrow\nSPK-SIM ‚Üë\\uparrow\nUTMOS ‚Üë\\uparrow\n\nWER (%) ‚Üì\\downarrow\nSPK-SIM ‚Üë\\uparrow\nUTMOS ‚Üë\\uparrow\n\nùùÅ\\boldsymbol{\\mu} ¬±\\pm 95% CI\n\n\n\n\nHuman\n-\n-\n-\n2.17\n0.734\n3.53\n\n2.30\n0.664\n4.10\n\n58.4 ¬±\\pm 2.5\n\n\n\n\n\nLarge\n\nCosyVoice\nBPE\n170k Multi.\n416M\n4.75\n0.635\n3.88\n\n3.75\n0.575\n4.09\n-\n\n\nSpark-TTS\nBPE\n102k Multi.\n507M\n3.29\n0.570\n3.94\n\n3.02\n0.513\n4.20\n-\n\n\nLlasa-1B\nBPE\n250k Multi.\n1000M\n3.18\n0.578\n4.08\n\n3.18\n0.490\n4.19\n-\n\n\nVoiceStar\nPhone\n65k EN\n840M\n2.91\n0.605\n3.92\n\n3.92\n0.509\n4.10\n-\n\n\nCosyVoice2\nBPE\n167k Multi.\n618M\n2.87\n0.656\n4.18\n\n2.97\n0.587\n4.23\n-\n\n\nFireRedTTS-1S\nBPE\n500k Multi.\n550M\n2.66\n0.633\n3.62\n\n6.43\n0.540\n3.82\n-\n\n\n\n\nMid\n\nVoiceCraft\nPhone\n9k EN\n830M\n3.77\n0.515\n3.63\n\n3.11\n0.444\n3.90\n\n53.6 ¬±\\pm 2.5\n\n\n\nXTTS-v2\nBPE\n27k Multi.\n470M\n3.64\n0.467\n3.57\n\n3.90\n0.444\n3.72\n\n53.8 ¬±\\pm 2.7\n\n\n\nVoXtream-NS\nPhone\n9k EN\n471M\n3.64\n0.537\n3.89\n\n2.99\n0.465\n4.07\n\n51.8 ¬±\\pm 2.6\n\n\n\n\n\nStream\n\n\nCosyVoice2:Out\n\nBPE\n167k Multi.\n618M\n2.70\n0.662\n4.05\n\n2.65\n0.592\n4.19\n\n60.6 ¬±\\pm 2.4\n\n\n\n\nXTTS-v2:Out\n\nBPE\n27k Multi.\n470M\n3.99\n0.480\n3.59\n\n4.06\n0.440\n3.64\n\n53.0 ¬±\\pm 2.7\n\n\n\n\nVoXtream:Out\n\nPhone\n9k EN\n471M\n3.82\n0.529\n3.88\n\n3.09\n0.461\n4.08\n\n53.4 ¬±\\pm 2.5\n\n\n\n\nVoXtream:Full\n\nPhone\n9k EN\n471M\n3.81\n0.529\n3.90\n\n3.15\n0.458\n4.07\n\n51.9 ¬±\\pm 2.6",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#Data(h)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SEED <span class=\"ltx_text ltx_font_italic\">test-en</span></span></td>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span></span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Naturalness <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Token</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SPK-SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SPK-SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<math alttext=\"\\boldsymbol{\\mu}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mi mathsize=\"0.800em\">&#120641;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\mu}</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\"> <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 95% CI</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.734</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.53</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.664</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.10</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">58.4 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.5</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text\" style=\"font-size:80%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:7.0pt;height:24.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.1pt;transform:translate(-8.5pt,-8.5pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large</span></span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">170k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">416M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.635</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.88</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.575</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.09</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">Spark-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">102k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">507M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.29</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.570</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.94</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.513</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">4.20</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">Llasa-1B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">250k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1000M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.578</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">4.08</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.490</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.19</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">VoiceStar</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Phone</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">65k EN</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">840M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.605</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.92</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.509</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.10</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">167k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">618M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">2.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.656</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.18</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.587</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.23</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">FireRedTTS-1S</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">500k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">550M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.66</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.633</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.62</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.43</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.540</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.82</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:80%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:5.6pt;height:17.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:17.4pt;transform:translate(-5.9pt,-5.9pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mid</span></span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">VoiceCraft</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Phone</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">9k EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">830M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.515</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.63</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.444</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.90</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">53.6 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.5</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">XTTS-v2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">27k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">470M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.467</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.57</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.444</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.72</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">53.8 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.7</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:80%;\">VoXtream-NS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Phone</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9k EN</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">471M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.537</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.465</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.07</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">51.8 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.6</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:80%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:5.5pt;height:30.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:30.7pt;transform:translate(-12.6pt,-12.6pt) rotate(-90deg) ;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stream</span></span>\n</span></span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2:</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Out</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">167k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">618M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.662</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.05</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.592</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.19</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">60.6</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.4</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">XTTS-v2:</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Out</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">BPE</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">27k Multi.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">470M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.480</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.59</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.440</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.64</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">53.0 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.7</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VoXtream:</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Out</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Phone</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9k EN</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">471M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.529</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.88</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.09</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.461</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">4.08</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">53.4 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.5</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VoXtream:</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Full</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">Phone</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">9k EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">471M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.529</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.90</span></td>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.458</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.07</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">51.9 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 2.6</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "cosyvoice",
            "text",
            "librispeech",
            "‚Üìdownarrow",
            "830m",
            "voxtreamout",
            "ùùÅboldsymbolmu",
            "utmos",
            "testclean",
            "65k",
            "¬±pm",
            "fireredtts1s",
            "multi",
            "507m",
            "mid",
            "voicecraft",
            "large",
            "xttsv2",
            "500k",
            "voicestar",
            "tts",
            "250k",
            "470m",
            "zeroshot",
            "27k",
            "xttsv2out",
            "human",
            "wer",
            "stream",
            "underline",
            "results",
            "cosyvoice2out",
            "voxtreamns",
            "840m",
            "471m",
            "0‚Äì100",
            "model",
            "spksim",
            "measured",
            "llasa1b",
            "params",
            "1000m",
            "phone",
            "nonstreaming",
            "evaluation",
            "167k",
            "result",
            "550m",
            "voxtreamfull",
            "denotes",
            "version",
            "seed",
            "token",
            "bold",
            "‚Üëuparrow",
            "cosyvoice2",
            "testen",
            "mushra",
            "sparktts",
            "170k",
            "datah",
            "416m",
            "102k",
            "scores",
            "best",
            "bpe",
            "618m",
            "secondbest"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the results for VoXtream and baselines. With comparable training data (Mid-scale), our model achieves the best SPK-SIM and UTMOS metrics and is comparable in terms of WER. We report two variants: a non-streaming model with effectively infinite look-ahead and a streaming model with limited phoneme look-ahead. VoXtream maintains high quality even with limited future context, introducing only slight degradations in WER and SPK-SIM relative to the non-streaming variant.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102&#8201;ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://herimor.github.io/voxtream\" title=\"\">https://herimor.github.io/voxtream</a>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nText-to-Speech, Speech Synthesis, Streaming TTS, Zero-shot TTS</p>\n\n",
                "matched_terms": [
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) and speech generation highlight the importance of low-latency, streaming text-to-speech (TTS) systems for real-time interaction. In applications such as voice assistants, simultaneous translation, and conversational AI, minimizing the delay between input and output &#8211; especially first-packet latency &#8211; is crucial for an engaging user experience. However, most existing TTS models either operate offline or rely on complex, multistage pipelines that limit responsiveness, require large input context, or introduce alignment challenges.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid development of generative models, together with expanding training data, has produced multiple zero-shot TTS models with high naturalness and intelligibility on unseen speakers. Among autoregressive (AR) systems, two-stage models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib2\" title=\"\">2</a>]</cite> first predict intermediate speech representations and then convert them to audio using a non-autoregressive decoder. Single-stage AR systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib4\" title=\"\">4</a>]</cite> directly map text to audio tokens with a single language model. While these models achieve high quality, they are fundamentally designed to process complete text segments before generating audio.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "text",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite> reduce latency via output streaming by generating speech in chunks as decoding proceeds. However, they do not address input-side latency and still require the entire text before speech generation begins.\nNon-autoregressive (NAR) models based on language models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib7\" title=\"\">7</a>]</cite>, factorized diffusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib8\" title=\"\">8</a>]</cite>, or flow matching <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib12\" title=\"\">12</a>]</cite> show strong offline performance, often surpassing AR models in speed and coherence, but their non-sequential architectures hinder streaming TTS.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work in streaming TTS focuses on real-time synthesis through interleaved or chunked text-speech processing. SpeakStream&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib13\" title=\"\">13</a>]</cite> employs a decoder-only transformer trained on interleaved text-speech sequences, achieving low first-packet latency but without exploring zero-shot capabilities. SyncSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib14\" title=\"\">14</a>]</cite> introduces a temporal-masked transformer with one text-token look-ahead and a two-stage NAR decoder, which, however, requires accumulating tokens before audio decoding and thus adds delay.\nIST-LM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite> and CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite> interleave text and speech at fixed ratios in an AR decoder to support full-stream operation, but both use the same NAR flow-matching decoder that operates on chunks, hurting first-packet latency.\nSpeak While You Think&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib17\" title=\"\">17</a>]</cite> distills from a non-streaming model with access to LLM embeddings and requires at least two words of look-ahead to maintain quality. LiveSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib18\" title=\"\">18</a>]</cite> breaks text into smaller segments and produces speech in a streaming manner but suffers from mismatches between speech generation and text fragments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "tts",
                    "nonstreaming",
                    "zeroshot",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoXtream, a fully autoregressive zero-shot TTS model that maps phoneme sequences to audio tokens with minimal delay. The architecture combines (1) an incremental Phoneme Transformer with dynamic phoneme look-ahead, (2) a Temporal Transformer (TT) that jointly predicts Mimi semantic and duration tokens, and (3) a Depth Transformer that generates acoustic tokens conditioned on outputs of TT and speaker embeddings. Unlike prior systems that require multi-stage decoding, complex alignment mechanisms, or large look-ahead windows, VoXtream combines these 3 components in a unified architecture. VoXtream begins outputting speech immediately after receiving the first word, achieving a first-packet latency as low as 102 ms on a modern GPU. Experiments conducted on SEED-TTS and LibriSpeech datasets show that the model generates highly intelligible speech, on par with state-of-the-art TTS models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "librispeech",
                    "zeroshot",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phoneme Transformer</span>: The Phoneme Transformer (PT) is a decoder-only transformer that receives a phoneme sequence and encodes each phoneme with a corresponding embedding. The model operates incrementally, so its input grows with each new word from the text stream. For more natural prosody, PT is allowed to look ahead (LA) up to <span class=\"ltx_text ltx_font_italic\">N</span> phoneme tokens. A key advantage is dynamic LA: the model does not wait for <span class=\"ltx_text ltx_font_italic\">N</span> phonemes before generating speech tokens but starts immediately after the first word. The minimal LA value is determined by the text-buffer size, and the maximal value is limited to 10 phonemes. We convert the input text stream to phonemes using g2pE<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Kyubyong/g2p\" title=\"\">https://github.com/Kyubyong/g2p</a></span></span></span> at the word level.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "stream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Models</span>: We chose multiple AR models with publicly available weights for comparison. For the non-streaming scenario we selected CosyVoice <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib2\" title=\"\">2</a>]</cite>, Spark-TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib3\" title=\"\">3</a>]</cite>, Llasa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib4\" title=\"\">4</a>]</cite>, VoiceStar <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib29\" title=\"\">29</a>]</cite>, VoiceCraft <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib25\" title=\"\">25</a>]</cite>, XTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>]</cite>, CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite>, and FireRedTTS-1S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite>. We split them into Large and Mid groups by training data size for fair comparison.</p>\n\n",
                "matched_terms": [
                    "sparktts",
                    "mid",
                    "cosyvoice",
                    "voicestar",
                    "nonstreaming",
                    "fireredtts1s",
                    "voicecraft",
                    "cosyvoice2",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For streaming evaluation, we selected XTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>]</cite> and CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite>. FireRedTTS-1S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite> was omitted because the authors did not release the streaming generation code. We also noticed that CosyVoice2 re-synthesizes the prompt in the full-stream scenario when the target text is shorter than the prompt, inflating WER, so we do not report those numbers. Instead, we compare VoXtream to CosyVoice2 in the full-stream scenario on LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span>, where this issue does not occur.</p>\n\n",
                "matched_terms": [
                    "text",
                    "evaluation",
                    "librispeech",
                    "wer",
                    "fireredtts1s",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoXtream on three test sets. The first is LibriSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib30\" title=\"\">30</a>]</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite>, we extracted a 2.2&#8201;h subset and evaluated the continuation task. The second is SEED-TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib31\" title=\"\">31</a>]</cite> <span class=\"ltx_text ltx_font_italic\">test-en</span> for a cross-sentence task. The third set targets long-sequence robustness in the full-stream setting: we selected utterances longer than 10&#8201;s from LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span>, yielding a 2.5&#8201;h subset with an average length of 15&#8201;s, denoted LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span>.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "testen",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used three reproducible model-based metrics. For intelligibility, we report WER between the transcription of synthesized speech and the input text. For SEED-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span> we used Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib32\" title=\"\">32</a>]</cite> and followed metric calculation from the official SEED test repository. For LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> we used a HuBERT-based ASR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib33\" title=\"\">33</a>]</cite> and prepended the audio prompt to the generated continuation for WER, as in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite>. For speaker similarity, we computed cosine similarity (SPK-SIM) between embeddings from a WavLM-based <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib34\" title=\"\">34</a>]</cite> ECAPA-TDNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib35\" title=\"\">35</a>]</cite> for the prompt and synthesized speech. For quality, we used the UTMOS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib36\" title=\"\">36</a>]</cite> MOS predictor.</p>\n\n",
                "matched_terms": [
                    "spksim",
                    "text",
                    "librispeech",
                    "utmos",
                    "testclean",
                    "seed",
                    "wer",
                    "testen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate naturalness, we conducted two user studies on the Prolific platform. Each study included multiple attention checks, and participants were compensated at an average rate of 9 GBP per hour. For a fair comparison, we sampled 100 unique sentences on which all systems achieved 0% WER. The first study focused on short-form TTS. We recruited 40 native listeners and asked them to rate the naturalness of each system on a 0&#8211;100 scale, following a MUSHRA-like protocol. The second study focused on full-stream evaluation. Here, 30 native listeners were asked to select their preferred system or indicate &#8220;no preference&#8221;.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "0‚Äì100",
                    "tts",
                    "evaluation",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the full-stream evaluation, text was provided word by word to simulate streaming input from an LLM. We also measured first-packet latency (FPL), the time to the first speech frame, and the real-time factor (RTF), defined as the ratio of generated speech duration to wall-clock generation time.</p>\n\n",
                "matched_terms": [
                    "text",
                    "evaluation",
                    "measured"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While less accurate than models trained on Large-scale datasets, VoXtream approaches Spark-TTS and CosyVoice despite much less data, while also providing streaming. Moreover, VoXtream attains the second-best WER on LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> among all systems, confirming high intelligibility.</p>\n\n",
                "matched_terms": [
                    "sparktts",
                    "cosyvoice",
                    "librispeech",
                    "testclean",
                    "wer",
                    "secondbest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In short-form streaming, VoXtream outperforms XTTS and is second only to CosyVoice2 in output streaming. In full-stream, WER is slightly higher because the input arrives word by word; SPK-SIM and UTMOS are on par with output streaming, where the full text is known before generation. Subjective evaluation also shows that our streaming model is comparable to non-streaming VoiceCraft and XTTS models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "spksim",
                    "text",
                    "nonstreaming",
                    "evaluation",
                    "utmos",
                    "wer",
                    "voicecraft",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the results of full-stream TTS on LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span> set. VoXtream delivers much lower WER compared to CosyVoice2, and naturalness favors our model in subjective evaluation (<math alttext=\"p&lt;5e^{-10}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>10</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p&lt;5e^{-10}</annotation></semantics></math>). CosyVoice2 has a higher speaker similarity due to its NAR flow-matching decoder, but this comes with a high FPL.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "tts",
                    "evaluation",
                    "librispeech",
                    "wer",
                    "cosyvoice2",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also conducted an ablation study to assess foundation-model components (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T4\" title=\"Table 4 &#8227; 3.2 Evaluation &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). As a baseline, we removed SPK-ENC and trained DT from scratch. We then added either SPK-ENC or CSM-DT, and finally both. CSM-DT substantially improves quality and speaker similarity via knowledge transfer. SPK-ENC increases SPK-SIM by 19% in zero-shot TTS when DT is unfrozen and by 6% even with a frozen DT. For intelligibility, the baseline achieves the best WER, highlighting the efficiency of the proposed method; the slight increase in WER of the final system is not significant.</p>\n\n",
                "matched_terms": [
                    "spksim",
                    "tts",
                    "zeroshot",
                    "best",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced VoXtream, a full-stream zero-shot TTS model. The method delivers real-time synthesis with ultra-low initial latency, achieving the best performance among publicly available streaming models. The use of foundation models helps VoXtream to match or exceed the quality of larger (in terms of parameters or training data) non-streaming and streaming baselines on several benchmarks, and provides strong performance in both output- and full-stream settings with only minor degradations relative to its non-streaming variant.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "nonstreaming",
                    "zeroshot",
                    "best"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency",
        "caption": "Table 2: Full-stream capabilities on the LibriSpeech long set. Pref. denotes naturalness preference.",
        "body": "Model\nWER (%) ‚Üì\\downarrow\nSPK-SIM ‚Üë\\uparrow\nUTMOS ‚Üë\\uparrow\nPref.(%) ‚Üë\\uparrow\n\n\nHuman\n1.97\n0.784\n4.16\n-\n\n\n\n\n\nCosyVoice2:Full\n\n6.11\n0.685\n4.19\n31\n\n\n\nVoXtream:Full\n\n3.24\n0.564\n4.23\n57",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SPK-SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Pref.(%) <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Human</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.97</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.784</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.16</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2:</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Full</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.685</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VoXtream:</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Full</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.564</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">57</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "model",
            "spksim",
            "cosyvoice2full",
            "fullstream",
            "librispeech",
            "pref",
            "preference",
            "denotes",
            "‚Üìdownarrow",
            "utmos",
            "voxtreamfull",
            "human",
            "wer",
            "long",
            "‚Üëuparrow",
            "set",
            "capabilities"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the results of full-stream TTS on LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span> set. VoXtream delivers much lower WER compared to CosyVoice2, and naturalness favors our model in subjective evaluation (<math alttext=\"p&lt;5e^{-10}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>10</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p&lt;5e^{-10}</annotation></semantics></math>). CosyVoice2 has a higher speaker similarity due to its NAR flow-matching decoder, but this comes with a high FPL.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The rapid development of generative models, together with expanding training data, has produced multiple zero-shot TTS models with high naturalness and intelligibility on unseen speakers. Among autoregressive (AR) systems, two-stage models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib2\" title=\"\">2</a>]</cite> first predict intermediate speech representations and then convert them to audio using a non-autoregressive decoder. Single-stage AR systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib4\" title=\"\">4</a>]</cite> directly map text to audio tokens with a single language model. While these models achieve high quality, they are fundamentally designed to process complete text segments before generating audio.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work in streaming TTS focuses on real-time synthesis through interleaved or chunked text-speech processing. SpeakStream&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib13\" title=\"\">13</a>]</cite> employs a decoder-only transformer trained on interleaved text-speech sequences, achieving low first-packet latency but without exploring zero-shot capabilities. SyncSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib14\" title=\"\">14</a>]</cite> introduces a temporal-masked transformer with one text-token look-ahead and a two-stage NAR decoder, which, however, requires accumulating tokens before audio decoding and thus adds delay.\nIST-LM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite> and CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite> interleave text and speech at fixed ratios in an AR decoder to support full-stream operation, but both use the same NAR flow-matching decoder that operates on chunks, hurting first-packet latency.\nSpeak While You Think&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib17\" title=\"\">17</a>]</cite> distills from a non-streaming model with access to LLM embeddings and requires at least two words of look-ahead to maintain quality. LiveSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib18\" title=\"\">18</a>]</cite> breaks text into smaller segments and produces speech in a streaming manner but suffers from mismatches between speech generation and text fragments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fullstream",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoXtream, a fully autoregressive zero-shot TTS model that maps phoneme sequences to audio tokens with minimal delay. The architecture combines (1) an incremental Phoneme Transformer with dynamic phoneme look-ahead, (2) a Temporal Transformer (TT) that jointly predicts Mimi semantic and duration tokens, and (3) a Depth Transformer that generates acoustic tokens conditioned on outputs of TT and speaker embeddings. Unlike prior systems that require multi-stage decoding, complex alignment mechanisms, or large look-ahead windows, VoXtream combines these 3 components in a unified architecture. VoXtream begins outputting speech immediately after receiving the first word, achieving a first-packet latency as low as 102 ms on a modern GPU. Experiments conducted on SEED-TTS and LibriSpeech datasets show that the model generates highly intelligible speech, on par with state-of-the-art TTS models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For streaming evaluation, we selected XTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>]</cite> and CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite>. FireRedTTS-1S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite> was omitted because the authors did not release the streaming generation code. We also noticed that CosyVoice2 re-synthesizes the prompt in the full-stream scenario when the target text is shorter than the prompt, inflating WER, so we do not report those numbers. Instead, we compare VoXtream to CosyVoice2 in the full-stream scenario on LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span>, where this issue does not occur.</p>\n\n",
                "matched_terms": [
                    "long",
                    "fullstream",
                    "librispeech",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoXtream on three test sets. The first is LibriSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib30\" title=\"\">30</a>]</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite>, we extracted a 2.2&#8201;h subset and evaluated the continuation task. The second is SEED-TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib31\" title=\"\">31</a>]</cite> <span class=\"ltx_text ltx_font_italic\">test-en</span> for a cross-sentence task. The third set targets long-sequence robustness in the full-stream setting: we selected utterances longer than 10&#8201;s from LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span>, yielding a 2.5&#8201;h subset with an average length of 15&#8201;s, denoted LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span>.</p>\n\n",
                "matched_terms": [
                    "long",
                    "set",
                    "fullstream",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used three reproducible model-based metrics. For intelligibility, we report WER between the transcription of synthesized speech and the input text. For SEED-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span> we used Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib32\" title=\"\">32</a>]</cite> and followed metric calculation from the official SEED test repository. For LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> we used a HuBERT-based ASR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib33\" title=\"\">33</a>]</cite> and prepended the audio prompt to the generated continuation for WER, as in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite>. For speaker similarity, we computed cosine similarity (SPK-SIM) between embeddings from a WavLM-based <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib34\" title=\"\">34</a>]</cite> ECAPA-TDNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib35\" title=\"\">35</a>]</cite> for the prompt and synthesized speech. For quality, we used the UTMOS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib36\" title=\"\">36</a>]</cite> MOS predictor.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "spksim",
                    "librispeech",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate naturalness, we conducted two user studies on the Prolific platform. Each study included multiple attention checks, and participants were compensated at an average rate of 9 GBP per hour. For a fair comparison, we sampled 100 unique sentences on which all systems achieved 0% WER. The first study focused on short-form TTS. We recruited 40 native listeners and asked them to rate the naturalness of each system on a 0&#8211;100 scale, following a MUSHRA-like protocol. The second study focused on full-stream evaluation. Here, 30 native listeners were asked to select their preferred system or indicate &#8220;no preference&#8221;.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "fullstream",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the results for VoXtream and baselines. With comparable training data (Mid-scale), our model achieves the best SPK-SIM and UTMOS metrics and is comparable in terms of WER. We report two variants: a non-streaming model with effectively infinite look-ahead and a streaming model with limited phoneme look-ahead. VoXtream maintains high quality even with limited future context, introducing only slight degradations in WER and SPK-SIM relative to the non-streaming variant.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "model",
                    "spksim",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While less accurate than models trained on Large-scale datasets, VoXtream approaches Spark-TTS and CosyVoice despite much less data, while also providing streaming. Moreover, VoXtream attains the second-best WER on LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> among all systems, confirming high intelligibility.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In short-form streaming, VoXtream outperforms XTTS and is second only to CosyVoice2 in output streaming. In full-stream, WER is slightly higher because the input arrives word by word; SPK-SIM and UTMOS are on par with output streaming, where the full text is known before generation. Subjective evaluation also shows that our streaming model is comparable to non-streaming VoiceCraft and XTTS models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "spksim",
                    "fullstream",
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also conducted an ablation study to assess foundation-model components (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T4\" title=\"Table 4 &#8227; 3.2 Evaluation &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). As a baseline, we removed SPK-ENC and trained DT from scratch. We then added either SPK-ENC or CSM-DT, and finally both. CSM-DT substantially improves quality and speaker similarity via knowledge transfer. SPK-ENC increases SPK-SIM by 19% in zero-shot TTS when DT is unfrozen and by 6% even with a frozen DT. For intelligibility, the baseline achieves the best WER, highlighting the efficiency of the proposed method; the slight increase in WER of the final system is not significant.</p>\n\n",
                "matched_terms": [
                    "spksim",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced VoXtream, a full-stream zero-shot TTS model. The method delivers real-time synthesis with ultra-low initial latency, achieving the best performance among publicly available streaming models. The use of foundation models helps VoXtream to match or exceed the quality of larger (in terms of parameters or training data) non-streaming and streaming baselines on several benchmarks, and provides strong performance in both output- and full-stream settings with only minor degradations relative to its non-streaming variant.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fullstream"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency",
        "caption": "Table 3: Performance in FP16 on an A100 GPU. TC denotes torch.compile and DS denotes DeepSpeed. Metrics for XTTS-v2 are for output streaming; other models are full-stream.",
        "body": "Model\nFPL(ms) ‚Üì\\downarrow\n\nRTF ‚Üì\\downarrow\n\n\n\n\n\nCosyVoice2\n1643\n0.85\n\n\nXTTS-v2\n295\n0.37\n\n\nXTTS-v2:DS\n196\n0.26\n\n\nVoXtream\n171\n1.00\n\n\nVoXtream:TC\n102\n0.17",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">FPL(ms) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">RTF</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">CosyVoice2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">1643</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.85</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">XTTS-v2</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">295</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">XTTS-v2:DS</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">196</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">VoXtream</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">171</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">VoXtream:TC</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">102</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.17</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "voxtream",
            "torchcompile",
            "output",
            "‚Üìdownarrow",
            "voxtreamtc",
            "xttsv2",
            "metrics",
            "fullstream",
            "rtf",
            "streaming",
            "model",
            "a100",
            "denotes",
            "performance",
            "cosyvoice2",
            "fp16",
            "gpu",
            "fplms",
            "models",
            "xttsv2ds",
            "deepspeed"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A key contribution is the extremely low initial latency (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T3\" title=\"Table 3 &#8227; 3.2 Evaluation &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). VoXtream has the lowest FPL among publicly available streaming TTS models and, with <span class=\"ltx_text ltx_font_typewriter\">torch.compile</span>, reaches 102&#8201;ms. VoXtream runs in real time on the GPU without extra acceleration and, with <span class=\"ltx_text ltx_font_typewriter\">torch.compile</span>, runs more than <math alttext=\"5\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">5\\times</annotation></semantics></math> faster than real time, achieving the lowest RTF among streaming competitors.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102&#8201;ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://herimor.github.io/voxtream\" title=\"\">https://herimor.github.io/voxtream</a>.</p>\n\n",
                "matched_terms": [
                    "gpu",
                    "voxtream",
                    "output",
                    "metrics",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) and speech generation highlight the importance of low-latency, streaming text-to-speech (TTS) systems for real-time interaction. In applications such as voice assistants, simultaneous translation, and conversational AI, minimizing the delay between input and output &#8211; especially first-packet latency &#8211; is crucial for an engaging user experience. However, most existing TTS models either operate offline or rely on complex, multistage pipelines that limit responsiveness, require large input context, or introduce alignment challenges.</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid development of generative models, together with expanding training data, has produced multiple zero-shot TTS models with high naturalness and intelligibility on unseen speakers. Among autoregressive (AR) systems, two-stage models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib2\" title=\"\">2</a>]</cite> first predict intermediate speech representations and then convert them to audio using a non-autoregressive decoder. Single-stage AR systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib4\" title=\"\">4</a>]</cite> directly map text to audio tokens with a single language model. While these models achieve high quality, they are fundamentally designed to process complete text segments before generating audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite> reduce latency via output streaming by generating speech in chunks as decoding proceeds. However, they do not address input-side latency and still require the entire text before speech generation begins.\nNon-autoregressive (NAR) models based on language models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib7\" title=\"\">7</a>]</cite>, factorized diffusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib8\" title=\"\">8</a>]</cite>, or flow matching <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib12\" title=\"\">12</a>]</cite> show strong offline performance, often surpassing AR models in speed and coherence, but their non-sequential architectures hinder streaming TTS.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "output",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work in streaming TTS focuses on real-time synthesis through interleaved or chunked text-speech processing. SpeakStream&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib13\" title=\"\">13</a>]</cite> employs a decoder-only transformer trained on interleaved text-speech sequences, achieving low first-packet latency but without exploring zero-shot capabilities. SyncSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib14\" title=\"\">14</a>]</cite> introduces a temporal-masked transformer with one text-token look-ahead and a two-stage NAR decoder, which, however, requires accumulating tokens before audio decoding and thus adds delay.\nIST-LM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite> and CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite> interleave text and speech at fixed ratios in an AR decoder to support full-stream operation, but both use the same NAR flow-matching decoder that operates on chunks, hurting first-packet latency.\nSpeak While You Think&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib17\" title=\"\">17</a>]</cite> distills from a non-streaming model with access to LLM embeddings and requires at least two words of look-ahead to maintain quality. LiveSpeech2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib18\" title=\"\">18</a>]</cite> breaks text into smaller segments and produces speech in a streaming manner but suffers from mismatches between speech generation and text fragments.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "model",
                    "fullstream",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoXtream, a fully autoregressive zero-shot TTS model that maps phoneme sequences to audio tokens with minimal delay. The architecture combines (1) an incremental Phoneme Transformer with dynamic phoneme look-ahead, (2) a Temporal Transformer (TT) that jointly predicts Mimi semantic and duration tokens, and (3) a Depth Transformer that generates acoustic tokens conditioned on outputs of TT and speaker embeddings. Unlike prior systems that require multi-stage decoding, complex alignment mechanisms, or large look-ahead windows, VoXtream combines these 3 components in a unified architecture. VoXtream begins outputting speech immediately after receiving the first word, achieving a first-packet latency as low as 102 ms on a modern GPU. Experiments conducted on SEED-TTS and LibriSpeech datasets show that the model generates highly intelligible speech, on par with state-of-the-art TTS models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gpu",
                    "voxtream",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Mimi decoder converts semantic and acoustic tokens for each frame into 80&#8201;ms of speech in a streaming fashion. We train VoXtream by minimizing the negative log-likelihood of TT and DT outputs.</p>\n\n",
                "matched_terms": [
                    "voxtream",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Models were trained on two NVIDIA A100-80GB GPUs with a batch size of 128 per GPU for 9 epochs. We used fixed 20&#8201;s audio chunks and their corresponding phoneme sequences as input. Because most utterances are shorter, we concatenated multiple utterances within the same speaker. We used AdamW <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib28\" title=\"\">28</a>]</cite>, warming up the learning rate for the first epoch to a peak of <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gpu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Models</span>: We chose multiple AR models with publicly available weights for comparison. For the non-streaming scenario we selected CosyVoice <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib2\" title=\"\">2</a>]</cite>, Spark-TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib3\" title=\"\">3</a>]</cite>, Llasa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib4\" title=\"\">4</a>]</cite>, VoiceStar <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib29\" title=\"\">29</a>]</cite>, VoiceCraft <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib25\" title=\"\">25</a>]</cite>, XTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>]</cite>, CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite>, and FireRedTTS-1S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite>. We split them into Large and Mid groups by training data size for fair comparison.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For streaming evaluation, we selected XTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib5\" title=\"\">5</a>]</cite> and CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib16\" title=\"\">16</a>]</cite>. FireRedTTS-1S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib6\" title=\"\">6</a>]</cite> was omitted because the authors did not release the streaming generation code. We also noticed that CosyVoice2 re-synthesizes the prompt in the full-stream scenario when the target text is shorter than the prompt, inflating WER, so we do not report those numbers. Instead, we compare VoXtream to CosyVoice2 in the full-stream scenario on LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span>, where this issue does not occur.</p>\n\n",
                "matched_terms": [
                    "voxtream",
                    "fullstream",
                    "cosyvoice2",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoXtream on three test sets. The first is LibriSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib30\" title=\"\">30</a>]</cite> <span class=\"ltx_text ltx_font_italic\">test-clean</span>. Following <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite>, we extracted a 2.2&#8201;h subset and evaluated the continuation task. The second is SEED-TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib31\" title=\"\">31</a>]</cite> <span class=\"ltx_text ltx_font_italic\">test-en</span> for a cross-sentence task. The third set targets long-sequence robustness in the full-stream setting: we selected utterances longer than 10&#8201;s from LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span>, yielding a 2.5&#8201;h subset with an average length of 15&#8201;s, denoted LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span>.</p>\n\n",
                "matched_terms": [
                    "voxtream",
                    "fullstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the full-stream evaluation, text was provided word by word to simulate streaming input from an LLM. We also measured first-packet latency (FPL), the time to the first speech frame, and the real-time factor (RTF), defined as the ratio of generated speech duration to wall-clock generation time.</p>\n\n",
                "matched_terms": [
                    "fullstream",
                    "rtf",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the results for VoXtream and baselines. With comparable training data (Mid-scale), our model achieves the best SPK-SIM and UTMOS metrics and is comparable in terms of WER. We report two variants: a non-streaming model with effectively infinite look-ahead and a streaming model with limited phoneme look-ahead. VoXtream maintains high quality even with limited future context, introducing only slight degradations in WER and SPK-SIM relative to the non-streaming variant.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "voxtream",
                    "model",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While less accurate than models trained on Large-scale datasets, VoXtream approaches Spark-TTS and CosyVoice despite much less data, while also providing streaming. Moreover, VoXtream attains the second-best WER on LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> among all systems, confirming high intelligibility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "voxtream",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In short-form streaming, VoXtream outperforms XTTS and is second only to CosyVoice2 in output streaming. In full-stream, WER is slightly higher because the input arrives word by word; SPK-SIM and UTMOS are on par with output streaming, where the full text is known before generation. Subjective evaluation also shows that our streaming model is comparable to non-streaming VoiceCraft and XTTS models.</p>\n\n",
                "matched_terms": [
                    "voxtream",
                    "model",
                    "models",
                    "output",
                    "fullstream",
                    "cosyvoice2",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the results of full-stream TTS on LibriSpeech <span class=\"ltx_text ltx_font_italic\">long</span> set. VoXtream delivers much lower WER compared to CosyVoice2, and naturalness favors our model in subjective evaluation (<math alttext=\"p&lt;5e^{-10}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>e</mi><mrow><mo>&#8722;</mo><mn>10</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p&lt;5e^{-10}</annotation></semantics></math>). CosyVoice2 has a higher speaker similarity due to its NAR flow-matching decoder, but this comes with a high FPL.</p>\n\n",
                "matched_terms": [
                    "voxtream",
                    "model",
                    "fullstream",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced VoXtream, a full-stream zero-shot TTS model. The method delivers real-time synthesis with ultra-low initial latency, achieving the best performance among publicly available streaming models. The use of foundation models helps VoXtream to match or exceed the quality of larger (in terms of parameters or training data) non-streaming and streaming baselines on several benchmarks, and provides strong performance in both output- and full-stream settings with only minor degradations relative to its non-streaming variant.</p>\n\n",
                "matched_terms": [
                    "voxtream",
                    "model",
                    "models",
                    "output",
                    "fullstream",
                    "performance",
                    "streaming"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency",
        "caption": "Table 4: Ablation study on foundation-model components on SEED test-en.",
        "body": "CSM-DT\nSPK-ENC\nWER (%) ‚Üì\\downarrow\nSPK-SIM ‚Üë\\uparrow\nUTMOS ‚Üë\\uparrow\n\n\n\n\n‚úó\n‚úó\n3.53\n0.471\n3.39\n\n\n‚úì\n‚úó\n3.70\n0.504\n3.90\n\n\n‚úó\n‚úì\n3.65\n0.558\n3.39\n\n\n‚úì\n‚úì\n3.64\n0.537\n3.89",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">CSM-DT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SPK-ENC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SPK-SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.471</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.504</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.558</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.537</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">3.89</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "components",
            "spksim",
            "study",
            "ablation",
            "csmdt",
            "spkenc",
            "foundationmodel",
            "‚Üìdownarrow",
            "seed",
            "utmos",
            "wer",
            "‚Üëuparrow",
            "testen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We also conducted an ablation study to assess foundation-model components (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T4\" title=\"Table 4 &#8227; 3.2 Evaluation &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). As a baseline, we removed SPK-ENC and trained DT from scratch. We then added either SPK-ENC or CSM-DT, and finally both. CSM-DT substantially improves quality and speaker similarity via knowledge transfer. SPK-ENC increases SPK-SIM by 19% in zero-shot TTS when DT is unfrozen and by 6% even with a frozen DT. For intelligibility, the baseline achieves the best WER, highlighting the efficiency of the proposed method; the slight increase in WER of the final system is not significant.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model</span>: We used a Llama-style <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib27\" title=\"\">27</a>]</cite> transformer for all modules. The Temporal Transformer has 12 layers, 16 attention heads, an embedding dimension of 1024, and a feed-forward dimension of 4096, similar to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib1\" title=\"\">1</a>]</cite>. The Phoneme and Depth transformers share a similar design with minor differences: the Phoneme Transformer has 6 layers and 8 heads; the Depth Transformer has 4 layers, 8 heads, and a feed-forward dimension of 8192. We borrowed the Depth Transformer from the CSM model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SesameAILabs/csm\" title=\"\">https://github.com/SesameAILabs/csm</a></span></span></span> (CSM-DT) trained on a large-scale dataset and kept its weights frozen during training. As a speaker encoder, we used ReDimNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib22\" title=\"\">22</a>]</cite> (SPK-ENC) trained on 100k+ identities.</p>\n\n",
                "matched_terms": [
                    "spkenc",
                    "csmdt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used three reproducible model-based metrics. For intelligibility, we report WER between the transcription of synthesized speech and the input text. For SEED-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span> we used Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib32\" title=\"\">32</a>]</cite> and followed metric calculation from the official SEED test repository. For LibriSpeech <span class=\"ltx_text ltx_font_italic\">test-clean</span> we used a HuBERT-based ASR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib33\" title=\"\">33</a>]</cite> and prepended the audio prompt to the generated continuation for WER, as in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib15\" title=\"\">15</a>]</cite>. For speaker similarity, we computed cosine similarity (SPK-SIM) between embeddings from a WavLM-based <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib34\" title=\"\">34</a>]</cite> ECAPA-TDNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib35\" title=\"\">35</a>]</cite> for the prompt and synthesized speech. For quality, we used the UTMOS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#bib.bib36\" title=\"\">36</a>]</cite> MOS predictor.</p>\n\n",
                "matched_terms": [
                    "spksim",
                    "seed",
                    "utmos",
                    "wer",
                    "testen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate naturalness, we conducted two user studies on the Prolific platform. Each study included multiple attention checks, and participants were compensated at an average rate of 9 GBP per hour. For a fair comparison, we sampled 100 unique sentences on which all systems achieved 0% WER. The first study focused on short-form TTS. We recruited 40 native listeners and asked them to rate the naturalness of each system on a 0&#8211;100 scale, following a MUSHRA-like protocol. The second study focused on full-stream evaluation. Here, 30 native listeners were asked to select their preferred system or indicate &#8220;no preference&#8221;.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15969v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Setup &#8227; 3 Experiments &#8227; VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the results for VoXtream and baselines. With comparable training data (Mid-scale), our model achieves the best SPK-SIM and UTMOS metrics and is comparable in terms of WER. We report two variants: a non-streaming model with effectively infinite look-ahead and a streaming model with limited phoneme look-ahead. VoXtream maintains high quality even with limited future context, introducing only slight degradations in WER and SPK-SIM relative to the non-streaming variant.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "spksim",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In short-form streaming, VoXtream outperforms XTTS and is second only to CosyVoice2 in output streaming. In full-stream, WER is slightly higher because the input arrives word by word; SPK-SIM and UTMOS are on par with output streaming, where the full text is known before generation. Subjective evaluation also shows that our streaming model is comparable to non-streaming VoiceCraft and XTTS models.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "spksim",
                    "wer"
                ]
            }
        ]
    }
}