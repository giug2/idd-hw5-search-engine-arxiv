{
    "S4.T1": {
        "caption": "TABLE I: Effectiveness of the two baseline attacks.\n``/'' denotes that the B2 attack is not applicable to these models since they do not support audio input.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Category</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Baseline 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Baseline 2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">R1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">R2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">TS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">R1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">R2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">TS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Conventional</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Google TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Self</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Ethos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Mul-ZH</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">IndexTTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Self</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Ethos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Mul-ZH</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"9\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">LLAMs</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">GPT-4o-mini-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Self</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">61.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.6e-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">91.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">85.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.5e-2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Ethos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">87.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">80.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5.0e-2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">97.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">94.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.1e-3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Mul-ZH</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">36.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.4e-1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">58.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">44.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.8e-1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">GPT-4o-mini-tts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Self</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">8.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6.3e-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Ethos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">42.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">13.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">4.0e-1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Mul-ZH</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5.4e-1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">/</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen-omni-turbo</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Self</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">7.2e-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">6.3e-1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Ethos</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">9.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">9.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5.4e-1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">47.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">21.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.7e-1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Mul-ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.7e-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.6e-1</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "applicable",
            "self",
            "36e1",
            "denotes",
            "40e1",
            "not",
            "two",
            "50e2",
            "effectiveness",
            "conventional",
            "tts",
            "mulzh",
            "31e3",
            "audio",
            "gpt4ominitts",
            "72e1",
            "63e1",
            "16e1",
            "37e1",
            "category",
            "15e2",
            "input",
            "gpt4ominiaudio",
            "qwenomniturbo",
            "34e1",
            "18e1",
            "google",
            "llams",
            "attack",
            "indextts",
            "attacks",
            "model",
            "since",
            "support",
            "54e1",
            "ethos",
            "dataset",
            "baseline",
            "they"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on conventional TTS models.</span>\nAs shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S4.T1\" title=\"Table I &#8227; 4.3 Effectiveness of Baseline Attacks &#8227; 4 Initial Harmful Voice Generation Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, both baseline attacks achieve 0% refusal rates (R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub>) and very high toxicity scores across all models and datasets. This indicates that these models replicate input harmful text into synthesized audio without any safety mechanisms in place.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Modern text-to-speech (TTS) systems, particularly those built on Large Audio-Language Models (LALMs), generate high-fidelity speech that faithfully reproduces input text and mimics specified speaker identities. While prior misuse studies have focused primarily on speaker impersonation, this work explores a distinct content-centric threat: exploiting TTS systems to produce speech containing explicitly harmful<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>In this paper, we interchangeably use the terms &#8220;harmful&#8221; (&#8220;harmfulness&#8221;) and &#8220;toxic&#8221; (&#8220;toxicity&#8221;).</span></span></span> content. Realizing such threats poses two core challenges: (1) LALM safety alignment frequently rejects harmful prompts, yet existing jailbreak attacks are ill-suited for TTS because these systems are designed to faithfully vocalize any input text, and (2) real-world deployment pipelines often employ input/output filters that block harmful text and audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "audio",
                    "attacks",
                    "input",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span>, a suite of five attacks organized into two families that address these challenges. The first family employs semantic obfuscation techniques (Concat, Shuffle) that conceal harmful content within text. The second leverages audio-modality exploits (Read, Spell, Phoneme) that inject harmful content through auxiliary audio channels while maintaining ostensibly benign textual prompts. Through evaluation across five commercial LALMs-based TTS systems and three datasets spanning two languages, we demonstrate that our attacks substantially reduce refusal rates and increase the toxicity of generated speech.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks",
                    "tts",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further assess both reactive countermeasures deployed by audio-streaming platforms and proactive defenses implemented by TTS providers. Our analysis reveals critical vulnerabilities: state-of-the-art deepfake audio detectors underperform on high-fidelity audio outputs; reactive text moderation can be circumvented through adversarial perturbations; while proactive moderation of model-emitted text detects 57&#8211;93% of attack instances. Our findings highlight a previously underexplored content-centric misuse vector for TTS systems and underscore the need for robust cross-modal safeguards throughout the model training and deployment lifecycle.</p>\n\n",
                "matched_terms": [
                    "attack",
                    "audio",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) conversion, which translates written text into natural-sounding speech, has become a fundamental technique underlying numerous human-computer interaction applications. From digital voice assistants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib1\" title=\"\">1</a>]</cite> and audiobook dubbing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib2\" title=\"\">2</a>]</cite> to public announcements&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib3\" title=\"\">3</a>]</cite> and virtual avatars&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib4\" title=\"\">4</a>]</cite>, TTS has evolved from a niche accessibility tool into an essential component of modern digital communication. The recent emergence of Large Audio-Language Models (LALMs) represents a paradigm shift in this field, enabling TTS systems to generate remarkably natural and expressive speech, exemplified by OpenAI's GPT-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib5\" title=\"\">5</a>]</cite>, that surpasses the quality and naturalness achievable with conventional TTS models.</p>\n\n",
                "matched_terms": [
                    "gpt4ominitts",
                    "models",
                    "tts",
                    "conventional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, TTS can also be abused for malicious purposes. Prior work on deepfake audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib8\" title=\"\">8</a>]</cite> has shown that synthesized speech can convincingly mimic a target speaker's voice to deceive biometric systems or listeners.\nTypically, speech can be factorized into speaker identity and linguistic content&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib9\" title=\"\">9</a>]</cite>; the former characterizes who is speaking, while the latter specifies what is being said.\nHowever, to the best of our knowledge, no prior work has focused on the misuse of TTS regarding linguistic content.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Work.</span>\nIn this work, we bridge this gap by examining a novel threat scenario: leveraging advanced TTS models to generate audio containing explicitly harmful speech content that violates ethical norms. Thoroughly investigating and mitigating such content-centric TTS abuse is imperative for two key reasons. First, compared to harmful text, spoken harmful content carries highly realistic vocal qualities, such as prosodic cues (e.g., tone, emphasis, pacing), that increase credibility and emotional arousal, thereby amplifying downstream harms, including radicalization dissemination and hate propagation. Second, with the proliferation of off-the-shelf TTS models and the rise of audio streaming infrastructures, spoken harmful content can now be generated and disseminated at scale, rapidly reaching vulnerable populations with minimal cost and fine-grained personalization, thereby posing severe risks to social stability, institutional credibility, and personal safety.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "tts",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet, to realize such attacks, we identify two major technical challenges.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge 1: Safety Alignment.</span>\nState-of-the-art LALMs have typically undergone safety alignment with human ethical standards and thus usually refuse to synthesize audio containing harmful content.\nFor instance, GPT-4o-mini-audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib10\" title=\"\">10</a>]</cite> refuses to synthesize over 80% of sentences in the Ethos dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib11\" title=\"\">11</a>]</cite>.\nOne straightforward approach is to adapt jailbreak attacks against large language models to bypass the safety alignment.\nHowever, we identify several reasons why straightforward adaptations of known textual and audio jailbreaks are inadequate for our scenario.\nFirst, most LALMs are not specialized for TTS, so simply combining toxic text with jailbreak prompts is insufficient to elicit spoken harmful audio. Second, textual jailbreaks introduce extraneous artifacts (e.g., role-play instructions, special characters, paraphrases) that are literally spoken in TTS-oriented LALMs, causing the output to deviate from the intended harmful content and preventing scaling to longer content due to context limits. Finally, prior audio jailbreak techniques are not applicable because our inputs are text rather than audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "applicable",
                    "tts",
                    "audio",
                    "attacks",
                    "ethos",
                    "dataset",
                    "not",
                    "gpt4ominiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we exploit the audio modality that is unique to LALMs and has no analogue in pure text models, and propose several multi-modal attacks.\nOur initial attack encodes the entire harmful text within audio and instructs LALMs to transcribe the audio and synthesize new audio with the transcription. However, we find that this attack is ineffective since the system may examine the audio and block it once any inappropriate content is detected.\nThus, we introduce techniques that use reading, spelling, and phoneme injection to bypass safety alignment.\nIn these attacks, the harmful words within the harmful sentence are supplied to the TTS model via the audio modality, while other words are still provided in text form.\nIn the reading, spelling, and phoneme attacks, the harmful word is encoded in an audio snippet where a speaker is reading it, spelling it letter by letter (e.g., ``s h i t'' for ``shit''), and reading out its phoneme symbols (e.g., ``S I t'' for ``shit''), respectively.\nBy exploiting the audio modality in these ways, our attacks smuggle disallowed content through the TTS pipeline under alternate guises, then deliver it with full fidelity in the spoken output.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attack",
                    "tts",
                    "audio",
                    "attacks",
                    "model",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge 2: Input Filtering and Output Moderation.</span>\nWe find that in addition to the built-in safety alignment of LALMs, many commercial systems (e.g., Qwen2.5-omni-turbo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib12\" title=\"\">12</a>]</cite>) deploy input filtering and output moderation. The input will not reach LALMs once it is deemed harmful by the input filter, and the synthesized output will not be released to users when it is regarded as harmful by the output moderation.\nThis necessitates more covert methods of delivering harmful content without triggering those safeguards.</p>\n\n",
                "matched_terms": [
                    "input",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we propose harmful semantic concealment techniques with two concrete text-based attacks.\nThese techniques circumvent content moderation by concealing the apparent harmfulness of both input and output, while still guaranteeing that we can obtain the desired audio.\nThe key idea is to break or disguise the toxic sentence so that the filters do not recognize it as such.\nThe first is a concatenation attack that splits a harmful sentence into benign-looking segments and then has the TTS model speak each segment in sequence, yielding the full harmful utterance when the audio segments are combined.\nThe second is a word shuffling attack that permutes word order to mask the toxic semantics, yet uses a post-processing step to restore the correct ordering in the audible output.\nThese methods ensure that both the input text and output speech remain neutral or nonsensical to AI moderators, while the final output reassembles the original toxic content.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "attack",
                    "audio",
                    "attacks",
                    "model",
                    "input",
                    "not",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement all these strategies in an attack tool named <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span> and perform extensive experiments to evaluate its effectiveness using five modern commercial LALMs and three datasets covering two languages.\nWe first demonstrate experimentally that the proposed attacks significantly reduce refusal rates and successfully produce the exact harmful phrases in audio form. For instance, under at least one of our attacks, all five models that initially refused a majority of hate-speech prompts were compelled to synthesize 100% of them, with high toxicity scores in the resulting audio.\nNext, we validate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span> across different output voice styles, across different harmful categories, in synthesizing audio containing harmful text without harmful words, and show that combining individual attacks boosts attack efficacy.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Case studies with audio samples are available at our anonymous website: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://harmgen.netlify.app\" title=\"\">https://harmgen.netlify.app</a>.</span></span></span>\nFinally, we discuss potential countermeasures to mitigate <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span>. We found that reactive defenses adopted by audio-streaming platform maintainers are ineffective: state-of-the-art deepfake audio detectors fail on high-fidelity adversarial outputs; reactive transcribe-then-text-moderation is easily evaded by adversarial perturbations. Thus, we propose employing proactive defense by TTS providers, which moderates the model-emitted text accompanying audio and effectively detects 57&#8211;93% of attacks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "tts",
                    "attack",
                    "audio",
                    "attacks",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Contributions.</span>\nTo the best of our knowledge, this represents the first work to explore linguistic content-oriented abuse of TTS models in generating harmful audio with harmful text content. Our contributions can be summarized as follows:</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose three attacks that exploit the audio modality to encode harmful words and successfully bypass the safety alignment of LALMs-based TTS models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "attacks",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose two harmful semantic concealment attacks to evade input and output moderation.</p>\n\n",
                "matched_terms": [
                    "input",
                    "attacks",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discuss and evaluate potential countermeasures to mitigate our attacks and safeguard TTS systems.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern TTS models vary in their architectural design and scale. In particular, we distinguish between conventional TTS models and those built on Large Audio-Language Models (LALMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib16\" title=\"\">16</a>]</cite>, reflecting whether the system leverages large multi-modal foundational models\nor not. Below, we discuss each category in turn.</p>\n\n",
                "matched_terms": [
                    "models",
                    "conventional",
                    "tts",
                    "category",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Non-LALMs-based TTS models.</span> These TTS systems are typically built on specialized small or medium-sized neural networks designed specifically for the speech synthesis task.\nExamples include dedicated TTS models like IndexTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib17\" title=\"\">17</a>]</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib18\" title=\"\">18</a>]</cite>, and Google&#8217;s conventional TTS system&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib19\" title=\"\">19</a>]</cite>.\nBecause they are purpose-built for TTS, their behavior is more straightforward: given a textual input, they will directly synthesize it to speech as instructed, without the need for\nprompt engineering.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "conventional",
                    "indextts",
                    "input",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LALMs-based TTS models.</span>\nLALMs, such as OpenAI&#8217;s GPT-4o and GPT-4o-mini-audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib10\" title=\"\">10</a>]</cite>, Qwen-2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib12\" title=\"\">12</a>]</cite>, and Google&#8217;s Gemini Live&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib20\" title=\"\">20</a>]</cite>, are massive models that can process and generate multiple modalities, including text and audio. While they are not exclusively designed for TTS,\nthey can be coerced or guided to generate audio outputs containing the spoken form of the provided text with suitable prompt engineering.\nTypically, the input prompt is crafted with two components: (1) an instruction that guides the model to act as a TTS system (for instance, asking the model to ``read the following text aloud'' or produce an audio output), and (2) the target text content that should be spoken.\nThe exception is that some LALMs, e.g., GPT-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib5\" title=\"\">5</a>]</cite>, are already well-tailored and purpose-built for TTS, which act as conventional TTS systems without the need for the complex prompt engineering.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "conventional",
                    "gpt4ominitts",
                    "audio",
                    "model",
                    "input",
                    "not",
                    "gpt4ominiaudio",
                    "two",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, using LALMs for TTS also brings challenges, such as ensuring the model follows the prompt precisely (since these models have a broad and sometimes unpredictable generative ability) and their integrated multi-layer safety mechanisms preventing them from producing disallowed content.\nOur work in this paper\ndemonstrates that with carefully crafted prompts (attacks), even LALM-based TTS models can be induced to generate harmful speech content despite built-in safeguards.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "attacks",
                    "model",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on deepfake audio has primarily focused on voice impersonation and speaker identity mimicry. In a typical deepfake audio attack (e.g.,&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib8\" title=\"\">8</a>]</cite>), the goal is to generate an audio clip that both sounds like a particular target speaker (to human listeners) and is recognized by speaker verification systems as that target. These attacks leverage advanced voice cloning and style transfer techniques to reproduce the vocal characteristics of the target speaker, raising concerns about security (e.g., bypassing voice authentication) and misinformation. For instance, an attacker might synthesize an arbitrary phrase spoken in a politician&#8217;s voice to create a fake speech recording.\nResearch in this area has led to improved deepfake generation methods and parallel efforts in deepfake audio detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib22\" title=\"\">22</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to those works, our study centers on the content of the speech (the linguistic message) rather than the speaker&#8217;s identity. We evaluate the potential of TTS models to generate audio that is harmful or toxic in content (e.g., hate speech, harassment).\nIn other words, while deepfake audio attacks and our work both deal with synthetic speech generation, they address two different threats: identity impersonation versus harmful content generation.\nBoth are serious concerns, but our focus is on the ethical and social violations arising from disallowed or harmful speech content produced by TTS systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "attacks",
                    "audio",
                    "two",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior works designed adversarial prompts to jailbreak textual large language models, either manually or automatically via optimization techniques. Despite differences in technique, these attacks depend on introduced artifacts to succeed, e.g., the virtual nested scenario in DeepInception&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib23\" title=\"\">23</a>]</cite>, the demonstration examples used by the In-Context Attack (ICA)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib24\" title=\"\">24</a>]</cite>, the language switch in the Multilingual attack&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib25\" title=\"\">25</a>]</cite>, the role-play instructions of the &#8220;Do Anything Now&#8221; (DAN) attack&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib26\" title=\"\">26</a>]</cite>, and the nonsensical or special-character sequences of the Greedy Coordinate Gradient (GCG) attack&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib27\" title=\"\">27</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attacks",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, many LALMs are developed as general-purpose models rather than being specialized for TTS. Simply feeding toxic text plus a jailbreak prompt is therefore insufficient since we must additionally instruct the model to render the text as speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, a subset of LALMs are specifically tailored for TTS and behave like conventional TTS systems: they will convert <em class=\"ltx_emph ltx_font_italic\">any</em> input text into audio. Textual jailbreaks typically introduce attack artifacts, either by inserting phrases irrelevant to the harmful text or by modifying the harmful content (e.g., via paraphrasing). In our attack scenario, every token in the prompt, including these artifacts, will be spoken verbatim in the output, producing undesirable extraneous speech that deviates from the intended harmful message. Worse, these artifacts increase prompt length, so such attacks have difficulty scaling to long harmful content under context-window constraints.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "conventional",
                    "attack",
                    "audio",
                    "attacks",
                    "input",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior jailbreak attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib33\" title=\"\">33</a>]</cite> in this area typically assume a usage scenario where the LALM is employed for tasks such as audio-based dialogue or audio understanding/analysis, and the adversary's goal is to trigger some unintended or disallowed textual or audio response from the model.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, we consider a different threat scenario: we use LALMs specifically as TTS engines, instructing them (via text prompts) to convert supplied harmful text into spoken audio. The adversary's challenge here is that the model may refuse to vocalize certain harmful or disallowed text because of its safety mechanisms. Prior audio-jailbreak techniques that alter the input audio, e.g., by injecting adversarial perturbations or modifying audio characteristics, cannot be transferred to our new threat scenario, since the input to LALMs in our scenario is text. We therefore introduce a suite of novel attacks explicitly designed to overcome a model's refusal and produce verbatim harmful speech audio. To the best of our knowledge, this is the first work to demonstrate and systematically evaluate such content-centric attacks against TTS-capable LALMs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "audio",
                    "attacks",
                    "model",
                    "since",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The closest related work is AudioHeel&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib34\" title=\"\">34</a>]</cite>, which also studies word-reading and spelling attacks similar to ours. The differences are: (1) AudioHeel uses audio to facilitate text jailbreaks where the model output is text, whereas our outputs are audio; (2) our prompts for the word-reading and spelling attacks are specifically tailored to the our new threat scenario and thus differ from theirs; and (3) we design a larger set of attacks, especially two harmful semantic concealment attacks that generally outperform word-reading and spelling attacks and can additionally avoid triggering input/output moderation. Combining harmful semantic concealment attacks with word-reading/spelling attacks yields the strongest overall performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "attacks",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our work, we focus on harmful voices generated by TTS models.\nSynthesized harmful audio offers at least two unique advantages to adversaries over human-spoken hate or harassment. (1) Enhanced stealthiness and anonymity: TTS systems provide options to select different speaker identities or voice styles, e.g., from a list of built-in voice presets.\nAs such, an attacker can generate toxic speech in someone else's voice rather than their own.\nThis makes it far more difficult to trace the source of malicious speech or hold the correct individual accountable, compared to scenarios where attackers personally utter harmful words (and might be recognized by their voice).\n(2) Potential to damage the TTS service's reputation: If harmful audio is labeled as being generated by a particular TTS provider's system, the attacker could disseminate the audio to incite public backlash against that provider. In other words, beyond the immediate harm caused by the content itself, there is a secondary attack objective of undermining trust in the TTS provider. For example, an attacker might publicly release a collection of hate speech audio clips purportedly spoken by a TTS model's signature voice (e.g., the ``Cove'' voice of OpenAI), leading listeners to question the company's content safeguards.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attack",
                    "tts",
                    "audio",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Harmfulness:</span> releasing harmful speech in the form of audio online, in order to incite hatred, discredit groups or individuals, manipulate opinions, etc. Theses lead to a toxic internet.\nMany platforms supporting audio/video can be affected and even help them spread: short-form social platforms (TikTok, Instagram&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib43\" title=\"\">43</a>]</cite>, X, Facebook&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib44\" title=\"\">44</a>]</cite>) enable lightning-fast viral spread; podcasts and streaming services reach dedicated, repeat listeners and enable prolonged messaging; professional networks (LinkedIn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib45\" title=\"\">45</a>]</cite>) lend false authority; messaging apps and voice chats (WhatsApp&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib46\" title=\"\">46</a>]</cite>, Discord&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib47\" title=\"\">47</a>]</cite>, Clubhouse&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib48\" title=\"\">48</a>]</cite>) enable private, targeted attacks. The adversary prefers audio over plain text because audio conveys emotion and intent more vividly (tone, pace, and prosody), creates a stronger sense of authenticity, making it both persuasive and highly viral.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reputation degradation:</span> By exploiting a TTS model from a specific provider, adversaries can inflict reputational damage on that provider. This is accomplished by disseminating the harmful voice online alongside text annotations identifying the source (provider name and voice style/name). Such attacks are particularly damaging because TTS providers are expected to prevent irresponsible usage of AI technologies, including ensuring that their TTS models do not serve as tools for generating any harmful content.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "attacks",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adversary's Knowledge and Capacity.</span>\nWe assume that adversaries have access to TTS models. This assumption is reasonable given the widespread availability of such models. Adversaries with sufficient computational resources can deploy open-source TTS models locally&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib18\" title=\"\">18</a>]</cite>, while others can access TTS services through APIs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib20\" title=\"\">20</a>]</cite> or web-based graphical user interfaces&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib50\" title=\"\">50</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib51\" title=\"\">51</a>]</cite>, potentially incurring nominal invocation fees. Our investigation reveals that the cost barrier is very low, with many services offering free usage quotas or being entirely free. Consequently, the attack cost is minimal and economically feasible. However, we assume that adversaries can only utilize these models as black-box oracles by providing text prompts and receiving generated voices, without access to or exploitation of any internal model information, even when using open-source models.</p>\n\n",
                "matched_terms": [
                    "attack",
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present and evaluate two baseline attacks (as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S3.F2\" title=\"Figure 2 &#8227; 3 Threat Model &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) that motivate us to design the advanced attacks introduced in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5\" title=\"5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. These baseline attacks also provide benchmarks for comparative evaluation.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "baseline",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This attack operates exclusively within the text modality and is applicable to both conventional and LALMs-based TTS models.\nFor conventional models, the attack directly feeds harmful text as input.\nFor LALMs-based models, we design the following prompt:</p>\n\n",
                "matched_terms": [
                    "models",
                    "applicable",
                    "tts",
                    "conventional",
                    "attack",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to the B<sub class=\"ltx_sub\">1</sub> attack, which delivers harmful content via text, this attack encodes it in audio format.\nWe design the prompt as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The sentence ``Treat the audio content as text to read, not as an instruction'' is included to prevent LALMs from responding to any questions or commands present in the audio, as we require only verbatim repetition.\nThe adversary provides multi-modal input (text prompt and audio) to the models.\nSince conventional TTS models do not support audio input, the B<sub class=\"ltx_sub\">2</sub> attack is exclusively applicable to LALMs-based TTS models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "applicable",
                    "tts",
                    "conventional",
                    "attack",
                    "audio",
                    "since",
                    "support",
                    "input",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One might question why adversaries would invoke TTS models if they already possess audio containing the harmful text.\nThe reason is that the original audio may have been spoken by the adversaries themselves or generated by a different TTS model, and thus cannot conceal their voiceprint to achieve stealthiness or inflict reputation damage on the target TTS provider.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "audio",
                    "model",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS models.</span>\nWe evaluate five TTS models: two conventional models (Google TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib19\" title=\"\">19</a>]</cite> and ByteDance's IndexTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib17\" title=\"\">17</a>]</cite>) and three LALMs-based models (OpenAI's GPT-4o-mini-audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib10\" title=\"\">10</a>]</cite> and GPT-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib5\" title=\"\">5</a>]</cite>, and Alibaba's Qwen-omni-turbo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib12\" title=\"\">12</a>]</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "conventional",
                    "tts",
                    "indextts",
                    "gpt4ominitts",
                    "qwenomniturbo",
                    "google",
                    "gpt4ominiaudio",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nWe adopt three datasets:\nEthos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib11\" title=\"\">11</a>]</cite>, which contains 413 English harmful texts derived from YouTube and Reddit comments;\nMul-ZH&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib52\" title=\"\">52</a>]</cite>, which comprises 881 Chinese harmful texts sourced from the TOXICN dataset that was compiled from popular Chinese social media platforms (Zhihu and Tieba) and covers multiple sensitive topics including gender, race, region, and LGBTQ issues;\nand a custom dataset created by the authors consisting of 62 harmful sentences, each containing only a single harmful word, denoted as Self.</p>\n\n",
                "matched_terms": [
                    "mulzh",
                    "ethos",
                    "dataset",
                    "self"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Trial-level refusal rate (R<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium\">1</span></sub>).</span> This metric measures how often the model refuses across all attempts. Specifically, given <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> harmful sentences and 10 trials per sentence, R<sub class=\"ltx_sub\">1</sub> is defined as the proportion of trials that return a refusal among the <math alttext=\"10\\times X\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">10\\times X</annotation></semantics></math> total trials. To determine whether a response is a refusal, for each model and dataset, we first run the baseline attack B<sub class=\"ltx_sub\">1</sub>, manually inspect the refusal responses, and summarize their common prefix patterns. Any response starting with one of these prefixes is counted as a refusal.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "baseline",
                    "model",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Toxicity score (TS).</span> This metric quantifies the harmfulness of the generated speech. We compute toxicity over the transcript of the output audio using Detoxify&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib53\" title=\"\">53</a>]</cite> for English datasets and COLD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib54\" title=\"\">54</a>]</cite> for Chinese datasets. Lower toxicity indicates that the model output is more aligned with safety and less likely to contain harmful semantic content.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on LALMs-based TTS models.</span>\nThese models refuse to synthesize audio from harmful text due to their safety mechanisms, as evidenced by the generally high refusal rates R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub>.\nFor instance, GPT-4o-mini-audio rejects over 80% of harmful text in the Ethos dataset.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "audio",
                    "ethos",
                    "dataset",
                    "gpt4ominiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observe variations across models and datasets. Qwen-omni-turbo demonstrates significantly lower resilience than the other two models, and GPT-4o-mini-tts, a dedicated TTS model, exhibits lower resilience than GPT-4o-mini-audio, upon which it is built. This suggests that fine-tuning may compromise the effectiveness of safety mechanisms and consequently increase vulnerability.\nRegarding datasets, both GPT-4o-mini-audio and GPT-4o-mini-tts show lower robustness on the Chinese dataset compared to the two English datasets, likely due to the greater availability of English resources for safety training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "tts",
                    "qwenomniturbo",
                    "gpt4ominitts",
                    "model",
                    "dataset",
                    "gpt4ominiaudio",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing the two baselines B<sub class=\"ltx_sub\">1</sub> and B<sub class=\"ltx_sub\">2</sub>, B<sub class=\"ltx_sub\">2</sub> typically demonstrates inferior attack capability compared to B<sub class=\"ltx_sub\">1</sub>, indicating that embedding harmful content entirely within audio is ineffective.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attack",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since conventional TTS models can be exploited to synthesize harmful voices without additional effort, the remainder of this paper focuses on LALMs-based TTS models, where we design and evaluate advanced attacks to bypass their safety mechanisms and compel them to produce harmful voices. Considering the superior audio quality generated by LALMs-based models (cf.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.SS1.SSS1\" title=\"6.1.1 Deepfake detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6.1.1</span></a>) and the possibility that adversaries may specifically target the reputation of LALMs-based TTS providers, we argue that the investment in developing advanced attacks is justified.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "conventional",
                    "attacks",
                    "audio",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose five advanced attacks to compel LALMs-based TTS models to produce voices containing harmful text.\nThese attacks operate from two distinct perspectives:\nharmful semantic concealment and audio modality exploitation, corresponding to text-modality and multi-modal attacks, respectively. They are illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S3.F2\" title=\"Figure 2 &#8227; 3 Threat Model &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "attacks",
                    "audio",
                    "two",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This category of attacks ensures that inputs to LALMs-based TTS models do not contain harmful intent or semantics (i.e., remain neutral), thereby avoiding\ntriggering input/output moderation and safety alignment mechanisms.\nWe observe that harmful text conveys harmful semantics holistically,\nand that the relative positions of words within a sentence are crucial in conveying harmfulness.\nThese observations motivate us to propose the concatenation attack and the word position shuffling attack, respectively.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attack",
                    "tts",
                    "attacks",
                    "category",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although harmful text as a whole conveys harmful messages and can be readily detected by the safety mechanisms of LALMs, individual substrings or words may be less toxic or even neutral, and thus can be synthesized by LALMs.\nBased on this hypothesis, this attack synthesizes harmful voices using a ``divide-and-conquer'' strategy.\nAs shown in Algo.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>,\nthe attack first locates toxic words using toxic span detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib56\" title=\"\">56</a>]</cite>, which identifies the indices of harmful words within a sentence (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>); then uses the identified harmful words as delimiters to partition the entire sentence into several segments to be ``conquered'', including substrings that do not contain any toxic words and the toxic words themselves (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>); next, it feeds the substrings and toxic words into LALMs and obtains the returned voices; and finally, it concatenates these voices to produce the desired harmful voice (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). To enhance the naturalness and interpretability of the harmful voice, we inject brief silences between consecutive voice segments. When the adversary chooses not to use toxic span detection or when the toxic spans are empty (i.e., no harmful words detected; Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), the attack simply synthesizes each word individually (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "not",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The relative positions of words within a sentence play a crucial role in determining semantics.\nHence, this attack conceals harmfulness by modifying word positions, as shown in Algo.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nIn the first while-loop,\nthe attack iteratively shuffles word positions randomly (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and feeds the shuffled harmful text to the model for at most <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> trials (to account for the randomness of model responses; the second while-loop). Once a trial does not receive a refusal response (cf. the R<sub class=\"ltx_sub\">1</sub> metric in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S4.SS3\" title=\"4.3 Effectiveness of Baseline Attacks &#8227; 4 Initial Harmful Voice Generation Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.3</span></a> for the refusal detection mechanism), the effective shuffled text and corresponding generated voice are identified (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Finally, adversaries recover the original word order in the voice.\nThey exploit a forced aligner, e.g., Montreal Forced Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib57\" title=\"\">57</a>]</cite>, to obtain word boundaries (i.e., word timestamps) in the voice (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), which are then used to segment the voice into individual words (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and concatenate these voice segments with intermediate silences in the correct word order (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). If an effective shuffled text cannot be found within a preset number of iterations, the attack considers the harmful text intractable to synthesize and exits (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "not",
                    "attack",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, this category of attacks exploits vulnerabilities introduced by the audio modality of LALMs to bypass safety alignment,\nwithout neutralizing the harmful intent within the harmful text.\nWhile the baseline attack B<sub class=\"ltx_sub\">2</sub> embeds the entire harmful text into audio, these attacks embed only the harmful words, while the remaining content of the harmful text and the instructions to LALMs for synthesis are provided via text prompts, similar to the baseline attack B<sub class=\"ltx_sub\">1</sub>.\nConsidering that a word can be expressed in different ways, including reading, spelling, and phoneme pronunciation, we accordingly propose three distinct attacks.</p>\n\n",
                "matched_terms": [
                    "attack",
                    "audio",
                    "attacks",
                    "category",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The procedure of multi-modal attacks is shown in Algo.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nFirst, toxic words are located using toxic span detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib56\" title=\"\">56</a>]</cite>, which identifies the indices of harmful words within a sentence (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), based on which the harmful words are extracted (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Then, according to the attack strategy, input audios are prepared by iterating through the harmful words and applying a mapping function that can be prepared by the adversary in advance (Lines&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), and the text prompt is selected accordingly (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Finally, the list of audios and the text prompt are fed to the LALM to obtain the generated voice (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). When no harmful words are detected, the attack exits (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nBelow we elaborate on the details of the audio and text prompt for each attack.</p>\n\n",
                "matched_terms": [
                    "input",
                    "audio",
                    "attacks",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that it is more challenging for LALMs to correctly recognize the [word] in this attack compared to the other two attacks. This is likely due to two main reasons: (1) The same IPA phonemes (i.e., pronunciation) may correspond to different words. (2) LALMs have encountered fewer word IPA phonemes than word readings and spellings during training.\nTo enhance LALMs' ability to recognize the correct word from the IPA phonemes and thereby improve attack efficacy, we provide guidance to LALMs by constraining them to select the [word] from a candidate list. We found that even when the candidate list contains only the correct word, the attack remains effective.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "attack",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When there are multiple harmful words, the text prompts of the three attacks can be easily adapted by inserting additional placeholders [word_i] (where i corresponds to the audio index within the audio list) at appropriate positions using the toxic span detection information.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Models.</span>\nWe evaluate five commercial LALMs-based TTS models,\nincluding OpenAI's GPT-4o-mini-audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib10\" title=\"\">10</a>]</cite>, GPT-4o-mini-tts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib5\" title=\"\">5</a>]</cite>, and GPT-5o-nano&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib61\" title=\"\">61</a>]</cite>,\nGoogle's Gemini-2.5-live&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib20\" title=\"\">20</a>]</cite>, and Alibaba's Qwen-omni-turbo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib12\" title=\"\">12</a>]</cite>.\nSince GPT-5o-nano does not support audio capability, we augment it with the Cosyvoice 2.0 model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib18\" title=\"\">18</a>]</cite> to synthesize its text responses into voices.\nWe do not include open-source models because commercial models require substantially lower computational resources and thus have broader adoption by adversaries. Moreover, the safety mechanisms of open-source models can be removed through more straightforward strategies, including fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "qwenomniturbo",
                    "gpt4ominitts",
                    "audio",
                    "model",
                    "since",
                    "support",
                    "not",
                    "gpt4ominiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Settings.</span>\nFor the Concat and all three multi-modal attacks, we use Mudes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib55\" title=\"\">55</a>]</cite> as <math alttext=\"\\tt TSD\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#120451;&#120450;&#120435;</mi><annotation encoding=\"application/x-tex\">\\tt TSD</annotation></semantics></math> for toxic span detection. For the Shuffle attacks, we employ Montreal Forced Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib57\" title=\"\">57</a>]</cite> as <math alttext=\"\\tt FA\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#120437;&#120432;</mi><annotation encoding=\"application/x-tex\">\\tt FA</annotation></semantics></math> and set <math alttext=\"N=20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">N=20</annotation></semantics></math> and <math alttext=\"T=10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">T=10</annotation></semantics></math>. For the Read and Spell attacks, the mapping function <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> is constructed by using IndexTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib17\" title=\"\">17</a>]</cite> to synthesize audio of harmful words and each letter of harmful words, respectively. For the Phoneme attack, we use the CMU Pronouncing Dictionary&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib59\" title=\"\">59</a>]</cite> and grapheme-to-phoneme conversion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib60\" title=\"\">60</a>]</cite> as <math alttext=\"\\tt PH\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><mi>&#120447;&#120439;</mi><annotation encoding=\"application/x-tex\">\\tt PH</annotation></semantics></math>, and the mapping function <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> contains audios provided on the website&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib62\" title=\"\">62</a>]</cite>.\nFor all attacks, we set the silence duration <math alttext=\"sd=50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">sd=50</annotation></semantics></math> milliseconds.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks",
                    "attack",
                    "indextts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experimental Design.</span>\nFirst, we compare the overall performance of different attacks. Second, we examine the impact of output voice style and the effectiveness across harmful categories. Third, we measure the efficacy of generating harmful text without any harmful words and the effectiveness of combining advanced attacks.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Experimental Setting &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nFirst, all five advanced attacks achieve substantially lower refusal rates and significantly higher toxicity scores than the two baseline attacks. For instance, the Concat and Shuffle attacks achieve 0% R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub> for the GPT-4o-mini-tts and Gemini-2.5-live models on the Self dataset, compared with 100% R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub> for the baseline attacks. This demonstrates the high effectiveness of these advanced attacks in generating harmful voices.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "self",
                    "gpt4ominitts",
                    "attacks",
                    "dataset",
                    "baseline",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we compare and rank the attacks lexicographically by (R<sub class=\"ltx_sub\">2</sub>, R<sub class=\"ltx_sub\">1</sub>, TS): primarily by R<sub class=\"ltx_sub\">2</sub>, breaking ties with R<sub class=\"ltx_sub\">1</sub>, and then with TS.\nIn general, the text-modality harmful semantic concealment attacks (Concat and Shuffle) outperform the multi-modal attacks (Read, Spell, and Phoneme). This is likely because the models fail to recognize some harmful words in the audio for the multi-modal attacks.\nHowever, upon listening, we find that the multi-modal attacks offer the advantage of more natural voice pitch compared to the text-modality attacks. This is because they synthesize from the entire harmful text, whereas the Concat attack synthesizes separately from different segments of the text, and the Shuffle attack synthesizes from shuffled text.\nHowever, the pitch of words within human voices is highly context-dependent.\nThis indicates that when selecting an attack method, adversaries need to balance efficacy and naturalness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attack",
                    "attacks",
                    "audio",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the text-modality attacks, the Concat and Shuffle attacks are comparable in terms of R<sub class=\"ltx_sub\">2</sub>, although the Shuffle attack often exhibits higher R<sub class=\"ltx_sub\">1</sub> since it requires searching for successful random word positions.\nAmong the multi-modal attacks, the Spell attack typically outperforms the others. However, it is not applicable to Chinese harmful text, whereas the Read attack is applicable.</p>\n\n",
                "matched_terms": [
                    "applicable",
                    "attack",
                    "attacks",
                    "since",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LALMs-based TTS models often allow control over the style of output voices by selecting from a list of predefined voices.\nWe evaluate this impact by using four different voices of GPT-4o-mini-audio to synthesize the Ethos dataset with the Spell attack (the best-performing attack under this model and dataset configuration according to Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Experimental Setting &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), specifically: ``alloy'' (female), ``ash'' (male), ``ballad'' (male), and ``coral'' (female).\nThe results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F4\" title=\"Figure 4 &#8227; 5.4.2 Impact of output voice style &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nAlthough the three metrics vary across voices, the differences are minor, indicating the effectiveness of our attacks across different output voice styles.\nInterestingly, we find that our attack with the two male voices (``ballad'' and ``ash'') consistently achieves lower refusal rates and higher toxicity scores than with the two female voices (``alloy'' and ``coral''). This is likely due to differences in the quantity of safety training data between male and female voices.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "tts",
                    "attack",
                    "attacks",
                    "model",
                    "ethos",
                    "dataset",
                    "gpt4ominiaudio",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize OpenAI's moderation API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib64\" title=\"\">64</a>]</cite> to classify the harmful category of each harmful text. Specifically, the API returns harmfulness scores across six major categories, and we assign the category with the highest score as the harmful type. Ultimately, we identify two harmful categories (harassment and hate) in the Self dataset, and four categories (with the additional inclusion of illicit and violence) in the Ethos and Mul-ZH datasets. The results for individual categories are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F5\" title=\"Figure 5 &#8227; 5.4.3 Effectiveness across different harmful categories &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The difficulty varies across harmful categories, with attacks achieving the highest efficacy for harassment on the Self dataset, for illicit on the Ethos dataset, and for violence on the Mul-ZH dataset. However, all five advanced attacks demonstrate effectiveness across all categories by substantially reducing refusal rates and increasing toxicity scores compared to the two baseline attacks, thereby demonstrating their generalizability.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "mulzh",
                    "self",
                    "attacks",
                    "category",
                    "ethos",
                    "dataset",
                    "baseline",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some sentences with harmful semantics may not contain any explicit harmful words. We identify such sentences in the Ethos dataset by applying span detection and selecting those with empty spans. We evaluate the two baseline attacks B<sub class=\"ltx_sub\">1</sub> and B<sub class=\"ltx_sub\">2</sub> and the advanced attacks Concat and Shuffle on the GPT-4o-mini-audio TTS model. Other attacks are excluded since they require harmful words for audio input. The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F6\" title=\"Figure 6 &#8227; 5.4.4 Effectiveness for harmful text without harmful words &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. First, the refusal rate of the two baseline attacks exceeds 60%, indicating the evident harmful semantics of these sentences despite the absence of harmful words. Both the Concat and Shuffle attacks significantly reduce the refusal rate, achieving 0% R<sub class=\"ltx_sub\">2</sub>, meaning they successfully compel the model to synthesize all sentences in at least one trial. They also improve the toxicity score by a substantial margin.\nThese results demonstrate the effectiveness of our attacks in handling this special case of harmful text.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "tts",
                    "audio",
                    "attacks",
                    "model",
                    "since",
                    "ethos",
                    "input",
                    "dataset",
                    "baseline",
                    "not",
                    "gpt4ominiaudio",
                    "two",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, we evaluated the advanced attacks individually. Here, we explore whether combining different attacks can further enhance attack efficacy. Specifically, we combine the Shuffle attack with each of the Read, Spell, and Phoneme attacks. Other combinations are either infeasible or unnecessary. The results on GPT-4o-mini-audio and the Ethos dataset are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F7\" title=\"Figure 7 &#8227; 5.4.5 Combining advanced attacks &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nGenerally, combining Shuffle with X (where X is Read, Spell, or Phoneme) yields lower refusal rates and higher toxicity scores than both Shuffle and X alone. For instance, the combined attack Shuffle+Spell achieves an R<sub class=\"ltx_sub\">1</sub> of 13.6%, substantially lower than that of Shuffle (75.5%) and Spell (35.2%).</p>\n\n",
                "matched_terms": [
                    "attack",
                    "attacks",
                    "ethos",
                    "dataset",
                    "gpt4ominiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the negative effects of harmful audio generated by our attacks, we discuss countermeasures taken by platform maintainers (where harmful voices are released) and TTS providers separately. The former constitutes reactive defense since harmful voices have already been created, while the latter represents proactive defense that prevents harmful voices from being generated for adversaries.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks",
                    "tts",
                    "since"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform detection on the audio modality using the state-of-the-art deepfake audio detector AASIST2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib65\" title=\"\">65</a>]</cite>, which ranks first on the leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib22\" title=\"\">22</a>]</cite>. We do not perform detection on the text modality since the harmful texts originate from humans. Given an audio sample, the model produces a score representing the confidence that the audio is fake. We utilize the scores in two ways: (1) applying the sigmoid function to the score to obtain a probability, and classifying the input as ``deepfake'' when the probability is at least 0.5. We report the detection accuracy. (2) Classifying the input as ``deepfake'' when the score is at least a specified threshold.\nTo reduce the impact of threshold selection on evaluation, we randomly select 10,000 human-uttered (real) voices from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib66\" title=\"\">66</a>]</cite> and use them along with the generated audio to calculate the Equal Error Rate (EER), where the false positive rate equals the false negative rate. We evaluate on the Self dataset using two models: GPT-4o-mini-audio and GPT-5o-nano, which are representative of end-to-end and cascaded models, respectively. The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.F8\" title=\"Figure 8 &#8227; 6.1.1 Deepfake detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The detection is more effective against voices generated by GPT-5o-nano. This is because GPT-5o-nano relies on an external conventional TTS model (CosyVoice 2.0) to synthesize audio, indicating that LALMs-based TTS models produce voices of substantially higher quality. For both models, the defense fails to detect a large portion of audio, e.g., achieving no more than 25% (resp. 70%) accuracy and over 75% (resp. 55%) EER on the GPT-4o-mini-audio (resp. GPT-5o-nano) model. These results indicate that current state-of-the-art deepfake audio detection models fail to detect audio generated by our attacks due to their high quality and minimal artifacts compared to real audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "conventional",
                    "tts",
                    "self",
                    "audio",
                    "attacks",
                    "model",
                    "since",
                    "input",
                    "dataset",
                    "not",
                    "gpt4ominiaudio",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform detection on the text modality by using the state-of-the-art speech recognition model Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib67\" title=\"\">67</a>]</cite> to obtain transcriptions of harmful voices and applying OpenAI's moderation API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib64\" title=\"\">64</a>]</cite> to detect harmfulness. We do not perform detection on the audio modality since we are not aware of any available harmful audio detectors that operate directly on the audio modality. We evaluate on the GPT-4o-mini-audio model and the Self dataset. We report both the ratio of texts flagged as harmful and the average harmfulness scores (cf.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.SS4.SSS3\" title=\"5.4.3 Effectiveness across different harmful categories &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.4.3</span></a>). The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.T2\" title=\"Table II &#8227; 6.1.2 Text harmfulness detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>. The defense can detect at least 66% of harmful audio, regardless of the attack method. However, since the defense is reactive, adversaries can bypass it by post-processing the harmful voices returned by TTS models prior to release. We implement this by crafting adversarial perturbations on the Whisper model using the attack described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib68\" title=\"\">68</a>]</cite>. This attack minimizes the distance between the intermediate outputs of the Whisper model for the adversarial audio and a targeted audio with randomly selected text content, while also employing a psychoacoustic model to render the perturbations inaudible. When perturbations are added to harmful voices, the effectiveness of the defense decreases significantly, exhibiting a substantially lower detection ratio and lower moderation scores. This occurs because the defense detects incorrect transcripts.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "tts",
                    "attack",
                    "self",
                    "audio",
                    "model",
                    "since",
                    "dataset",
                    "not",
                    "gpt4ominiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, TTS providers can moderate output audios and refuse to return them to users if any inappropriate content is detected. We again utilize OpenAI's moderation API to moderate the text of the output audio.\nCompared with the detection in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.SS1.SSS2\" title=\"6.1.2 Text harmfulness detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6.1.2</span></a>, the key difference is that many LALMs simultaneously produce both text and audio in their response; hence, the text can be directly used instead of relying on external speech recognition models.\nMoreover, adversaries have no opportunity to bypass the detection since the audio has not yet been released.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "audio",
                    "since",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting audios generated by the two baseline attacks and the Read, Spell, and Phoneme attacks is straightforward since the texts are complete and their harmful semantics are not concealed.\nFor the Concat attack, the defender can maintain a buffer storing the output texts from users' consecutive queries and initiate detection at an appropriate time (e.g., when the buffer is full). For the Shuffle attack, the defender can attempt to restore the word positions (using tools or simply random reordering) and flag the text as harmful once any version of the reordered text is identified as harmful.</p>\n\n",
                "matched_terms": [
                    "attack",
                    "attacks",
                    "since",
                    "baseline",
                    "not",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the defense on the GPT-4o-mini-audio model and the Self dataset.\nThe results are shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.T3\" title=\"Table III &#8227; 6.2 Proactive Defense by TTS Providers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.\nThe defense can detect at least 57% of harmful audio with a minimum harmfulness score of 0.49, regardless of the attack method.\nEven with the shuffled word positions employed by adversaries, the defense is highly effective in detecting the Shuffle attack. When the defender can recover the correct positions, the detection becomes substantially more effective, with the detection ratio increasing from 72.48% to 93.29%.\nAlthough this defense is simple and effective, we find that it has surprisingly not been incorporated into OpenAI's models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attack",
                    "self",
                    "audio",
                    "model",
                    "dataset",
                    "not",
                    "gpt4ominiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Naturalness.</span>\nThe Concat attack synthesizes short substrings (or individual words) independently and then concatenates them, while the Shuffle attack generates speech for a syntactically scrambled sentence and subsequently reorders word segments. Because intonation, rhythm, and coarticulation are context-dependent,\nthese two attacks disrupt pitch contours and timing, yielding less natural prosody and occasional discontinuities.\nAdditionally, for the Shuffle attack, we rely on a forced aligner to obtain word-level time indices. Any alignment errors (e.g., partial words or spillover into neighboring words) degrade reconstruction quality and further reduce naturalness.\nThis limitation can be alleviated by switching the shuffling units from words to substrings, which we leave for future work.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "attack",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Toxic word localization and counts.</span>\nThe Read, Spell, and Shuffle attacks depend on a toxic span detection module to localize harmful words. Misclassification, either missing true toxic spans or flagging benign tokens, can (i) suppress attack success by omitting crucial words, or (ii) introduce unnecessary placeholders that confuse the model.\nAdditionally, when the harmful text contains numerous toxic words, the prompt accumulates many ``[word]'' placeholders; models then face a more challenging disambiguation and placement problem (mapping each placeholder to the correct target and position), which can reduce the completeness of the final utterance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attacks",
                    "model",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability.</span>\nThis limitation persists in the multi-modal strategies (Spell and Phoneme).\nThe Spell attack assumes alphabetic spelling conventions and is less applicable to languages without letter-wise spelling, e.g., Chinese. The Phoneme attack requires reliable grapheme-to-phoneme resources and robust recognition of phonetic sequences, which may be scarce for low-resource languages.</p>\n\n",
                "matched_terms": [
                    "applicable",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robustness via safety training.</span>\nBeyond the detection-based countermeasures evaluated in our work, we identify a complementary direction: integrating our attacks as adversarial data augmentation during alignment/safety training, encouraging models to (i) resist segmented or shuffled prompts that reconstitute toxic content, and (ii) refuse cross-modal smuggling of harmful words (Read/Spell/Phoneme).</p>\n\n",
                "matched_terms": [
                    "models",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work reframes TTS misuse and safety by shifting attention from who is speaking (speaker identity) to what is being said (linguistic content). We demonstrate that LALMs-based TTS models, despite safety alignment and input/output moderation layers, can be coerced into vocalizing toxic sentences with high fidelity. Our text-modality strategies (Concat, Shuffle) neutralize apparent harmful semantics to avoid refusals, while multi-modal strategies (Read, Spell, Phoneme) covertly reintroduce toxic words via audio,\ndriving down refusal rates across models and languages. In combination attacks such as Shuffle+Spell, we achieve the strongest overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "effectiveness",
                    "tts",
                    "attacks",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We advocate integrating our attacks as adversarial data during alignment and safety training, strengthening cross-modal safeguards against audio-smuggled content, and deploying provider-side proactive moderation by default. We believe that broader adoption of these practices is essential to mitigate emerging risks as TTS models continue to scale in capability and reach.</p>\n\n",
                "matched_terms": [
                    "models",
                    "attacks",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have contacted OpenAI, Google, and Alibaba regarding the vulnerability of their LALMs in generating harmful audio under our attack <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span>.</p>\n\n",
                "matched_terms": [
                    "google",
                    "audio",
                    "attack"
                ]
            }
        ]
    },
    "S6.T2": {
        "caption": "TABLE II: Results of reactive text harmfulness detection",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">w/</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">bypass</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "bypass",
            "detection",
            "harmfulness",
            "results",
            "text",
            "reactive"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We perform detection on the text modality by using the state-of-the-art speech recognition model Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib67\" title=\"\">67</a>]</cite> to obtain transcriptions of harmful voices and applying OpenAI's moderation API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib64\" title=\"\">64</a>]</cite> to detect harmfulness. We do not perform detection on the audio modality since we are not aware of any available harmful audio detectors that operate directly on the audio modality. We evaluate on the GPT-4o-mini-audio model and the Self dataset. We report both the ratio of texts flagged as harmful and the average harmfulness scores (cf.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.SS4.SSS3\" title=\"5.4.3 Effectiveness across different harmful categories &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.4.3</span></a>). The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.T2\" title=\"Table II &#8227; 6.1.2 Text harmfulness detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>. The defense can detect at least 66% of harmful audio, regardless of the attack method. However, since the defense is reactive, adversaries can bypass it by post-processing the harmful voices returned by TTS models prior to release. We implement this by crafting adversarial perturbations on the Whisper model using the attack described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib68\" title=\"\">68</a>]</cite>. This attack minimizes the distance between the intermediate outputs of the Whisper model for the adversarial audio and a targeted audio with randomly selected text content, while also employing a psychoacoustic model to render the perturbations inaudible. When perturbations are added to harmful voices, the effectiveness of the defense decreases significantly, exhibiting a substantially lower detection ratio and lower moderation scores. This occurs because the defense detects incorrect transcripts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We further assess both reactive countermeasures deployed by audio-streaming platforms and proactive defenses implemented by TTS providers. Our analysis reveals critical vulnerabilities: state-of-the-art deepfake audio detectors underperform on high-fidelity audio outputs; reactive text moderation can be circumvented through adversarial perturbations; while proactive moderation of model-emitted text detects 57&#8211;93% of attack instances. Our findings highlight a previously underexplored content-centric misuse vector for TTS systems and underscore the need for robust cross-modal safeguards throughout the model training and deployment lifecycle.</p>\n\n",
                "matched_terms": [
                    "text",
                    "reactive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Challenge 1: Safety Alignment.</span>\nState-of-the-art LALMs have typically undergone safety alignment with human ethical standards and thus usually refuse to synthesize audio containing harmful content.\nFor instance, GPT-4o-mini-audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib10\" title=\"\">10</a>]</cite> refuses to synthesize over 80% of sentences in the Ethos dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib11\" title=\"\">11</a>]</cite>.\nOne straightforward approach is to adapt jailbreak attacks against large language models to bypass the safety alignment.\nHowever, we identify several reasons why straightforward adaptations of known textual and audio jailbreaks are inadequate for our scenario.\nFirst, most LALMs are not specialized for TTS, so simply combining toxic text with jailbreak prompts is insufficient to elicit spoken harmful audio. Second, textual jailbreaks introduce extraneous artifacts (e.g., role-play instructions, special characters, paraphrases) that are literally spoken in TTS-oriented LALMs, causing the output to deviate from the intended harmful content and preventing scaling to longer content due to context limits. Finally, prior audio jailbreak techniques are not applicable because our inputs are text rather than audio.</p>\n\n",
                "matched_terms": [
                    "bypass",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we exploit the audio modality that is unique to LALMs and has no analogue in pure text models, and propose several multi-modal attacks.\nOur initial attack encodes the entire harmful text within audio and instructs LALMs to transcribe the audio and synthesize new audio with the transcription. However, we find that this attack is ineffective since the system may examine the audio and block it once any inappropriate content is detected.\nThus, we introduce techniques that use reading, spelling, and phoneme injection to bypass safety alignment.\nIn these attacks, the harmful words within the harmful sentence are supplied to the TTS model via the audio modality, while other words are still provided in text form.\nIn the reading, spelling, and phoneme attacks, the harmful word is encoded in an audio snippet where a speaker is reading it, spelling it letter by letter (e.g., ``s h i t'' for ``shit''), and reading out its phoneme symbols (e.g., ``S I t'' for ``shit''), respectively.\nBy exploiting the audio modality in these ways, our attacks smuggle disallowed content through the TTS pipeline under alternate guises, then deliver it with full fidelity in the spoken output.</p>\n\n",
                "matched_terms": [
                    "bypass",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we propose harmful semantic concealment techniques with two concrete text-based attacks.\nThese techniques circumvent content moderation by concealing the apparent harmfulness of both input and output, while still guaranteeing that we can obtain the desired audio.\nThe key idea is to break or disguise the toxic sentence so that the filters do not recognize it as such.\nThe first is a concatenation attack that splits a harmful sentence into benign-looking segments and then has the TTS model speak each segment in sequence, yielding the full harmful utterance when the audio segments are combined.\nThe second is a word shuffling attack that permutes word order to mask the toxic semantics, yet uses a post-processing step to restore the correct ordering in the audible output.\nThese methods ensure that both the input text and output speech remain neutral or nonsensical to AI moderators, while the final output reassembles the original toxic content.</p>\n\n",
                "matched_terms": [
                    "text",
                    "harmfulness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement all these strategies in an attack tool named <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span> and perform extensive experiments to evaluate its effectiveness using five modern commercial LALMs and three datasets covering two languages.\nWe first demonstrate experimentally that the proposed attacks significantly reduce refusal rates and successfully produce the exact harmful phrases in audio form. For instance, under at least one of our attacks, all five models that initially refused a majority of hate-speech prompts were compelled to synthesize 100% of them, with high toxicity scores in the resulting audio.\nNext, we validate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span> across different output voice styles, across different harmful categories, in synthesizing audio containing harmful text without harmful words, and show that combining individual attacks boosts attack efficacy.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Case studies with audio samples are available at our anonymous website: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://harmgen.netlify.app\" title=\"\">https://harmgen.netlify.app</a>.</span></span></span>\nFinally, we discuss potential countermeasures to mitigate <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span>. We found that reactive defenses adopted by audio-streaming platform maintainers are ineffective: state-of-the-art deepfake audio detectors fail on high-fidelity adversarial outputs; reactive transcribe-then-text-moderation is easily evaded by adversarial perturbations. Thus, we propose employing proactive defense by TTS providers, which moderates the model-emitted text accompanying audio and effectively detects 57&#8211;93% of attacks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "reactive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Harmfulness:</span> releasing harmful speech in the form of audio online, in order to incite hatred, discredit groups or individuals, manipulate opinions, etc. Theses lead to a toxic internet.\nMany platforms supporting audio/video can be affected and even help them spread: short-form social platforms (TikTok, Instagram&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib43\" title=\"\">43</a>]</cite>, X, Facebook&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib44\" title=\"\">44</a>]</cite>) enable lightning-fast viral spread; podcasts and streaming services reach dedicated, repeat listeners and enable prolonged messaging; professional networks (LinkedIn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib45\" title=\"\">45</a>]</cite>) lend false authority; messaging apps and voice chats (WhatsApp&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib46\" title=\"\">46</a>]</cite>, Discord&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib47\" title=\"\">47</a>]</cite>, Clubhouse&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib48\" title=\"\">48</a>]</cite>) enable private, targeted attacks. The adversary prefers audio over plain text because audio conveys emotion and intent more vividly (tone, pace, and prosody), creates a stronger sense of authenticity, making it both persuasive and highly viral.</p>\n\n",
                "matched_terms": [
                    "text",
                    "harmfulness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on conventional TTS models.</span>\nAs shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S4.T1\" title=\"Table I &#8227; 4.3 Effectiveness of Baseline Attacks &#8227; 4 Initial Harmful Voice Generation Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, both baseline attacks achieve 0% refusal rates (R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub>) and very high toxicity scores across all models and datasets. This indicates that these models replicate input harmful text into synthesized audio without any safety mechanisms in place.</p>\n\n",
                "matched_terms": [
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results on LALMs-based TTS models.</span>\nThese models refuse to synthesize audio from harmful text due to their safety mechanisms, as evidenced by the generally high refusal rates R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub>.\nFor instance, GPT-4o-mini-audio rejects over 80% of harmful text in the Ethos dataset.</p>\n\n",
                "matched_terms": [
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This category of attacks ensures that inputs to LALMs-based TTS models do not contain harmful intent or semantics (i.e., remain neutral), thereby avoiding\ntriggering input/output moderation and safety alignment mechanisms.\nWe observe that harmful text conveys harmful semantics holistically,\nand that the relative positions of words within a sentence are crucial in conveying harmfulness.\nThese observations motivate us to propose the concatenation attack and the word position shuffling attack, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "harmfulness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although harmful text as a whole conveys harmful messages and can be readily detected by the safety mechanisms of LALMs, individual substrings or words may be less toxic or even neutral, and thus can be synthesized by LALMs.\nBased on this hypothesis, this attack synthesizes harmful voices using a ``divide-and-conquer'' strategy.\nAs shown in Algo.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>,\nthe attack first locates toxic words using toxic span detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib56\" title=\"\">56</a>]</cite>, which identifies the indices of harmful words within a sentence (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>); then uses the identified harmful words as delimiters to partition the entire sentence into several segments to be ``conquered'', including substrings that do not contain any toxic words and the toxic words themselves (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>); next, it feeds the substrings and toxic words into LALMs and obtains the returned voices; and finally, it concatenates these voices to produce the desired harmful voice (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). To enhance the naturalness and interpretability of the harmful voice, we inject brief silences between consecutive voice segments. When the adversary chooses not to use toxic span detection or when the toxic spans are empty (i.e., no harmful words detected; Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), the attack simply synthesizes each word individually (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg1\" title=\"Algorithm 1 &#8227; 5.1.1 Concatenation attack (Concat) &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The relative positions of words within a sentence play a crucial role in determining semantics.\nHence, this attack conceals harmfulness by modifying word positions, as shown in Algo.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nIn the first while-loop,\nthe attack iteratively shuffles word positions randomly (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and feeds the shuffled harmful text to the model for at most <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> trials (to account for the randomness of model responses; the second while-loop). Once a trial does not receive a refusal response (cf. the R<sub class=\"ltx_sub\">1</sub> metric in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S4.SS3\" title=\"4.3 Effectiveness of Baseline Attacks &#8227; 4 Initial Harmful Voice Generation Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.3</span></a> for the refusal detection mechanism), the effective shuffled text and corresponding generated voice are identified (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Finally, adversaries recover the original word order in the voice.\nThey exploit a forced aligner, e.g., Montreal Forced Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib57\" title=\"\">57</a>]</cite>, to obtain word boundaries (i.e., word timestamps) in the voice (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), which are then used to segment the voice into individual words (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and concatenate these voice segments with intermediate silences in the correct word order (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). If an effective shuffled text cannot be found within a preset number of iterations, the attack considers the harmful text intractable to synthesize and exits (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg2\" title=\"Algorithm 2 &#8227; 5.1.2 Word position shuffling attack &#8227; 5.1 Text-Modality Attacks: Harmful Semantic Concealment &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "harmfulness",
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, this category of attacks exploits vulnerabilities introduced by the audio modality of LALMs to bypass safety alignment,\nwithout neutralizing the harmful intent within the harmful text.\nWhile the baseline attack B<sub class=\"ltx_sub\">2</sub> embeds the entire harmful text into audio, these attacks embed only the harmful words, while the remaining content of the harmful text and the instructions to LALMs for synthesis are provided via text prompts, similar to the baseline attack B<sub class=\"ltx_sub\">1</sub>.\nConsidering that a word can be expressed in different ways, including reading, spelling, and phoneme pronunciation, we accordingly propose three distinct attacks.</p>\n\n",
                "matched_terms": [
                    "bypass",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The procedure of multi-modal attacks is shown in Algo.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nFirst, toxic words are located using toxic span detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib56\" title=\"\">56</a>]</cite>, which identifies the indices of harmful words within a sentence (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), based on which the harmful words are extracted (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Then, according to the attack strategy, input audios are prepared by iterating through the harmful words and applying a mapping function that can be prepared by the adversary in advance (Lines&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), and the text prompt is selected accordingly (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Finally, the list of audios and the text prompt are fed to the LALM to obtain the generated voice (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). When no harmful words are detected, the attack exits (Line&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#alg3\" title=\"Algorithm 3 &#8227; 5.2.3 Word phoneme reading attack (Phoneme) &#8227; 5.2 Multi-Modal Attacks: Audio Modality Exploitation &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nBelow we elaborate on the details of the audio and text prompt for each attack.</p>\n\n",
                "matched_terms": [
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When there are multiple harmful words, the text prompts of the three attacks can be easily adapted by inserting additional placeholders [word_i] (where i corresponds to the audio index within the audio list) at appropriate positions using the toxic span detection information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize OpenAI's moderation API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib64\" title=\"\">64</a>]</cite> to classify the harmful category of each harmful text. Specifically, the API returns harmfulness scores across six major categories, and we assign the category with the highest score as the harmful type. Ultimately, we identify two harmful categories (harassment and hate) in the Self dataset, and four categories (with the additional inclusion of illicit and violence) in the Ethos and Mul-ZH datasets. The results for individual categories are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F5\" title=\"Figure 5 &#8227; 5.4.3 Effectiveness across different harmful categories &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The difficulty varies across harmful categories, with attacks achieving the highest efficacy for harassment on the Self dataset, for illicit on the Ethos dataset, and for violence on the Mul-ZH dataset. However, all five advanced attacks demonstrate effectiveness across all categories by substantially reducing refusal rates and increasing toxicity scores compared to the two baseline attacks, thereby demonstrating their generalizability.</p>\n\n",
                "matched_terms": [
                    "results",
                    "text",
                    "harmfulness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some sentences with harmful semantics may not contain any explicit harmful words. We identify such sentences in the Ethos dataset by applying span detection and selecting those with empty spans. We evaluate the two baseline attacks B<sub class=\"ltx_sub\">1</sub> and B<sub class=\"ltx_sub\">2</sub> and the advanced attacks Concat and Shuffle on the GPT-4o-mini-audio TTS model. Other attacks are excluded since they require harmful words for audio input. The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F6\" title=\"Figure 6 &#8227; 5.4.4 Effectiveness for harmful text without harmful words &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. First, the refusal rate of the two baseline attacks exceeds 60%, indicating the evident harmful semantics of these sentences despite the absence of harmful words. Both the Concat and Shuffle attacks significantly reduce the refusal rate, achieving 0% R<sub class=\"ltx_sub\">2</sub>, meaning they successfully compel the model to synthesize all sentences in at least one trial. They also improve the toxicity score by a substantial margin.\nThese results demonstrate the effectiveness of our attacks in handling this special case of harmful text.</p>\n\n",
                "matched_terms": [
                    "results",
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering the dual nature of harmful voices generated by our attacks, namely, harmfulness and deepfake characteristics, we can detect them through either deepfake detection or harmfulness detection (i.e., content moderation).</p>\n\n",
                "matched_terms": [
                    "harmfulness",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform detection on the audio modality using the state-of-the-art deepfake audio detector AASIST2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib65\" title=\"\">65</a>]</cite>, which ranks first on the leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib22\" title=\"\">22</a>]</cite>. We do not perform detection on the text modality since the harmful texts originate from humans. Given an audio sample, the model produces a score representing the confidence that the audio is fake. We utilize the scores in two ways: (1) applying the sigmoid function to the score to obtain a probability, and classifying the input as ``deepfake'' when the probability is at least 0.5. We report the detection accuracy. (2) Classifying the input as ``deepfake'' when the score is at least a specified threshold.\nTo reduce the impact of threshold selection on evaluation, we randomly select 10,000 human-uttered (real) voices from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib66\" title=\"\">66</a>]</cite> and use them along with the generated audio to calculate the Equal Error Rate (EER), where the false positive rate equals the false negative rate. We evaluate on the Self dataset using two models: GPT-4o-mini-audio and GPT-5o-nano, which are representative of end-to-end and cascaded models, respectively. The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.F8\" title=\"Figure 8 &#8227; 6.1.1 Deepfake detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The detection is more effective against voices generated by GPT-5o-nano. This is because GPT-5o-nano relies on an external conventional TTS model (CosyVoice 2.0) to synthesize audio, indicating that LALMs-based TTS models produce voices of substantially higher quality. For both models, the defense fails to detect a large portion of audio, e.g., achieving no more than 25% (resp. 70%) accuracy and over 75% (resp. 55%) EER on the GPT-4o-mini-audio (resp. GPT-5o-nano) model. These results indicate that current state-of-the-art deepfake audio detection models fail to detect audio generated by our attacks due to their high quality and minimal artifacts compared to real audio.</p>\n\n",
                "matched_terms": [
                    "results",
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, TTS providers can moderate output audios and refuse to return them to users if any inappropriate content is detected. We again utilize OpenAI's moderation API to moderate the text of the output audio.\nCompared with the detection in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.SS1.SSS2\" title=\"6.1.2 Text harmfulness detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6.1.2</span></a>, the key difference is that many LALMs simultaneously produce both text and audio in their response; hence, the text can be directly used instead of relying on external speech recognition models.\nMoreover, adversaries have no opportunity to bypass the detection since the audio has not yet been released.</p>\n\n",
                "matched_terms": [
                    "bypass",
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting audios generated by the two baseline attacks and the Read, Spell, and Phoneme attacks is straightforward since the texts are complete and their harmful semantics are not concealed.\nFor the Concat attack, the defender can maintain a buffer storing the output texts from users' consecutive queries and initiate detection at an appropriate time (e.g., when the buffer is full). For the Shuffle attack, the defender can attempt to restore the word positions (using tools or simply random reordering) and flag the text as harmful once any version of the reordered text is identified as harmful.</p>\n\n",
                "matched_terms": [
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the defense on the GPT-4o-mini-audio model and the Self dataset.\nThe results are shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.T3\" title=\"Table III &#8227; 6.2 Proactive Defense by TTS Providers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.\nThe defense can detect at least 57% of harmful audio with a minimum harmfulness score of 0.49, regardless of the attack method.\nEven with the shuffled word positions employed by adversaries, the defense is highly effective in detecting the Shuffle attack. When the defender can recover the correct positions, the detection becomes substantially more effective, with the detection ratio increasing from 72.48% to 93.29%.\nAlthough this defense is simple and effective, we find that it has surprisingly not been incorporated into OpenAI's models.</p>\n\n",
                "matched_terms": [
                    "results",
                    "harmfulness",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Toxic word localization and counts.</span>\nThe Read, Spell, and Shuffle attacks depend on a toxic span detection module to localize harmful words. Misclassification, either missing true toxic spans or flagging benign tokens, can (i) suppress attack success by omitting crucial words, or (ii) introduce unnecessary placeholders that confuse the model.\nAdditionally, when the harmful text contains numerous toxic words, the prompt accumulates many ``[word]'' placeholders; models then face a more challenging disambiguation and placement problem (mapping each placeholder to the correct target and position), which can reduce the completeness of the final utterance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Defensive analysis reveals critical gaps: current deepfake audio detection is unreliable for high-quality LALM outputs; reactive transcribe-then-moderate pipelines are brittle and vulnerable to adversarial perturbations; whereas proactively moderating model-emitted text (possibly with buffering and reordering checks) is simple yet substantially more effective.</p>\n\n",
                "matched_terms": [
                    "text",
                    "reactive",
                    "detection"
                ]
            }
        ]
    },
    "S6.T3": {
        "caption": "TABLE III: Results of proactive detection defense",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Concat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Shuffle</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Shuffle-R</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Read</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Spell</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Phoneme</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ratio (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">84.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">72.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">93.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">80.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">82.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Score</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.71</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "phoneme",
            "score",
            "spell",
            "iii",
            "detection",
            "proactive",
            "defense",
            "read",
            "shuffler",
            "results",
            "concat",
            "shuffle",
            "ratio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the defense on the GPT-4o-mini-audio model and the Self dataset.\nThe results are shown in TABLE&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.T3\" title=\"Table III &#8227; 6.2 Proactive Defense by TTS Providers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.\nThe defense can detect at least 57% of harmful audio with a minimum harmfulness score of 0.49, regardless of the attack method.\nEven with the shuffled word positions employed by adversaries, the defense is highly effective in detecting the Shuffle attack. When the defender can recover the correct positions, the detection becomes substantially more effective, with the detection ratio increasing from 72.48% to 93.29%.\nAlthough this defense is simple and effective, we find that it has surprisingly not been incorporated into OpenAI's models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span>, a suite of five attacks organized into two families that address these challenges. The first family employs semantic obfuscation techniques (Concat, Shuffle) that conceal harmful content within text. The second leverages audio-modality exploits (Read, Spell, Phoneme) that inject harmful content through auxiliary audio channels while maintaining ostensibly benign textual prompts. Through evaluation across five commercial LALMs-based TTS systems and three datasets spanning two languages, we demonstrate that our attacks substantially reduce refusal rates and increase the toxicity of generated speech.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell",
                    "read",
                    "concat",
                    "shuffle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement all these strategies in an attack tool named <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span> and perform extensive experiments to evaluate its effectiveness using five modern commercial LALMs and three datasets covering two languages.\nWe first demonstrate experimentally that the proposed attacks significantly reduce refusal rates and successfully produce the exact harmful phrases in audio form. For instance, under at least one of our attacks, all five models that initially refused a majority of hate-speech prompts were compelled to synthesize 100% of them, with high toxicity scores in the resulting audio.\nNext, we validate the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span> across different output voice styles, across different harmful categories, in synthesizing audio containing harmful text without harmful words, and show that combining individual attacks boosts attack efficacy.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Case studies with audio samples are available at our anonymous website: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://harmgen.netlify.app\" title=\"\">https://harmgen.netlify.app</a>.</span></span></span>\nFinally, we discuss potential countermeasures to mitigate <span class=\"ltx_text ltx_font_smallcaps\">HarmGen</span>. We found that reactive defenses adopted by audio-streaming platform maintainers are ineffective: state-of-the-art deepfake audio detectors fail on high-fidelity adversarial outputs; reactive transcribe-then-text-moderation is easily evaded by adversarial perturbations. Thus, we propose employing proactive defense by TTS providers, which moderates the model-emitted text accompanying audio and effectively detects 57&#8211;93% of attacks.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "proactive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Settings.</span>\nFor the Concat and all three multi-modal attacks, we use Mudes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib55\" title=\"\">55</a>]</cite> as <math alttext=\"\\tt TSD\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#120451;&#120450;&#120435;</mi><annotation encoding=\"application/x-tex\">\\tt TSD</annotation></semantics></math> for toxic span detection. For the Shuffle attacks, we employ Montreal Forced Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib57\" title=\"\">57</a>]</cite> as <math alttext=\"\\tt FA\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#120437;&#120432;</mi><annotation encoding=\"application/x-tex\">\\tt FA</annotation></semantics></math> and set <math alttext=\"N=20\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">N=20</annotation></semantics></math> and <math alttext=\"T=10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">T=10</annotation></semantics></math>. For the Read and Spell attacks, the mapping function <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> is constructed by using IndexTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib17\" title=\"\">17</a>]</cite> to synthesize audio of harmful words and each letter of harmful words, respectively. For the Phoneme attack, we use the CMU Pronouncing Dictionary&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib59\" title=\"\">59</a>]</cite> and grapheme-to-phoneme conversion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib60\" title=\"\">60</a>]</cite> as <math alttext=\"\\tt PH\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><mi>&#120447;&#120439;</mi><annotation encoding=\"application/x-tex\">\\tt PH</annotation></semantics></math>, and the mapping function <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> contains audios provided on the website&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib62\" title=\"\">62</a>]</cite>.\nFor all attacks, we set the silence duration <math alttext=\"sd=50\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">sd=50</annotation></semantics></math> milliseconds.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell",
                    "detection",
                    "read",
                    "concat",
                    "shuffle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Experimental Setting &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nFirst, all five advanced attacks achieve substantially lower refusal rates and significantly higher toxicity scores than the two baseline attacks. For instance, the Concat and Shuffle attacks achieve 0% R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub> for the GPT-4o-mini-tts and Gemini-2.5-live models on the Self dataset, compared with 100% R<sub class=\"ltx_sub\">1</sub> and R<sub class=\"ltx_sub\">2</sub> for the baseline attacks. This demonstrates the high effectiveness of these advanced attacks in generating harmful voices.</p>\n\n",
                "matched_terms": [
                    "shuffle",
                    "results",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we compare and rank the attacks lexicographically by (R<sub class=\"ltx_sub\">2</sub>, R<sub class=\"ltx_sub\">1</sub>, TS): primarily by R<sub class=\"ltx_sub\">2</sub>, breaking ties with R<sub class=\"ltx_sub\">1</sub>, and then with TS.\nIn general, the text-modality harmful semantic concealment attacks (Concat and Shuffle) outperform the multi-modal attacks (Read, Spell, and Phoneme). This is likely because the models fail to recognize some harmful words in the audio for the multi-modal attacks.\nHowever, upon listening, we find that the multi-modal attacks offer the advantage of more natural voice pitch compared to the text-modality attacks. This is because they synthesize from the entire harmful text, whereas the Concat attack synthesizes separately from different segments of the text, and the Shuffle attack synthesizes from shuffled text.\nHowever, the pitch of words within human voices is highly context-dependent.\nThis indicates that when selecting an attack method, adversaries need to balance efficacy and naturalness.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell",
                    "read",
                    "concat",
                    "shuffle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the text-modality attacks, the Concat and Shuffle attacks are comparable in terms of R<sub class=\"ltx_sub\">2</sub>, although the Shuffle attack often exhibits higher R<sub class=\"ltx_sub\">1</sub> since it requires searching for successful random word positions.\nAmong the multi-modal attacks, the Spell attack typically outperforms the others. However, it is not applicable to Chinese harmful text, whereas the Read attack is applicable.</p>\n\n",
                "matched_terms": [
                    "shuffle",
                    "read",
                    "concat",
                    "spell"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LALMs-based TTS models often allow control over the style of output voices by selecting from a list of predefined voices.\nWe evaluate this impact by using four different voices of GPT-4o-mini-audio to synthesize the Ethos dataset with the Spell attack (the best-performing attack under this model and dataset configuration according to Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Experimental Setting &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), specifically: ``alloy'' (female), ``ash'' (male), ``ballad'' (male), and ``coral'' (female).\nThe results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F4\" title=\"Figure 4 &#8227; 5.4.2 Impact of output voice style &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nAlthough the three metrics vary across voices, the differences are minor, indicating the effectiveness of our attacks across different output voice styles.\nInterestingly, we find that our attack with the two male voices (``ballad'' and ``ash'') consistently achieves lower refusal rates and higher toxicity scores than with the two female voices (``alloy'' and ``coral''). This is likely due to differences in the quantity of safety training data between male and female voices.</p>\n\n",
                "matched_terms": [
                    "results",
                    "spell"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize OpenAI's moderation API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib64\" title=\"\">64</a>]</cite> to classify the harmful category of each harmful text. Specifically, the API returns harmfulness scores across six major categories, and we assign the category with the highest score as the harmful type. Ultimately, we identify two harmful categories (harassment and hate) in the Self dataset, and four categories (with the additional inclusion of illicit and violence) in the Ethos and Mul-ZH datasets. The results for individual categories are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F5\" title=\"Figure 5 &#8227; 5.4.3 Effectiveness across different harmful categories &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The difficulty varies across harmful categories, with attacks achieving the highest efficacy for harassment on the Self dataset, for illicit on the Ethos dataset, and for violence on the Mul-ZH dataset. However, all five advanced attacks demonstrate effectiveness across all categories by substantially reducing refusal rates and increasing toxicity scores compared to the two baseline attacks, thereby demonstrating their generalizability.</p>\n\n",
                "matched_terms": [
                    "score",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some sentences with harmful semantics may not contain any explicit harmful words. We identify such sentences in the Ethos dataset by applying span detection and selecting those with empty spans. We evaluate the two baseline attacks B<sub class=\"ltx_sub\">1</sub> and B<sub class=\"ltx_sub\">2</sub> and the advanced attacks Concat and Shuffle on the GPT-4o-mini-audio TTS model. Other attacks are excluded since they require harmful words for audio input. The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F6\" title=\"Figure 6 &#8227; 5.4.4 Effectiveness for harmful text without harmful words &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. First, the refusal rate of the two baseline attacks exceeds 60%, indicating the evident harmful semantics of these sentences despite the absence of harmful words. Both the Concat and Shuffle attacks significantly reduce the refusal rate, achieving 0% R<sub class=\"ltx_sub\">2</sub>, meaning they successfully compel the model to synthesize all sentences in at least one trial. They also improve the toxicity score by a substantial margin.\nThese results demonstrate the effectiveness of our attacks in handling this special case of harmful text.</p>\n\n",
                "matched_terms": [
                    "score",
                    "detection",
                    "results",
                    "concat",
                    "shuffle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previously, we evaluated the advanced attacks individually. Here, we explore whether combining different attacks can further enhance attack efficacy. Specifically, we combine the Shuffle attack with each of the Read, Spell, and Phoneme attacks. Other combinations are either infeasible or unnecessary. The results on GPT-4o-mini-audio and the Ethos dataset are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.F7\" title=\"Figure 7 &#8227; 5.4.5 Combining advanced attacks &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nGenerally, combining Shuffle with X (where X is Read, Spell, or Phoneme) yields lower refusal rates and higher toxicity scores than both Shuffle and X alone. For instance, the combined attack Shuffle+Spell achieves an R<sub class=\"ltx_sub\">1</sub> of 13.6%, substantially lower than that of Shuffle (75.5%) and Spell (35.2%).</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell",
                    "read",
                    "results",
                    "shuffle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the negative effects of harmful audio generated by our attacks, we discuss countermeasures taken by platform maintainers (where harmful voices are released) and TTS providers separately. The former constitutes reactive defense since harmful voices have already been created, while the latter represents proactive defense that prevents harmful voices from being generated for adversaries.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "proactive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform detection on the audio modality using the state-of-the-art deepfake audio detector AASIST2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib65\" title=\"\">65</a>]</cite>, which ranks first on the leaderboard&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib22\" title=\"\">22</a>]</cite>. We do not perform detection on the text modality since the harmful texts originate from humans. Given an audio sample, the model produces a score representing the confidence that the audio is fake. We utilize the scores in two ways: (1) applying the sigmoid function to the score to obtain a probability, and classifying the input as ``deepfake'' when the probability is at least 0.5. We report the detection accuracy. (2) Classifying the input as ``deepfake'' when the score is at least a specified threshold.\nTo reduce the impact of threshold selection on evaluation, we randomly select 10,000 human-uttered (real) voices from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib66\" title=\"\">66</a>]</cite> and use them along with the generated audio to calculate the Equal Error Rate (EER), where the false positive rate equals the false negative rate. We evaluate on the Self dataset using two models: GPT-4o-mini-audio and GPT-5o-nano, which are representative of end-to-end and cascaded models, respectively. The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.F8\" title=\"Figure 8 &#8227; 6.1.1 Deepfake detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. The detection is more effective against voices generated by GPT-5o-nano. This is because GPT-5o-nano relies on an external conventional TTS model (CosyVoice 2.0) to synthesize audio, indicating that LALMs-based TTS models produce voices of substantially higher quality. For both models, the defense fails to detect a large portion of audio, e.g., achieving no more than 25% (resp. 70%) accuracy and over 75% (resp. 55%) EER on the GPT-4o-mini-audio (resp. GPT-5o-nano) model. These results indicate that current state-of-the-art deepfake audio detection models fail to detect audio generated by our attacks due to their high quality and minimal artifacts compared to real audio.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "score",
                    "results",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform detection on the text modality by using the state-of-the-art speech recognition model Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib67\" title=\"\">67</a>]</cite> to obtain transcriptions of harmful voices and applying OpenAI's moderation API&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib64\" title=\"\">64</a>]</cite> to detect harmfulness. We do not perform detection on the audio modality since we are not aware of any available harmful audio detectors that operate directly on the audio modality. We evaluate on the GPT-4o-mini-audio model and the Self dataset. We report both the ratio of texts flagged as harmful and the average harmfulness scores (cf.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S5.SS4.SSS3\" title=\"5.4.3 Effectiveness across different harmful categories &#8227; 5.4 Experimental Results &#8227; 5 Advanced Attacks &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">&#167;</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.4.3</span></a>). The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#S6.T2\" title=\"Table II &#8227; 6.1.2 Text harmfulness detection &#8227; 6.1 Reactive Defense by Platform Maintainers &#8227; 6 Countermeasures &#8227; Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>. The defense can detect at least 66% of harmful audio, regardless of the attack method. However, since the defense is reactive, adversaries can bypass it by post-processing the harmful voices returned by TTS models prior to release. We implement this by crafting adversarial perturbations on the Whisper model using the attack described in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10913v1#bib.bib68\" title=\"\">68</a>]</cite>. This attack minimizes the distance between the intermediate outputs of the Whisper model for the adversarial audio and a targeted audio with randomly selected text content, while also employing a psychoacoustic model to render the perturbations inaudible. When perturbations are added to harmful voices, the effectiveness of the defense decreases significantly, exhibiting a substantially lower detection ratio and lower moderation scores. This occurs because the defense detects incorrect transcripts.</p>\n\n",
                "matched_terms": [
                    "results",
                    "defense",
                    "ratio",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting audios generated by the two baseline attacks and the Read, Spell, and Phoneme attacks is straightforward since the texts are complete and their harmful semantics are not concealed.\nFor the Concat attack, the defender can maintain a buffer storing the output texts from users' consecutive queries and initiate detection at an appropriate time (e.g., when the buffer is full). For the Shuffle attack, the defender can attempt to restore the word positions (using tools or simply random reordering) and flag the text as harmful once any version of the reordered text is identified as harmful.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell",
                    "detection",
                    "read",
                    "concat",
                    "shuffle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Naturalness.</span>\nThe Concat attack synthesizes short substrings (or individual words) independently and then concatenates them, while the Shuffle attack generates speech for a syntactically scrambled sentence and subsequently reorders word segments. Because intonation, rhythm, and coarticulation are context-dependent,\nthese two attacks disrupt pitch contours and timing, yielding less natural prosody and occasional discontinuities.\nAdditionally, for the Shuffle attack, we rely on a forced aligner to obtain word-level time indices. Any alignment errors (e.g., partial words or spillover into neighboring words) degrade reconstruction quality and further reduce naturalness.\nThis limitation can be alleviated by switching the shuffling units from words to substrings, which we leave for future work.</p>\n\n",
                "matched_terms": [
                    "shuffle",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Toxic word localization and counts.</span>\nThe Read, Spell, and Shuffle attacks depend on a toxic span detection module to localize harmful words. Misclassification, either missing true toxic spans or flagging benign tokens, can (i) suppress attack success by omitting crucial words, or (ii) introduce unnecessary placeholders that confuse the model.\nAdditionally, when the harmful text contains numerous toxic words, the prompt accumulates many ``[word]'' placeholders; models then face a more challenging disambiguation and placement problem (mapping each placeholder to the correct target and position), which can reduce the completeness of the final utterance.</p>\n\n",
                "matched_terms": [
                    "shuffle",
                    "read",
                    "detection",
                    "spell"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability.</span>\nThis limitation persists in the multi-modal strategies (Spell and Phoneme).\nThe Spell attack assumes alphabetic spelling conventions and is less applicable to languages without letter-wise spelling, e.g., Chinese. The Phoneme attack requires reliable grapheme-to-phoneme resources and robust recognition of phonetic sequences, which may be scarce for low-resource languages.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work reframes TTS misuse and safety by shifting attention from who is speaking (speaker identity) to what is being said (linguistic content). We demonstrate that LALMs-based TTS models, despite safety alignment and input/output moderation layers, can be coerced into vocalizing toxic sentences with high fidelity. Our text-modality strategies (Concat, Shuffle) neutralize apparent harmful semantics to avoid refusals, while multi-modal strategies (Read, Spell, Phoneme) covertly reintroduce toxic words via audio,\ndriving down refusal rates across models and languages. In combination attacks such as Shuffle+Spell, we achieve the strongest overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "spell",
                    "read",
                    "concat",
                    "shuffle"
                ]
            }
        ]
    }
}