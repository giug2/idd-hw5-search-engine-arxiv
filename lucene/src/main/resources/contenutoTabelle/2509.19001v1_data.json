{
    "S2.T1": {
        "source_file": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS",
        "caption": "Table 1: Comparison of results on TextrolSpeech and EmoVoice-DB test sets.",
        "body": "Model\nSubjective\nObjective\n\n\nMOS-N ↑\nMOS-S ↑\nDNSMOS ↑\nEMO-SIM ↑\nWER ↓\n\n\n\n\nPromptStyle\n2.674 ±\\pm 0.145\n2.420 ±\\pm 0.147\n3.68\n0.529\n17.92%\n\n\nPromptTTS\n2.920 ±\\pm 0.137\n2.601 ±\\pm 0.148\n3.65\n0.588\n4.38%\n\n\nCosyVoice\n3.240 ±\\pm 0.138\n3.028 ±\\pm 0.149\n3.77\n0.635\n6.10%\n\n\nCosyVoice2\n3.920 ±\\pm 0.112\n3.885 ±\\pm 0.116\n3.83\n0.714\n5.71%\n\n\nEmoVoice-PP\n3.694 ±\\pm 0.123\n3.594 ±\\pm 0.128\n3.87\n0.613\n8.56%\n\n\nHD-PPT (Ours)\n4.108 ±\\pm 0.105\n4.167 ±\\pm 0.103\n3.84\n0.753\n5.18%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Subjective</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Objective</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MOS-N&#160;&#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MOS-S&#160;&#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">DNSMOS&#160;&#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">EMO-SIM&#160;&#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER&#160;&#8595;</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">PromptStyle</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.674 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.145</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.420 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.147</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.529</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">17.92%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PromptTTS</th>\n<td class=\"ltx_td ltx_align_center\">2.920 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.137</td>\n<td class=\"ltx_td ltx_align_center\">2.601 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.148</td>\n<td class=\"ltx_td ltx_align_center\">3.65</td>\n<td class=\"ltx_td ltx_align_center\">0.588</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.38%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center\">3.240 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.138</td>\n<td class=\"ltx_td ltx_align_center\">3.028 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.149</td>\n<td class=\"ltx_td ltx_align_center\">3.77</td>\n<td class=\"ltx_td ltx_align_center\">0.635</td>\n<td class=\"ltx_td ltx_align_center\">6.10%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice2</th>\n<td class=\"ltx_td ltx_align_center\">3.920 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.112</td>\n<td class=\"ltx_td ltx_align_center\">3.885 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.116</td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n<td class=\"ltx_td ltx_align_center\">0.714</td>\n<td class=\"ltx_td ltx_align_center\">5.71%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EmoVoice-PP</th>\n<td class=\"ltx_td ltx_align_center\">3.694 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.123</td>\n<td class=\"ltx_td ltx_align_center\">3.594 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.128</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.87</span></td>\n<td class=\"ltx_td ltx_align_center\">0.613</td>\n<td class=\"ltx_td ltx_align_center\">8.56%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">HD-PPT (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.108 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.105</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.167 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.103</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">3.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.753</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">5.18%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sets",
            "subjective",
            "cosyvoice",
            "ours",
            "dnsmos",
            "±pm",
            "emosim",
            "objective",
            "moss",
            "test",
            "prompttts",
            "wer",
            "results",
            "model",
            "hdppt",
            "mosn",
            "cosyvoice2",
            "emovoicedb",
            "textrolspeech",
            "promptstyle",
            "emovoicepp",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S2.T1\" title=\"Table 1 &#8227; 2.2 LLM&#8217;s Hierarchical Decoding &#8227; 2 METHODOLOGY &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comprehensive comparison between HD-PPT and the five baseline models on the combined test sets of TextrolSpeech and EmoVoice-DB. HD-PPT achieves superior performance across the board. In subjective tests, it received the highest MOS-N and MOS-S scores, which prove its excellent naturalness and stylistic consistency. Objectively, it achieved the best EMO-SIM score for controllable emotional expression. These high scores directly validate that our hierarchical structure improved instruction adherence and stylistic control. Furthermore, HD-PPT also achieved a competitive DNSMOS and the second lowest WER, demonstrating its ability to generate high-fidelity and intelligible speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at &#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xxh333.github.io/\" title=\"\">https://xxh333.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "hdppt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the preference tokens capture distinct speech attributes, we impose specific supervision mechanisms. The content-preference tokens are supervised by an ASR task, using a Whisper-Small decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib16\" title=\"\">16</a>]</cite> to predict text, thus exposing them to semantic information. The prompt-preference tokens are supervised by a CLAP-based contrastive loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib14\" title=\"\">14</a>]</cite> to capture prosody and emotion. A cross-attention module maps these tokens to a fixed-length embedding. This embedding is trained to maximize cosine similarity with the text embedding of the corresponding prompt (from a pre-trained RoBERTa-base model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib17\" title=\"\">17</a>]</cite>), while minimizing similarity to embeddings of noncorresponding prompts. This objective compels the prompt-preference tokens to encode fine-grained stylistic attributes correlated with the prompt.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">1) Datasets and Baselines.</span> We conducted experiments on two public datasets to ensure a comprehensive evaluation: TextrolSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib11\" title=\"\">11</a>]</cite> for fine-grained style control and EmoVoice-DB&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib6\" title=\"\">6</a>]</cite> for emotional control. All audio was resampled to 24kHz. We compared HD-PPT against two categories of baselines: 1) <span class=\"ltx_text ltx_font_bold\">Explicit style encoding</span>: PromptTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib4\" title=\"\">4</a>]</cite> and PromptStyle&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib8\" title=\"\">8</a>]</cite>; and 2) <span class=\"ltx_text ltx_font_bold\">LLM-driven</span>: CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib1\" title=\"\">1</a>]</cite>, EmoVoice-PP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib6\" title=\"\">6</a>]</cite>, and our main baseline, CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>]</cite>, which represents one of the current state-of-the-art systems in Instruct-TTS.</p>\n\n",
                "matched_terms": [
                    "textrolspeech",
                    "cosyvoice",
                    "promptstyle",
                    "prompttts",
                    "hdppt",
                    "emovoicepp",
                    "cosyvoice2",
                    "emovoicedb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">2) Evaluation metrics</span>. Our evaluation employed a combination of subjective and objective metrics. For subjective tests, 18 participants rated speech naturalness (MOS-N) and stylistic consistency (MOS-S) with the instructional text on a 5-point Likert scale. For objective evaluation, we used three metrics. We used the CV3-Eval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib20\" title=\"\">20</a>]</cite> to obtain perceptual quality through the Deep Noise Suppression Mean Opinion Score (DNSMOS) and the word error rate (WER). Furthermore, emotional similarity (EMO-SIM) was quantified by the cosine similarity of the emotional feature vectors between real and synthesized audio, extracted using the pre-trained emotion2vec-plus-large model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib21\" title=\"\">21</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "moss",
                    "model",
                    "subjective",
                    "dnsmos",
                    "wer",
                    "mosn",
                    "emosim",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">3) Implementation details</span>. We first trained our speech token codec, which consists of a 5-layer conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib22\" title=\"\">22</a>]</cite> extractor and a 4-layer causal transformer combiner. The FSQ codebook sizes for the prompt- and content-preference tokens were set to 64 and 1296, respectively, both operating at a rate of 25Hz. The codec was trained for 50 epochs on 4 NVIDIA 4090 GPUs using the AdamW optimizer with a learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>. Following this, we trained the modified LLM, which uses Qwen2.5-0.5B as its backbone. For this model, we employed a lightweight 2-layer auto-regressive transformer with a fixed length of 3 as the hierarchical Decoder. The LLM was trained for 16 epochs on the same hardware using the AdamW optimizer, but with a learning rate of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>. For the final audio generation, we used the official pre-trained vocoder from CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>]</cite>, which combines a flow-matching model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib23\" title=\"\">23</a>]</cite> and HifiGAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib24\" title=\"\">24</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the efficacy of preference tokens, we conducted ablation experiments across four variants: 1) <span class=\"ltx_text ltx_font_bold\">w/o Content-Pref.</span>: removing content-preference tokens from the decoding process; 2) <span class=\"ltx_text ltx_font_bold\">w/o Prompt-Pref.</span>: removing prompt-preference tokens; 3) <span class=\"ltx_text ltx_font_bold\">w/o Dual-Pref.</span>: bypassing both preference tokens; and 4) <span class=\"ltx_text ltx_font_bold\">w/o Instruct Text</span>: generating speech without the style prompt. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Comparison with Baselines &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, removing either preference token led to a performance drop. The removal of content-preference tokens caused a significant increase in WER, highlighting their role in maintaining semantic integrity. The absence of prompt-preference tokens led to a notable decrease in EMO-SIM, underscoring their necessity for stylistic nuances. When both were removed, all metrics degraded, confirming the importance of our structured intermediate representations. Furthermore, the drastic drop in EMO-SIM without instruction text proves that the model&#8217;s stylistic control is directly derived from the prompt rather than dataset bias.</p>\n\n",
                "matched_terms": [
                    "emosim",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated our hierarchical decoding strategy against two alternatives: 1) <span class=\"ltx_text ltx_font_bold\">Parallel</span>: predicting all three token types (content, prompt, speech) simultaneously from the LLM&#8217;s hidden state. 2) <span class=\"ltx_text ltx_font_bold\">Single-step</span>: directly predicting the final speech tokens, bypassing the intermediate preference tokens. Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S3.T3\" title=\"Table 3 &#8227; 3.2.3 Ablation Study on Hierarchical Decoding Strategy &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show that our hierarchical approach outperforms both. The suboptimal results of the parallel approach demonstrated that an explicit conditional dependency is needed for effective output structuring. The weaker performance of the single-step model further affirmed the need for structured intermediate representations. These findings confirmed that the sequential, layer-by-layer process of our hierarchical strategy was crucial for its success, enabling precise control via the preference extraction process. Importantly, this structured approach only added a 34% inference latency overhead to the LLM component compared to the single-step baseline.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced HD-PPT, a novel framework for Instruct-TTS that resolves the hierarchical mismatch between textual instructions and speech signals. By employing a specialized codec to extract dual preference tokens from speech tokens and a hierarchical decoding strategy to generate them sequentially, our method significantly enhances fine-grained control and expressiveness. Extensive experiments demonstrated that HD-PPT outperforms state-of-the-art baselines in both instruction adherence and speech naturalness. The results validate that aligning the generative process with the intrinsic structure of speech is a robust paradigm for precise and controllable synthesis. Future work could explore extending this hierarchical approach to other domains, such as singing voice synthesis or cross-lingual TTS.</p>\n\n",
                "matched_terms": [
                    "hdppt",
                    "results"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS",
        "caption": "Table 2: Ablation study on preference tokens.",
        "body": "Model\nDNSMOS ↑\nEMO-SIM ↑\nWER ↓\n\n\n\n\nw/o Content-Pref.\n3.76\n0.742\n8.04%\n\n\nw/o Prompt-Pref.\n3.76\n0.728\n5.49%\n\n\nw/o Dual-Pref.\n3.73\n0.716\n10.10%\n\n\nw/o Instruct Text\n3.78\n0.605\n5.44%\n\n\nProposed\n3.84\n0.753\n5.18%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DNSMOS &#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">EMO-SIM &#8593;</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER &#8595;</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">w/o Content-Pref.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.742</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">8.04%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">w/o Prompt-Pref.</th>\n<td class=\"ltx_td ltx_align_center\">3.76</td>\n<td class=\"ltx_td ltx_align_center\">0.728</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5.49%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">w/o Dual-Pref.</th>\n<td class=\"ltx_td ltx_align_center\">3.73</td>\n<td class=\"ltx_td ltx_align_center\">0.716</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">10.10%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">w/o Instruct Text</th>\n<td class=\"ltx_td ltx_align_center\">3.78</td>\n<td class=\"ltx_td ltx_align_center\">0.605</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5.44%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Proposed</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.753</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.18%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "instruct",
            "model",
            "study",
            "ablation",
            "text",
            "preference",
            "contentpref",
            "proposed",
            "dnsmos",
            "promptpref",
            "wer",
            "dualpref",
            "tokens",
            "emosim"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To validate the efficacy of preference tokens, we conducted ablation experiments across four variants: 1) <span class=\"ltx_text ltx_font_bold\">w/o Content-Pref.</span>: removing content-preference tokens from the decoding process; 2) <span class=\"ltx_text ltx_font_bold\">w/o Prompt-Pref.</span>: removing prompt-preference tokens; 3) <span class=\"ltx_text ltx_font_bold\">w/o Dual-Pref.</span>: bypassing both preference tokens; and 4) <span class=\"ltx_text ltx_font_bold\">w/o Instruct Text</span>: generating speech without the style prompt. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Comparison with Baselines &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, removing either preference token led to a performance drop. The removal of content-preference tokens caused a significant increase in WER, highlighting their role in maintaining semantic integrity. The absence of prompt-preference tokens led to a notable decrease in EMO-SIM, underscoring their necessity for stylistic nuances. When both were removed, all metrics degraded, confirming the importance of our structured intermediate representations. Furthermore, the drastic drop in EMO-SIM without instruction text proves that the model&#8217;s stylistic control is directly derived from the prompt rather than dataset bias.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at &#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xxh333.github.io/\" title=\"\">https://xxh333.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "proposed",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current approaches fall largely into two main categories: explicit style encoding methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib9\" title=\"\">9</a>]</cite> and Large Language Model (LLM)-driven methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib12\" title=\"\">12</a>]</cite>. Although explicit style encoding methods can achieve basic prompt control, they are limited by their inefficient structure and coarse-grained control. In contrast, LLM-based methods offer a more flexible architecture and stronger control for interpreting nuanced textual instructions. Despite their promise, these methods often struggle with precision and robustness, particularly when faced with complex or subtle prompts. This is primarily because they directly map the style information from the text instructions onto the speech tokens, making fine-grained control difficult, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a). In addition, they treat speech tokens as a monolithic sequence. We posit that this limitation stems from a fundamental hierarchical mismatch: they attempt to map a single-level text instruction directly onto multilevel speech tokens. This approach overlooks the inherently hierarchical nature of speech, which involves three types of information: linguistic, paralinguistic, and extralinguistic, corresponding to spoken content, prosody/emotion, and speaker/scenario, respectively&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To effectively extract fine-grained preference representations from speech, we designed a speech token codec based on finite-scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib15\" title=\"\">15</a>]</cite>, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(a). The model is optimized via a combination of a reconstruction loss and two auxiliary supervision tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "preference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The codec employs a transformer-based architecture. A preference token extractor first encodes the input speech tokens (from the pre-trained CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>]</cite> tokenizer) into a continuous representation <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>. Subsequently, two independent FSQ modules quantize <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> into distinct, discrete preference tokens. Finally, a causal transformer-based speech token combiner fuses these preference tokens to reconstruct the original speech tokens. This causal design enforces temporal alignment between the representations. In addition, slight random noise is injected during training to enhance robustness.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "preference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the preference tokens capture distinct speech attributes, we impose specific supervision mechanisms. The content-preference tokens are supervised by an ASR task, using a Whisper-Small decoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib16\" title=\"\">16</a>]</cite> to predict text, thus exposing them to semantic information. The prompt-preference tokens are supervised by a CLAP-based contrastive loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib14\" title=\"\">14</a>]</cite> to capture prosody and emotion. A cross-attention module maps these tokens to a fixed-length embedding. This embedding is trained to maximize cosine similarity with the text embedding of the corresponding prompt (from a pre-trained RoBERTa-base model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib17\" title=\"\">17</a>]</cite>), while minimizing similarity to embeddings of noncorresponding prompts. This objective compels the prompt-preference tokens to encode fine-grained stylistic attributes correlated with the prompt.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens",
                    "model",
                    "preference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the preference speech tokens established, we leverage an LLM to generate them from textual instructions. We chose Qwen2.5-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib18\" title=\"\">18</a>]</cite> as backbone, paired with a lightweight transformer decoder to perform hierarchical generation.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "preference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM auto-regressively generates a sequence of hidden states <math alttext=\"T_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">T_{h}</annotation></semantics></math> based on input text <math alttext=\"T_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">T_{t}</annotation></semantics></math>. At each step, the hidden state is fed into the lightweight hierarchical decoder to sequentially predict the tokens. Assume that the content-preference tokens, prompt-preference tokens, and speech tokens are <math alttext=\"T_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">T_{c}</annotation></semantics></math>, <math alttext=\"T_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">T_{p}</annotation></semantics></math>, and <math alttext=\"T_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">T_{s}</annotation></semantics></math>, respectively. Additionally, let <math alttext=\"\\theta_{LM}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\theta_{LM}</annotation></semantics></math>, <math alttext=\"\\theta_{HD}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\theta_{HD}</annotation></semantics></math> represent the parameters of the LLM and the decoder. The generation process at step <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m8\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> is as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the model robustly learns this hierarchical process, we employ two regularization strategies during training. First, we introduce stochasticity by probabilistically masking the hidden states and prompt tokens, and by concatenating token logits with the token embeddings as input to the lightweight decoder. These interventions compel the model to integrate the information from all available sources rather than relying on a single one. Second, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b), an auxiliary linear layer is added to directly project the LLM&#8217;s hidden states into the speech tokens, ensuring that its internal representations remain acoustically grounded.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">2) Evaluation metrics</span>. Our evaluation employed a combination of subjective and objective metrics. For subjective tests, 18 participants rated speech naturalness (MOS-N) and stylistic consistency (MOS-S) with the instructional text on a 5-point Likert scale. For objective evaluation, we used three metrics. We used the CV3-Eval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib20\" title=\"\">20</a>]</cite> to obtain perceptual quality through the Deep Noise Suppression Mean Opinion Score (DNSMOS) and the word error rate (WER). Furthermore, emotional similarity (EMO-SIM) was quantified by the cosine similarity of the emotional feature vectors between real and synthesized audio, extracted using the pre-trained emotion2vec-plus-large model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib21\" title=\"\">21</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "dnsmos",
                    "wer",
                    "emosim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">3) Implementation details</span>. We first trained our speech token codec, which consists of a 5-layer conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib22\" title=\"\">22</a>]</cite> extractor and a 4-layer causal transformer combiner. The FSQ codebook sizes for the prompt- and content-preference tokens were set to 64 and 1296, respectively, both operating at a rate of 25Hz. The codec was trained for 50 epochs on 4 NVIDIA 4090 GPUs using the AdamW optimizer with a learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>. Following this, we trained the modified LLM, which uses Qwen2.5-0.5B as its backbone. For this model, we employed a lightweight 2-layer auto-regressive transformer with a fixed length of 3 as the hierarchical Decoder. The LLM was trained for 16 epochs on the same hardware using the AdamW optimizer, but with a learning rate of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>. For the final audio generation, we used the official pre-trained vocoder from CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>]</cite>, which combines a flow-matching model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib23\" title=\"\">23</a>]</cite> and HifiGAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib24\" title=\"\">24</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S2.T1\" title=\"Table 1 &#8227; 2.2 LLM&#8217;s Hierarchical Decoding &#8227; 2 METHODOLOGY &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comprehensive comparison between HD-PPT and the five baseline models on the combined test sets of TextrolSpeech and EmoVoice-DB. HD-PPT achieves superior performance across the board. In subjective tests, it received the highest MOS-N and MOS-S scores, which prove its excellent naturalness and stylistic consistency. Objectively, it achieved the best EMO-SIM score for controllable emotional expression. These high scores directly validate that our hierarchical structure improved instruction adherence and stylistic control. Furthermore, HD-PPT also achieved a competitive DNSMOS and the second lowest WER, demonstrating its ability to generate high-fidelity and intelligible speech.</p>\n\n",
                "matched_terms": [
                    "emosim",
                    "dnsmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated our hierarchical decoding strategy against two alternatives: 1) <span class=\"ltx_text ltx_font_bold\">Parallel</span>: predicting all three token types (content, prompt, speech) simultaneously from the LLM&#8217;s hidden state. 2) <span class=\"ltx_text ltx_font_bold\">Single-step</span>: directly predicting the final speech tokens, bypassing the intermediate preference tokens. Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S3.T3\" title=\"Table 3 &#8227; 3.2.3 Ablation Study on Hierarchical Decoding Strategy &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show that our hierarchical approach outperforms both. The suboptimal results of the parallel approach demonstrated that an explicit conditional dependency is needed for effective output structuring. The weaker performance of the single-step model further affirmed the need for structured intermediate representations. These findings confirmed that the sequential, layer-by-layer process of our hierarchical strategy was crucial for its success, enabling precise control via the preference extraction process. Importantly, this structured approach only added a 34% inference latency overhead to the LLM component compared to the single-step baseline.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "model",
                    "preference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced HD-PPT, a novel framework for Instruct-TTS that resolves the hierarchical mismatch between textual instructions and speech signals. By employing a specialized codec to extract dual preference tokens from speech tokens and a hierarchical decoding strategy to generate them sequentially, our method significantly enhances fine-grained control and expressiveness. Extensive experiments demonstrated that HD-PPT outperforms state-of-the-art baselines in both instruction adherence and speech naturalness. The results validate that aligning the generative process with the intrinsic structure of speech is a robust paradigm for precise and controllable synthesis. Future work could explore extending this hierarchical approach to other domains, such as singing voice synthesis or cross-lingual TTS.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "preference"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS",
        "caption": "Table 3: Ablation study on hierarchical decoding strategy.",
        "body": "Model\nDNSMOS ↑\nEMO-SIM ↑\nWER ↓\n\n\n\n\nParallel\n3.76\n0.736\n5.99%\n\n\nSingle-step\n3.80\n0.713\n5.93%\n\n\nHierarchical\n3.84\n0.753\n5.18%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DNSMOS &#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">EMO-SIM &#8593;</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER &#8595;</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Parallel</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.736</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">5.99%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Single-step</th>\n<td class=\"ltx_td ltx_align_center\">3.80</td>\n<td class=\"ltx_td ltx_align_center\">0.713</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5.93%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Hierarchical</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.753</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">5.18%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "model",
            "strategy",
            "study",
            "ablation",
            "hierarchical",
            "dnsmos",
            "singlestep",
            "wer",
            "emosim",
            "parallel",
            "decoding"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluated our hierarchical decoding strategy against two alternatives: 1) <span class=\"ltx_text ltx_font_bold\">Parallel</span>: predicting all three token types (content, prompt, speech) simultaneously from the LLM&#8217;s hidden state. 2) <span class=\"ltx_text ltx_font_bold\">Single-step</span>: directly predicting the final speech tokens, bypassing the intermediate preference tokens. Results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S3.T3\" title=\"Table 3 &#8227; 3.2.3 Ablation Study on Hierarchical Decoding Strategy &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show that our hierarchical approach outperforms both. The suboptimal results of the parallel approach demonstrated that an explicit conditional dependency is needed for effective output structuring. The weaker performance of the single-step model further affirmed the need for structured intermediate representations. These findings confirmed that the sequential, layer-by-layer process of our hierarchical strategy was crucial for its success, enabling precise control via the preference extraction process. Importantly, this structured approach only added a 34% inference latency overhead to the LLM component compared to the single-step baseline.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at &#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xxh333.github.io/\" title=\"\">https://xxh333.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "model",
                    "strategy",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current approaches fall largely into two main categories: explicit style encoding methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib9\" title=\"\">9</a>]</cite> and Large Language Model (LLM)-driven methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib12\" title=\"\">12</a>]</cite>. Although explicit style encoding methods can achieve basic prompt control, they are limited by their inefficient structure and coarse-grained control. In contrast, LLM-based methods offer a more flexible architecture and stronger control for interpreting nuanced textual instructions. Despite their promise, these methods often struggle with precision and robustness, particularly when faced with complex or subtle prompts. This is primarily because they directly map the style information from the text instructions onto the speech tokens, making fine-grained control difficult, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a). In addition, they treat speech tokens as a monolithic sequence. We posit that this limitation stems from a fundamental hierarchical mismatch: they attempt to map a single-level text instruction directly onto multilevel speech tokens. This approach overlooks the inherently hierarchical nature of speech, which involves three types of information: linguistic, paralinguistic, and extralinguistic, corresponding to spoken content, prosody/emotion, and speaker/scenario, respectively&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To resolve this hierarchical mismatch, we reframe the synthesis task from monolithic generation to a structured process. We propose <span class=\"ltx_text ltx_font_bold\">HD-PPT</span>, a framework for <span class=\"ltx_text ltx_font_bold\">H</span>ierarchical <span class=\"ltx_text ltx_font_bold\">D</span>ecoding of Content- and <span class=\"ltx_text ltx_font_bold\">P</span>rompt-<span class=\"ltx_text ltx_font_bold\">P</span>reference <span class=\"ltx_text ltx_font_bold\">T</span>okens for Instruct-TTS. As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b), our approach is founded on two key innovations designed to bridge the gap between instruction and audio. To enable fine-grained control, we introduce a novel speech token codec. Jointly supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib14\" title=\"\">14</a>]</cite>, it distinguishes between prompt-preference tokens to capture fine-grained style and content-preference tokens to anchor semantics. To bridge the hierarchical gap, we design a hierarchical decoding strategy. This guides the LLM to generate these representations sequentially: first establishing the semantic foundation, then layering stylistic details, and finally rendering the complete acoustic representation. This structured generation process dramatically enhances the model&#8217;s ability to execute instructions with precision and fidelity.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "strategy",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, our main contributions are as follows.\n1) A novel speech codec to extract and differentiate content- and prompt-preference tokens, providing a fine-grained intermediate modeling target for the LLM.\n2) A hierarchical decoding strategy that aligns generation with the intrinsic structure of speech, guiding the LLM to render audio hierarchically to improve complex instruction execution.\n3) Extensive validation of our method&#8217;s effectiveness, demonstrating state-of-the-art performance in both naturalness and control accuracy.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "strategy",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{rec}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rec}</annotation></semantics></math> is the cross-entropy loss for reconstruction, and the weights <math alttext=\"\\lambda_{asr}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{asr}</annotation></semantics></math> and <math alttext=\"\\lambda_{clap}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{clap}</annotation></semantics></math> are set to 2.0 and 0.8, respectively. Through this joint optimization strategy, the codec effectively learns to extract different preference representations tailored for hierarchical synthesis.</p>\n\n",
                "matched_terms": [
                    "strategy",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that the model robustly learns this hierarchical process, we employ two regularization strategies during training. First, we introduce stochasticity by probabilistically masking the hidden states and prompt tokens, and by concatenating token logits with the token embeddings as input to the lightweight decoder. These interventions compel the model to integrate the information from all available sources rather than relying on a single one. Second, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(b), an auxiliary linear layer is added to directly project the LLM&#8217;s hidden states into the speech tokens, ensuring that its internal representations remain acoustically grounded.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">2) Evaluation metrics</span>. Our evaluation employed a combination of subjective and objective metrics. For subjective tests, 18 participants rated speech naturalness (MOS-N) and stylistic consistency (MOS-S) with the instructional text on a 5-point Likert scale. For objective evaluation, we used three metrics. We used the CV3-Eval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib20\" title=\"\">20</a>]</cite> to obtain perceptual quality through the Deep Noise Suppression Mean Opinion Score (DNSMOS) and the word error rate (WER). Furthermore, emotional similarity (EMO-SIM) was quantified by the cosine similarity of the emotional feature vectors between real and synthesized audio, extracted using the pre-trained emotion2vec-plus-large model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib21\" title=\"\">21</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "emosim",
                    "model",
                    "dnsmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">3) Implementation details</span>. We first trained our speech token codec, which consists of a 5-layer conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib22\" title=\"\">22</a>]</cite> extractor and a 4-layer causal transformer combiner. The FSQ codebook sizes for the prompt- and content-preference tokens were set to 64 and 1296, respectively, both operating at a rate of 25Hz. The codec was trained for 50 epochs on 4 NVIDIA 4090 GPUs using the AdamW optimizer with a learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>. Following this, we trained the modified LLM, which uses Qwen2.5-0.5B as its backbone. For this model, we employed a lightweight 2-layer auto-regressive transformer with a fixed length of 3 as the hierarchical Decoder. The LLM was trained for 16 epochs on the same hardware using the AdamW optimizer, but with a learning rate of <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>. For the final audio generation, we used the official pre-trained vocoder from CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib10\" title=\"\">10</a>]</cite>, which combines a flow-matching model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib23\" title=\"\">23</a>]</cite> and HifiGAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#bib.bib24\" title=\"\">24</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S2.T1\" title=\"Table 1 &#8227; 2.2 LLM&#8217;s Hierarchical Decoding &#8227; 2 METHODOLOGY &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a comprehensive comparison between HD-PPT and the five baseline models on the combined test sets of TextrolSpeech and EmoVoice-DB. HD-PPT achieves superior performance across the board. In subjective tests, it received the highest MOS-N and MOS-S scores, which prove its excellent naturalness and stylistic consistency. Objectively, it achieved the best EMO-SIM score for controllable emotional expression. These high scores directly validate that our hierarchical structure improved instruction adherence and stylistic control. Furthermore, HD-PPT also achieved a competitive DNSMOS and the second lowest WER, demonstrating its ability to generate high-fidelity and intelligible speech.</p>\n\n",
                "matched_terms": [
                    "emosim",
                    "dnsmos",
                    "hierarchical",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the efficacy of preference tokens, we conducted ablation experiments across four variants: 1) <span class=\"ltx_text ltx_font_bold\">w/o Content-Pref.</span>: removing content-preference tokens from the decoding process; 2) <span class=\"ltx_text ltx_font_bold\">w/o Prompt-Pref.</span>: removing prompt-preference tokens; 3) <span class=\"ltx_text ltx_font_bold\">w/o Dual-Pref.</span>: bypassing both preference tokens; and 4) <span class=\"ltx_text ltx_font_bold\">w/o Instruct Text</span>: generating speech without the style prompt. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19001v1#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Comparison with Baselines &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, removing either preference token led to a performance drop. The removal of content-preference tokens caused a significant increase in WER, highlighting their role in maintaining semantic integrity. The absence of prompt-preference tokens led to a notable decrease in EMO-SIM, underscoring their necessity for stylistic nuances. When both were removed, all metrics degraded, confirming the importance of our structured intermediate representations. Furthermore, the drastic drop in EMO-SIM without instruction text proves that the model&#8217;s stylistic control is directly derived from the prompt rather than dataset bias.</p>\n\n",
                "matched_terms": [
                    "emosim",
                    "wer",
                    "decoding",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduced HD-PPT, a novel framework for Instruct-TTS that resolves the hierarchical mismatch between textual instructions and speech signals. By employing a specialized codec to extract dual preference tokens from speech tokens and a hierarchical decoding strategy to generate them sequentially, our method significantly enhances fine-grained control and expressiveness. Extensive experiments demonstrated that HD-PPT outperforms state-of-the-art baselines in both instruction adherence and speech naturalness. The results validate that aligning the generative process with the intrinsic structure of speech is a robust paradigm for precise and controllable synthesis. Future work could explore extending this hierarchical approach to other domains, such as singing voice synthesis or cross-lingual TTS.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "strategy",
                    "hierarchical"
                ]
            }
        ]
    }
}