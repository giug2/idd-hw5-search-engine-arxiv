{
    "S3.T1": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 1: Proportion of participants who thought speaker was human, sorted.",
        "body": "Speaker\nProportion\n\n\n\n\nOpenAI\n0.625\n\n\nGoogle\n0.571\n\n\nAzure\n0.338\n\n\nVITS\n0.207\n\n\nPolly\n0.075",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:195.1pt;\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Proportion</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.625</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Google</th>\n<td class=\"ltx_td ltx_align_right\">0.571</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Azure</th>\n<td class=\"ltx_td ltx_align_right\">0.338</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VITS</th>\n<td class=\"ltx_td ltx_align_right\">0.207</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.075</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sorted",
            "openai",
            "speaker",
            "thought",
            "human",
            "azure",
            "proportion",
            "google",
            "who",
            "participants",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "human",
                    "participants"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The simpler one looks solely at the proportion of speakers who agree with a model (i.e., place an event where the model does) at a particular point. We call this proportion, calculated for each word, the &#8220;agreement score&#8221; signal. More formally, we define a discrete signal comprising <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> words as <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\{0,1\\}</annotation></semantics></math>, where <math alttext=\"x_{i}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x_{i}=1</annotation></semantics></math> if there is an event at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word, and <math alttext=\"x_{i}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x_{i}=0</annotation></semantics></math> otherwise. Our goal is to compare a machine signal <math alttext=\"p=(p_{1},p_{2},\\dots,p_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p=(p_{1},p_{2},\\dots,p_{n})</annotation></semantics></math> against a set of human signals <math alttext=\"\\mathcal{S}=\\{s_{1},s_{2},\\dots,s_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{s_{1},s_{2},\\dots,s_{m}\\}</annotation></semantics></math>, where <math alttext=\"s_{i}=(s_{i,1},s_{i,2},\\dots,s_{i,n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i}=(s_{i,1},s_{i,2},\\dots,s_{i,n})</annotation></semantics></math>. Then, we define the agreement score of a model at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m9\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word as</p>\n\n",
                "matched_terms": [
                    "proportion",
                    "who",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can then establish a fixed threshold <math alttext=\"c\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m10\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in[0,1]</annotation></semantics></math> that <math alttext=\"\\alpha_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m11\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\alpha_{i}</annotation></semantics></math> must exceed for the model to be &#8220;correct&#8221;&#8212;that is, the model is correct for word <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m12\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> if and only if <math alttext=\"\\alpha_{i}\\geq c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m13\" intent=\":literal\"><semantics><mrow><msub><mi>&#945;</mi><mi>i</mi></msub><mo>&#8805;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha_{i}\\geq c</annotation></semantics></math>. In our experiments, we used <math alttext=\"c=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m14\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">c=0.5</annotation></semantics></math>, a simple majority. Put simply, if the proportion of speakers who agree with the model (i.e., decide to put or not put an event where the model does) at a given point exceeds <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m15\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, the model is considered correct for that event.</p>\n\n",
                "matched_terms": [
                    "proportion",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "human",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MOS experiment, participants were recruited through two pools: (1) 91 university students, and (2) 49 Prolific users (32 female, 17 male; mean age 40.84 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.89). Prolific participants were required to have an 85&#8211;100% approval rate, be based in the United States, and list English as their native language. Participants with outlier completion times and those who self-rated as being &#8220;between focused and unfocused&#8221; or worse were manually verified for attention. One participant was excluded because of irregular behavior. Participants were shown a random selection of 150 sentences across all speakers. As a control, roughly 15% of stimuli presented to participants were human speech. For each sentence, participants rated the speaker&#8217;s naturalness on the following scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>: (1) Completely unnatural, (2) Mostly unnatural, (3) In between unnatural and natural, (4) Mostly natural, or (5) Completely natural. Participants were additionally asked to answer &#8220;Yes&#8221; or &#8220;No&#8221; to the question, &#8220;Do you believe this recording was spoken by a real person?&#8221;</p>\n\n",
                "matched_terms": [
                    "who",
                    "human",
                    "participants"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pairwise comparison experiment, 97 participants were recruited from the university student population. Similar participation requirements were enforced. Each participant was presented with 115 random pairings of models speaking the same sentence, and was asked to choose which was more natural. Human speakers were not included in the stimuli.</p>\n\n",
                "matched_terms": [
                    "human",
                    "participants"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "proportion",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "human",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "human",
                    "openai"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 2: MOS of speakers, sorted.",
        "body": "Speaker\nMOS\n\n\n\n\nOpenAI\n3.55 ±0.05{}_{\\pm\\text{0.05}}\n\n\n\nGoogle\n3.41 ±0.05{}_{\\pm\\text{0.05}}\n\n\n\nAzure\n2.80 ±0.05{}_{\\pm\\text{0.05}}\n\n\n\nVITS\n2.09 ±0.05{}_{\\pm\\text{0.05}}\n\n\n\nPolly\n1.84 ±0.05{}_{\\pm\\text{0.05}}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:195.1pt;\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">3.55 <math alttext=\"{}_{\\pm\\text{0.05}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><msub><mi/><mrow><mo>&#177;</mo><mtext>0.05</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\pm\\text{0.05}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Google</th>\n<td class=\"ltx_td ltx_align_right\">3.41 <math alttext=\"{}_{\\pm\\text{0.05}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><msub><mi/><mrow><mo>&#177;</mo><mtext>0.05</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\pm\\text{0.05}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Azure</th>\n<td class=\"ltx_td ltx_align_right\">2.80 <math alttext=\"{}_{\\pm\\text{0.05}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><msub><mi/><mrow><mo>&#177;</mo><mtext>0.05</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\pm\\text{0.05}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VITS</th>\n<td class=\"ltx_td ltx_align_right\">2.09 <math alttext=\"{}_{\\pm\\text{0.05}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><msub><mi/><mrow><mo>&#177;</mo><mtext>0.05</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\pm\\text{0.05}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">1.84 <math alttext=\"{}_{\\pm\\text{0.05}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><msub><mi/><mrow><mo>&#177;</mo><mtext>0.05</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\pm\\text{0.05}}</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sorted",
            "openai",
            "±005pmtext005",
            "speakers",
            "speaker",
            "azure",
            "mos",
            "google",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
            "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "mos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "speakers",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MOS experiment, participants were recruited through two pools: (1) 91 university students, and (2) 49 Prolific users (32 female, 17 male; mean age 40.84 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.89). Prolific participants were required to have an 85&#8211;100% approval rate, be based in the United States, and list English as their native language. Participants with outlier completion times and those who self-rated as being &#8220;between focused and unfocused&#8221; or worse were manually verified for attention. One participant was excluded because of irregular behavior. Participants were shown a random selection of 150 sentences across all speakers. As a control, roughly 15% of stimuli presented to participants were human speech. For each sentence, participants rated the speaker&#8217;s naturalness on the following scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>: (1) Completely unnatural, (2) Mostly unnatural, (3) In between unnatural and natural, (4) Mostly natural, or (5) Completely natural. Participants were additionally asked to answer &#8220;Yes&#8221; or &#8220;No&#8221; to the question, &#8220;Do you believe this recording was spoken by a real person?&#8221;</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "mos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "mos",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "mos",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "mos",
                    "openai"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 3: BTM scores of models, sorted",
        "body": "OpenAI\nGoogle\nAzure\nPolly\nVITS\n\n\nScore\n0.946\n0.575\n0.136\n\n−-0.780\n\n−-0.877",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">OpenAI</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Google</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Azure</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Polly</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VITS</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.946</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.575</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.136</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>0.780</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>0.877</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sorted",
            "openai",
            "score",
            "btm",
            "models",
            "−0877",
            "azure",
            "google",
            "−0780",
            "scores",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Problematically, the most common techniques for evaluating the prosodic accuracy of models remain resource-intensive and oftentimes opaque. These techniques can be broadly partitioned into subjective methods, which typically involve some sort of perception experiment where listeners rate or compare generated stimuli, and objective methods, which seek to score models purely based on the acoustic outputs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores",
                    "score",
                    "btm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While subjective methods are still commonly recognized as the gold standard, there have been increasing attempts to address some of their limitations by using objective methods. For example, given the plethora of MOS rating data available, a natural idea would be to predict MOS scores directly from the TTS output using supervised machine learning models. Indeed, several attempts have been made in this general direction. MOSNet uses spectrograms as the input to predict MOS on a frame-by-frame basis, taking advantage of convolutional and recurrent layers in a bidirectional long short-term memory network to capture temporal and local information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib22\" title=\"\">22</a>]</cite>. LDNet, which takes inspiration from MOSNet, additionally incorporates the listener&#8217;s identity as an input, allowing for prediction for a specific listener <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib23\" title=\"\">23</a>]</cite>. Other models like SSL-MOS use pre-trained embeddings, rather than pure acoustic or spectral information as the input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib24\" title=\"\">24</a>]</cite>. Some also include more specific linguistic features, such as <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math>, POS tags, etc. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib25\" title=\"\">25</a>]</cite>. Crucially, however, even if these models are able to successfully model MOS ratings, they necessarily possess the same limitations as the metric they mimic: inconsistency and linguistic opaqueness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unsupervised models have also been used to calculate the probability of the naturalness of synthetic speech. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib26\" title=\"\">26</a>]</cite>, for example, trains Hidden Markov models (HMMs) on natural speech, then calculates the log-likelihood of synthetic speech under those models. Performance, however, shows substantial gender differences (a separate HMM is used for each gender, using a simple <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> frequency threshold to discriminate between them) and is largely limited to checking temporal features against scores like MOS on older, non-neural models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A subset of objective methods evaluate speech using explicit linguistic cues. Rather than having a model predict a MOS score, these methods calculate scores directly from the linguistic features of the speech signal. One of the most common strategies to accomplish this is to compare synthetic speech data with a natural speech corpus. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib28\" title=\"\">28</a>]</cite> measures the Euclidean distance between the Mel-frequency cepstral coefficients of synthetic and reference speech, aligned via dynamic time warping. Similarly, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib29\" title=\"\">29</a>]</cite> calculates quantitative measures of rhythm and intonation based on acoustic features and compares them to a database of natural speech, combining the results into an overall score.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "models",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "models",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "scores",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "score",
                    "models",
                    "scores",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "models",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "score",
                    "models",
                    "openai"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these advances lay the groundwork for a transparent, linguistically principled evaluation standard that moves beyond mean opinion scores and toward a more systematic understanding of how expressive, human-like speech can be achieved in next-generation TTS models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 4: tt-tests for self-validation across all humans and models",
        "body": "Feature\nMetric\n\ntt-value\n\n\npp-value\n\nWinner\n\n\n\n\nDuration\nℓ0/1∗\\ell_{0/1}^{*}\n8.521\n1.13e-16\nHuman\n\n\nF1F_{1}\n\n−-4.488\n2.07e-05\nHuman\n\n\nError\n4.399\n1.20e-05\nHuman\n\n\nPitch\nℓ0/1∗\\ell_{0/1}^{*}\n18.167\n8.63e-60\nHuman\n\n\nF1F_{1}\n\n−-7.420\n3.37e-13\nHuman\n\n\nError\n13.278\n5.68e-37\nHuman\n\n\nIntensity\nℓ0/1∗\\ell_{0/1}^{*}\n17.275\n3.59e-55\nHuman\n\n\nF1F_{1}\n\n−-8.901\n3.53e-18\nHuman\n\n\nError\n16.610\n9.20e-55\nHuman\n\n\nAlpha ratio\nℓ0/1∗\\ell_{0/1}^{*}\n14.544\n1.29e-41\nHuman\n\n\nF1F_{1}\n\n−-4.871\n1.28e-06\nHuman\n\n\nError\n12.192\n3.49e-32\nHuman\n\n\nL1–L0\nℓ0/1∗\\ell_{0/1}^{*}\n18.013\n5.50e-59\nHuman\n\n\nF1F_{1}\n\n−-10.283\n1.84e-23\nHuman\n\n\nError\n17.592\n6.14e-61\nHuman\n\n\nCPPS\nℓ0/1∗\\ell_{0/1}^{*}\n16.574\n1.35e-51\nHuman\n\n\nF1F_{1}\n\n−-8.897\n2.82e-18\nHuman\n\n\nError\n16.329\n1.36e-53\nHuman",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Feature</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Metric</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-value</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-value</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Winner</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Duration</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m5\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">8.521</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.13e-16</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m7\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>4.488</td>\n<td class=\"ltx_td ltx_align_right\">2.07e-05</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<td class=\"ltx_td ltx_align_right\">4.399</td>\n<td class=\"ltx_td ltx_align_right\">1.20e-05</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Pitch</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m8\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">18.167</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">8.63e-60</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m9\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m10\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>7.420</td>\n<td class=\"ltx_td ltx_align_right\">3.37e-13</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<td class=\"ltx_td ltx_align_right\">13.278</td>\n<td class=\"ltx_td ltx_align_right\">5.68e-37</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Intensity</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m11\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">17.275</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">3.59e-55</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m12\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m13\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>8.901</td>\n<td class=\"ltx_td ltx_align_right\">3.53e-18</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<td class=\"ltx_td ltx_align_right\">16.610</td>\n<td class=\"ltx_td ltx_align_right\">9.20e-55</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Alpha ratio</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m14\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">14.544</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.29e-41</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m15\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m16\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>4.871</td>\n<td class=\"ltx_td ltx_align_right\">1.28e-06</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<td class=\"ltx_td ltx_align_right\">12.192</td>\n<td class=\"ltx_td ltx_align_right\">3.49e-32</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">L1&#8211;L0</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m17\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">18.013</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">5.50e-59</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m18\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m19\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>10.283</td>\n<td class=\"ltx_td ltx_align_right\">1.84e-23</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<td class=\"ltx_td ltx_align_right\">17.592</td>\n<td class=\"ltx_td ltx_align_right\">6.14e-61</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"3\">CPPS</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m20\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">16.574</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.35e-51</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m21\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m22\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>8.897</td>\n<td class=\"ltx_td ltx_align_right\">2.82e-18</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Error</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">16.329</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">1.36e-53</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">Human</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "−10283",
            "tttests",
            "129e41",
            "359e55",
            "282e18",
            "120e05",
            "alpha",
            "920e55",
            "207e05",
            "metric",
            "−7420",
            "humans",
            "113e16",
            "pitch",
            "error",
            "feature",
            "568e37",
            "135e51",
            "ℓ01∗ell01",
            "winner",
            "f1f1",
            "136e53",
            "−8897",
            "ttvalue",
            "−4871",
            "ppvalue",
            "selfvalidation",
            "cpps",
            "intensity",
            "−8901",
            "128e06",
            "l1–l0",
            "863e60",
            "614e61",
            "across",
            "353e18",
            "all",
            "337e13",
            "ratio",
            "models",
            "550e59",
            "human",
            "349e32",
            "duration",
            "−4488",
            "184e23"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transformative advancements in deep neural net (DNN) speech synthesis systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib2\" title=\"\">2</a>]</cite> have produced TTS models that, particularly in short segments, are nearly indistinguishable from humans. While text inputs have continued to increase in length, content, and context, however, TTS models have yet to fully encapsulate the full range of human expression, inviting increased focus on modeling human prosody in speech technologies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib5\" title=\"\">5</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human",
                    "humans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Challengingly for TTS systems, prosody is complex. It is not solvable by, for example, adding SSML (Speech Synthesis Markup Language) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib15\" title=\"\">15</a>]</cite> tags to the inputs of TTS models&#8212;something which, even if capable of encapsulating the full range of human prosody, adds a laborious step to what is intended to be an automated process. Rather, we know that prosody can be accidental or intentional, covert or overt, varying across a broad swath of linguistic and social contexts while maintaining the same emotional &#8220;label&#8221; in layman&#8217;s terms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While subjective methods are still commonly recognized as the gold standard, there have been increasing attempts to address some of their limitations by using objective methods. For example, given the plethora of MOS rating data available, a natural idea would be to predict MOS scores directly from the TTS output using supervised machine learning models. Indeed, several attempts have been made in this general direction. MOSNet uses spectrograms as the input to predict MOS on a frame-by-frame basis, taking advantage of convolutional and recurrent layers in a bidirectional long short-term memory network to capture temporal and local information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib22\" title=\"\">22</a>]</cite>. LDNet, which takes inspiration from MOSNet, additionally incorporates the listener&#8217;s identity as an input, allowing for prediction for a specific listener <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib23\" title=\"\">23</a>]</cite>. Other models like SSL-MOS use pre-trained embeddings, rather than pure acoustic or spectral information as the input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib24\" title=\"\">24</a>]</cite>. Some also include more specific linguistic features, such as <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math>, POS tags, etc. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib25\" title=\"\">25</a>]</cite>. Crucially, however, even if these models are able to successfully model MOS ratings, they necessarily possess the same limitations as the metric they mimic: inconsistency and linguistic opaqueness.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "human",
                    "humans",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although these methods capture certain aspects of linguistic form, they are limited by their lack of sensitivity to variation. Frame-level acoustic comparisons assume that every utterance has a single optimal realization, unfairly penalizing valid prosodic variation that occur within the natural range of human expression. In practice, two speakers&#8212;or even two utterances by the same speaker&#8212;may express the same prosodic target with different pitch ranges, voice qualities, or timing patterns, all of which are perceptually valid. Evaluations based on rigid acoustic distances therefore risk rewarding uniformity rather than communicative adequacy.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important insight from prosodic theory is that prosodic encodings of human natural speech are inherently two-layered: they consist of discrete structural targets and their continuous phonetic realizations. In intonational phonology (e.g., Autosegmental&#8211;Metrical theory), the discrete layer comprises categories such as pitch accents, phrase accents, and boundary tones, which define what events occur and where <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib31\" title=\"\">31</a>]</cite>. These targets are then realized through continuous parameters&#8212;<math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alignment, scaling, interpolation&#8212;modulated by duration, intensity, and voice quality adjustments.</p>\n\n",
                "matched_terms": [
                    "human",
                    "pitch",
                    "intensity",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational models of human speech prosody, though developed in different traditions, share the goal of linking abstract representations of prosodic events to their continuous acoustic realizations. The command&#8211;response model represents targets as underlying commands generating smooth contours <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib32\" title=\"\">32</a>]</cite>; the Tilt model parameterizes each event in shape and amplitude <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib33\" title=\"\">33</a>]</cite>; MOMEL/INTSINT stylizes contours by extracting sparse targets and interpolating between them <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib34\" title=\"\">34</a>]</cite>; the Target Approximation (qTA/PENTA) model further links target selection to communicative goals and models the dynamics of their realization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib35\" title=\"\">35</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, prosodic structure is realized through a rich, high-dimensional set of acoustic cues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib36\" title=\"\">36</a>]</cite>. This multidimensionality is evident in both phrasing and prominence, the two most important aspects of prosodic structure cross-linguistically. Phrasing is primarily related to durational cues such as word duration and pause duration, but also related to pitch cues (e.g., final lowering, reset, boundary tone) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib37\" title=\"\">37</a>]</cite>, and voice quality cues (e.g., creakiness) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>. Prominence, meanwhile, can be variably realized through a number of acoustic cues: for example, in American English, stressed or accented words tend to have higher pitch, longer duration, tenser voice quality (stronger energy in the high-frequency region of the spectrum and greater periodicity), and greater intensity. And while traditional prosodic evaluations often only focus on pitch, intensity, and duration, recent studies have shown voice quality to contribute important acoustic-prosodic cues as well <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib41\" title=\"\">41</a>]</cite>. Because speakers can use different combinations of cues to achieve the same communicative function, this cue multidimensionality thus contributes to an additional source of variation in prosodic realization. Our model therefore aims to capture a range of acoustic representations for prosodic structure, as well as their potential variability. Importantly, these cues are interpretable and linguistically meaningful. The goal of our approach is not only to provide objective evaluations for the prosodic naturalness of TTS systems, but also to diagnose and identify the limitations of state-of-the-art models, especially when the specific factors that make synthetic speech sound &#8220;unnatural&#8221; remain elusive to the untrained ear.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pitch",
                    "intensity",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "across",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pitch",
                    "across",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As established in the linguistic literature based on natural human speech, the prosodic signal can be broken down into targets and interpolation. Here, &#8220;prosodic events&#8221; or simply &#8220;events&#8221; refer to those linguistic targets, including but not limited to pitch and phrase accents, boundary tones, and pauses. Indeed, the correct placement of such events&#8212;for example, accenting the right words and pausing in the appropriate places&#8212;is crucial in natural speech.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first tier therefore evaluates whether models place prosodic events on the same words where human speakers typically produce them. Events are automatically detected as local extrema in acoustic signals (e.g., peaks in the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> contour). We frame this as a binary classification task: for each word, the model predicts whether an event should occur. Because human productions vary, there is no single &#8220;correct&#8221; reference. To address this, we define two complementary criteria for correctness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S2.F1\" title=\"Figure 1 &#8227; 2.1.1 Binary event evaluation &#8227; 2.1 Overview &#8227; 2 Methods &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m2\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> is a generalized Gaussian function, with a smooth peak (i.e., high loss) when there&#8217;s no or low agreement, and a rapid decline as agreement increases. Then, we can define a &#8220;smooth&#8221; loss <math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math> by plugging in <math alttext=\"\\varepsilon(\\alpha_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m4\" intent=\":literal\"><semantics><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})</annotation></semantics></math> for <math alttext=\"\\mathbbm{1}(\\alpha_{i}&lt;c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m5\" intent=\":literal\"><semantics><mrow><mn>&#120793;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#945;</mi><mi>i</mi></msub><mo>&lt;</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbbm{1}(\\alpha_{i}&lt;c)</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S2.E2\" title=\"In 2.1.1 Binary event evaluation &#8227; 2.1 Overview &#8227; 2 Methods &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Practically, this means that we&#8217;re always awarding &#8220;partial credit&#8221; to models based on the proportion of speakers concurring with them: if all or almost all of the speakers agree with a model, it&#8217;ll get all (or almost all) of the points, but as the percentage of people agreeing with it decays, its correctness will rapidly fall to zero. Thus, variability is enabled with a bias toward the majority.</p>\n\n",
                "matched_terms": [
                    "models",
                    "ℓ01∗ell01",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "human",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Still, we run into the issue of comparing signals with a <span class=\"ltx_text ltx_font_italic\">corpus</span> of human data; it&#8217;s easy to compare a model with one human, but not all of them. To account for multiple speakers and the variation they exhibit, we look at &#8220;normalized&#8221; error: the mean of the squared <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-scores of each element in the TTS signal. Formally, if we let <math alttext=\"p=(p_{1},\\dots,p_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p=(p_{1},\\dots,p_{n})</annotation></semantics></math> be the TTS model&#8217;s acoustic measurements and <math alttext=\"\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo>=</mo><msub><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}</annotation></semantics></math> be the set of measurements of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word among all human speakers (e.g., <math alttext=\"\\mathcal{S}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{1}</annotation></semantics></math> is the set of measurements at the first word across all speakers), we can define error as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "across",
                    "human",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\overline{\\mathcal{S}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{\\mathcal{S}_{i}}</annotation></semantics></math> is the mean of <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> and <math alttext=\"\\mathrm{std}(\\mathcal{S}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m8\" intent=\":literal\"><semantics><mrow><mi>std</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{std}(\\mathcal{S}_{i})</annotation></semantics></math> is its standard deviation. This is motivated by the fact that human speakers will likely have a high degree of agreement in certain places and low agreement elsewhere. By dividing out variation, our error metric weights model&#8211;human agreement based on whether people agree among themselves.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "human",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "all",
                    "ratio",
                    "cpps",
                    "alpha",
                    "intensity",
                    "human",
                    "duration",
                    "f1f1",
                    "l1–l0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MOS experiment, participants were recruited through two pools: (1) 91 university students, and (2) 49 Prolific users (32 female, 17 male; mean age 40.84 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.89). Prolific participants were required to have an 85&#8211;100% approval rate, be based in the United States, and list English as their native language. Participants with outlier completion times and those who self-rated as being &#8220;between focused and unfocused&#8221; or worse were manually verified for attention. One participant was excluded because of irregular behavior. Participants were shown a random selection of 150 sentences across all speakers. As a control, roughly 15% of stimuli presented to participants were human speech. For each sentence, participants rated the speaker&#8217;s naturalness on the following scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>: (1) Completely unnatural, (2) Mostly unnatural, (3) In between unnatural and natural, (4) Mostly natural, or (5) Completely natural. Participants were additionally asked to answer &#8220;Yes&#8221; or &#8220;No&#8221; to the question, &#8220;Do you believe this recording was spoken by a real person?&#8221;</p>\n\n",
                "matched_terms": [
                    "across",
                    "human",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pairwise comparison experiment, 97 participants were recruited from the university student population. Similar participation requirements were enforced. Each participant was presented with 115 random pairings of models speaking the same sentence, and was asked to choose which was more natural. Human speakers were not included in the stimuli.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "tttests"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "error",
                    "human",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "across",
                    "error",
                    "all",
                    "ratio",
                    "alpha",
                    "intensity",
                    "duration",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "across",
                    "error",
                    "intensity",
                    "duration",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "across",
                    "error",
                    "all",
                    "ratio",
                    "models",
                    "cpps",
                    "alpha",
                    "intensity",
                    "duration",
                    "f1f1",
                    "l1–l0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "selfvalidation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "selfvalidation",
                    "models",
                    "feature",
                    "human",
                    "duration",
                    "f1f1",
                    "l1–l0",
                    "humans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "across",
                    "ratio",
                    "models",
                    "alpha",
                    "l1–l0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "all",
                    "selfvalidation",
                    "error",
                    "models",
                    "human",
                    "duration",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "models",
                    "cpps",
                    "intensity",
                    "human",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 5: Evaluation metrics for duration.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nPolly\n0.023\n0.013\n0.125\n0.194\n0.611\n0.049\n\n\nGoogle\n0.017\n0.007\n0.375\n0.417\n0.684\n0.035\n\n\nAzure\n0.013\n0.007\n0.196\n0.773\n0.723\n0.042\n\n\nOpenAI\n0.014\n0.006\n0.657\n0.540\n0.767\n0.047\n\n\nVITS\n0.021\n0.013\n0.140\n0.227\n0.375\n0.033",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.023</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.013</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.125</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.194</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.611</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.049</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Google</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.017</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.375</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.417</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.684</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.035</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Azure</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.013</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.196</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.773</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.723</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.042</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.014</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.006</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.657</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.540</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.767</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.047</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VITS</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.021</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.013</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.140</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.227</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.375</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.033</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "openai",
            "evaluation",
            "err",
            "speaker",
            "azure",
            "duration",
            "rec",
            "prec",
            "google",
            "ℓ01ell01",
            "f1f1",
            "ℓ01∗ell01",
            "metrics",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "duration",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "duration",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "duration",
                    "google",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "duration",
                    "f1f1",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "evaluation",
                    "duration",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "azure",
                    "duration",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "duration",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 6: Evaluation metrics for mean pitch.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nPolly\n0.124\n0.038\n0.514\n0.257\n0.418\n0.566\n\n\nGoogle\n0.134\n0.030\n0.485\n0.220\n0.383\n0.468\n\n\nAzure\n0.136\n0.037\n0.568\n0.231\n0.410\n0.660\n\n\nOpenAI\n0.121\n0.032\n0.334\n0.205\n0.311\n0.582\n\n\nVITS\n0.158\n0.071\n0.184\n0.078\n0.143\n1.188",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.124</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.038</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.514</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.257</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.418</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.566</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Google</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.134</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.030</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.485</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.220</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.383</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.468</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Azure</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.136</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.037</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.568</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.231</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.410</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.660</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.121</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.032</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.334</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.205</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.311</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.582</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VITS</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.158</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.071</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.184</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.078</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.143</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1.188</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pitch",
            "openai",
            "evaluation",
            "err",
            "ℓ01ell01",
            "speaker",
            "azure",
            "rec",
            "prec",
            "google",
            "metrics",
            "f1f1",
            "ℓ01∗ell01",
            "mean",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most popular general measures of TTS quality&#8212;subjective or otherwise&#8212;is the Mean Opinion Score (MOS), which is calculated by conducting experiments that ask many participants to rate the &#8220;naturalness&#8221; of model outputs on a scale from 1 to 5, then taking the mean. Aside from being costly and time-intensive, MOS and similar evaluation methods have been found to be inconsistent across the literature and poorly defined, frequently leading to different results when performed under different circumstances <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib17\" title=\"\">17</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib18\" title=\"\">18</a>]</cite> performed a meta-analysis of evaluations used at INTERSPEECH 2014 and found that more than 60% of papers used fewer than 20 listeners for their evaluations. Meanwhile, they showed that at least 30 participants were necessary to enable a stable level of significance for MOS.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "evaluation",
                    "speaker",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "pitch",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "azure",
                    "google",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "f1f1",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "evaluation",
                    "azure",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "evaluation",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "evaluation",
                    "speaker",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these advances lay the groundwork for a transparent, linguistically principled evaluation standard that moves beyond mean opinion scores and toward a more systematic understanding of how expressive, human-like speech can be achieved in next-generation TTS models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 7: Evaluation metrics on mean intensity",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nPolly\n0.163\n0.049\n0.352\n0.203\n0.310\n0.576\n\n\nGoogle\n0.154\n0.041\n0.479\n0.251\n0.387\n0.498\n\n\nAzure\n0.143\n0.034\n0.507\n0.304\n0.446\n0.532\n\n\nOpenAI\n0.126\n0.023\n0.494\n0.342\n0.461\n0.438\n\n\nVITS\n0.180\n0.063\n0.440\n0.229\n0.358\n0.760",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.163</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.049</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.352</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.203</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.310</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.576</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Google</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.154</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.041</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.479</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.251</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.387</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.498</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Azure</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.143</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.034</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.507</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.304</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.446</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.532</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.126</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.023</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.494</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.342</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.461</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.438</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VITS</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.180</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.063</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.440</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.229</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.358</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.760</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "openai",
            "evaluation",
            "err",
            "ℓ01ell01",
            "speaker",
            "intensity",
            "azure",
            "rec",
            "prec",
            "google",
            "metrics",
            "f1f1",
            "ℓ01∗ell01",
            "mean",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most popular general measures of TTS quality&#8212;subjective or otherwise&#8212;is the Mean Opinion Score (MOS), which is calculated by conducting experiments that ask many participants to rate the &#8220;naturalness&#8221; of model outputs on a scale from 1 to 5, then taking the mean. Aside from being costly and time-intensive, MOS and similar evaluation methods have been found to be inconsistent across the literature and poorly defined, frequently leading to different results when performed under different circumstances <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib17\" title=\"\">17</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib18\" title=\"\">18</a>]</cite> performed a meta-analysis of evaluations used at INTERSPEECH 2014 and found that more than 60% of papers used fewer than 20 listeners for their evaluations. Meanwhile, they showed that at least 30 participants were necessary to enable a stable level of significance for MOS.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "intensity",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "intensity",
                    "f1f1",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "azure",
                    "intensity",
                    "google",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "intensity",
                    "f1f1",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "azure",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "openai",
                    "metrics",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these advances lay the groundwork for a transparent, linguistically principled evaluation standard that moves beyond mean opinion scores and toward a more systematic understanding of how expressive, human-like speech can be achieved in next-generation TTS models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            }
        ]
    },
    "A1.T8": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 8: Evaluation metrics on spectral tilt (alpha ratio)",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nPolly\n0.132\n0.042\n0.675\n0.507\n0.599\n0.487\n\n\nGoogle\n0.124\n0.035\n0.715\n0.540\n0.641\n0.409\n\n\nAzure\n0.118\n0.035\n0.716\n0.573\n0.642\n0.469\n\n\nOpenAI\n0.086\n0.025\n0.805\n0.652\n0.747\n0.316\n\n\nVITS\n0.142\n0.054\n0.584\n0.502\n0.588\n0.651",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.132</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.675</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.507</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.599</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.487</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Google</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.124</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.035</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.715</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.540</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.641</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.409</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Azure</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.118</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.035</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.716</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.573</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.642</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.469</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.086</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.025</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.805</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.652</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.747</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.316</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VITS</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.142</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.054</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.584</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.502</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.588</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.651</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tilt",
            "openai",
            "evaluation",
            "ratio",
            "err",
            "spectral",
            "alpha",
            "speaker",
            "azure",
            "rec",
            "prec",
            "google",
            "ℓ01ell01",
            "f1f1",
            "ℓ01∗ell01",
            "metrics",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "ratio",
                    "spectral",
                    "alpha",
                    "speaker",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "openai",
                    "ratio",
                    "spectral",
                    "alpha",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "azure",
                    "google",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "ratio",
                    "spectral",
                    "alpha",
                    "f1f1",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "openai",
                    "evaluation",
                    "spectral",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "azure",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "ratio",
                    "spectral",
                    "alpha",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "openai",
                    "evaluation",
                    "spectral",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will extend the approach in several directions. Incorporating additional acoustic and spectral measures may capture finer-grained dimensions of prosodic expressiveness, while developing weighted composite indices could integrate multiple prosodic dimensions without sacrificing interpretability. Cross-linguistic applications and analyses of conversational or emotionally expressive speech will further test the framework&#8217;s generalizability. Finally, embedding these interpretable metrics directly into training or fine-tuning pipelines could transform them from diagnostic tools into guiding objectives for adaptive model optimization.</p>\n\n",
                "matched_terms": [
                    "spectral",
                    "metrics"
                ]
            }
        ]
    },
    "A1.T9": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 9: Evaluation metrics on spectral tilt (L1–L0)",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nPolly\n0.213\n0.067\n0.372\n0.166\n0.267\n0.873\n\n\nGoogle\n0.163\n0.049\n0.442\n0.257\n0.363\n0.726\n\n\nAzure\n0.170\n0.043\n0.492\n0.232\n0.374\n0.628\n\n\nOpenAI\n0.163\n0.042\n0.482\n0.258\n0.375\n0.631\n\n\nVITS\n0.171\n0.046\n0.367\n0.216\n0.313\n0.718",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.213</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.067</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.372</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.166</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.267</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.873</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Google</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.163</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.049</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.442</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.257</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.363</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.726</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Azure</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.170</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.043</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.492</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.232</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.374</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.628</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.163</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.042</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.482</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.258</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.375</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.631</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VITS</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.171</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.046</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.367</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.216</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.313</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.718</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tilt",
            "openai",
            "evaluation",
            "err",
            "spectral",
            "speaker",
            "azure",
            "rec",
            "ℓ01∗ell01",
            "prec",
            "google",
            "ℓ01ell01",
            "f1f1",
            "l1–l0",
            "metrics",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "speaker",
                    "f1f1",
                    "l1–l0",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "openai",
                    "spectral",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "azure",
                    "google",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "f1f1",
                    "l1–l0",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "openai",
                    "evaluation",
                    "spectral",
                    "f1f1",
                    "l1–l0",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "azure",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "azure",
                    "google",
                    "l1–l0",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "openai",
                    "evaluation",
                    "spectral",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will extend the approach in several directions. Incorporating additional acoustic and spectral measures may capture finer-grained dimensions of prosodic expressiveness, while developing weighted composite indices could integrate multiple prosodic dimensions without sacrificing interpretability. Cross-linguistic applications and analyses of conversational or emotionally expressive speech will further test the framework&#8217;s generalizability. Finally, embedding these interpretable metrics directly into training or fine-tuning pipelines could transform them from diagnostic tools into guiding objectives for adaptive model optimization.</p>\n\n",
                "matched_terms": [
                    "spectral",
                    "metrics"
                ]
            }
        ]
    },
    "A1.T10": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 10: Evaluation metrics for CPPS.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nPolly\n0.170\n0.044\n0.503\n0.373\n0.462\n0.620\n\n\nGoogle\n0.169\n0.048\n0.595\n0.364\n0.509\n0.556\n\n\nAzure\n0.162\n0.031\n0.587\n0.408\n0.518\n0.585\n\n\nOpenAI\n0.156\n0.034\n0.636\n0.405\n0.549\n0.542\n\n\nVITS\n0.203\n0.063\n0.414\n0.272\n0.365\n0.757",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T10.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T10.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T10.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Polly</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.170</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.044</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.503</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.373</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.462</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.620</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Google</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.169</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.048</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.595</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.364</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.509</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.556</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Azure</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.162</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.031</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.587</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.408</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.518</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.585</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.156</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.034</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.636</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.405</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.549</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.542</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VITS</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.203</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.063</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.414</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.272</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.365</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.757</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "openai",
            "evaluation",
            "err",
            "cpps",
            "speaker",
            "azure",
            "rec",
            "prec",
            "google",
            "ℓ01ell01",
            "f1f1",
            "ℓ01∗ell01",
            "metrics",
            "vits",
            "polly"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "cpps",
                    "speaker",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "azure",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "cpps",
                    "f1f1",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "azure",
                    "google",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "azure",
                    "google",
                    "metrics",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "speaker",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "cpps",
                    "openai",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            }
        ]
    },
    "A2.T11": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 11: Human evaluation metrics for duration.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nS1\n0.009\n0.000\n0.903\n0.586\n0.864\n0.031\n\n\nS2\n0.007\n0.000\n0.815\n0.677\n0.907\n0.024\n\n\nS3\n0.012\n0.000\n0.695\n0.657\n0.775\n0.026\n\n\nS4\n0.008\n0.000\n0.750\n0.721\n0.905\n0.025\n\n\nS5\n0.008\n0.000\n0.968\n0.646\n0.910\n0.032",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T11.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T11.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T11.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.009</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.903</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.586</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.864</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S2</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.007</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.815</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.677</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.907</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.024</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S3</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.012</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.695</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.657</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.775</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.026</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S4</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.008</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.750</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.721</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.905</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.025</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S5</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.008</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.968</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.646</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.910</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.032</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluation",
            "err",
            "speaker",
            "human",
            "duration",
            "rec",
            "prec",
            "ℓ01ell01",
            "f1f1",
            "ℓ01∗ell01",
            "metrics"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important insight from prosodic theory is that prosodic encodings of human natural speech are inherently two-layered: they consist of discrete structural targets and their continuous phonetic realizations. In intonational phonology (e.g., Autosegmental&#8211;Metrical theory), the discrete layer comprises categories such as pitch accents, phrase accents, and boundary tones, which define what events occur and where <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib31\" title=\"\">31</a>]</cite>. These targets are then realized through continuous parameters&#8212;<math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alignment, scaling, interpolation&#8212;modulated by duration, intensity, and voice quality adjustments.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "duration",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "duration",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "duration",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "duration",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "duration",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "duration",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "duration",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            }
        ]
    },
    "A2.T12": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 12: Human evaluation metrics for mean pitch.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nS1\n0.108\n0.000\n0.729\n0.310\n0.543\n0.384\n\n\nS2\n0.109\n0.000\n0.739\n0.308\n0.555\n0.375\n\n\nS3\n0.097\n0.000\n0.666\n0.315\n0.535\n0.382\n\n\nS4\n0.121\n0.000\n0.585\n0.253\n0.462\n0.519\n\n\nS5\n0.090\n0.000\n0.769\n0.359\n0.588\n0.353",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T12.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T12.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T12.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.108</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.729</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.310</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.543</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.384</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S2</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.109</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.739</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.308</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.555</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.375</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S3</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.097</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.666</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.315</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.535</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.382</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S4</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.121</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.585</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.253</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.462</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.519</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S5</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.090</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.769</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.359</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.588</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.353</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pitch",
            "evaluation",
            "err",
            "ℓ01ell01",
            "speaker",
            "human",
            "rec",
            "prec",
            "metrics",
            "f1f1",
            "ℓ01∗ell01",
            "mean"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most popular general measures of TTS quality&#8212;subjective or otherwise&#8212;is the Mean Opinion Score (MOS), which is calculated by conducting experiments that ask many participants to rate the &#8220;naturalness&#8221; of model outputs on a scale from 1 to 5, then taking the mean. Aside from being costly and time-intensive, MOS and similar evaluation methods have been found to be inconsistent across the literature and poorly defined, frequently leading to different results when performed under different circumstances <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib17\" title=\"\">17</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib18\" title=\"\">18</a>]</cite> performed a meta-analysis of evaluations used at INTERSPEECH 2014 and found that more than 60% of papers used fewer than 20 listeners for their evaluations. Meanwhile, they showed that at least 30 participants were necessary to enable a stable level of significance for MOS.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although these methods capture certain aspects of linguistic form, they are limited by their lack of sensitivity to variation. Frame-level acoustic comparisons assume that every utterance has a single optimal realization, unfairly penalizing valid prosodic variation that occur within the natural range of human expression. In practice, two speakers&#8212;or even two utterances by the same speaker&#8212;may express the same prosodic target with different pitch ranges, voice qualities, or timing patterns, all of which are perceptually valid. Evaluations based on rigid acoustic distances therefore risk rewarding uniformity rather than communicative adequacy.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important insight from prosodic theory is that prosodic encodings of human natural speech are inherently two-layered: they consist of discrete structural targets and their continuous phonetic realizations. In intonational phonology (e.g., Autosegmental&#8211;Metrical theory), the discrete layer comprises categories such as pitch accents, phrase accents, and boundary tones, which define what events occur and where <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib31\" title=\"\">31</a>]</cite>. These targets are then realized through continuous parameters&#8212;<math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alignment, scaling, interpolation&#8212;modulated by duration, intensity, and voice quality adjustments.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As established in the linguistic literature based on natural human speech, the prosodic signal can be broken down into targets and interpolation. Here, &#8220;prosodic events&#8221; or simply &#8220;events&#8221; refer to those linguistic targets, including but not limited to pitch and phrase accents, boundary tones, and pauses. Indeed, the correct placement of such events&#8212;for example, accenting the right words and pausing in the appropriate places&#8212;is crucial in natural speech.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Still, we run into the issue of comparing signals with a <span class=\"ltx_text ltx_font_italic\">corpus</span> of human data; it&#8217;s easy to compare a model with one human, but not all of them. To account for multiple speakers and the variation they exhibit, we look at &#8220;normalized&#8221; error: the mean of the squared <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-scores of each element in the TTS signal. Formally, if we let <math alttext=\"p=(p_{1},\\dots,p_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p=(p_{1},\\dots,p_{n})</annotation></semantics></math> be the TTS model&#8217;s acoustic measurements and <math alttext=\"\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo>=</mo><msub><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}</annotation></semantics></math> be the set of measurements of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word among all human speakers (e.g., <math alttext=\"\\mathcal{S}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{1}</annotation></semantics></math> is the set of measurements at the first word across all speakers), we can define error as follows:</p>\n\n",
                "matched_terms": [
                    "mean",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\overline{\\mathcal{S}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{\\mathcal{S}_{i}}</annotation></semantics></math> is the mean of <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> and <math alttext=\"\\mathrm{std}(\\mathcal{S}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m8\" intent=\":literal\"><semantics><mrow><mi>std</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{std}(\\mathcal{S}_{i})</annotation></semantics></math> is its standard deviation. This is motivated by the fact that human speakers will likely have a high degree of agreement in certain places and low agreement elsewhere. By dividing out variation, our error metric weights model&#8211;human agreement based on whether people agree among themselves.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "evaluation",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MOS experiment, participants were recruited through two pools: (1) 91 university students, and (2) 49 Prolific users (32 female, 17 male; mean age 40.84 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.89). Prolific participants were required to have an 85&#8211;100% approval rate, be based in the United States, and list English as their native language. Participants with outlier completion times and those who self-rated as being &#8220;between focused and unfocused&#8221; or worse were manually verified for attention. One participant was excluded because of irregular behavior. Participants were shown a random selection of 150 sentences across all speakers. As a control, roughly 15% of stimuli presented to participants were human speech. For each sentence, participants rated the speaker&#8217;s naturalness on the following scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>: (1) Completely unnatural, (2) Mostly unnatural, (3) In between unnatural and natural, (4) Mostly natural, or (5) Completely natural. Participants were additionally asked to answer &#8220;Yes&#8221; or &#8220;No&#8221; to the question, &#8220;Do you believe this recording was spoken by a real person?&#8221;</p>\n\n",
                "matched_terms": [
                    "mean",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "pitch",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "pitch",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "pitch",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "evaluation",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these advances lay the groundwork for a transparent, linguistically principled evaluation standard that moves beyond mean opinion scores and toward a more systematic understanding of how expressive, human-like speech can be achieved in next-generation TTS models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            }
        ]
    },
    "A2.T13": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 13: Human evaluation metrics for mean intensity.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nS1\n0.103\n0.000\n0.705\n0.423\n0.597\n0.339\n\n\nS2\n0.086\n0.000\n0.834\n0.493\n0.689\n0.235\n\n\nS3\n0.125\n0.000\n0.643\n0.350\n0.512\n0.326\n\n\nS4\n0.099\n0.000\n0.724\n0.416\n0.589\n0.321\n\n\nS5\n0.100\n0.000\n0.692\n0.447\n0.606\n0.294",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T13.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T13.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T13.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.103</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.705</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.423</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.597</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.339</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S2</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.086</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.834</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.493</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.689</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.235</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S3</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.125</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.643</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.350</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.512</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.326</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S4</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.099</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.724</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.416</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.589</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.321</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S5</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.100</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.692</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.447</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.606</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.294</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluation",
            "err",
            "ℓ01ell01",
            "speaker",
            "intensity",
            "human",
            "rec",
            "prec",
            "metrics",
            "f1f1",
            "ℓ01∗ell01",
            "mean"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most popular general measures of TTS quality&#8212;subjective or otherwise&#8212;is the Mean Opinion Score (MOS), which is calculated by conducting experiments that ask many participants to rate the &#8220;naturalness&#8221; of model outputs on a scale from 1 to 5, then taking the mean. Aside from being costly and time-intensive, MOS and similar evaluation methods have been found to be inconsistent across the literature and poorly defined, frequently leading to different results when performed under different circumstances <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib17\" title=\"\">17</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib18\" title=\"\">18</a>]</cite> performed a meta-analysis of evaluations used at INTERSPEECH 2014 and found that more than 60% of papers used fewer than 20 listeners for their evaluations. Meanwhile, they showed that at least 30 participants were necessary to enable a stable level of significance for MOS.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important insight from prosodic theory is that prosodic encodings of human natural speech are inherently two-layered: they consist of discrete structural targets and their continuous phonetic realizations. In intonational phonology (e.g., Autosegmental&#8211;Metrical theory), the discrete layer comprises categories such as pitch accents, phrase accents, and boundary tones, which define what events occur and where <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib31\" title=\"\">31</a>]</cite>. These targets are then realized through continuous parameters&#8212;<math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alignment, scaling, interpolation&#8212;modulated by duration, intensity, and voice quality adjustments.</p>\n\n",
                "matched_terms": [
                    "human",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Still, we run into the issue of comparing signals with a <span class=\"ltx_text ltx_font_italic\">corpus</span> of human data; it&#8217;s easy to compare a model with one human, but not all of them. To account for multiple speakers and the variation they exhibit, we look at &#8220;normalized&#8221; error: the mean of the squared <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-scores of each element in the TTS signal. Formally, if we let <math alttext=\"p=(p_{1},\\dots,p_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p=(p_{1},\\dots,p_{n})</annotation></semantics></math> be the TTS model&#8217;s acoustic measurements and <math alttext=\"\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo>=</mo><msub><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}</annotation></semantics></math> be the set of measurements of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word among all human speakers (e.g., <math alttext=\"\\mathcal{S}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{1}</annotation></semantics></math> is the set of measurements at the first word across all speakers), we can define error as follows:</p>\n\n",
                "matched_terms": [
                    "mean",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\overline{\\mathcal{S}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{\\mathcal{S}_{i}}</annotation></semantics></math> is the mean of <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> and <math alttext=\"\\mathrm{std}(\\mathcal{S}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m8\" intent=\":literal\"><semantics><mrow><mi>std</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{std}(\\mathcal{S}_{i})</annotation></semantics></math> is its standard deviation. This is motivated by the fact that human speakers will likely have a high degree of agreement in certain places and low agreement elsewhere. By dividing out variation, our error metric weights model&#8211;human agreement based on whether people agree among themselves.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "intensity",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the MOS experiment, participants were recruited through two pools: (1) 91 university students, and (2) 49 Prolific users (32 female, 17 male; mean age 40.84 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 13.89). Prolific participants were required to have an 85&#8211;100% approval rate, be based in the United States, and list English as their native language. Participants with outlier completion times and those who self-rated as being &#8220;between focused and unfocused&#8221; or worse were manually verified for attention. One participant was excluded because of irregular behavior. Participants were shown a random selection of 150 sentences across all speakers. As a control, roughly 15% of stimuli presented to participants were human speech. For each sentence, participants rated the speaker&#8217;s naturalness on the following scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>: (1) Completely unnatural, (2) Mostly unnatural, (3) In between unnatural and natural, (4) Mostly natural, or (5) Completely natural. Participants were additionally asked to answer &#8220;Yes&#8221; or &#8220;No&#8221; to the question, &#8220;Do you believe this recording was spoken by a real person?&#8221;</p>\n\n",
                "matched_terms": [
                    "mean",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "intensity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "intensity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "intensity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "intensity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these advances lay the groundwork for a transparent, linguistically principled evaluation standard that moves beyond mean opinion scores and toward a more systematic understanding of how expressive, human-like speech can be achieved in next-generation TTS models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mean"
                ]
            }
        ]
    },
    "A2.T14": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 14: Human evaluation metrics on spectral tilt (alpha ratio).",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nS1\n0.120\n0.000\n0.644\n0.593\n0.633\n0.369\n\n\nS2\n0.079\n0.000\n0.828\n0.653\n0.749\n0.296\n\n\nS3\n0.098\n0.000\n0.741\n0.608\n0.703\n0.310\n\n\nS4\n0.074\n0.000\n0.854\n0.675\n0.762\n0.287\n\n\nS5\n0.059\n0.000\n0.888\n0.720\n0.806\n0.207",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T14.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T14.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T14.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.120</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.644</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.593</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.633</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.369</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S2</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.079</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.828</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.653</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.749</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.296</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S3</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.098</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.741</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.608</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.703</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.310</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S4</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.074</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.854</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.675</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.762</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.287</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S5</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.059</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.888</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.720</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.806</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.207</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tilt",
            "evaluation",
            "ratio",
            "err",
            "spectral",
            "alpha",
            "human",
            "speaker",
            "rec",
            "prec",
            "ℓ01ell01",
            "f1f1",
            "ℓ01∗ell01",
            "metrics"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational models of human speech prosody, though developed in different traditions, share the goal of linking abstract representations of prosodic events to their continuous acoustic realizations. The command&#8211;response model represents targets as underlying commands generating smooth contours <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib32\" title=\"\">32</a>]</cite>; the Tilt model parameterizes each event in shape and amplitude <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib33\" title=\"\">33</a>]</cite>; MOMEL/INTSINT stylizes contours by extracting sparse targets and interpolating between them <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib34\" title=\"\">34</a>]</cite>; the Target Approximation (qTA/PENTA) model further links target selection to communicative goals and models the dynamics of their realization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib35\" title=\"\">35</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "ratio",
                    "spectral",
                    "alpha",
                    "human",
                    "speaker",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "ratio",
                    "spectral",
                    "alpha",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "metrics",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "ratio",
                    "spectral",
                    "alpha",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "evaluation",
                    "spectral",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "ratio",
                    "spectral",
                    "alpha",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "evaluation",
                    "spectral",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will extend the approach in several directions. Incorporating additional acoustic and spectral measures may capture finer-grained dimensions of prosodic expressiveness, while developing weighted composite indices could integrate multiple prosodic dimensions without sacrificing interpretability. Cross-linguistic applications and analyses of conversational or emotionally expressive speech will further test the framework&#8217;s generalizability. Finally, embedding these interpretable metrics directly into training or fine-tuning pipelines could transform them from diagnostic tools into guiding objectives for adaptive model optimization.</p>\n\n",
                "matched_terms": [
                    "spectral",
                    "metrics"
                ]
            }
        ]
    },
    "A2.T15": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 15: Human evaluation metrics on spectral tilt (L1–L0).",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nS1\n0.102\n0.000\n0.618\n0.409\n0.521\n0.422\n\n\nS2\n0.123\n0.000\n0.771\n0.392\n0.593\n0.384\n\n\nS3\n0.119\n0.000\n0.648\n0.361\n0.527\n0.458\n\n\nS4\n0.124\n0.000\n0.699\n0.363\n0.525\n0.518\n\n\nS5\n0.109\n0.000\n0.838\n0.441\n0.641\n0.413",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T15.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T15.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T15.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.102</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.618</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.409</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.521</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.422</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S2</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.123</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.771</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.392</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.593</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.384</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S3</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.119</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.648</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.361</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.527</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.458</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S4</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.124</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.699</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.363</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.525</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.518</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S5</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.109</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.838</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.441</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.641</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.413</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tilt",
            "evaluation",
            "err",
            "spectral",
            "speaker",
            "human",
            "rec",
            "ℓ01∗ell01",
            "prec",
            "ℓ01ell01",
            "f1f1",
            "l1–l0",
            "metrics"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational models of human speech prosody, though developed in different traditions, share the goal of linking abstract representations of prosodic events to their continuous acoustic realizations. The command&#8211;response model represents targets as underlying commands generating smooth contours <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib32\" title=\"\">32</a>]</cite>; the Tilt model parameterizes each event in shape and amplitude <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib33\" title=\"\">33</a>]</cite>; MOMEL/INTSINT stylizes contours by extracting sparse targets and interpolating between them <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib34\" title=\"\">34</a>]</cite>; the Target Approximation (qTA/PENTA) model further links target selection to communicative goals and models the dynamics of their realization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib35\" title=\"\">35</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "speaker",
                    "human",
                    "f1f1",
                    "l1–l0",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "metrics",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "metrics",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "spectral",
                    "f1f1",
                    "l1–l0",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "evaluation",
                    "spectral",
                    "human",
                    "f1f1",
                    "l1–l0",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "spectral",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "l1–l0",
                    "spectral",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "tilt",
                    "evaluation",
                    "spectral",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will extend the approach in several directions. Incorporating additional acoustic and spectral measures may capture finer-grained dimensions of prosodic expressiveness, while developing weighted composite indices could integrate multiple prosodic dimensions without sacrificing interpretability. Cross-linguistic applications and analyses of conversational or emotionally expressive speech will further test the framework&#8217;s generalizability. Finally, embedding these interpretable metrics directly into training or fine-tuning pipelines could transform them from diagnostic tools into guiding objectives for adaptive model optimization.</p>\n\n",
                "matched_terms": [
                    "spectral",
                    "metrics"
                ]
            }
        ]
    },
    "A2.T16": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 16: Human evaluation metrics for CPPS.",
        "body": "Speaker\nℓ0/1\\ell_{0/1}\nℓ0/1∗\\ell_{0/1}^{*}\nRec.\nPrec.\nF1F_{1}\nErr.\n\n\n\n\nS1\n0.103\n0.000\n0.724\n0.510\n0.646\n0.359\n\n\nS2\n0.093\n0.000\n0.801\n0.553\n0.715\n0.304\n\n\nS3\n0.104\n0.000\n0.772\n0.554\n0.679\n0.349\n\n\nS4\n0.136\n0.000\n0.677\n0.440\n0.587\n0.447\n\n\nS5\n0.119\n0.000\n0.724\n0.516\n0.661\n0.360",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T16.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\ell_{0/1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T16.m2\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Rec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T16.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Err.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S1</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.103</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.724</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.510</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.646</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.359</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S2</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.093</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.801</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.553</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.715</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.304</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S3</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.104</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.772</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.554</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.679</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.349</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S4</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.136</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.677</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.440</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.587</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.447</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S5</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.119</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.724</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.516</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.661</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.360</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evaluation",
            "err",
            "cpps",
            "speaker",
            "human",
            "rec",
            "prec",
            "ℓ01ell01",
            "f1f1",
            "ℓ01∗ell01",
            "metrics"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Introduces a linguistically informed, objective, interpretable metrics for text-to-speech prosody evaluation</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address some of the subjectivity and context-based limitations of direct rating tasks, paired comparison tests are also common for evaluating systems. For example, the classic AB preference test has participants choose between the tested system and a baseline using several different stimuli <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib20\" title=\"\">20</a>]</cite>. Based on these results, some sort of statistical model is then used to produce a final ranking or score. One example is the Bradley&#8211;Terry model (BTM) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib21\" title=\"\">21</a>]</cite>, which estimates latent competitiveness scores from pairwise comparisons. While providing a robust statistical foundation for comparing models, this class of evaluation metrics suffers a similar pitfall to MOS and MUSHRA, failing to provide useful linguistic details about the final score. A secondary limitation, which is in fact shared by all subjective methods, is that perception experiments can be resource-intensive, especially when performed at a scale that can ensure statistical significance. This is particularly true for paired comparison methods, where the number of pairings that must be tested increases quadratically in the number of evaluated models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing linguistically informed objective metrics for speech prosody is essential for advancing both scientific understanding and technological performance. Our proposed evaluation framework is designed to meet four key goals. First, it provides objective and reproducible measures of prosodic naturalness. Second, it adopts a two-tier architecture that reflects how human prosody operates&#8212;linking discrete structural events to their continuous phonetic realizations. Third, it accounts for variability across cue dimensions and individual speakers, capturing the natural diversity of prosodic expression. Finally, it is interpretable, enabling clear diagnosis of why and how synthetic speech diverges from human performance. To validate the robustness and perceptual relevance of these metrics, we compare model-based evaluations with human listener ratings, bridging quantitative analysis and perceptual judgment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can now apply traditional binary classification evaluation metrics. To briefly illustrate, we define a slightly modified zero-one loss as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "cpps",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarize the results of the perceptual evaluation, presenting the proportion of listeners who judged each speaker as human and the MOS for overall naturalness, respectively. The MOS ratings followed a five-point scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib49\" title=\"\">49</a>]</cite>, where 1 corresponds to <span class=\"ltx_text ltx_font_italic\">completely unnatural</span> and 5 to <span class=\"ltx_text ltx_font_italic\">completely natural</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "cpps",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "f1f1",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speaker",
                    "human",
                    "f1f1",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "cpps",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "metrics"
                ]
            }
        ]
    },
    "A2.T17": {
        "source_file": "Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach",
        "caption": "Table 17: tt-tests for self-validation for best model vs. worst human",
        "body": "Feature\nMetric\nTTS\nHuman\n\ntt-value\n\n\npp-value\n\nWinner\n\n\n\n\nDuration\nℓ0/1∗\\ell_{0/1}^{*}\nOpenAI\nS1\n3.076\n2.56e-03\nHuman\n\n\nF1F_{1}\nOpenAI\nS3\n\n−-0.104\n9.17e-01\nHuman\n\n\nError\nVITS\nS5\n0.147\n8.83e-01\nHuman\n\n\nPitch\nℓ0/1∗\\ell_{0/1}^{*}\nGoogle\nS4\n6.977\n1.56e-10\nHuman\n\n\nF1F_{1}\nPolly\nS4\n\n−-0.712\n4.77e-01\nHuman\n\n\nError\nGoogle\nS4\n\n−-1.527\n1.28e-01\nTTS\n\n\nIntensity\nℓ0/1∗\\ell_{0/1}^{*}\nOpenAI\nS3\n5.750\n6.14e-08\nHuman\n\n\nF1F_{1}\nOpenAI\nS3\n\n−-0.943\n3.47e-01\nHuman\n\n\nError\nOpenAI\nS1\n3.388\n8.31e-04\nHuman\n\n\nAlpha ratio\nℓ0/1∗\\ell_{0/1}^{*}\nOpenAI\nS1\n5.626\n1.09e-07\nHuman\n\n\nF1F_{1}\nOpenAI\nS1\n2.791\n5.78e-03\nTTS\n\n\nError\nOpenAI\nS1\n\n−-1.906\n5.79e-02\nTTS\n\n\nL1–L0\nℓ0/1∗\\ell_{0/1}^{*}\nOpenAI\nS4\n7.181\n4.90e-11\nHuman\n\n\nF1F_{1}\nOpenAI\nS1\n\n−-2.806\n5.62e-03\nHuman\n\n\nError\nAzure\nS4\n3.437\n6.88e-04\nHuman\n\n\nCPPS\nℓ0/1∗\\ell_{0/1}^{*}\nAzure\nS4\n6.937\n1.78e-10\nHuman\n\n\nF1F_{1}\nOpenAI\nS4\n\n−-0.876\n3.82e-01\nHuman\n\n\nError\nOpenAI\nS4\n2.892\n4.21e-03\nHuman",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Feature</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Metric</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">TTS</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Human</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-value</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-value</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Winner</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Duration</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m5\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">S1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">3.076</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2.56e-03</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S3</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m7\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>0.104</td>\n<td class=\"ltx_td ltx_align_right\">9.17e-01</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VITS</th>\n<td class=\"ltx_td ltx_align_right\">S5</td>\n<td class=\"ltx_td ltx_align_right\">0.147</td>\n<td class=\"ltx_td ltx_align_right\">8.83e-01</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Pitch</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m8\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Google</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">S4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">6.977</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.56e-10</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m9\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Polly</th>\n<td class=\"ltx_td ltx_align_right\">S4</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m10\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>0.712</td>\n<td class=\"ltx_td ltx_align_right\">4.77e-01</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Google</th>\n<td class=\"ltx_td ltx_align_right\">S4</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m11\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>1.527</td>\n<td class=\"ltx_td ltx_align_right\">1.28e-01</td>\n<td class=\"ltx_td ltx_align_right\">TTS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Intensity</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m12\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">S3</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">5.750</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">6.14e-08</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m13\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S3</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m14\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>0.943</td>\n<td class=\"ltx_td ltx_align_right\">3.47e-01</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S1</td>\n<td class=\"ltx_td ltx_align_right\">3.388</td>\n<td class=\"ltx_td ltx_align_right\">8.31e-04</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">Alpha ratio</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m15\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">S1</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">5.626</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.09e-07</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m16\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S1</td>\n<td class=\"ltx_td ltx_align_right\">2.791</td>\n<td class=\"ltx_td ltx_align_right\">5.78e-03</td>\n<td class=\"ltx_td ltx_align_right\">TTS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S1</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m17\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>1.906</td>\n<td class=\"ltx_td ltx_align_right\">5.79e-02</td>\n<td class=\"ltx_td ltx_align_right\">TTS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">L1&#8211;L0</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m18\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">S4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">7.181</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">4.90e-11</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m19\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S1</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m20\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>2.806</td>\n<td class=\"ltx_td ltx_align_right\">5.62e-03</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Error</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Azure</th>\n<td class=\"ltx_td ltx_align_right\">S4</td>\n<td class=\"ltx_td ltx_align_right\">3.437</td>\n<td class=\"ltx_td ltx_align_right\">6.88e-04</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"3\">CPPS</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m21\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Azure</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">S4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">6.937</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.78e-10</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m22\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right\">S4</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"-\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T17.m23\" intent=\":literal\"><semantics><mo>&#8722;</mo><annotation encoding=\"application/x-tex\">-</annotation></semantics></math>0.876</td>\n<td class=\"ltx_td ltx_align_right\">3.82e-01</td>\n<td class=\"ltx_td ltx_align_right\">Human</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Error</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">OpenAI</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">S4</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">2.892</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">4.21e-03</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">Human</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "347e01",
            "tttests",
            "688e04",
            "156e10",
            "alpha",
            "917e01",
            "google",
            "831e04",
            "metric",
            "490e11",
            "pitch",
            "−1527",
            "−0712",
            "382e01",
            "error",
            "best",
            "feature",
            "−2806",
            "−0876",
            "winner",
            "f1f1",
            "579e02",
            "vits",
            "polly",
            "ttvalue",
            "256e03",
            "openai",
            "ppvalue",
            "578e03",
            "selfvalidation",
            "178e10",
            "cpps",
            "614e08",
            "477e01",
            "intensity",
            "562e03",
            "l1–l0",
            "883e01",
            "ratio",
            "model",
            "azure",
            "human",
            "tts",
            "109e07",
            "duration",
            "−0943",
            "−1906",
            "421e03",
            "−0104",
            "128e01",
            "ℓ01∗ell01",
            "worst"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Prosody is essential for speech technology, shaping comprehension, naturalness, and expressiveness. However, current text-to-speech (TTS) systems still struggle to accurately capture human-like prosodic variation, in part because existing evaluation methods for prosody remain limited. Traditional metrics like Mean Opinion Score (MOS) are resource-intensive, inconsistent, and offer little insight into why a system sounds unnatural. This study introduces a linguistically informed, semi-automatic framework for evaluating TTS prosody through a two-tier architecture that mirrors human prosodic organization. The method uses quantitative linguistic criteria to evaluate synthesized speech against human speech corpora across multiple acoustic dimensions. By integrating discrete and continuous prosodic measures, it provides objective and interpretable metrics of both event placement and cue realization, while accounting for the natural variability observed across speakers and prosodic cues. Results show strong correlations with perceptual MOS ratings while revealing model-specific weaknesses that traditional perceptual tests alone cannot capture. This approach provides a principled path toward diagnosing, benchmarking, and ultimately improving the prosodic naturalness of next-generation TTS systems.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transformative advancements in deep neural net (DNN) speech synthesis systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib2\" title=\"\">2</a>]</cite> have produced TTS models that, particularly in short segments, are nearly indistinguishable from humans. While text inputs have continued to increase in length, content, and context, however, TTS models have yet to fully encapsulate the full range of human expression, inviting increased focus on modeling human prosody in speech technologies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib5\" title=\"\">5</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Challengingly for TTS systems, prosody is complex. It is not solvable by, for example, adding SSML (Speech Synthesis Markup Language) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib15\" title=\"\">15</a>]</cite> tags to the inputs of TTS models&#8212;something which, even if capable of encapsulating the full range of human prosody, adds a laborious step to what is intended to be an automated process. Rather, we know that prosody can be accidental or intentional, covert or overt, varying across a broad swath of linguistic and social contexts while maintaining the same emotional &#8220;label&#8221; in layman&#8217;s terms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib16\" title=\"\">16</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the most popular general measures of TTS quality&#8212;subjective or otherwise&#8212;is the Mean Opinion Score (MOS), which is calculated by conducting experiments that ask many participants to rate the &#8220;naturalness&#8221; of model outputs on a scale from 1 to 5, then taking the mean. Aside from being costly and time-intensive, MOS and similar evaluation methods have been found to be inconsistent across the literature and poorly defined, frequently leading to different results when performed under different circumstances <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib17\" title=\"\">17</a>]</cite>. For example, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib18\" title=\"\">18</a>]</cite> performed a meta-analysis of evaluations used at INTERSPEECH 2014 and found that more than 60% of papers used fewer than 20 listeners for their evaluations. Meanwhile, they showed that at least 30 participants were necessary to enable a stable level of significance for MOS.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another common technique is MUSHRA, in which participants evaluate several TTS models simultaneously along a sliding scale from 0 to 100 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib19\" title=\"\">19</a>]</cite>. The models are mixed in with an open reference produced by a human, as well as other lower-quality &#8220;anchor&#8221; references such as low-pass filtered speech samples. MUSHRA and similar techniques rely heavily, however, on a small sample of reference materials used in an artificial evaluation environment. Considering recent advances in TTS quality, this can prove problematic when synthetic outputs differ from the reference in a plausible manner. Low-quality anchors may also prove less useful in these settings.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While subjective methods are still commonly recognized as the gold standard, there have been increasing attempts to address some of their limitations by using objective methods. For example, given the plethora of MOS rating data available, a natural idea would be to predict MOS scores directly from the TTS output using supervised machine learning models. Indeed, several attempts have been made in this general direction. MOSNet uses spectrograms as the input to predict MOS on a frame-by-frame basis, taking advantage of convolutional and recurrent layers in a bidirectional long short-term memory network to capture temporal and local information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib22\" title=\"\">22</a>]</cite>. LDNet, which takes inspiration from MOSNet, additionally incorporates the listener&#8217;s identity as an input, allowing for prediction for a specific listener <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib23\" title=\"\">23</a>]</cite>. Other models like SSL-MOS use pre-trained embeddings, rather than pure acoustic or spectral information as the input <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib24\" title=\"\">24</a>]</cite>. Some also include more specific linguistic features, such as <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math>, POS tags, etc. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib25\" title=\"\">25</a>]</cite>. Crucially, however, even if these models are able to successfully model MOS ratings, they necessarily possess the same limitations as the metric they mimic: inconsistency and linguistic opaqueness.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Word error rate (WER) is a third approach that looks at how well automatic speech recognition (ASR) systems recognize TTS outputs as a proxy for how well humans might, sometimes being correlated with human ratings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib27\" title=\"\">27</a>]</cite>. Still, WER and similar techniques focus on segmental sequences and are generally insensitive to prosodic variation. Additionally, as TTS systems improve in quality, the focus in evaluation has shifted from the simpler task of understanding to the more complex one of employing accurate prosody.</p>\n\n",
                "matched_terms": [
                    "human",
                    "tts",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although these methods capture certain aspects of linguistic form, they are limited by their lack of sensitivity to variation. Frame-level acoustic comparisons assume that every utterance has a single optimal realization, unfairly penalizing valid prosodic variation that occur within the natural range of human expression. In practice, two speakers&#8212;or even two utterances by the same speaker&#8212;may express the same prosodic target with different pitch ranges, voice qualities, or timing patterns, all of which are perceptually valid. Evaluations based on rigid acoustic distances therefore risk rewarding uniformity rather than communicative adequacy.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these limitations, our framework extends existing objective evaluation techniques in two key ways. First, we explicitly model human variation, incorporating the natural flexibility observed in prosodic realization to prevent over-penalization of legitimate variation in TTS outputs. In other words, our framework recognizes that there is not a single correct prosodic realization, but rather a range of acceptable patterns that convey the same communicative function. More importantly, we introduce evaluation using a two-tier framework that is grounded in prosodic theory.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important insight from prosodic theory is that prosodic encodings of human natural speech are inherently two-layered: they consist of discrete structural targets and their continuous phonetic realizations. In intonational phonology (e.g., Autosegmental&#8211;Metrical theory), the discrete layer comprises categories such as pitch accents, phrase accents, and boundary tones, which define what events occur and where <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib31\" title=\"\">31</a>]</cite>. These targets are then realized through continuous parameters&#8212;<math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alignment, scaling, interpolation&#8212;modulated by duration, intensity, and voice quality adjustments.</p>\n\n",
                "matched_terms": [
                    "human",
                    "pitch",
                    "intensity",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational models of human speech prosody, though developed in different traditions, share the goal of linking abstract representations of prosodic events to their continuous acoustic realizations. The command&#8211;response model represents targets as underlying commands generating smooth contours <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib32\" title=\"\">32</a>]</cite>; the Tilt model parameterizes each event in shape and amplitude <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib33\" title=\"\">33</a>]</cite>; MOMEL/INTSINT stylizes contours by extracting sparse targets and interpolating between them <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib34\" title=\"\">34</a>]</cite>; the Target Approximation (qTA/PENTA) model further links target selection to communicative goals and models the dynamics of their realization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib35\" title=\"\">35</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">What unites these approaches is the recognition that the continuous layer is not fixed: variation can come from multiple sources, including intrinsic speaker differences (e.g., pitch range, voice quality), contextual influences (e.g., syntax, discourse structure, and information status), and communicative intent (e.g., emphasis, affect). This means that evaluating prosody requires not only checking whether events are placed in the right locations, but also measuring how closely their continuous realization matches the range of natural variability observed in human speech. This dual perspective&#8212;structure plus execution&#8212;is the core principle guiding our two-tier evaluation.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, prosodic structure is realized through a rich, high-dimensional set of acoustic cues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib36\" title=\"\">36</a>]</cite>. This multidimensionality is evident in both phrasing and prominence, the two most important aspects of prosodic structure cross-linguistically. Phrasing is primarily related to durational cues such as word duration and pause duration, but also related to pitch cues (e.g., final lowering, reset, boundary tone) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib37\" title=\"\">37</a>]</cite>, and voice quality cues (e.g., creakiness) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>. Prominence, meanwhile, can be variably realized through a number of acoustic cues: for example, in American English, stressed or accented words tend to have higher pitch, longer duration, tenser voice quality (stronger energy in the high-frequency region of the spectrum and greater periodicity), and greater intensity. And while traditional prosodic evaluations often only focus on pitch, intensity, and duration, recent studies have shown voice quality to contribute important acoustic-prosodic cues as well <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib41\" title=\"\">41</a>]</cite>. Because speakers can use different combinations of cues to achieve the same communicative function, this cue multidimensionality thus contributes to an additional source of variation in prosodic realization. Our model therefore aims to capture a range of acoustic representations for prosodic structure, as well as their potential variability. Importantly, these cues are interpretable and linguistically meaningful. The goal of our approach is not only to provide objective evaluations for the prosodic naturalness of TTS systems, but also to diagnose and identify the limitations of state-of-the-art models, especially when the specific factors that make synthetic speech sound &#8220;unnatural&#8221; remain elusive to the untrained ear.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "tts",
                    "intensity",
                    "duration",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend and validate a new prosodic evaluation method we first introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib42\" title=\"\">42</a>]</cite>, which automatically and objectively evaluates TTS outputs using acoustic measurements against a reference corpus of human speech. The method computes differences between TTS and human utterances of the same sentences across multiple acoustic measures, enabling interpretable, fine-grained analysis of model performance. Whereas perception MOS experiments incur significant costs whenever a model is updated, this method only has a one-time setup cost of collecting a test corpus, greatly decreasing evaluation costs and providing clarity into the specific acoustic domains in which particular models struggle. To capture the inherent variability of natural speech, evaluation is organized around two prosodic tiers&#8212;binary events (e.g., phrasing and prominence targets) and continuous signals (e.g., pitch and spectral trajectories). Audio signals are force-aligned at the word level, and aggregate acoustic measurements are taken for each segment. These acoustic measurements serve as the basis for objective evaluation. Notably, rather than producing a single numerical score, the framework serves as an analytical tool for quantitatively identifying where and how TTS models diverge from human prosody.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "pitch",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As established in the linguistic literature based on natural human speech, the prosodic signal can be broken down into targets and interpolation. Here, &#8220;prosodic events&#8221; or simply &#8220;events&#8221; refer to those linguistic targets, including but not limited to pitch and phrase accents, boundary tones, and pauses. Indeed, the correct placement of such events&#8212;for example, accenting the right words and pausing in the appropriate places&#8212;is crucial in natural speech.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first tier therefore evaluates whether models place prosodic events on the same words where human speakers typically produce them. Events are automatically detected as local extrema in acoustic signals (e.g., peaks in the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> contour). We frame this as a binary classification task: for each word, the model predicts whether an event should occur. Because human productions vary, there is no single &#8220;correct&#8221; reference. To address this, we define two complementary criteria for correctness.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The simpler one looks solely at the proportion of speakers who agree with a model (i.e., place an event where the model does) at a particular point. We call this proportion, calculated for each word, the &#8220;agreement score&#8221; signal. More formally, we define a discrete signal comprising <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> words as <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\{0,1\\}</annotation></semantics></math>, where <math alttext=\"x_{i}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x_{i}=1</annotation></semantics></math> if there is an event at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word, and <math alttext=\"x_{i}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x_{i}=0</annotation></semantics></math> otherwise. Our goal is to compare a machine signal <math alttext=\"p=(p_{1},p_{2},\\dots,p_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p=(p_{1},p_{2},\\dots,p_{n})</annotation></semantics></math> against a set of human signals <math alttext=\"\\mathcal{S}=\\{s_{1},s_{2},\\dots,s_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><msub><mi>s</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{s_{1},s_{2},\\dots,s_{m}\\}</annotation></semantics></math>, where <math alttext=\"s_{i}=(s_{i,1},s_{i,2},\\dots,s_{i,n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m8\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{i}=(s_{i,1},s_{i,2},\\dots,s_{i,n})</annotation></semantics></math>. Then, we define the agreement score of a model at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p3.m9\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word as</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It stands to reason, though, that being in the minority of human speakers does not preclude the possibility of a model being deemed natural. Indeed, the crux of the evaluation problem&#8217;s difficulty is that natural variation permits multiple &#8220;correct&#8221; utterances. This leads us to the second method for determining &#8220;correctness,&#8221; which is a more lax version of the first. Rather than being strictly correct or incorrect, we assign a continuous correctness score <math alttext=\"\\varepsilon(\\alpha_{i})\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})\\in(0,1]</annotation></semantics></math>, where</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S2.F1\" title=\"Figure 1 &#8227; 2.1.1 Binary event evaluation &#8227; 2.1 Overview &#8227; 2 Methods &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m2\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> is a generalized Gaussian function, with a smooth peak (i.e., high loss) when there&#8217;s no or low agreement, and a rapid decline as agreement increases. Then, we can define a &#8220;smooth&#8221; loss <math alttext=\"\\ell_{0/1}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}</annotation></semantics></math> by plugging in <math alttext=\"\\varepsilon(\\alpha_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m4\" intent=\":literal\"><semantics><mrow><mi>&#949;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon(\\alpha_{i})</annotation></semantics></math> for <math alttext=\"\\mathbbm{1}(\\alpha_{i}&lt;c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS1.p5.m5\" intent=\":literal\"><semantics><mrow><mn>&#120793;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#945;</mi><mi>i</mi></msub><mo>&lt;</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbbm{1}(\\alpha_{i}&lt;c)</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S2.E2\" title=\"In 2.1.1 Binary event evaluation &#8227; 2.1 Overview &#8227; 2 Methods &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Practically, this means that we&#8217;re always awarding &#8220;partial credit&#8221; to models based on the proportion of speakers concurring with them: if all or almost all of the speakers agree with a model, it&#8217;ll get all (or almost all) of the points, but as the percentage of people agreeing with it decays, its correctness will rapidly fall to zero. Thus, variability is enabled with a bias toward the majority.</p>\n\n",
                "matched_terms": [
                    "ℓ01∗ell01",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While binary measures are our primary criterion for evaluating prosody, precise acoustic realizations are also crucial for simulating human speech. In the second class of evaluation metrics, we assess whether the &#8220;continuous signals&#8221; of TTS acoustic features fall within an appropriate range compared to natural human speech. This approach bears a closer resemblance to prior works, calculating the distance between acoustic measurements of human and synthetic utterances on some aligned basis. Here, we look at signals <math alttext=\"x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">x=(x_{1},x_{2},\\dots,x_{n}),x_{i}\\in\\mathbb{R}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is an acoustic feature&#8217;s measurement at the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word. Since each speaker uses the same script, all our word-aligned signals share the same dimensions, enabling evaluation via standard vector distance metrics. For example, we have a vector containing the <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> measurement for each word in the human sample, and an analogous vector for the model, both with the same length. (As an aside, if we remove this assumption, a similar analysis is still possible, using alternate series comparison techniques like dynamic time warping.)</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Still, we run into the issue of comparing signals with a <span class=\"ltx_text ltx_font_italic\">corpus</span> of human data; it&#8217;s easy to compare a model with one human, but not all of them. To account for multiple speakers and the variation they exhibit, we look at &#8220;normalized&#8221; error: the mean of the squared <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m1\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-scores of each element in the TTS signal. Formally, if we let <math alttext=\"p=(p_{1},\\dots,p_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p=(p_{1},\\dots,p_{n})</annotation></semantics></math> be the TTS model&#8217;s acoustic measurements and <math alttext=\"\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo>=</mo><msub><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo lspace=\"0em\" rspace=\"0.0835em\">.</mo><mo lspace=\"0.0835em\" rspace=\"0.167em\">.</mo><mi>m</mi><mo stretchy=\"false\">]</mo></mrow></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{i}=\\{s_{j,i}\\}_{j\\in[1..m]}</annotation></semantics></math> be the set of measurements of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th word among all human speakers (e.g., <math alttext=\"\\mathcal{S}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">\\mathcal{S}_{1}</annotation></semantics></math> is the set of measurements at the first word across all speakers), we can define error as follows:</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\overline{\\mathcal{S}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"true\">&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\overline{\\mathcal{S}_{i}}</annotation></semantics></math> is the mean of <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> and <math alttext=\"\\mathrm{std}(\\mathcal{S}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS3.p2.m8\" intent=\":literal\"><semantics><mrow><mi>std</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{std}(\\mathcal{S}_{i})</annotation></semantics></math> is its standard deviation. This is motivated by the fact that human speakers will likely have a high degree of agreement in certain places and low agreement elsewhere. By dividing out variation, our error metric weights model&#8211;human agreement based on whether people agree among themselves.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "human",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, human and TTS signals were force-aligned using Charsiu <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib44\" title=\"\">44</a>]</cite>, a transformer-based aligner. Then, the following acoustic features were extracted on the word level using Praat: (1) duration (ms), including word duration and pause duration&#8212;important indicators for phrasing and temporal organization; (2) <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> pitch (Hz), an important indicator for intonation, prominence, and phrasing); and (3) intensity (dB), which is important for prominence. In addition to these traditional prosodic features, we included three spectral measures that are important indicators for voice quality and prominence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib39\" title=\"\">39</a>]</cite>: (4) alpha ratio, the energy difference between the 1&#8211;<math alttext=\"5\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">5\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> and 50&#8211;<math alttext=\"1\\text{\\,}\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">kHz</mi></mrow><annotation encoding=\"application/x-tex\">1\\text{\\,}\\mathrm{kHz}</annotation></semantics></math> regions in the spectrum; (5) L1&#8211;L0, the difference between the <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> (300&#8211;<math alttext=\"800\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><mn>800</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">800\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) and <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> (0&#8211;<math alttext=\"300\\text{\\,}\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m7\" intent=\":literal\"><semantics><mrow><mn>300</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\">Hz</mi></mrow><annotation encoding=\"application/x-tex\">300\\text{\\,}\\mathrm{Hz}</annotation></semantics></math>) regions in the spectrum; and (5) cepstral peak prominence-smoothed (CPPS, dB). Before serving as the inputs of our evaluation metrics, all measurements were <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS4.p1.m8\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math>-score normalized by speaker and sentence. As an aside, we also performed a similar analysis at the phone, rather than word, level, but found inconsistent results due to high variation. Further, from the view of sentence-level prosody, the phone level emerged less relevant.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "ratio",
                    "cpps",
                    "alpha",
                    "intensity",
                    "human",
                    "tts",
                    "duration",
                    "f1f1",
                    "l1–l0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We performed our analysis using a spoken corpus of Jane Austen&#8217;s <span class=\"ltx_text ltx_font_italic\">Emma</span> (Volume II, Chapter 10), obtained through LibriVox <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib46\" title=\"\">46</a>]</cite>. This dataset ensured a uniform narrative tone paired with moments of heightened emotion, especially in dialogue. Unlike spontaneous speech, these recordings contained relatively few disfluencies, a trait they share with synthetic speech. To facilitate meaningful comparisons with TTS outputs, our analysis centered on speakers 1 through 5, who exhibit North American English accents and characteristic female pitch ranges. The chapter comprises 136 sentences with an average length of 15.1 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 11.4 words, ranging from 2 to 63.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides human speakers, we synthesized each sentence using the following five models, representing a range of capabilities across open-source and commercial models, and exhibiting the same accent characteristics as our corpus: Google TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Studio-O</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://cloud.google.com/text-to-speech?hl=en</span></span></span> OpenAI TTS (<span class=\"ltx_text ltx_font_typewriter\">tts-1</span>; <span class=\"ltx_text ltx_font_typewriter\">nova</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span> Amazon Polly (<span class=\"ltx_text ltx_font_typewriter\">Joanna</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://docs.aws.amazon.com/polly/</span></span></span> Microsoft Azure TTS (<span class=\"ltx_text ltx_font_typewriter\">en-US-Emma</span>),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://azure.microsoft.com/en-us/products/ai-services/ai-speech</span></span></span> and VITS (<span class=\"ltx_text ltx_font_typewriter\">facebook/mms-tts-eng</span>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#bib.bib47\" title=\"\">47</a>]</cite>. For each of these models, we used the most recent generally available version as of February 2025.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "human",
                    "tts",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, to assess our framework&#8217;s treatment of natural variation, we also applied it to the human reference corpus itself using a leave-one-out design, evaluating each speaker against the remaining four. This allowed direct comparison between human and TTS prosodic performance under identical metrics.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, despite recent advances in TTS, both metrics reveal that there remains a significant gap between synthetic and human performance in this reading task. For both MOS and the ratings on humanness, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests reveal that even the best-performing TTS model, OpenAI, was rated statistically lower than the worst-rated human speakers (<math alttext=\"p=0.00011\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.00011</mn></mrow><annotation encoding=\"application/x-tex\">p=0.00011</annotation></semantics></math>). This underscores the persistent challenges that even the most advanced TTS systems face in achieving truly human-like naturalness.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "tttests",
                    "tts",
                    "human",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MOS scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T2\" title=\"Table 2 &#8227; 3.1.1 Overall human-likeness and Mean Opinion Scores &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> suggest a general hierarchy of perceived quality (from best to worst): OpenAI, Google, Azure, VITS, and finally Polly. On the higher end, OpenAI and Google scored within the moderate naturalness range (3.0&#8211;4.0: borderline to mostly natural). VITS and Polly both occupied the lower end of the scale, in the range of &#8220;mostly unnatural&#8221; (<math alttext=\"&lt;3.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>3.0</mn></mrow><annotation encoding=\"application/x-tex\">&lt;3.0</annotation></semantics></math>), with VITS demonstrating a small but measurable advantage over Polly.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "best",
                    "azure",
                    "google",
                    "worst",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MOS and human-likeness ratings summarize listeners&#8217; general impressions, the results from the pairwise comparison task provide a more detailed view of relative preferences among TTS models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> models these pairwise comparisons as a directed graph, where each edge indicates the dominant model in a given pairing and edge weights correspond to the proportion of trials favoring that model. A clear transitive ranking emerges from the figure: OpenAI (no outgoing edges) occupies the highest rank, followed by Google (out-degree 1), Microsoft Azure (out-degree 2), Amazon Polly (out-degree 3), and VITS (out-degree 4). To validate this structure, we fitted a Bradley&#8211;Terry model (BTM) to the same dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T3\" title=\"Table 3 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the BTM-derived scores reproduce the same ranking observed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.2 Pairwise comparison &#8227; 3.1 Human Perceptual Evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "tts",
                    "google",
                    "model",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the ranking from pairwise comparisons differs from that of MOS, with VITS rated lower than Polly. This suggests that when listeners compared samples directly, they preferred Polly slightly more often. The discrepancy highlights instability and context dependence of perceptual evaluations: even when rated by the same listeners, models were ranked differently when they were evaluated in isolation versus compared in pairs. This inconsistency underscores the limitations of traditional perceptual measures, calling into question the reliability of measures like MOS as standalone evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarize overall performance across all our measured acoustic-prosodic features using two different distance metrics. Full evaluation results for each measurement are in Appendix A. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates categorical (binary) accuracy using <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores, which were min&#8211;max normalized for comparability, while Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.F4\" title=\"Figure 4 &#8227; 3.2 Objective prosodic evaluation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> depicts continuous performance using the complement of normalized error (1 &#8211; error), with higher values indicating closer approximations of human speech. Together, they capture complementary perspectives on prosodic control&#8212;discrete event placement versus continuous acoustic implementation &#8212; and reveal broadly consistent patterns across metrics.</p>\n\n",
                "matched_terms": [
                    "f1f1",
                    "human",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both categorical and continuous metrics, OpenAI exhibits the most balanced and consistent prosodic control. It ranks highest in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> for duration (0.767), intensity (0.461), and spectral tilt (alpha ratio = 0.747), and maintains relatively low continuous errors across all cues, indicating accurate implementation of phrasing, prominence, and voice quality. However, its pitch accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.311; error = 0.582) slightly lags behind other cues, indicating persistent difficulty in reproducing natural intonation patterns and pitch accent placement. This limitation echoes the perceptual findings that, although OpenAI&#8217;s speech sounds more natural overall, its tonal modulation remains less human-like.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "openai",
                    "error",
                    "ratio",
                    "alpha",
                    "intensity",
                    "duration",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both Google Cloud and Microsoft Azure performed competitively across most prosodic dimensions.\nWhile Azure achieved slightly higher scores on the binary metrics&#8212;reflecting more accurate placement of prosodic events such as prominence peaks or boundaries (e.g., duration <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.723 vs. Google&#8217;s 0.684; spectral tilt <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.642 vs. 0.641)&#8212;Google consistently showed lower normalized errors across the continuous metrics, indicating more precise acoustic realization of these cues once placed (e.g., duration error = 0.035 vs. Azure&#8217;s 0.042; pitch error = 0.468 vs. 0.660; spectral tilt error = 0.409 vs. 0.469). This distinction likely explains why Google&#8217;s speech was judged to sound more natural overall. Listeners are sensitive not only to whether prosodic events occur in the right locations but also to how smoothly and accurately those events are implemented acoustically. Fine-grained control over pitch movement, duration, and intensity variation contributes strongly to perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "error",
                    "azure",
                    "intensity",
                    "duration",
                    "google",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Amazon Polly and VITS perform consistently below the top three systems across both categorical and continuous metrics. Polly shows its relative strength in pitch modeling, achieving higher categorical accuracy (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.418) and lower pitch error (0.566) than several other systems. Its performance on duration (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.611; error = 0.049) and CPPS (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.462; error = 0.620) is moderate, suggesting basic control over temporal structure and voice periodicity. However, Polly performs poorly on intensity (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.310; error = 0.576) and spectral tilt (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.599 for alpha ratio; 0.267 for L1&#8211;L0), suggesting limited variation in loudness and voice quality. VITS ranks the lowest among all systems, showing pervasive weaknesses in prosodic control across both categorical and continuous dimensions. It exhibits the lowest <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores (e.g., duration = 0.375, pitch = 0.143, intensity = 0.358) and the largest continuous deviations (pitch error = 1.188; CPPS error = 0.757). The placement of phrasing and prominence is often inaccurate. Nonetheless, VITS shows some limited strengths. It achieves the lowest duration error among all models (0.033), suggesting consistent temporal pacing once a rhythm is established, even though its categorical accuracy for duration remains low (<math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> = 0.375).</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "error",
                    "ratio",
                    "cpps",
                    "alpha",
                    "intensity",
                    "duration",
                    "f1f1",
                    "l1–l0",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the rankings derived from the acoustic&#8211;prosodic metrics using our proposed method are largely consistent with the perceptual results: OpenAI emerges as the most human-like system, followed by Google, Azure, Polly, and finally VITS.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "azure",
                    "google",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further validate the proposed acoustic&#8211;prosodic metrics, we conducted a self-validation analysis using human speech. If the metrics are meaningful, human speech should consistently outperform synthetic speech while still showing subtle variability among speakers.</p>\n\n",
                "matched_terms": [
                    "selfvalidation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complete results for all speakers and acoustic measures are presented in Appendix B. Despite the decreased variation available for comparison with the self-validation task, human speakers score significantly better than the models do using our metrics. For duration, for example, the majority of speakers have an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> score of at least 0.9, whereas the best-performing model (OpenAI) scored merely 0.767. The most &#8220;difficult&#8221; feature to get right for both models and humans was L1&#8211;L0 spectral tilt, and even here the best model scores an <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> of merely 0.375 compared to the worst human speaker&#8217;s 0.521. The most striking measure is smoothed zero-one loss: across each of the five features, every human has a loss of 0.000, while no model ever achieves such a feat. As one of the metrics that intelligently encodes variation in the source dataset, it is a positive sign that our evaluation metrics can deftly handle multiple potentially correct utterances.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "selfvalidation",
                    "best",
                    "model",
                    "feature",
                    "human",
                    "duration",
                    "f1f1",
                    "l1–l0",
                    "worst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More robustly, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.02104v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Human speaker prosodic self-validation &#8227; 3 Results &#8227; Toward Objective and Interpretable Prosody Evaluation in Text-to-Speech: A Linguistically Motivated Approach\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests between all humans and all TTS models for each feature using smoothed zero-one loss, <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math>, and normalized error. For each of the 18 shown settings, the human speakers perform statistically better than the TTS models do. Similar <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-tests comparing the best TTS model against the worst human for each setting show the human performing better in 15 out of 18 settings for this more difficult task.</p>\n\n",
                "matched_terms": [
                    "tttests",
                    "error",
                    "best",
                    "feature",
                    "tts",
                    "human",
                    "f1f1",
                    "model",
                    "worst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although state-of-the-art neural TTS systems now produce speech that is clear, smooth, and intelligible, our study demonstrates that a substantial gap remains between human and machine prosody. Both perceptual judgments and our acoustic&#8211;prosodic analyses converge on this conclusion: even the best-performing models fail to reproduce truly natural, human-like prosody. This gap calls for the need for effective and interpretable evaluation metrics&#8212;tools that can meaningfully quantify how far current systems remain from human prosodic behavior and, more importantly, why.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on linguistic insights regarding the dual-tier nature of prosody, our proposed evaluation framework fills this gap by combining both discrete and continuous metrics. The discrete tier captures the correct placement of prosodic events&#8212;phrasing, boundaries, and prominence&#8212;while the continuous tier measures how accurately those events are realized in the signal. This dual perspective enables a deeper diagnostic view: systems may succeed in event placement but fail in execution, or vice versa. For example, while Azure achieved higher binary accuracy than Google, it underperformed in continuous realization, leading to less natural-sounding speech. This finding illustrates that accurate event timing alone does not guarantee prosodic naturalness&#8212;proper fine-grained control over pitch movement, duration, and spectral balance is equally essential.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "pitch",
                    "azure",
                    "google"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is immediately clear that the overall trends observed in our perception experiments are reflected in the acoustic data. Our evaluation metrics uniquely highlight the varying strengths and weaknesses of TTS models across different acoustic dimensions. OpenAI&#8217;s model exhibited the most balanced overall performance, but its relatively weak pitch control suggests lingering challenges in generating natural intonation contours. Google Cloud, while achieving slightly lower categorical accuracy, demonstrated superior continuous precision, producing more stable cue realizations once prosodic events were correctly placed. Azure excelled in event placement yet lagged in voice-quality modulation and spectral variation, leading to overall lower naturalness. The lower-performing systems, Polly and VITS, showed greater inconsistencies in temporal alignment and pitch control, reflecting persistent difficulties in maintaining prosodic coherence within open-source architectures. The multidimensional results reveal that each system adopts a distinct prosodic strategy. Importantly, our metrics revealed that voice-quality measures&#8212;including alpha ratio, L1&#8211;L0, and CPPS&#8212;played a more significant role than previously assumed. Systems that exhibited richer spectral variability tended to sound more natural, even when their pitch trajectories were less accurate. These findings reinforce that prosodic expressiveness depends on the coordination of multiple acoustic dimensions&#8212;pitch, timing, and spectral quality&#8212;rather than on <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> alone.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "ratio",
                    "model",
                    "alpha",
                    "azure",
                    "tts",
                    "google",
                    "l1–l0",
                    "vits",
                    "polly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, a key advantage of this framework lies in its treatment of speaker variability. Prosody is inherently flexible: the same sentence may be realized with multiple acceptable contours, and speakers differ systematically in their use of pitch range, rhythm, and voice quality. Rigid reference-based metrics would penalize this natural variability as error. Our framework instead encodes variation within the human reference set, allowing models to be evaluated against the distribution of human utterances rather than a single canonical form. The human self-validation results confirm that the metrics behave sensibly: all human speakers scored near ceiling, with perfect smoothed loss (<math alttext=\"\\ell_{0/1}^{*}=0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>0</mn><mo>/</mo><mn>1</mn></mrow><mo>&#8727;</mo></msubsup><mo>=</mo><mn>0.000</mn></mrow><annotation encoding=\"application/x-tex\">\\ell_{0/1}^{*}=0.000</annotation></semantics></math>) and <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p5.m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> scores exceeding 0.86 for duration, indicating that natural variability is preserved rather than punished. This ensures that the evaluation reflects genuine differences in prosodic control, not artifacts of inter-speaker diversity.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "error",
                    "selfvalidation",
                    "human",
                    "duration",
                    "f1f1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our results show that this flexibility extends beyond individual speakers to the multidimensional nature of prosodic realization. Among the prominence-related measures (intensity, pitch, spectral tilts, and CPPS), the high-performing models excelled in at least several of these dimensions. For example, while OpenAI did not score highly in pitch, it performed well in intensity, spectral tilt, and CPPS. This demonstrates that our evaluation metric objectively assesses synthesized speech naturalness while allowing for variability in the realization of prosodic prominence, as observed in natural speech. Different models, like human speakers, may use distinct acoustic strategies to achieve perceptually equivalent effects&#8212;an interpretive nuance that traditional single-dimensional metrics such as MOS cannot reveal.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "openai",
                    "cpps",
                    "intensity",
                    "human",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduced and validated a novel linguistically informed, semi-automatic prosodic evaluation technique for TTS models using a two-layered approach. This design reflects how human prosody operates&#8212;linking discrete categories such as phrasing and prominence to their continuous acoustic correlates&#8212;and allows both objective quantification and interpretive insight.</p>\n\n",
                "matched_terms": [
                    "human",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation metrics used in this study align closely with the naturalness rankings produced by perception-based human experiments, but more significantly, provide not only a broad performance comparison but also insight into specific prosodic weaknesses of different TTS models. For example, some models needed more work on having appropriate voice quality throughout, while others needed work in accenting the correct words. The general framework for evaluation presented&#8212;separately evaluating continuous and binary signals against human speech corpora&#8212;importantly provides a better-defined path toward TTS improvement, not just relative to other models, but in line with objective linguistic features.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "human"
                ]
            }
        ]
    }
}