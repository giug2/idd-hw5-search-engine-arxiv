{
    "S3.T1": {
        "caption": "Table 1: Objective evaluation results of zero-shot TTS using different models on objective sets, including content consistency (CER/WER) and speaker similarity (SIM).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ko</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.755</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.734</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.708</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.716</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Llasa-1B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.636</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.578</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-CER</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.672</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.580</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-NLL</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.674</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.581</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-CER-NLL</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.669</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.580</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.753</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.655</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.635</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.670</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-CER</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.34</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.751</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.655</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.645</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.37</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.677</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-NLL</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.753</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.659</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.662</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.59</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.682</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-CER-NLL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.753</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.659</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.656</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.85pt;padding-bottom:-0.85pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.680</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "grpocernll",
            "wer",
            "content",
            "llasa1b",
            "consistency",
            "grpocer",
            "evaluation",
            "↓downarrow",
            "similarity",
            "zeroshot",
            "tts",
            "objective",
            "speaker",
            "grponll",
            "sets",
            "cosyvoice2",
            "results",
            "↑uparrow",
            "model",
            "including",
            "sim",
            "human",
            "cer",
            "cerwer",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The objective evaluation results of zero-shot TTS are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1.2 GRPO setup &#8227; 3.1 Experimental setup &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As previously discussed, Llasa-1B </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relies on acoustic speech tokens, which lack alignment with the text, whereas CosyVoice2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> utilizes semantic tokens within an LLM, followed by a flow-matching model to refine acoustic details. Consequently, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves significantly better performance than </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Llasa-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on both metrics for Chinese and English.\nFurthermore, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> consistently improves CER/WER over both baseline models, demonstrating that our proposed method effectively enhances the semantic consistency of speech generated by LLM-based TTS models. Regarding speaker similarity (SIM), </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is comparable to the baseline models, except in cases where the baseline exhibits extremely high CER/WER. In such cases, excessive pronunciation errors likely lead the WavLM-based speaker verification model to produce unreliable similarity scores.\nCompared to the ablated models, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows overall superior performance across different subsets, except for slightly lower performance than </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on Chinese. This highlights the complementary role of NLL as an additional reward signal in improving synthesis quality.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper proposes a GRPO-based approach to enhance the performance of large language model (LLM)-based text-to-speech (TTS) models by deriving rewards from an off-the-shelf automatic speech recognition (ASR) model. Compared to previous reinforcement learning methods for LLM-based TTS, our method requires no dedicated model for reward computation or training. Moreover, we design a composite reward function that combines character error rate (CER) with negative log-likelihood (NLL) obtained from the ASR model, providing more informative and accurate reward signals. We apply GRPO fine-tuning to pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance. Experimental results show that the proposed method substantially improves both the intelligibility and naturalness of synthesized speech. Ablation studies and further analyses confirm the effectiveness of integrating the two reward components.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "model",
                    "results",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nSpeech synthesis, TTS, large language models, reinforcement learning, GRPO</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLM)-based TTS models have become the mainstream approach for speech synthesis due to their ability to generate highly natural-sounding speech and their powerful zero-shot cloning capabilities </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nGiven only a few seconds of prompt speech, these models can effectively capture speaker timbre, prosody, and speaking style, and synthesize speech corresponding to arbitrary text inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "speaker",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">LLM-based TTS models can be broadly divided into two categories.\nThe first uses large language models to model acoustic speech token extracted by speech codec models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In this paradigm, the predicted speech token can be simply converted to speech waveform through the decoder of codec models. However, acoustic tokens contain rich information and lack alignment with text, which can easily lead to synthesized speech content inconsistency with the text, or even synthesis failure.</span>\n</p>\n\n",
                "matched_terms": [
                    "content",
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The second category models semantic speech tokens within the LLM, followed by a non-autoregressive flow-matching model to supplement acoustic details beyond the semantic information </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe semantic tokens are usually extracted from an automatic speech recognition (ASR) models with a vector quantization layers, providing better alignment with text.\nTherefore, these models exhibit fewer synthesis failures, while the flow-matching module further improves the acoustic fidelity and naturalness of the generated speech. They usually outperform the first type.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While autoregressive sampling in LLM-based TTS models enables the generation of diverse and prosody-consistent speech, it also causes the model to sometimes generate speech that does not align with human preferences.\nTherefore, some studies apply reinforcement learning (RL) to fine-tune LLM-based TTS models, thereby achieving better performance.\nSeed-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed to integrate RL using word error rate (WER) and speaker similarity (SIM) as rewards within the Proximal Policy Optimization (PPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and REINFORCE </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> frameworks.\nHowever, this approach requires maintaining and training multiple models simultaneously, which results in a complex and unstable training process.\nTo address this, many studies adopt Direct Preference Optimization (DPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and its variants to enhance LLM-based TTS models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAlthough DPO does not require additional models, it heavily relies on high-quality paired preference data, making it sensitive to noisy or inconsistent annotations and costly to scale. Moreover, it offers limited fine-grained control over the reward function and shows restricted generalization ability </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDiffRO </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposes a differentiable reward framework for fine-tuning speech token LLMs in a supervised manner. Specifically, Gumbel-Softmax is employed to approximate the sampling of output tokens at each step, while an ASR-style token-to-text model is built to evaluate the negative log-likelihood (NLL) between the generated speech tokens and the corresponding ground-truth transcripts, which serves as the reward signal. Owing to the differentiability of Gumbel-Softmax, the optimization can be directly formulated as minimizing the NLL. Nevertheless, this approach requires pre-training a token-to-text model, which incurs additional computational and data costs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "similarity",
                    "tts",
                    "wer",
                    "model",
                    "speaker",
                    "sim",
                    "human",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, no prior study has applied Group Relative Policy Optimization (GRPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to fine-tune LLM-based TTS models. GRPO eliminates the value model used in PPO, thereby reducing resource consumption and training complexity. Unlike DPO, it does not require large amounts of paired positive and negative preference data in advance and can instead be trained solely with text data. In addition, by applying group-wise normalization, GRPO keeps the advantage function within a stable range, which contributes to stabilizing the training process.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose a GRPO-based fine-tuning approach applicable to both categories of LLM-based TTS models introduced earlier. Our method leverages an off-the-shelf ASR model to directly derive rewards from generated speech waveforms and compute group-relative advantages, thereby substantially simplifying and stabilizing the training pipeline. Consequently, it obviates the need for a dedicated token-to-text model as required by DiffRO </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAdditionally, we use a reward function that combines character error rate (CER) with negative log-likelihood (NLL), effectively balancing alignment accuracy and probabilistic confidence to achieve improved performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We fine-tune both categories of LLM-based TTS models using the proposed method and evaluate their zero-shot performance. Experimental results show that our approach significantly enhances both the semantic consistency and naturalness of the synthesized speech. Moreover, both ablation studies and additional analyses demonstrate the effectiveness of integrating the two reward components. Audio demos, codes and models are available at </span>\n  <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ryuclc.github.io/LLM-TTS-GRPO\" style=\"font-size:90%;\" title=\"\">https://ryuclc.github.io/LLM-TTS-GRPO</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "consistency",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper focuses on enhancing semantic consistency and proposes a reward function applicable to both categories of LLM-based TTS models. Specifically, given the reconstructed speech waveform from the LLM outputs, rewards are computed using an off-the-shelf ASR model combining character error rate (CER) and negative log-likelihood (NLL).</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "consistency",
                    "model",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">CER is a widely adopted metric for evaluating speech intelligibility. As an intuitive measure based on edit distance, it directly reflects the transcription accuracy of synthesized speech relative to the ground-truth text. Its simplicity and interpretability make CER a reliable indicator of how well the generated speech conveys the intended content.\nHowever, relying solely on CER as the reward function may introduce several limitations. First, CER only measures surface-level transcription accuracy, and therefore ignores the ASR model&#8217;s confidence in its predictions. This may lead to ambiguous optimization signals, especially when different speech outputs yield the same CER but differ significantly in quality. Second, CER is insensitive to acoustic nuances such as prosody or fluency, which are essential for natural-sounding speech synthesis. Third, CER is discrete, which may result in sparse or unstable reward signals during reinforcement learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "content",
                    "different",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, NLL is introduced as a complementary reward. NLL captures the probability distribution of the ASR model over the ground-truth tokens, providing a continuous, fine-grained signal that reflects the model&#8217;s confidence and sensitivity to subtle variations. By combining CER and NLL, the reward function balances objective intelligibility with model confidence, resulting in more stable training and higher-quality speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Formally, the ASR model transcribes input speech </span>\n  <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119857;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into predicted text </span>\n  <math alttext=\"\\mathbf{\\hat{y}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119858;</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\mathbf{\\hat{y}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, from which CER is computed against the ground-truth </span>\n  <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119858;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For NLL, as shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.1 Reward function &#8227; 2 Proposed Method &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the true text is tokenized as </span>\n  <math alttext=\"\\mathbf{\\bar{y}}=(\\bar{y}_{1},\\dots,\\bar{y}_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119858;</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">y</mi>\n              <mo mathsize=\"0.900em\">&#175;</mo>\n            </mover>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">y</mi>\n              <mo mathsize=\"0.900em\">&#175;</mo>\n            </mover>\n            <mi mathsize=\"0.900em\">N</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{\\bar{y}}=(\\bar{y}_{1},\\dots,\\bar{y}_{N})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and passed through the ASR decoder to obtain logits. NLL is then calculated as:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we enhance pre-trained LLM-based TTS models using the GRPO algorithm </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The fine-tuning process is illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.1 Reward function &#8227; 2 Proposed Method &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The pre-trained speech token LLM serves as the policy model </span>\n  <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#952;</mi>\n      <annotation encoding=\"application/x-tex\">\\theta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the trainable parameters. These parameters are also used to initialize the reference model </span>\n  <math alttext=\"\\pi_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">f</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{ref}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which remains frozen during fine-tuning.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given an input text </span>\n  <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119858;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the policy model autoregressively samples </span>\n  <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">G</mi>\n      <annotation encoding=\"application/x-tex\">G</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> times, producing a set of outputs denoted as </span>\n  <math alttext=\"O=\\{\\mathbf{o}_{1},\\mathbf{o}_{2},\\cdots,\\mathbf{o}_{G}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">O</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119848;</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119848;</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8943;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119848;</mi>\n            <mi mathsize=\"0.900em\">G</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">O=\\{\\mathbf{o}_{1},\\mathbf{o}_{2},\\cdots,\\mathbf{o}_{G}\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Here, </span>\n  <math alttext=\"\\mathbf{o}_{i}=[o_{i,1},o_{i,2},\\cdots,o_{i,t},\\cdots]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119848;</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mn mathsize=\"0.900em\">2</mn>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8943;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8943;</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{o}_{i}=[o_{i,1},o_{i,2},\\cdots,o_{i,t},\\cdots]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The outputs are then used to compute the Kullback&#8211;Leibler (KL) divergence between the policy and reference models:</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Here, </span>\n  <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the dataset, </span>\n  <math alttext=\"|\\mathbf{o}_{i}|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119848;</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">|\\mathbf{o}_{i}|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the length of </span>\n  <math alttext=\"\\mathbf{o}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119848;</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{o}_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the coefficient of the KL penalty. In GRPO, the old policy </span>\n  <math alttext=\"\\pi_{\\theta_{old}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <msub>\n          <mi mathsize=\"0.900em\">&#952;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">l</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">d</mi>\n          </mrow>\n        </msub>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta_{old}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is usually initialized to the current policy </span>\n  <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> at the start of each update and remains fixed during the gradient computation. Finally, the policy model parameters are updated by maximizing this objective function.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of the proposed GRPO method across different categories of LLM-based TTS models, we fine-tune two open-source baseline models with training codes: CosyVoice2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Llasa-1B </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which correspond to the two types of models introduced earlier. CosyVoice2 supports four languages: Chinese, English, Japanese, and Korean, whereas Llasa-1B supports only Chinese and English.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "cosyvoice2",
                    "llasa1b",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For GRPO fine-tuning, we randomly sampled 4,000 sentences from the Emilia dataset, covering Chinese, English, Japanese, and Korean. Chinese and English accounted for approximately 90% of the total, with the remainder consisting of Japanese and Korean. The full set of 4,000 sentences was used to fine-tune CosyVoice2, while Llasa-1B was fine-tuned solely on the Chinese and English subsets.</span>\n</p>\n\n",
                "matched_terms": [
                    "llasa1b",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our proposed method is denoted as GRPO-CER-NLL. In addition to comparing the baseline models with this approach, we conduct ablation studies to evaluate the contribution of each reward component. Specifically, GRPO-CER refers to fine-tuning using only the CER reward, while GRPO-NLL uses only the NLL reward. In our implementation, we adopt Whisper-large-v3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the ASR model to obtain reward values.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "grpocernll",
                    "grpocer",
                    "model",
                    "cer",
                    "grponll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the performance of zero-shot speech synthesis across different TTS models, we prepared both objective and subjective test sets. For Chinese (zh) and English (en), we adopted the open-source benchmarks provided by seed-tts-eval </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, containing 2,020 and 1,088 samples, respectively. For Japanese (ja) and Korean (ko), we additionally collected 1,000 test samples per language from the Common Voice dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, the subjective set is used for listening tests, as it includes high expressiveness and diverse styles. We collect approximately 100 samples per language.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "sets",
                    "objective",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employed two objective metrics for evaluation:\n1) content consistency (CER/WER),\nwhich is used to assess speech intelligibility.For Chinese, we used Paraformer-zh </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while for the other languages we used Whisper-large-v3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to transcribe the generated speech.\nword error rate (WER) was calculated for English, and character error rate (CER) for the remaining languages. For each subset, the average CER/WER was computed by dividing the total edit distance by the total reference length.\n2) speaker similarity (SIM), which evaluates how closely the synthesized speech matches the reference speaker. It is calculated as the cosine similarity between the speaker embeddings of the generated and reference speech. The embeddings were extracted using a speaker verification model fine-tuned on WavLM </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "wer",
                    "content",
                    "consistency",
                    "model",
                    "objective",
                    "evaluation",
                    "speaker",
                    "sim",
                    "cer",
                    "cerwer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the impact of GRPO on the naturalness of synthesized speech, we conducted subjective listening tests. Specifically, we performed MOS evaluations on CosyVoice2 and its variants fine-tuned using different methods. For each language, 30 samples were randomly selected from the subjective evaluation sets for zero-shot synthesis. We then conducted listening tests by recruiting 10 native speakers for each language group, who rated the synthesized speech in terms of overall naturalness on a 1&#8211;5 scale with 0.5-point intervals.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "sets",
                    "evaluation",
                    "cosyvoice2",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The MOS results are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1.2 GRPO setup &#8227; 3.1 Experimental setup &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The </span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-value of paired </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test was employed to measure the significance of the difference between two models. Across all languages, the proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model outperformed the baseline models (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), indicating that combining GRPO with the proposed reward function effectively enhances the performance of LLM-based TTS models. Additionally, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieved significant higher MOS scores than the baseline across all languages (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) except Chinese (</span>\n  <math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&gt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), while </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outperformed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on all languages (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) except Korean (</span>\n  <math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&gt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). These results further demonstrate the benefits of integrating CER and NLL as complementary reward signals.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "grpocernll",
                    "grpocer",
                    "model",
                    "results",
                    "cer",
                    "grponll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Llasa-1B and its fine-tuned models, we also conduct subjective listening tests. However, the experimental results show that there are no significant improvements on naturalness with reinforcement learning. Therefore, we do not list the results here. The underlying reasons will be investigated in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "llasa1b",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">First, we examined the correlation between CER and NLL. We computed the rewards </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for synthesized speech generated by the two baseline models on the objective test sets across four languages. We observed that shorter speech segments tend to have lower </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as the ASR model exhibits lower confidence with limited context. To mitigate this, we removed excessively short sentences, resulting in approximately 8,000 data points. A scatter plot was then created with </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the horizontal axis and </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the vertical axis as show in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The plot shows no strong correlation, and in most cases </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> equals 1 while </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides additional discriminative information, demonstrating the effectiveness of incorporating NLL as a complementary reward. The Pearson correlation coefficient between </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was </span>\n  <math alttext=\"r=0.3371\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.3371</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=0.3371</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which, despite being statistically significant (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), indicates a negligible linear relationship in practice.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "sets",
                    "model",
                    "objective",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, we compared pronunciations errors in speech generated by baseline models with those generated by models fine-tuned using our proposed GRPO-based methods.\nFigure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows a Chinese example synthesized by Llasa-1B and its variants through spectrogram. We can find that </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Llasa-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced pronunciation errors on &#8220;&#20110;&#8221; and &#8220;&#32440;&#8221;, and omitted &#8220;&#24046;&#36317;&#8221; due to alignment issues. After GRPO fine-tuning, all variants corrected the omission of &#8220;&#24046;&#36317;&#8221; and the error in &#8220;&#32440;&#8221;. However, the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> still mispronounced &#8220;&#20110;&#8221;, while </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and our proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced fully correct pronunciations. In addition, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> exhibited more natural prosodic breaks.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "grpocernll",
                    "grpocer",
                    "grponll",
                    "llasa1b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Besides, Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustates the spectrogram of a English case generated by CosyVoice2 and its variants. They shows the similar results as in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese analyses provide intuitive evidence for the superiority of our proposed method.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we proposed a GRPO approach to enhance the pre-trained LLM-based TTS models. This method adopts an off-the-shell ASR model to obtain rewards, reducing the reliance on extra models to make the training process simpler and more stable. Besides, this paper adopts a reward function which is derived from the CER and NLL. This reward combines objective intelligibility with ASR model confidence, resulting in improved semantic consistency and naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "consistency",
                    "model",
                    "objective",
                    "cer"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: MOS results of zero-shot TTS using different models on subjective sets with 95% confidence intervals.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ko</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.42 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.05</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.22 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.10 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.18 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-CER</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.44 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.26 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.15 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.23 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-NLL</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.52 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.05</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.31 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.21 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.24 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.85pt 2.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">+ GRPO-CER-NLL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.58 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.05</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.43 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.29 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.85pt 2.3pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.30 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.08</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "mos",
            "models",
            "zeroshot",
            "tts",
            "intervals",
            "grpocernll",
            "sets",
            "grpocer",
            "model",
            "±pm",
            "cosyvoice2",
            "results",
            "confidence",
            "subjective",
            "different",
            "grponll"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The MOS results are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1.2 GRPO setup &#8227; 3.1 Experimental setup &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The </span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-value of paired </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-test was employed to measure the significance of the difference between two models. Across all languages, the proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model outperformed the baseline models (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), indicating that combining GRPO with the proposed reward function effectively enhances the performance of LLM-based TTS models. Additionally, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieved significant higher MOS scores than the baseline across all languages (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) except Chinese (</span>\n  <math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&gt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), while </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outperformed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on all languages (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) except Korean (</span>\n  <math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&gt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). These results further demonstrate the benefits of integrating CER and NLL as complementary reward signals.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper proposes a GRPO-based approach to enhance the performance of large language model (LLM)-based text-to-speech (TTS) models by deriving rewards from an off-the-shelf automatic speech recognition (ASR) model. Compared to previous reinforcement learning methods for LLM-based TTS, our method requires no dedicated model for reward computation or training. Moreover, we design a composite reward function that combines character error rate (CER) with negative log-likelihood (NLL) obtained from the ASR model, providing more informative and accurate reward signals. We apply GRPO fine-tuning to pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance. Experimental results show that the proposed method substantially improves both the intelligibility and naturalness of synthesized speech. Ablation studies and further analyses confirm the effectiveness of integrating the two reward components.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nSpeech synthesis, TTS, large language models, reinforcement learning, GRPO</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLM)-based TTS models have become the mainstream approach for speech synthesis due to their ability to generate highly natural-sounding speech and their powerful zero-shot cloning capabilities </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nGiven only a few seconds of prompt speech, these models can effectively capture speaker timbre, prosody, and speaking style, and synthesize speech corresponding to arbitrary text inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">LLM-based TTS models can be broadly divided into two categories.\nThe first uses large language models to model acoustic speech token extracted by speech codec models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In this paradigm, the predicted speech token can be simply converted to speech waveform through the decoder of codec models. However, acoustic tokens contain rich information and lack alignment with text, which can easily lead to synthesized speech content inconsistency with the text, or even synthesis failure.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The second category models semantic speech tokens within the LLM, followed by a non-autoregressive flow-matching model to supplement acoustic details beyond the semantic information </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe semantic tokens are usually extracted from an automatic speech recognition (ASR) models with a vector quantization layers, providing better alignment with text.\nTherefore, these models exhibit fewer synthesis failures, while the flow-matching module further improves the acoustic fidelity and naturalness of the generated speech. They usually outperform the first type.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While autoregressive sampling in LLM-based TTS models enables the generation of diverse and prosody-consistent speech, it also causes the model to sometimes generate speech that does not align with human preferences.\nTherefore, some studies apply reinforcement learning (RL) to fine-tune LLM-based TTS models, thereby achieving better performance.\nSeed-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposed to integrate RL using word error rate (WER) and speaker similarity (SIM) as rewards within the Proximal Policy Optimization (PPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and REINFORCE </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> frameworks.\nHowever, this approach requires maintaining and training multiple models simultaneously, which results in a complex and unstable training process.\nTo address this, many studies adopt Direct Preference Optimization (DPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and its variants to enhance LLM-based TTS models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAlthough DPO does not require additional models, it heavily relies on high-quality paired preference data, making it sensitive to noisy or inconsistent annotations and costly to scale. Moreover, it offers limited fine-grained control over the reward function and shows restricted generalization ability </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nDiffRO </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proposes a differentiable reward framework for fine-tuning speech token LLMs in a supervised manner. Specifically, Gumbel-Softmax is employed to approximate the sampling of output tokens at each step, while an ASR-style token-to-text model is built to evaluate the negative log-likelihood (NLL) between the generated speech tokens and the corresponding ground-truth transcripts, which serves as the reward signal. Owing to the differentiability of Gumbel-Softmax, the optimization can be directly formulated as minimizing the NLL. Nevertheless, this approach requires pre-training a token-to-text model, which incurs additional computational and data costs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, no prior study has applied Group Relative Policy Optimization (GRPO) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to fine-tune LLM-based TTS models. GRPO eliminates the value model used in PPO, thereby reducing resource consumption and training complexity. Unlike DPO, it does not require large amounts of paired positive and negative preference data in advance and can instead be trained solely with text data. In addition, by applying group-wise normalization, GRPO keeps the advantage function within a stable range, which contributes to stabilizing the training process.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose a GRPO-based fine-tuning approach applicable to both categories of LLM-based TTS models introduced earlier. Our method leverages an off-the-shelf ASR model to directly derive rewards from generated speech waveforms and compute group-relative advantages, thereby substantially simplifying and stabilizing the training pipeline. Consequently, it obviates the need for a dedicated token-to-text model as required by DiffRO </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAdditionally, we use a reward function that combines character error rate (CER) with negative log-likelihood (NLL), effectively balancing alignment accuracy and probabilistic confidence to achieve improved performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We fine-tune both categories of LLM-based TTS models using the proposed method and evaluate their zero-shot performance. Experimental results show that our approach significantly enhances both the semantic consistency and naturalness of the synthesized speech. Moreover, both ablation studies and additional analyses demonstrate the effectiveness of integrating the two reward components. Audio demos, codes and models are available at </span>\n  <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ryuclc.github.io/LLM-TTS-GRPO\" style=\"font-size:90%;\" title=\"\">https://ryuclc.github.io/LLM-TTS-GRPO</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper focuses on enhancing semantic consistency and proposes a reward function applicable to both categories of LLM-based TTS models. Specifically, given the reconstructed speech waveform from the LLM outputs, rewards are computed using an off-the-shelf ASR model combining character error rate (CER) and negative log-likelihood (NLL).</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">CER is a widely adopted metric for evaluating speech intelligibility. As an intuitive measure based on edit distance, it directly reflects the transcription accuracy of synthesized speech relative to the ground-truth text. Its simplicity and interpretability make CER a reliable indicator of how well the generated speech conveys the intended content.\nHowever, relying solely on CER as the reward function may introduce several limitations. First, CER only measures surface-level transcription accuracy, and therefore ignores the ASR model&#8217;s confidence in its predictions. This may lead to ambiguous optimization signals, especially when different speech outputs yield the same CER but differ significantly in quality. Second, CER is insensitive to acoustic nuances such as prosody or fluency, which are essential for natural-sounding speech synthesis. Third, CER is discrete, which may result in sparse or unstable reward signals during reinforcement learning.</span>\n</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, NLL is introduced as a complementary reward. NLL captures the probability distribution of the ASR model over the ground-truth tokens, providing a continuous, fine-grained signal that reflects the model&#8217;s confidence and sensitivity to subtle variations. By combining CER and NLL, the reward function balances objective intelligibility with model confidence, resulting in more stable training and higher-quality speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathbf{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119838;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{e}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the encoder output, </span>\n  <math alttext=\"\\bar{y}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">&#175;</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">n</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bar{y}_{n}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th ground-truth token, and </span>\n  <math alttext=\"P(\\bar{y}_{n}|\\bar{y}_{&lt;n},\\mathbf{e})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">P</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mover accent=\"true\">\n                <mi mathsize=\"0.900em\">y</mi>\n                <mo mathsize=\"0.900em\">&#175;</mo>\n              </mover>\n              <mi mathsize=\"0.900em\">n</mi>\n            </msub>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mrow>\n              <msub>\n                <mover accent=\"true\">\n                  <mi mathsize=\"0.900em\">y</mi>\n                  <mo mathsize=\"0.900em\">&#175;</mo>\n                </mover>\n                <mrow>\n                  <mi/>\n                  <mo mathsize=\"0.900em\">&lt;</mo>\n                  <mi mathsize=\"0.900em\">n</mi>\n                </mrow>\n              </msub>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">&#119838;</mi>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">P(\\bar{y}_{n}|\\bar{y}_{&lt;n},\\mathbf{e})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the probability assigned by the ASR model. Lower NLL values indicate higher confidence and better alignment with the reference text.</span>\n</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we enhance pre-trained LLM-based TTS models using the GRPO algorithm </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The fine-tuning process is illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.1 Reward function &#8227; 2 Proposed Method &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The pre-trained speech token LLM serves as the policy model </span>\n  <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#952;</mi>\n      <annotation encoding=\"application/x-tex\">\\theta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the trainable parameters. These parameters are also used to initialize the reference model </span>\n  <math alttext=\"\\pi_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">f</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\pi_{ref}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which remains frozen during fine-tuning.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given an input text </span>\n  <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119858;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the policy model autoregressively samples </span>\n  <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">G</mi>\n      <annotation encoding=\"application/x-tex\">G</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> times, producing a set of outputs denoted as </span>\n  <math alttext=\"O=\\{\\mathbf{o}_{1},\\mathbf{o}_{2},\\cdots,\\mathbf{o}_{G}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">O</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119848;</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119848;</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8943;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119848;</mi>\n            <mi mathsize=\"0.900em\">G</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">O=\\{\\mathbf{o}_{1},\\mathbf{o}_{2},\\cdots,\\mathbf{o}_{G}\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Here, </span>\n  <math alttext=\"\\mathbf{o}_{i}=[o_{i,1},o_{i,2},\\cdots,o_{i,t},\\cdots]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119848;</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mn mathsize=\"0.900em\">1</mn>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mn mathsize=\"0.900em\">2</mn>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8943;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8943;</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{o}_{i}=[o_{i,1},o_{i,2},\\cdots,o_{i,t},\\cdots]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The outputs are then used to compute the Kullback&#8211;Leibler (KL) divergence between the policy and reference models:</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of the proposed GRPO method across different categories of LLM-based TTS models, we fine-tune two open-source baseline models with training codes: CosyVoice2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Llasa-1B </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which correspond to the two types of models introduced earlier. CosyVoice2 supports four languages: Chinese, English, Japanese, and Korean, whereas Llasa-1B supports only Chinese and English.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "cosyvoice2",
                    "different",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our proposed method is denoted as GRPO-CER-NLL. In addition to comparing the baseline models with this approach, we conduct ablation studies to evaluate the contribution of each reward component. Specifically, GRPO-CER refers to fine-tuning using only the CER reward, while GRPO-NLL uses only the NLL reward. In our implementation, we adopt Whisper-large-v3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the ASR model to obtain reward values.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "grpocernll",
                    "grpocer",
                    "model",
                    "grponll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the performance of zero-shot speech synthesis across different TTS models, we prepared both objective and subjective test sets. For Chinese (zh) and English (en), we adopted the open-source benchmarks provided by seed-tts-eval </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, containing 2,020 and 1,088 samples, respectively. For Japanese (ja) and Korean (ko), we additionally collected 1,000 test samples per language from the Common Voice dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, the subjective set is used for listening tests, as it includes high expressiveness and diverse styles. We collect approximately 100 samples per language.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "sets",
                    "subjective",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The objective evaluation results of zero-shot TTS are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1.2 GRPO setup &#8227; 3.1 Experimental setup &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As previously discussed, Llasa-1B </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relies on acoustic speech tokens, which lack alignment with the text, whereas CosyVoice2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> utilizes semantic tokens within an LLM, followed by a flow-matching model to refine acoustic details. Consequently, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves significantly better performance than </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Llasa-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on both metrics for Chinese and English.\nFurthermore, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> consistently improves CER/WER over both baseline models, demonstrating that our proposed method effectively enhances the semantic consistency of speech generated by LLM-based TTS models. Regarding speaker similarity (SIM), </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is comparable to the baseline models, except in cases where the baseline exhibits extremely high CER/WER. In such cases, excessive pronunciation errors likely lead the WavLM-based speaker verification model to produce unreliable similarity scores.\nCompared to the ablated models, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows overall superior performance across different subsets, except for slightly lower performance than </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on Chinese. This highlights the complementary role of NLL as an additional reward signal in improving synthesis quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "grpocernll",
                    "tts",
                    "model",
                    "cosyvoice2",
                    "results",
                    "grponll",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the impact of GRPO on the naturalness of synthesized speech, we conducted subjective listening tests. Specifically, we performed MOS evaluations on CosyVoice2 and its variants fine-tuned using different methods. For each language, 30 samples were randomly selected from the subjective evaluation sets for zero-shot synthesis. We then conducted listening tests by recruiting 10 native speakers for each language group, who rated the synthesized speech in terms of overall naturalness on a 1&#8211;5 scale with 0.5-point intervals.</span>\n</p>\n\n",
                "matched_terms": [
                    "mos",
                    "zeroshot",
                    "intervals",
                    "sets",
                    "cosyvoice2",
                    "subjective",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For Llasa-1B and its fine-tuned models, we also conduct subjective listening tests. However, the experimental results show that there are no significant improvements on naturalness with reinforcement learning. Therefore, we do not list the results here. The underlying reasons will be investigated in future work.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "subjective",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">First, we examined the correlation between CER and NLL. We computed the rewards </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for synthesized speech generated by the two baseline models on the objective test sets across four languages. We observed that shorter speech segments tend to have lower </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as the ASR model exhibits lower confidence with limited context. To mitigate this, we removed excessively short sentences, resulting in approximately 8,000 data points. A scatter plot was then created with </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the horizontal axis and </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the vertical axis as show in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The plot shows no strong correlation, and in most cases </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> equals 1 while </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides additional discriminative information, demonstrating the effectiveness of incorporating NLL as a complementary reward. The Pearson correlation coefficient between </span>\n  <math alttext=\"R_{CER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{CER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"R_{NLL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">R_{NLL}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> was </span>\n  <math alttext=\"r=0.3371\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.3371</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=0.3371</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which, despite being statistically significant (</span>\n  <math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">0.05</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p&lt;0.05</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), indicates a negligible linear relationship in practice.</span>\n</p>\n\n",
                "matched_terms": [
                    "sets",
                    "models",
                    "confidence",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, we compared pronunciations errors in speech generated by baseline models with those generated by models fine-tuned using our proposed GRPO-based methods.\nFigure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows a Chinese example synthesized by Llasa-1B and its variants through spectrogram. We can find that </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Llasa-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced pronunciation errors on &#8220;&#20110;&#8221; and &#8220;&#32440;&#8221;, and omitted &#8220;&#24046;&#36317;&#8221; due to alignment issues. After GRPO fine-tuning, all variants corrected the omission of &#8220;&#24046;&#36317;&#8221; and the error in &#8220;&#32440;&#8221;. However, the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> still mispronounced &#8220;&#20110;&#8221;, while </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and our proposed </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced fully correct pronunciations. In addition, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GRPO-CER-NLL</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> exhibited more natural prosodic breaks.</span>\n</p>\n\n",
                "matched_terms": [
                    "grponll",
                    "models",
                    "grpocer",
                    "grpocernll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Besides, Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F5\" style=\"font-size:90%;\" title=\"Figure 5 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustates the spectrogram of a English case generated by CosyVoice2 and its variants. They shows the similar results as in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18798v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.2.4 Analysis experiments &#8227; 3.2 Experimental results &#8227; 3 Experiments &#8227; Group Relative Policy Optimization for Text-to-Speech with Large Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese analyses provide intuitive evidence for the superiority of our proposed method.</span>\n</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we proposed a GRPO approach to enhance the pre-trained LLM-based TTS models. This method adopts an off-the-shell ASR model to obtain rewards, reducing the reliance on extra models to make the training process simpler and more stable. Besides, this paper adopts a reward function which is derived from the CER and NLL. This reward combines objective intelligibility with ASR model confidence, resulting in improved semantic consistency and naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "confidence",
                    "models",
                    "model",
                    "tts"
                ]
            }
        ]
    }
}