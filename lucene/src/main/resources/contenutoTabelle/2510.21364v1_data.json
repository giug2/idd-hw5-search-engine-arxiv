{
    "S2.T1": {
        "caption": "Table 1: Overview of models evaluated in this work. We only consider cased variants even if uncased versions exist.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_font_bold\">Architecture</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\"><span class=\"ltx_text ltx_font_bold\">Pre-training Data</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold\">Corpus Size</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BERTurk<sub class=\"ltx_sub\">32k,128k</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">BERT base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">OSCAR, Wikipedia, OPUS, non-public</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">35 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DistilBERTurk</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">DistilBERT</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">Distilled from BERTurk (subset)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">7 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ELECTRA<sub class=\"ltx_sub\">small</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">ELECTRA small</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">OSCAR, Wikipedia, OPUS, non-public</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">35 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ELECTRA<sub class=\"ltx_sub\">base</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">ELECTRA base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">OSCAR, Wikipedia, OPUS, non-public</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">35 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ELECTRA<sub class=\"ltx_sub\">mC4</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">ELECTRA base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">mC4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">242 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ConvBERTurk</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">ConvBERT base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">OSCAR, Wikipedia, OPUS, non-public</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">35 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ConvBERTurk<sub class=\"ltx_sub\">mC4</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">ConvBERT base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">mC4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">242 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RoBERTurk</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">RoBERTa-mid (12L, 1024H)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">OSCAR, Turkish C4 subset (1 GB)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">28 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SindBERT<sub class=\"ltx_sub\">base</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">RoBERTa base</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">mC4, OSCAR23, Wikipedia</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">312 GB</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">SindBERT<sub class=\"ltx_sub\">large</sub>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">RoBERTa large</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:184.9pt;\">mC4, OSCAR23, Wikipedia</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\">312 GB</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "sindbertlarge",
            "evaluated",
            "electramc4",
            "roberturk",
            "convbert",
            "even",
            "convberturkmc4",
            "corpus",
            "oscar23",
            "size",
            "nonpublic",
            "versions",
            "electrabase",
            "uncased",
            "exist",
            "distilberturk",
            "opus",
            "distilbert",
            "base",
            "electra",
            "architecture",
            "oscar",
            "distilled",
            "pretraining",
            "from",
            "small",
            "robertamid",
            "sindbertbase",
            "electrasmall",
            "1024h",
            "turkish",
            "berturk",
            "consider",
            "roberta",
            "overview",
            "only",
            "bert",
            "mc4",
            "wikipedia",
            "convberturk",
            "variants",
            "model",
            "berturk32k128k",
            "large",
            "work",
            "data",
            "12l",
            "subset",
            "cased"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">An overview of existing Turkish transformer-based language models is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts.\nWith SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish.\nTrained from scratch on 312&#160;GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish.\nWe evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> linguistic acceptability benchmark.\nOur results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall.\nThis flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated.\nAt the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume.\nTaken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages.\nThe SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wikipedia",
                    "base",
                    "oscar23",
                    "model",
                    "corpus",
                    "turkish",
                    "large",
                    "berturk",
                    "pretraining",
                    "from",
                    "data",
                    "mc4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The advent of transformer-based models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> has reshaped natural language processing (NLP), providing contextualized word representations that generalize across a wide range of tasks. While early efforts focused on English and multilingual approaches, research has consistently shown that monolingual pre-training on large, high-quality corpora yields superior results for the target language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "large",
                    "pretraining",
                    "roberta",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish NLP, several transformer-based encoders have been introduced in recent years. Notable examples include BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>, trained on a 35 GB corpus of Turkish OSCAR, Wikipedia, and OPUS data; ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib14\" title=\"\">2021</a>)</cite> models trained on both OSCAR and mC4 (35&#8211;242 GB)<cite class=\"ltx_cite ltx_citemacro_cite\">Jiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite>. While these models provide important milestones, most are relatively small encoder models trained with earlier-generation methods or focus on architectures other than RoBERTa. The only RoBERTa models out there were not computed in its fullest extend, but rather with small batch size for relatively small period <cite class=\"ltx_cite ltx_citemacro_cite\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>); Tas (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite>. Futher, Turkish still lacks a large-scale, high-quality encoder-only model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "opus",
                    "wikipedia",
                    "convbert",
                    "electra",
                    "model",
                    "corpus",
                    "turkish",
                    "size",
                    "oscar",
                    "berturk",
                    "data",
                    "small",
                    "roberta",
                    "only",
                    "mc4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce SindBERT, a RoBERTa-based encoder model pre-trained specifically for Turkish. SindBERT builds on the design principles of the German model GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> and adapts them to the morphological richness and agglutinative structure of Turkish. We construct a byte-level BPE vocabulary optimized for Turkish, train both base and large variants with fairseq&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ott et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib21\" title=\"\">2019</a>)</cite>, and leverage TPUv4 hardware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jouppi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite> for efficient large-scale pre-training. SindBERT is designed to combine scalability and reproducibility while directly targeting Turkish, resulting in the first large-scale RoBERTa-style encoder model for Turkish.\nOur contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "base",
                    "variants",
                    "model",
                    "large",
                    "turkish",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We release SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub>, trained from scratch on Turkish web-text.</p>\n\n",
                "matched_terms": [
                    "sindbertlarge",
                    "from",
                    "sindbertbase",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark SindBERT against existing Turkish and multilingual models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of transformer-based language models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> marked a paradigm shift in NLP, enabling significant improvements across a wide range of tasks. Building on these foundations, multilingual extensions such as mBERT and in particular XLM-RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib5\" title=\"\">2020</a>)</cite> became widely used as strong general-purpose baselines across more than 100 languages. At the same time, a wave of monolingual adaptations demonstrated that language-specific pre-training often outperforms multilingual alternatives when sufficient high-quality data is available&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib9\" title=\"\">2020b</a>; Martin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib20\" title=\"\">2020</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib6\" title=\"\">2020</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretraining",
                    "data",
                    "roberta",
                    "bert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, multilingual encoder-only models have seen a revival. EuroBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boizard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib3\" title=\"\">2025</a>)</cite> revisits the encoder paradigm with innovations from decoder-only models, introducing a family of multilingual encoders for European and global languages with native support for sequences up to 8,192 tokens. Similarly, mmBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib19\" title=\"\">2025</a>)</cite> scales encoder pretraining to 3T tokens across 1,800+ languages, introducing novel sampling schedules and showing strong performance on both high- and low-resource languages. These developments highlight that encoder-based architectures remain competitive even in an era dominated by large decoder models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "even",
                    "large",
                    "pretraining",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish, the first widely adopted transformer encoder was BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib27\" title=\"\">2020</a>)</cite>, trained on a 35&#160;GB mixture of OSCAR, Wikipedia, OPUS, and additional resources. Variants included cased/uncased models and vocabularies of 32k or 128k tokens. Distilled versions (DistilBERTurk)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite> and subsequent models such as ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERTurk expanded the model zoo, with some trained on the Turkish portion of mC4 (up to 242&#160;GB) <cite class=\"ltx_cite ltx_citemacro_cite\">Schweter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>. These provided important baselines but generally followed smaller encoder configurations or explored alternative pre-training architectures rather than scaling RoBERTa.</p>\n\n",
                "matched_terms": [
                    "models",
                    "opus",
                    "wikipedia",
                    "electra",
                    "convberturk",
                    "variants",
                    "model",
                    "turkish",
                    "oscar",
                    "distilled",
                    "berturk",
                    "versions",
                    "pretraining",
                    "roberta",
                    "distilberturk",
                    "mc4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line of work, RoBERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tas, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite> introduced a RoBERTa-style encoder specifically adapted for Turkish, showing that refined pre-training objectives and tokenizer design can yield competitive results.\nIn parallel, research has underscored the critical role of tokenization in morphologically rich languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> systematically analyzed the impact of vocabulary size and segmentation strategy, showing that larger vocabularies can notably improve performance in morphosyntactic evaluations.\nHowever, all these RoBERTa-based models were not extensively trained, typically using moderate batch sizes and relatively few update steps, resulting in comparatively shallow pretraining regimes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roberturk",
                    "turkish",
                    "size",
                    "work",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, these contributions highlight steady progress in Turkish NLP. However, despite the availability of increasingly large corpora and modern training infrastructure, Turkish has lacked a RoBERTa-based encoder model trained from scratch at scale. SindBERT addresses this gap by providing the first large-scale RoBERTa encoder dedicated to Turkish, trained on modern corpora and released openly to the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "large",
                    "turkish",
                    "from",
                    "roberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT was trained on three Turkish corpora: Wikipedia, OSCAR23 <cite class=\"ltx_cite ltx_citemacro_cite\">Jansen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib13\" title=\"\">2022</a>)</cite>, and mC4.\nThe corpus was shuffled and lightly filtered, restricted to the removal of documents containing invalid character encodings.\nThe extracted sizes are approximately 242&#160;GB for mC4, 69&#160;GB for OSCAR, and 0.6&#160;GB for Wikipedia, resulting in a combined pre-training corpus of about 312&#160;GB of Turkish text.</p>\n\n",
                "matched_terms": [
                    "wikipedia",
                    "oscar23",
                    "corpus",
                    "turkish",
                    "oscar",
                    "pretraining",
                    "mc4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to RoBERTa, SindBERT relies on byte pair encoding (BPE) <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib23\" title=\"\">2019</a>)</cite> for subword segmentation, which directly operates on raw text without the need for pre-tokenization or auxiliary tools such as Moses <cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib17\" title=\"\">2007</a>)</cite>. Since the original GPT-2 tokenizer was designed for English, we instead constructed a tokenizer tailored for Turkish. Following the strategy applied in GottBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we trained a dedicated vocabulary using 40 GB of randomly sampled Turkish text, resulting in a 52k subword inventory optimized for the language. In our experience, sampling around 40 GB of text is already enough for the subword statistics to stabilize, while scaling vocabulary training to the entire corpus would primarily increase computational cost without offering substantial gains. While we did not separately evaluate the effect of this adaptation on storage size or downstream accuracy, previous work in Dutch <cite class=\"ltx_cite ltx_citemacro_cite\">Delobelle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>)</cite> and German <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> indicates that language-specific tokenizers can yield improvements in both efficiency and performance.</p>\n\n",
                "matched_terms": [
                    "corpus",
                    "turkish",
                    "size",
                    "work",
                    "roberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the setup of GottBERT, we pre-trained both SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub> using the fairseq framework on a 128-core TPUv4 pod <cite class=\"ltx_cite ltx_citemacro_cite\">Jouppi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite>. Mixed-precision training (fp16/bfloat16) was not employed, so both models were trained entirely in full precision (fp32). This ensures that training dynamics can be attributed directly to model size, without numerical precision optimizations acting as additional factors.</p>\n\n",
                "matched_terms": [
                    "sindbertlarge",
                    "models",
                    "model",
                    "size",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "sindbertlarge",
                    "evaluated",
                    "base",
                    "model",
                    "large",
                    "size",
                    "pretraining",
                    "roberta",
                    "only",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "model",
                    "turkish",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For NER, we fine-tuned on the Turkish NER dataset introduced in the WikiANN corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib22\" title=\"\">2017</a>)</cite> and widely used for multilingual evaluation. We used the splits from <cite class=\"ltx_cite ltx_citemacro_citet\">Rahimi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib24\" title=\"\">2019</a>)</cite> and report micro F1 across all entity types.</p>\n\n",
                "matched_terms": [
                    "from",
                    "corpus",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness on user-generated content, we employed the OffensEval-TR 2020 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(&#199;&#246;ltekin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib4\" title=\"\">2020</a>)</cite>, a corpus of Turkish tweets annotated for the presence of offensive language. The dataset contains over 31k training and 3.5k test instances, labeled in a binary fashion as either <span class=\"ltx_text ltx_font_italic\">NOT</span> (not offensive) or <span class=\"ltx_text ltx_font_italic\">OFF</span> (offensive). Mentions and URLs were anonymized during preprocessing (e.g., replaced by @USER or URL), while the tweets otherwise preserve the linguistic and pragmatic properties of social media text.\nWe report performance using macro F1.</p>\n\n",
                "matched_terms": [
                    "corpus",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess fine-grained grammatical knowledge, we include evaluation on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#351;ar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib2\" title=\"\">2025</a>)</cite>, a benchmark of 16 core linguistic phenomena ranging from anaphor agreement and argument structure to scrambling and suspended affixation.\nEach phenomenon is represented by 1,000 minimal pairs, and models are scored following the BLiMP protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Warstadt et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib31\" title=\"\">2020</a>)</cite>, i.e., assigning higher probability to the grammatical sentence of each pair.\nFor each model we compute the accuracy within every phenomenon and report the average across all 16 categories as the overall <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> score.\nThis measure complements PoS tagging, NER, and sentiment classification by probing deeper syntactic and morphosyntactic competence.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.SS6\" title=\"3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">3.6</span></a> summarizes the vocabulary sizes and parameter counts of the Turkish and multilingual models included in our evaluation.\nThe smallest encoder is ELECTRA<sub class=\"ltx_sub\">small</sub> (13.7M parameters), followed by DistilBERTurk (67M).\nBase-scale Turkish encoders, such as ConvBERTurk (cased and mC4 variants), ELECTRA<sub class=\"ltx_sub\">base</sub> (cased and mC4), and BERTurk (cased/uncased), cluster between 106M and 111M parameters with 32k vocabularies.\nRoBERTurk, another RoBERTa-style encoder with a 50k vocabulary, is slightly larger at 125M parameters.\nSindBERT<sub class=\"ltx_sub\">base</sub> grows further to 126M owing to its 52k vocabulary and extended RoBERTa design.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roberturk",
                    "electrasmall",
                    "convberturk",
                    "variants",
                    "turkish",
                    "berturk",
                    "mc4",
                    "electrabase",
                    "roberta",
                    "distilberturk",
                    "sindbertbase",
                    "cased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the mid-scale, mBERT has 178M parameters with a WordPiece vocabulary of nearly 120k tokens, while the 128k-token BERTurk variants reach 184M.\nAmong larger models, XLM-R<sub class=\"ltx_sub\">base</sub> contains 278M parameters, while SindBERT<sub class=\"ltx_sub\">large</sub> grows to 357M.\nThe largest encoder considered is XLM-R<sub class=\"ltx_sub\">large</sub>, with 560M parameters and a 250k-token vocabulary.\nAll values were extracted using Hugging Face&#8217;s <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library.</p>\n\n",
                "matched_terms": [
                    "berturk",
                    "models",
                    "variants",
                    "sindbertlarge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pre-training, we monitored perplexity both on the training set (at each optimization step) and on the validation set (after each epoch; see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.F1\" title=\"Figure 1 &#8227; 4.1 Pre-training &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Across all configurations, the curves follow a consistent convergence pattern. An initial plateau phase can be observed, which is relatively brief for the base models but more pronounced for the large ones. Occasional short upward spikes appear in the training curves; if taken in isolation, these might be misread as divergence, yet they quickly subside as training progresses.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "pretraining",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The base models typically stabilize after 20k&#8211;30k steps, while the large models require slightly longer but consistently converge by around 40k steps. By the end of training, both configurations achieve comparably low perplexity, underscoring the efficiency of the pre-training setup. This trend is mirrored in the validation perplexity, which shows steady improvements after each epoch. Overall, training perplexity decreased from about 54.5k to 3.93 for the base models and from about 52.2k to 3.24 for the large models, reflecting robust and reliable convergence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "large",
                    "pretraining",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across base-scale models, performance on the Turkish Universal Dependencies treebank is consistently high, with micro-F1 values exceeding 93% for nearly all encoders.\nThe strongest overall results are achieved by ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (94.57), closely followed by SindBERT<sub class=\"ltx_sub\">base</sub> (94.47) and BERTurk<sub class=\"ltx_sub\">128k</sub> (94.44).\nInterestingly, both ConvBERTurk variants, trained with different corpora, maintain a narrow margin over ELECTRA-based and RoBERTa-style encoders, suggesting that architectural innovations like dynamic convolution offer slight but consistent gains in token-level syntactic tagging.\nThe relatively low score of RoBERTurk (87.99) indicates the limitations of early RoBERTa replications for Turkish, likely due to smaller corpora and shorter training schedules.\nSindBERT<sub class=\"ltx_sub\">base</sub> performs competitively within this saturated range, demonstrating strong generalization across tasks despite a larger 52k BPE vocabulary.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roberturk",
                    "convberturk",
                    "variants",
                    "convberturkmc4",
                    "turkish",
                    "roberta",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among large-scale encoders, SindBERT<sub class=\"ltx_sub\">large</sub> attains the highest F1 (94.63), marginally outperforming XLM-R<sub class=\"ltx_sub\">large</sub> (94.39).\nThis indicates that SindBERT&#8217;s pre-training on modern Turkish data contributes positively to syntactic coverage, even when compared to substantially larger multilingual models.\nThe weaker performance of EuroBERT<sub class=\"ltx_sub\">610M</sub> (93.33) may reflect its more domain-diverse, less Turkish-focused corpus composition.</p>\n\n",
                "matched_terms": [
                    "sindbertlarge",
                    "models",
                    "even",
                    "corpus",
                    "turkish",
                    "pretraining",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, POS tagging performance appears saturated across both scales, with nearly all base models exceeding 94 F1 and only marginal gains from scaling. SindBERT maintains parity with top-tier baselines, confirming that syntactic coverage in Turkish is largely solved for transformer-based encoders.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "turkish",
                    "from",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best base-scale performance is reached by BERTurk<sub class=\"ltx_sub\">32k</sub> (94.38), confirming its robustness for token-level classification.\nClose behind are ConvBERTurk (94.03) and BERTurk<sub class=\"ltx_sub\">128k</sub> (93.81), while SindBERT<sub class=\"ltx_sub\">base</sub> achieves a solid 93.19, comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> (93.49) and XLM-R<sub class=\"ltx_sub\">base</sub> (92.9).\nThis indicates that SindBERT&#8217;s RoBERTa-like setup neither clearly surpasses nor lags behind the most established Turkish encoders, suggesting that the NER task may already be approaching an upper limit with current dataset size and annotation quality.</p>\n\n",
                "matched_terms": [
                    "convberturk",
                    "turkish",
                    "size",
                    "electrabase",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the large scale, XLM-R<sub class=\"ltx_sub\">large</sub> slightly leads (94.44), followed closely by SindBERT<sub class=\"ltx_sub\">large</sub> (93.64).\nGiven that XLM-R was trained on over 2 TB of multilingual text, this narrow margin underscores the efficiency of SindBERT&#8217;s more compact, Turkish-focused pretraining corpus.</p>\n\n",
                "matched_terms": [
                    "large",
                    "sindbertlarge",
                    "pretraining",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "turkish",
                    "size",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For offensive language classification (OffensEval-TR 2020), we observe more pronounced differences between architectures.\nConvBERTurk reaches the highest macro-F1 among base models (81.99), with ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (81.90) and BERTurk<sub class=\"ltx_sub\">128k</sub> (81.77) performing almost identically.\nELECTRA variants and SindBERT<sub class=\"ltx_sub\">base</sub> (81.14) cluster slightly below, while distilled and multilingual models trail more clearly.\nThese results highlight that models trained on monolingual Turkish corpora still offer clear advantages for pragmatic and domain-sensitive tasks.\nSindBERT<sub class=\"ltx_sub\">base</sub> thus performs solidly but not at the very top, suggesting that further pre-training on informal or social-media text could enhance its stylistic robustness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "electra",
                    "base",
                    "convberturk",
                    "variants",
                    "convberturkmc4",
                    "turkish",
                    "distilled",
                    "pretraining",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the large model group, SindBERT<sub class=\"ltx_sub\">large</sub> again performs best (82.29), surpassing XLM-R<sub class=\"ltx_sub\">large</sub> (81.99) and far exceeding EuroBERT<sub class=\"ltx_sub\">610M</sub> (75.57).\nThis consistent lead across two of four downstream tasks emphasizes SindBERT&#8217;s balanced architecture and effective use of Turkish-specific corpora.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "sindbertlarge",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.SS2.SSS0.Px4\" title=\"TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> reports the detailed <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> results for all base and large models.\nOverall, SindBERT<sub class=\"ltx_sub\">base</sub> achieves an average score of 90.3, which is comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> and ELECTRA<sub class=\"ltx_sub\">mC4</sub> (both 89.9), while trailing behind the strongest baselines BERTurk<sub class=\"ltx_sub\">32k</sub> (93.8) and BERTurk<sub class=\"ltx_sub\">128k</sub> (95.1).\nA closer look at the per-phenomenon results shows that SindBERT<sub class=\"ltx_sub\">base</sub> is particularly strong on <span class=\"ltx_text ltx_font_italic\">scrambling</span>,\n<span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">subject agreement</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span> (all <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>98), which are central morphosyntactic phenomena of Turkish.\nAt the same time, it struggles with <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (59.0) and <span class=\"ltx_text ltx_font_italic\">island effects</span> (64.0), two categories that remain challenging across most models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "electramc4",
                    "base",
                    "large",
                    "turkish",
                    "electrabase",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the large models, SindBERT<sub class=\"ltx_sub\">large</sub> reaches an average of 89.8, placing it slightly below EuroBERT<sub class=\"ltx_sub\">610M</sub> (90.0) and XLM-R<sub class=\"ltx_sub\">large</sub> (92.7).\nIts strengths mirror the base variant: ceiling-level performance in morphologically rich categories such as <span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">scrambling</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span>.\nHowever, SindBERT<sub class=\"ltx_sub\">large</sub> shows a severe weakness in <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (27.8), which strongly lowers its overall average.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "large",
                    "sindbertlarge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that monolingual models like SindBERT capture Turkish-specific morphosyntax particularly well, while multilingual models such as XLM-R generalize more effectively to harder syntactic phenomena (e.g., ellipsis and binding).\nThis suggests a trade-off between specialization in language-specific structures and broader generalization capacities learned from multilingual corpora.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation shows that SindBERT<sub class=\"ltx_sub\">base</sub> performs competitively with other widely used Turkish encoders, confirming the robustness of its RoBERTa-style pretraining setup.\nAt the same time, SindBERT<sub class=\"ltx_sub\">large</sub> achieves the best overall results in two of four downstream tasks, notably in part-of-speech tagging and offensive language detection, and also performs strongly on several linguistic control tests.\nWhile scaling does not produce uniform gains across all benchmarks, these task-specific improvements suggest that larger contextual capacity primarily benefits pragmatically and syntactically complex settings.\nSimilar saturation effects are visible for EuroBERT and XLM-R, indicating that many Turkish benchmarks may no longer be sufficiently discriminative to reveal consistent scaling trends.\nNonetheless, diagnostic evaluations such as <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> underscore SindBERT&#8217;s strengths in Turkish-specific grammatical phenomena (e.g., scrambling, suspended affixation, subject agreement), highlighting the model&#8217;s linguistic depth beyond aggregate scores.</p>\n\n",
                "matched_terms": [
                    "sindbertlarge",
                    "pretraining",
                    "sindbertbase",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A likely factor explaining the limited scaling gains lies in the training corpus composition.\nSindBERT was trained on 312 GB of text&#8212;dominated by mC4 (242 GB), which provides broad coverage but is considerably noisier than smaller, curated datasets.\nBy contrast, BERTurk, trained on only a fraction of that volume but sourced from cleaner collections (OSCAR, Wikipedia, OPUS, and non-public), achieves excellent results, particularly on linguistically sensitive evaluations.\nThis mirrors trends observed in other monolingual models such as GottBERT, CamemBERT, and GeistBERT, where performance gains stemmed not merely from data size but from an effective balance of quality, domain diversity, and linguistic representativeness.\nOur findings therefore reinforce that corpus curation, not scale alone, is decisive for progress in Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "opus",
                    "wikipedia",
                    "corpus",
                    "turkish",
                    "size",
                    "oscar",
                    "nonpublic",
                    "berturk",
                    "from",
                    "data",
                    "only",
                    "mc4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further dimension concerns vocabulary design.\nSindBERT employs a 52k BPE vocabulary that balances coverage and efficiency, whereas BERTurk also released a 128k-token variant, which ranks among the strongest performers in our benchmarks, especially on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.\nRecent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> corroborates that vocabulary size has a substantial impact on Turkish models due to the language&#8217;s agglutinative morphology.\nThey report that optimal vocabulary scales differ by tokenization strategy: for BPE or WordPiece, vocabularies around 20% of model parameters tend to be most effective, while morphological or word-level tokenizers may benefit from substantially larger ratios.\nOur results align with this observation: BERTurk<sub class=\"ltx_sub\">128k</sub> profits from an expanded vocabulary despite its smaller corpus, whereas SindBERT&#8217;s 52k vocabulary remains sufficiently expressive to achieve competitive results given its broader but noisier training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "corpus",
                    "turkish",
                    "size",
                    "berturk",
                    "work",
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "sindbertlarge",
                    "base",
                    "model",
                    "corpus",
                    "turkish",
                    "large",
                    "from",
                    "sindbertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work may extend SindBERT in several directions.\nFirst, while GeistBERT built on the GottBERT checkpoint through continued pre-training on in-domain data <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>, and ChristBERT explored the effects of continued pre-training versus training from scratch using both general and domain-specific vocabularies, a similar ablation study has not yet been conducted for Turkish.\nSindBERT provides a natural starting point for replicating these approaches, enabling systematic comparisons of domain adaptation strategies in Turkish.</p>\n\n",
                "matched_terms": [
                    "turkish",
                    "work",
                    "data",
                    "pretraining",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, our findings indicate that many existing benchmarks are already saturated, as they fail to reveal consistent improvements from larger models.\nTo overcome this limitation, future evaluations should adopt more comprehensive and discriminative test suites.\nIn particular, the recently released TrGLUE benchmark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE\" title=\"\">https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE</a></span></span></span> offers a promising step in this direction, providing a diverse collection of tasks. It includes natural language inference, paraphrase detection, sentiment analysis, and question answering, that more closely mirror the breadth of the original GLUE suite.\nIncorporating TrGLUE into future experiments would enable a more fine-grained assessment of SindBERT&#8217;s generalization capabilities across both syntactic and semantic dimensions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sindbertlarge",
                    "model",
                    "corpus",
                    "turkish",
                    "size",
                    "berturk",
                    "data",
                    "from",
                    "roberta",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has several limitations. First, SindBERT was trained on three large-scale Turkish corpora (mC4, OSCAR23, Wikipedia) with only light filtering applied, restricted to the removal of documents containing invalid character encodings. No additional cleaning, quality filtering, or cross-source deduplication was performed. As a result, residual noise, duplicated content, and potential biases are likely to remain in the training data and may influence the learned representations.</p>\n\n",
                "matched_terms": [
                    "wikipedia",
                    "oscar23",
                    "turkish",
                    "work",
                    "data",
                    "only",
                    "mc4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the training data was drawn exclusively from web-based sources, without explicit control for dialectal or register variation (e.g., Ottoman vs. Modern Turkish, formal vs. colloquial, or regional varieties). This may limit the model&#8217;s robustness on underrepresented varieties or in specialized domains such as biomedical or legal text, unless additional domain-adaptive pre-training is performed.</p>\n\n",
                "matched_terms": [
                    "data",
                    "pretraining",
                    "from",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, SindBERT was pre-trained with conservative hyperparameter settings and without extensive exploration of alternative masking strategies (e.g., Whole Word Masking) or longer training schedules. Pre-training was also conducted without mixed precision, which increased computational cost and limited the feasibility of scaling to larger model sizes or more training steps.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fifth, baseline reproducibility introduces some uncertainty. ConvBERTurk and ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> are based on the ELECTRA codebase, but during conversion from the original checkpoints to HuggingFace Transformers the distinction between generator and discriminator is not explicit. While ELECTRA&#8217;s conversion script allows specifying this choice, ConvBERTurk appears to default to the discriminator. This may not invalidate comparisons, but it does leave open the possibility of subtle architectural differences and explains the suboptimal performance on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.</p>\n\n",
                "matched_terms": [
                    "convberturkmc4",
                    "convberturk",
                    "from",
                    "electra"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like all large-scale language models, SindBERT may inherit biases from its training data, which can influence downstream tasks such as classification or decision-making. While no deduplication was applied, the corpus may still contain redundancy and noise, as well as deeper societal or representational biases. Furthermore, training on large web-based corpora raises privacy concerns, as models may inadvertently retain sensitive information. Responsible deployment is especially important in high-stakes domains like legal, medical, or financial NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "large",
                    "corpus",
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite optimizations for efficiency, pre-training and evaluating transformer models remain computationally demanding, contributing to energy use and carbon emissions. These environmental costs highlight the need for balancing model performance with sustainable development goals.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pretraining",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "turkish"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: \nHyperparameter configurations for downstream fine-tuning. Each modeltask combination was trained with all permutations, yielding 10 runs per model and task. Reported scores are averaged across seeds for the best configuration.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\"><span class=\"ltx_text ltx_font_bold\">Values</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Batch Size</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">16, 32</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">5e-6, 7e-6, 1e-5, 2e-5, 5e-5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Epochs</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:128.0pt;\">up to 30 \n<br class=\"ltx_break\"/>(Early stopping, patience = 3)</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "yielding",
            "configurations",
            "stopping",
            "size",
            "modeltask",
            "each",
            "seeds",
            "parameter",
            "patience",
            "combination",
            "downstream",
            "5e6",
            "5e5",
            "rate",
            "batch",
            "reported",
            "learning",
            "runs",
            "7e6",
            "1e5",
            "trained",
            "scores",
            "hyperparameter",
            "across",
            "finetuning",
            "averaged",
            "permutations",
            "early",
            "configuration",
            "values",
            "task",
            "model",
            "best",
            "all",
            "epochs",
            "2e5"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
            "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts.\nWith SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish.\nTrained from scratch on 312&#160;GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish.\nWe evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> linguistic acceptability benchmark.\nOur results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall.\nThis flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated.\nAt the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume.\nTaken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages.\nThe SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "model",
                    "best",
                    "trained",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The advent of transformer-based models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> has reshaped natural language processing (NLP), providing contextualized word representations that generalize across a wide range of tasks. While early efforts focused on English and multilingual approaches, research has consistently shown that monolingual pre-training on large, high-quality corpora yields superior results for the target language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "early",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish NLP, several transformer-based encoders have been introduced in recent years. Notable examples include BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>, trained on a 35 GB corpus of Turkish OSCAR, Wikipedia, and OPUS data; ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib14\" title=\"\">2021</a>)</cite> models trained on both OSCAR and mC4 (35&#8211;242 GB)<cite class=\"ltx_cite ltx_citemacro_cite\">Jiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite>. While these models provide important milestones, most are relatively small encoder models trained with earlier-generation methods or focus on architectures other than RoBERTa. The only RoBERTa models out there were not computed in its fullest extend, but rather with small batch size for relatively small period <cite class=\"ltx_cite ltx_citemacro_cite\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>); Tas (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite>. Futher, Turkish still lacks a large-scale, high-quality encoder-only model.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "trained",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish, the first widely adopted transformer encoder was BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib27\" title=\"\">2020</a>)</cite>, trained on a 35&#160;GB mixture of OSCAR, Wikipedia, OPUS, and additional resources. Variants included cased/uncased models and vocabularies of 32k or 128k tokens. Distilled versions (DistilBERTurk)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite> and subsequent models such as ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERTurk expanded the model zoo, with some trained on the Turkish portion of mC4 (up to 242&#160;GB) <cite class=\"ltx_cite ltx_citemacro_cite\">Schweter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>. These provided important baselines but generally followed smaller encoder configurations or explored alternative pre-training architectures rather than scaling RoBERTa.</p>\n\n",
                "matched_terms": [
                    "model",
                    "configurations",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line of work, RoBERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tas, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite> introduced a RoBERTa-style encoder specifically adapted for Turkish, showing that refined pre-training objectives and tokenizer design can yield competitive results.\nIn parallel, research has underscored the critical role of tokenization in morphologically rich languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> systematically analyzed the impact of vocabulary size and segmentation strategy, showing that larger vocabularies can notably improve performance in morphosyntactic evaluations.\nHowever, all these RoBERTa-based models were not extensively trained, typically using moderate batch sizes and relatively few update steps, resulting in comparatively shallow pretraining regimes.</p>\n\n",
                "matched_terms": [
                    "size",
                    "all",
                    "trained",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, these contributions highlight steady progress in Turkish NLP. However, despite the availability of increasingly large corpora and modern training infrastructure, Turkish has lacked a RoBERTa-based encoder model trained from scratch at scale. SindBERT addresses this gap by providing the first large-scale RoBERTa encoder dedicated to Turkish, trained on modern corpora and released openly to the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to RoBERTa, SindBERT relies on byte pair encoding (BPE) <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib23\" title=\"\">2019</a>)</cite> for subword segmentation, which directly operates on raw text without the need for pre-tokenization or auxiliary tools such as Moses <cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib17\" title=\"\">2007</a>)</cite>. Since the original GPT-2 tokenizer was designed for English, we instead constructed a tokenizer tailored for Turkish. Following the strategy applied in GottBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we trained a dedicated vocabulary using 40 GB of randomly sampled Turkish text, resulting in a 52k subword inventory optimized for the language. In our experience, sampling around 40 GB of text is already enough for the subword statistics to stabilize, while scaling vocabulary training to the entire corpus would primarily increase computational cost without offering substantial gains. While we did not separately evaluate the effect of this adaptation on storage size or downstream accuracy, previous work in Dutch <cite class=\"ltx_cite ltx_citemacro_cite\">Delobelle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>)</cite> and German <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> indicates that language-specific tokenizers can yield improvements in both efficiency and performance.</p>\n\n",
                "matched_terms": [
                    "size",
                    "downstream",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the setup of GottBERT, we pre-trained both SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub> using the fairseq framework on a 128-core TPUv4 pod <cite class=\"ltx_cite ltx_citemacro_cite\">Jouppi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite>. Mixed-precision training (fp16/bfloat16) was not employed, so both models were trained entirely in full precision (fp32). This ensures that training dynamics can be attributed directly to model size, without numerical precision optimizations acting as additional factors.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model",
                    "size",
                    "batch",
                    "learning",
                    "each",
                    "epochs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For NER, we fine-tuned on the Turkish NER dataset introduced in the WikiANN corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib22\" title=\"\">2017</a>)</cite> and widely used for multilingual evaluation. We used the splits from <cite class=\"ltx_cite ltx_citemacro_citet\">Rahimi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib24\" title=\"\">2019</a>)</cite> and report micro F1 across all entity types.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess fine-grained grammatical knowledge, we include evaluation on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#351;ar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib2\" title=\"\">2025</a>)</cite>, a benchmark of 16 core linguistic phenomena ranging from anaphor agreement and argument structure to scrambling and suspended affixation.\nEach phenomenon is represented by 1,000 minimal pairs, and models are scored following the BLiMP protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Warstadt et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib31\" title=\"\">2020</a>)</cite>, i.e., assigning higher probability to the grammatical sentence of each pair.\nFor each model we compute the accuracy within every phenomenon and report the average across all 16 categories as the overall <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> score.\nThis measure complements PoS tagging, NER, and sentiment classification by probing deeper syntactic and morphosyntactic competence.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "across",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the mid-scale, mBERT has 178M parameters with a WordPiece vocabulary of nearly 120k tokens, while the 128k-token BERTurk variants reach 184M.\nAmong larger models, XLM-R<sub class=\"ltx_sub\">base</sub> contains 278M parameters, while SindBERT<sub class=\"ltx_sub\">large</sub> grows to 357M.\nThe largest encoder considered is XLM-R<sub class=\"ltx_sub\">large</sub>, with 560M parameters and a 250k-token vocabulary.\nAll values were extracted using Hugging Face&#8217;s <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library.</p>\n\n",
                "matched_terms": [
                    "all",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pre-training, we monitored perplexity both on the training set (at each optimization step) and on the validation set (after each epoch; see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.F1\" title=\"Figure 1 &#8227; 4.1 Pre-training &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Across all configurations, the curves follow a consistent convergence pattern. An initial plateau phase can be observed, which is relatively brief for the base models but more pronounced for the large ones. Occasional short upward spikes appear in the training curves; if taken in isolation, these might be misread as divergence, yet they quickly subside as training progresses.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "across",
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The base models typically stabilize after 20k&#8211;30k steps, while the large models require slightly longer but consistently converge by around 40k steps. By the end of training, both configurations achieve comparably low perplexity, underscoring the efficiency of the pre-training setup. This trend is mirrored in the validation perplexity, which shows steady improvements after each epoch. Overall, training perplexity decreased from about 54.5k to 3.93 for the base models and from about 52.2k to 3.24 for the large models, reflecting robust and reliable convergence.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across base-scale models, performance on the Turkish Universal Dependencies treebank is consistently high, with micro-F1 values exceeding 93% for nearly all encoders.\nThe strongest overall results are achieved by ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (94.57), closely followed by SindBERT<sub class=\"ltx_sub\">base</sub> (94.47) and BERTurk<sub class=\"ltx_sub\">128k</sub> (94.44).\nInterestingly, both ConvBERTurk variants, trained with different corpora, maintain a narrow margin over ELECTRA-based and RoBERTa-style encoders, suggesting that architectural innovations like dynamic convolution offer slight but consistent gains in token-level syntactic tagging.\nThe relatively low score of RoBERTurk (87.99) indicates the limitations of early RoBERTa replications for Turkish, likely due to smaller corpora and shorter training schedules.\nSindBERT<sub class=\"ltx_sub\">base</sub> performs competitively within this saturated range, demonstrating strong generalization across tasks despite a larger 52k BPE vocabulary.</p>\n\n",
                "matched_terms": [
                    "across",
                    "early",
                    "all",
                    "trained",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, POS tagging performance appears saturated across both scales, with nearly all base models exceeding 94 F1 and only marginal gains from scaling. SindBERT maintains parity with top-tier baselines, confirming that syntactic coverage in Turkish is largely solved for transformer-based encoders.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best base-scale performance is reached by BERTurk<sub class=\"ltx_sub\">32k</sub> (94.38), confirming its robustness for token-level classification.\nClose behind are ConvBERTurk (94.03) and BERTurk<sub class=\"ltx_sub\">128k</sub> (93.81), while SindBERT<sub class=\"ltx_sub\">base</sub> achieves a solid 93.19, comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> (93.49) and XLM-R<sub class=\"ltx_sub\">base</sub> (92.9).\nThis indicates that SindBERT&#8217;s RoBERTa-like setup neither clearly surpasses nor lags behind the most established Turkish encoders, suggesting that the NER task may already be approaching an upper limit with current dataset size and annotation quality.</p>\n\n",
                "matched_terms": [
                    "size",
                    "task",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "size",
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the large model group, SindBERT<sub class=\"ltx_sub\">large</sub> again performs best (82.29), surpassing XLM-R<sub class=\"ltx_sub\">large</sub> (81.99) and far exceeding EuroBERT<sub class=\"ltx_sub\">610M</sub> (75.57).\nThis consistent lead across two of four downstream tasks emphasizes SindBERT&#8217;s balanced architecture and effective use of Turkish-specific corpora.</p>\n\n",
                "matched_terms": [
                    "best",
                    "across",
                    "downstream",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.SS2.SSS0.Px4\" title=\"TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> reports the detailed <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> results for all base and large models.\nOverall, SindBERT<sub class=\"ltx_sub\">base</sub> achieves an average score of 90.3, which is comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> and ELECTRA<sub class=\"ltx_sub\">mC4</sub> (both 89.9), while trailing behind the strongest baselines BERTurk<sub class=\"ltx_sub\">32k</sub> (93.8) and BERTurk<sub class=\"ltx_sub\">128k</sub> (95.1).\nA closer look at the per-phenomenon results shows that SindBERT<sub class=\"ltx_sub\">base</sub> is particularly strong on <span class=\"ltx_text ltx_font_italic\">scrambling</span>,\n<span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">subject agreement</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span> (all <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>98), which are central morphosyntactic phenomena of Turkish.\nAt the same time, it struggles with <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (59.0) and <span class=\"ltx_text ltx_font_italic\">island effects</span> (64.0), two categories that remain challenging across most models.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation shows that SindBERT<sub class=\"ltx_sub\">base</sub> performs competitively with other widely used Turkish encoders, confirming the robustness of its RoBERTa-style pretraining setup.\nAt the same time, SindBERT<sub class=\"ltx_sub\">large</sub> achieves the best overall results in two of four downstream tasks, notably in part-of-speech tagging and offensive language detection, and also performs strongly on several linguistic control tests.\nWhile scaling does not produce uniform gains across all benchmarks, these task-specific improvements suggest that larger contextual capacity primarily benefits pragmatically and syntactically complex settings.\nSimilar saturation effects are visible for EuroBERT and XLM-R, indicating that many Turkish benchmarks may no longer be sufficiently discriminative to reveal consistent scaling trends.\nNonetheless, diagnostic evaluations such as <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> underscore SindBERT&#8217;s strengths in Turkish-specific grammatical phenomena (e.g., scrambling, suspended affixation, subject agreement), highlighting the model&#8217;s linguistic depth beyond aggregate scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "downstream",
                    "best",
                    "all",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A likely factor explaining the limited scaling gains lies in the training corpus composition.\nSindBERT was trained on 312 GB of text&#8212;dominated by mC4 (242 GB), which provides broad coverage but is considerably noisier than smaller, curated datasets.\nBy contrast, BERTurk, trained on only a fraction of that volume but sourced from cleaner collections (OSCAR, Wikipedia, OPUS, and non-public), achieves excellent results, particularly on linguistically sensitive evaluations.\nThis mirrors trends observed in other monolingual models such as GottBERT, CamemBERT, and GeistBERT, where performance gains stemmed not merely from data size but from an effective balance of quality, domain diversity, and linguistic representativeness.\nOur findings therefore reinforce that corpus curation, not scale alone, is decisive for progress in Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "size",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further dimension concerns vocabulary design.\nSindBERT employs a 52k BPE vocabulary that balances coverage and efficiency, whereas BERTurk also released a 128k-token variant, which ranks among the strongest performers in our benchmarks, especially on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.\nRecent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> corroborates that vocabulary size has a substantial impact on Turkish models due to the language&#8217;s agglutinative morphology.\nThey report that optimal vocabulary scales differ by tokenization strategy: for BPE or WordPiece, vocabularies around 20% of model parameters tend to be most effective, while morphological or word-level tokenizers may benefit from substantially larger ratios.\nOur results align with this observation: BERTurk<sub class=\"ltx_sub\">128k</sub> profits from an expanded vocabulary despite its smaller corpus, whereas SindBERT&#8217;s 52k vocabulary remains sufficiently expressive to achieve competitive results given its broader but noisier training data.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "across",
                    "task",
                    "model",
                    "configuration",
                    "parameter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "size",
                    "best",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, SindBERT was pre-trained with conservative hyperparameter settings and without extensive exploration of alternative masking strategies (e.g., Whole Word Masking) or longer training schedules. Pre-training was also conducted without mixed precision, which increased computational cost and limited the feasibility of scaling to larger model sizes or more training steps.</p>\n\n",
                "matched_terms": [
                    "hyperparameter",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, we did not perform a systematic error analysis of downstream results. Such an analysis could provide insights into systematic weaknesses (e.g., frequent PoS confusions, NER boundary errors, sentiment misclassifications, or <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> minimal pair failures) and help prioritize future improvements in model design and dataset composition.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like all large-scale language models, SindBERT may inherit biases from its training data, which can influence downstream tasks such as classification or decision-making. While no deduplication was applied, the corpus may still contain redundancy and noise, as well as deeper societal or representational biases. Furthermore, training on large web-based corpora raises privacy concerns, as models may inadvertently retain sensitive information. Responsible deployment is especially important in high-stakes domains like legal, medical, or financial NLP.</p>\n\n",
                "matched_terms": [
                    "all",
                    "downstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "task",
                    "model",
                    "best",
                    "all",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> is not reported, as the pipeline did not record training time.\nSince no hyperparameter search was involved, this omission is minor and corresponds to only a few additional hours.</p>\n\n",
                "matched_terms": [
                    "hyperparameter",
                    "reported"
                ]
            }
        ]
    },
    "S3.SS6.tab1": {
        "caption": "Table 3: Vocabulary size and total parameter count for Turkish transformer-based models.\nValues were extracted using Hugging Faces transformers library.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Vocab Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">#Params</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line =</th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "library",
            "line",
            "size",
            "extracted",
            "parameter",
            "transformers",
            "vocab",
            "csvreaderlate",
            "params",
            "after",
            "count",
            "csvcolii",
            "turkish",
            "faces",
            "vocabulary",
            "hugging",
            "values",
            "csvcoliii",
            "total",
            "model",
            "transformerbased"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts.\nWith SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish.\nTrained from scratch on 312&#160;GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish.\nWe evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> linguistic acceptability benchmark.\nOur results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall.\nThis flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated.\nAt the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume.\nTaken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages.\nThe SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The advent of transformer-based models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> has reshaped natural language processing (NLP), providing contextualized word representations that generalize across a wide range of tasks. While early efforts focused on English and multilingual approaches, research has consistently shown that monolingual pre-training on large, high-quality corpora yields superior results for the target language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "transformerbased",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish NLP, several transformer-based encoders have been introduced in recent years. Notable examples include BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>, trained on a 35 GB corpus of Turkish OSCAR, Wikipedia, and OPUS data; ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib14\" title=\"\">2021</a>)</cite> models trained on both OSCAR and mC4 (35&#8211;242 GB)<cite class=\"ltx_cite ltx_citemacro_cite\">Jiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite>. While these models provide important milestones, most are relatively small encoder models trained with earlier-generation methods or focus on architectures other than RoBERTa. The only RoBERTa models out there were not computed in its fullest extend, but rather with small batch size for relatively small period <cite class=\"ltx_cite ltx_citemacro_cite\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>); Tas (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite>. Futher, Turkish still lacks a large-scale, high-quality encoder-only model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "turkish",
                    "size",
                    "transformerbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce SindBERT, a RoBERTa-based encoder model pre-trained specifically for Turkish. SindBERT builds on the design principles of the German model GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> and adapts them to the morphological richness and agglutinative structure of Turkish. We construct a byte-level BPE vocabulary optimized for Turkish, train both base and large variants with fairseq&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ott et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib21\" title=\"\">2019</a>)</cite>, and leverage TPUv4 hardware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jouppi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite> for efficient large-scale pre-training. SindBERT is designed to combine scalability and reproducibility while directly targeting Turkish, resulting in the first large-scale RoBERTa-style encoder model for Turkish.\nOur contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "vocabulary",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark SindBERT against existing Turkish and multilingual models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of transformer-based language models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> marked a paradigm shift in NLP, enabling significant improvements across a wide range of tasks. Building on these foundations, multilingual extensions such as mBERT and in particular XLM-RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib5\" title=\"\">2020</a>)</cite> became widely used as strong general-purpose baselines across more than 100 languages. At the same time, a wave of monolingual adaptations demonstrated that language-specific pre-training often outperforms multilingual alternatives when sufficient high-quality data is available&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib9\" title=\"\">2020b</a>; Martin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib20\" title=\"\">2020</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib6\" title=\"\">2020</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "transformerbased",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish, the first widely adopted transformer encoder was BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib27\" title=\"\">2020</a>)</cite>, trained on a 35&#160;GB mixture of OSCAR, Wikipedia, OPUS, and additional resources. Variants included cased/uncased models and vocabularies of 32k or 128k tokens. Distilled versions (DistilBERTurk)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite> and subsequent models such as ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERTurk expanded the model zoo, with some trained on the Turkish portion of mC4 (up to 242&#160;GB) <cite class=\"ltx_cite ltx_citemacro_cite\">Schweter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>. These provided important baselines but generally followed smaller encoder configurations or explored alternative pre-training architectures rather than scaling RoBERTa.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line of work, RoBERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tas, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite> introduced a RoBERTa-style encoder specifically adapted for Turkish, showing that refined pre-training objectives and tokenizer design can yield competitive results.\nIn parallel, research has underscored the critical role of tokenization in morphologically rich languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> systematically analyzed the impact of vocabulary size and segmentation strategy, showing that larger vocabularies can notably improve performance in morphosyntactic evaluations.\nHowever, all these RoBERTa-based models were not extensively trained, typically using moderate batch sizes and relatively few update steps, resulting in comparatively shallow pretraining regimes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "line",
                    "turkish",
                    "size",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, these contributions highlight steady progress in Turkish NLP. However, despite the availability of increasingly large corpora and modern training infrastructure, Turkish has lacked a RoBERTa-based encoder model trained from scratch at scale. SindBERT addresses this gap by providing the first large-scale RoBERTa encoder dedicated to Turkish, trained on modern corpora and released openly to the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An overview of existing Turkish transformer-based language models is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "transformerbased",
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT was trained on three Turkish corpora: Wikipedia, OSCAR23 <cite class=\"ltx_cite ltx_citemacro_cite\">Jansen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib13\" title=\"\">2022</a>)</cite>, and mC4.\nThe corpus was shuffled and lightly filtered, restricted to the removal of documents containing invalid character encodings.\nThe extracted sizes are approximately 242&#160;GB for mC4, 69&#160;GB for OSCAR, and 0.6&#160;GB for Wikipedia, resulting in a combined pre-training corpus of about 312&#160;GB of Turkish text.</p>\n\n",
                "matched_terms": [
                    "extracted",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to RoBERTa, SindBERT relies on byte pair encoding (BPE) <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib23\" title=\"\">2019</a>)</cite> for subword segmentation, which directly operates on raw text without the need for pre-tokenization or auxiliary tools such as Moses <cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib17\" title=\"\">2007</a>)</cite>. Since the original GPT-2 tokenizer was designed for English, we instead constructed a tokenizer tailored for Turkish. Following the strategy applied in GottBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we trained a dedicated vocabulary using 40 GB of randomly sampled Turkish text, resulting in a 52k subword inventory optimized for the language. In our experience, sampling around 40 GB of text is already enough for the subword statistics to stabilize, while scaling vocabulary training to the entire corpus would primarily increase computational cost without offering substantial gains. While we did not separately evaluate the effect of this adaptation on storage size or downstream accuracy, previous work in Dutch <cite class=\"ltx_cite ltx_citemacro_cite\">Delobelle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>)</cite> and German <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> indicates that language-specific tokenizers can yield improvements in both efficiency and performance.</p>\n\n",
                "matched_terms": [
                    "size",
                    "vocabulary",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the setup of GottBERT, we pre-trained both SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub> using the fairseq framework on a 128-core TPUv4 pod <cite class=\"ltx_cite ltx_citemacro_cite\">Jouppi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite>. Mixed-precision training (fp16/bfloat16) was not employed, so both models were trained entirely in full precision (fp32). This ensures that training dynamics can be attributed directly to model size, without numerical precision optimizations acting as additional factors.</p>\n\n",
                "matched_terms": [
                    "size",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
                "matched_terms": [
                    "size",
                    "models",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess fine-grained grammatical knowledge, we include evaluation on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#351;ar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib2\" title=\"\">2025</a>)</cite>, a benchmark of 16 core linguistic phenomena ranging from anaphor agreement and argument structure to scrambling and suspended affixation.\nEach phenomenon is represented by 1,000 minimal pairs, and models are scored following the BLiMP protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Warstadt et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib31\" title=\"\">2020</a>)</cite>, i.e., assigning higher probability to the grammatical sentence of each pair.\nFor each model we compute the accuracy within every phenomenon and report the average across all 16 categories as the overall <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> score.\nThis measure complements PoS tagging, NER, and sentiment classification by probing deeper syntactic and morphosyntactic competence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "total",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.SS6\" title=\"3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">3.6</span></a> summarizes the vocabulary sizes and parameter counts of the Turkish and multilingual models included in our evaluation.\nThe smallest encoder is ELECTRA<sub class=\"ltx_sub\">small</sub> (13.7M parameters), followed by DistilBERTurk (67M).\nBase-scale Turkish encoders, such as ConvBERTurk (cased and mC4 variants), ELECTRA<sub class=\"ltx_sub\">base</sub> (cased and mC4), and BERTurk (cased/uncased), cluster between 106M and 111M parameters with 32k vocabularies.\nRoBERTurk, another RoBERTa-style encoder with a 50k vocabulary, is slightly larger at 125M parameters.\nSindBERT<sub class=\"ltx_sub\">base</sub> grows further to 126M owing to its 52k vocabulary and extended RoBERTa design.</p>\n\n",
                "matched_terms": [
                    "models",
                    "parameter",
                    "vocabulary",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the mid-scale, mBERT has 178M parameters with a WordPiece vocabulary of nearly 120k tokens, while the 128k-token BERTurk variants reach 184M.\nAmong larger models, XLM-R<sub class=\"ltx_sub\">base</sub> contains 278M parameters, while SindBERT<sub class=\"ltx_sub\">large</sub> grows to 357M.\nThe largest encoder considered is XLM-R<sub class=\"ltx_sub\">large</sub>, with 560M parameters and a 250k-token vocabulary.\nAll values were extracted using Hugging Face&#8217;s <span class=\"ltx_text ltx_font_typewriter\">transformers</span> library.</p>\n\n",
                "matched_terms": [
                    "models",
                    "transformers",
                    "library",
                    "values",
                    "extracted",
                    "faces",
                    "vocabulary",
                    "hugging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pre-training, we monitored perplexity both on the training set (at each optimization step) and on the validation set (after each epoch; see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.F1\" title=\"Figure 1 &#8227; 4.1 Pre-training &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Across all configurations, the curves follow a consistent convergence pattern. An initial plateau phase can be observed, which is relatively brief for the base models but more pronounced for the large ones. Occasional short upward spikes appear in the training curves; if taken in isolation, these might be misread as divergence, yet they quickly subside as training progresses.</p>\n\n",
                "matched_terms": [
                    "models",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The base models typically stabilize after 20k&#8211;30k steps, while the large models require slightly longer but consistently converge by around 40k steps. By the end of training, both configurations achieve comparably low perplexity, underscoring the efficiency of the pre-training setup. This trend is mirrored in the validation perplexity, which shows steady improvements after each epoch. Overall, training perplexity decreased from about 54.5k to 3.93 for the base models and from about 52.2k to 3.24 for the large models, reflecting robust and reliable convergence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across base-scale models, performance on the Turkish Universal Dependencies treebank is consistently high, with micro-F1 values exceeding 93% for nearly all encoders.\nThe strongest overall results are achieved by ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (94.57), closely followed by SindBERT<sub class=\"ltx_sub\">base</sub> (94.47) and BERTurk<sub class=\"ltx_sub\">128k</sub> (94.44).\nInterestingly, both ConvBERTurk variants, trained with different corpora, maintain a narrow margin over ELECTRA-based and RoBERTa-style encoders, suggesting that architectural innovations like dynamic convolution offer slight but consistent gains in token-level syntactic tagging.\nThe relatively low score of RoBERTurk (87.99) indicates the limitations of early RoBERTa replications for Turkish, likely due to smaller corpora and shorter training schedules.\nSindBERT<sub class=\"ltx_sub\">base</sub> performs competitively within this saturated range, demonstrating strong generalization across tasks despite a larger 52k BPE vocabulary.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vocabulary",
                    "values",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among large-scale encoders, SindBERT<sub class=\"ltx_sub\">large</sub> attains the highest F1 (94.63), marginally outperforming XLM-R<sub class=\"ltx_sub\">large</sub> (94.39).\nThis indicates that SindBERT&#8217;s pre-training on modern Turkish data contributes positively to syntactic coverage, even when compared to substantially larger multilingual models.\nThe weaker performance of EuroBERT<sub class=\"ltx_sub\">610M</sub> (93.33) may reflect its more domain-diverse, less Turkish-focused corpus composition.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, POS tagging performance appears saturated across both scales, with nearly all base models exceeding 94 F1 and only marginal gains from scaling. SindBERT maintains parity with top-tier baselines, confirming that syntactic coverage in Turkish is largely solved for transformer-based encoders.</p>\n\n",
                "matched_terms": [
                    "transformerbased",
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best base-scale performance is reached by BERTurk<sub class=\"ltx_sub\">32k</sub> (94.38), confirming its robustness for token-level classification.\nClose behind are ConvBERTurk (94.03) and BERTurk<sub class=\"ltx_sub\">128k</sub> (93.81), while SindBERT<sub class=\"ltx_sub\">base</sub> achieves a solid 93.19, comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> (93.49) and XLM-R<sub class=\"ltx_sub\">base</sub> (92.9).\nThis indicates that SindBERT&#8217;s RoBERTa-like setup neither clearly surpasses nor lags behind the most established Turkish encoders, suggesting that the NER task may already be approaching an upper limit with current dataset size and annotation quality.</p>\n\n",
                "matched_terms": [
                    "size",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "size",
                    "models",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For offensive language classification (OffensEval-TR 2020), we observe more pronounced differences between architectures.\nConvBERTurk reaches the highest macro-F1 among base models (81.99), with ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (81.90) and BERTurk<sub class=\"ltx_sub\">128k</sub> (81.77) performing almost identically.\nELECTRA variants and SindBERT<sub class=\"ltx_sub\">base</sub> (81.14) cluster slightly below, while distilled and multilingual models trail more clearly.\nThese results highlight that models trained on monolingual Turkish corpora still offer clear advantages for pragmatic and domain-sensitive tasks.\nSindBERT<sub class=\"ltx_sub\">base</sub> thus performs solidly but not at the very top, suggesting that further pre-training on informal or social-media text could enhance its stylistic robustness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.SS2.SSS0.Px4\" title=\"TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> reports the detailed <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> results for all base and large models.\nOverall, SindBERT<sub class=\"ltx_sub\">base</sub> achieves an average score of 90.3, which is comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> and ELECTRA<sub class=\"ltx_sub\">mC4</sub> (both 89.9), while trailing behind the strongest baselines BERTurk<sub class=\"ltx_sub\">32k</sub> (93.8) and BERTurk<sub class=\"ltx_sub\">128k</sub> (95.1).\nA closer look at the per-phenomenon results shows that SindBERT<sub class=\"ltx_sub\">base</sub> is particularly strong on <span class=\"ltx_text ltx_font_italic\">scrambling</span>,\n<span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">subject agreement</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span> (all <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>98), which are central morphosyntactic phenomena of Turkish.\nAt the same time, it struggles with <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (59.0) and <span class=\"ltx_text ltx_font_italic\">island effects</span> (64.0), two categories that remain challenging across most models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A likely factor explaining the limited scaling gains lies in the training corpus composition.\nSindBERT was trained on 312 GB of text&#8212;dominated by mC4 (242 GB), which provides broad coverage but is considerably noisier than smaller, curated datasets.\nBy contrast, BERTurk, trained on only a fraction of that volume but sourced from cleaner collections (OSCAR, Wikipedia, OPUS, and non-public), achieves excellent results, particularly on linguistically sensitive evaluations.\nThis mirrors trends observed in other monolingual models such as GottBERT, CamemBERT, and GeistBERT, where performance gains stemmed not merely from data size but from an effective balance of quality, domain diversity, and linguistic representativeness.\nOur findings therefore reinforce that corpus curation, not scale alone, is decisive for progress in Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "size",
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further dimension concerns vocabulary design.\nSindBERT employs a 52k BPE vocabulary that balances coverage and efficiency, whereas BERTurk also released a 128k-token variant, which ranks among the strongest performers in our benchmarks, especially on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.\nRecent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> corroborates that vocabulary size has a substantial impact on Turkish models due to the language&#8217;s agglutinative morphology.\nThey report that optimal vocabulary scales differ by tokenization strategy: for BPE or WordPiece, vocabularies around 20% of model parameters tend to be most effective, while morphological or word-level tokenizers may benefit from substantially larger ratios.\nOur results align with this observation: BERTurk<sub class=\"ltx_sub\">128k</sub> profits from an expanded vocabulary despite its smaller corpus, whereas SindBERT&#8217;s 52k vocabulary remains sufficiently expressive to achieve competitive results given its broader but noisier training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "turkish",
                    "size",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "parameter",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "size",
                    "models",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite optimizations for efficiency, pre-training and evaluating transformer models remain computationally demanding, contributing to energy use and carbon emissions. These environmental costs highlight the need for balancing model performance with sustainable development goals.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "total",
                    "turkish"
                ]
            }
        ]
    },
    "S4.SS2.SSS0.Px3.tab1": {
        "caption": "Table 4: \nEvaluation results across four Turkish downstream tasks.\nBest results are shown in bold and second-best results are underlined, with rankings reported separately for base and large model groups. For the 13 base models, third-best results are additionally marked with a dotted underline.\nPoS: micro-F1 on concatenated UD datasets.\nNER: entity-level F1 on WikiANN Turkish.\nSentiment: macro-F1 on OffensEval-TR 2020.\nTurBLiMP: average accuracy over 16 linguistic acceptability phenomena.\nReported scores for PoS, NER and classification are computed on the test set, with the best checkpoint per modeltask combination selected based on validation performance. TurBLiMP was evaluated using its predefined configuration.",
        "body": "<table class=\"ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PoS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WikiANN</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">OffensEval-TR 2020</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">TurBLiMP<span class=\"ltx_text ltx_font_upright\"> AVG</span></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line=</th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_ERROR undefined\">\\csvcolvi</span>\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line=</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcoliv</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcolvi</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_ERROR undefined\">\\csvcoliv</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "four",
            "validation",
            "evaluated",
            "marked",
            "pos",
            "phenomena",
            "line",
            "concatenated",
            "turblimp",
            "evaluation",
            "modeltask",
            "separately",
            "underline",
            "selected",
            "microf1",
            "combination",
            "downstream",
            "its",
            "based",
            "base",
            "linguistic",
            "checkpoint",
            "average",
            "thirdbest",
            "csvreaderlate",
            "after",
            "reported",
            "computed",
            "tasks",
            "rankings",
            "test",
            "wikiann",
            "accuracy",
            "sentiment",
            "scores",
            "performance",
            "over",
            "across",
            "avg",
            "csvcoliv",
            "acceptability",
            "entitylevel",
            "bold",
            "csvcolii",
            "turkish",
            "dotted",
            "classification",
            "results",
            "underlined",
            "configuration",
            "groups",
            "datasets",
            "set",
            "csvcoliii",
            "secondbest",
            "additionally",
            "model",
            "ner",
            "large",
            "offensevaltr",
            "best",
            "csvcolvi",
            "macrof1",
            "predefined"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts.\nWith SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish.\nTrained from scratch on 312&#160;GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish.\nWe evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> linguistic acceptability benchmark.\nOur results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall.\nThis flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated.\nAt the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume.\nTaken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages.\nThe SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "four",
                    "base",
                    "linguistic",
                    "acceptability",
                    "model",
                    "large",
                    "turblimp",
                    "turkish",
                    "best",
                    "tasks",
                    "results",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The advent of transformer-based models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> has reshaped natural language processing (NLP), providing contextualized word representations that generalize across a wide range of tasks. While early efforts focused on English and multilingual approaches, research has consistently shown that monolingual pre-training on large, high-quality corpora yields superior results for the target language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "large",
                    "tasks",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish NLP, several transformer-based encoders have been introduced in recent years. Notable examples include BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>, trained on a 35 GB corpus of Turkish OSCAR, Wikipedia, and OPUS data; ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib14\" title=\"\">2021</a>)</cite> models trained on both OSCAR and mC4 (35&#8211;242 GB)<cite class=\"ltx_cite ltx_citemacro_cite\">Jiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite>. While these models provide important milestones, most are relatively small encoder models trained with earlier-generation methods or focus on architectures other than RoBERTa. The only RoBERTa models out there were not computed in its fullest extend, but rather with small batch size for relatively small period <cite class=\"ltx_cite ltx_citemacro_cite\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>); Tas (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite>. Futher, Turkish still lacks a large-scale, high-quality encoder-only model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "its",
                    "model",
                    "turkish",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce SindBERT, a RoBERTa-based encoder model pre-trained specifically for Turkish. SindBERT builds on the design principles of the German model GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> and adapts them to the morphological richness and agglutinative structure of Turkish. We construct a byte-level BPE vocabulary optimized for Turkish, train both base and large variants with fairseq&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ott et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib21\" title=\"\">2019</a>)</cite>, and leverage TPUv4 hardware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jouppi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite> for efficient large-scale pre-training. SindBERT is designed to combine scalability and reproducibility while directly targeting Turkish, resulting in the first large-scale RoBERTa-style encoder model for Turkish.\nOur contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "large",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark SindBERT against existing Turkish and multilingual models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of transformer-based language models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> marked a paradigm shift in NLP, enabling significant improvements across a wide range of tasks. Building on these foundations, multilingual extensions such as mBERT and in particular XLM-RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib5\" title=\"\">2020</a>)</cite> became widely used as strong general-purpose baselines across more than 100 languages. At the same time, a wave of monolingual adaptations demonstrated that language-specific pre-training often outperforms multilingual alternatives when sufficient high-quality data is available&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib9\" title=\"\">2020b</a>; Martin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib20\" title=\"\">2020</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib6\" title=\"\">2020</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "tasks",
                    "marked"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, multilingual encoder-only models have seen a revival. EuroBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boizard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib3\" title=\"\">2025</a>)</cite> revisits the encoder paradigm with innovations from decoder-only models, introducing a family of multilingual encoders for European and global languages with native support for sequences up to 8,192 tokens. Similarly, mmBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib19\" title=\"\">2025</a>)</cite> scales encoder pretraining to 3T tokens across 1,800+ languages, introducing novel sampling schedules and showing strong performance on both high- and low-resource languages. These developments highlight that encoder-based architectures remain competitive even in an era dominated by large decoder models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "large",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish, the first widely adopted transformer encoder was BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib27\" title=\"\">2020</a>)</cite>, trained on a 35&#160;GB mixture of OSCAR, Wikipedia, OPUS, and additional resources. Variants included cased/uncased models and vocabularies of 32k or 128k tokens. Distilled versions (DistilBERTurk)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite> and subsequent models such as ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERTurk expanded the model zoo, with some trained on the Turkish portion of mC4 (up to 242&#160;GB) <cite class=\"ltx_cite ltx_citemacro_cite\">Schweter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>. These provided important baselines but generally followed smaller encoder configurations or explored alternative pre-training architectures rather than scaling RoBERTa.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line of work, RoBERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tas, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite> introduced a RoBERTa-style encoder specifically adapted for Turkish, showing that refined pre-training objectives and tokenizer design can yield competitive results.\nIn parallel, research has underscored the critical role of tokenization in morphologically rich languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> systematically analyzed the impact of vocabulary size and segmentation strategy, showing that larger vocabularies can notably improve performance in morphosyntactic evaluations.\nHowever, all these RoBERTa-based models were not extensively trained, typically using moderate batch sizes and relatively few update steps, resulting in comparatively shallow pretraining regimes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "line",
                    "turkish",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, these contributions highlight steady progress in Turkish NLP. However, despite the availability of increasingly large corpora and modern training infrastructure, Turkish has lacked a RoBERTa-based encoder model trained from scratch at scale. SindBERT addresses this gap by providing the first large-scale RoBERTa encoder dedicated to Turkish, trained on modern corpora and released openly to the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "large",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An overview of existing Turkish transformer-based language models is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to RoBERTa, SindBERT relies on byte pair encoding (BPE) <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib23\" title=\"\">2019</a>)</cite> for subword segmentation, which directly operates on raw text without the need for pre-tokenization or auxiliary tools such as Moses <cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib17\" title=\"\">2007</a>)</cite>. Since the original GPT-2 tokenizer was designed for English, we instead constructed a tokenizer tailored for Turkish. Following the strategy applied in GottBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we trained a dedicated vocabulary using 40 GB of randomly sampled Turkish text, resulting in a 52k subword inventory optimized for the language. In our experience, sampling around 40 GB of text is already enough for the subword statistics to stabilize, while scaling vocabulary training to the entire corpus would primarily increase computational cost without offering substantial gains. While we did not separately evaluate the effect of this adaptation on storage size or downstream accuracy, previous work in Dutch <cite class=\"ltx_cite ltx_citemacro_cite\">Delobelle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>)</cite> and German <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> indicates that language-specific tokenizers can yield improvements in both efficiency and performance.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "turkish",
                    "separately",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the setup of GottBERT, we pre-trained both SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub> using the fairseq framework on a 128-core TPUv4 pod <cite class=\"ltx_cite ltx_citemacro_cite\">Jouppi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite>. Mixed-precision training (fp16/bfloat16) was not employed, so both models were trained entirely in full precision (fp32). This ensures that training dynamics can be attributed directly to model size, without numerical precision optimizations acting as additional factors.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "four",
                    "evaluated",
                    "base",
                    "checkpoint",
                    "model",
                    "after",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
                "matched_terms": [
                    "models",
                    "over",
                    "downstream",
                    "evaluated",
                    "linguistic",
                    "acceptability",
                    "model",
                    "turkish",
                    "classification",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used the concatenation of five Turkish Universal Dependencies (UD) datasets: Atis, BOUN, FrameNet, IMST, and Tourism.\nThis diverse set reflects different domains such as spoken language, newswire, and tourism.\nProviding a measure of syntactic and morphological coverage, we report model&#8217;s performance using micro F1.</p>\n\n",
                "matched_terms": [
                    "set",
                    "datasets",
                    "turkish",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For NER, we fine-tuned on the Turkish NER dataset introduced in the WikiANN corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib22\" title=\"\">2017</a>)</cite> and widely used for multilingual evaluation. We used the splits from <cite class=\"ltx_cite ltx_citemacro_citet\">Rahimi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib24\" title=\"\">2019</a>)</cite> and report micro F1 across all entity types.</p>\n\n",
                "matched_terms": [
                    "across",
                    "ner",
                    "turkish",
                    "evaluation",
                    "wikiann"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness on user-generated content, we employed the OffensEval-TR 2020 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(&#199;&#246;ltekin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib4\" title=\"\">2020</a>)</cite>, a corpus of Turkish tweets annotated for the presence of offensive language. The dataset contains over 31k training and 3.5k test instances, labeled in a binary fashion as either <span class=\"ltx_text ltx_font_italic\">NOT</span> (not offensive) or <span class=\"ltx_text ltx_font_italic\">OFF</span> (offensive). Mentions and URLs were anonymized during preprocessing (e.g., replaced by @USER or URL), while the tweets otherwise preserve the linguistic and pragmatic properties of social media text.\nWe report performance using macro F1.</p>\n\n",
                "matched_terms": [
                    "over",
                    "linguistic",
                    "offensevaltr",
                    "turkish",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess fine-grained grammatical knowledge, we include evaluation on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#351;ar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib2\" title=\"\">2025</a>)</cite>, a benchmark of 16 core linguistic phenomena ranging from anaphor agreement and argument structure to scrambling and suspended affixation.\nEach phenomenon is represented by 1,000 minimal pairs, and models are scored following the BLiMP protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Warstadt et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib31\" title=\"\">2020</a>)</cite>, i.e., assigning higher probability to the grammatical sentence of each pair.\nFor each model we compute the accuracy within every phenomenon and report the average across all 16 categories as the overall <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> score.\nThis measure complements PoS tagging, NER, and sentiment classification by probing deeper syntactic and morphosyntactic competence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "pos",
                    "linguistic",
                    "phenomena",
                    "average",
                    "model",
                    "ner",
                    "turblimp",
                    "evaluation",
                    "classification",
                    "accuracy",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selected",
                    "models",
                    "across",
                    "downstream",
                    "based",
                    "pos",
                    "base",
                    "ner",
                    "model",
                    "large",
                    "classification",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.SS6\" title=\"3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">3.6</span></a> summarizes the vocabulary sizes and parameter counts of the Turkish and multilingual models included in our evaluation.\nThe smallest encoder is ELECTRA<sub class=\"ltx_sub\">small</sub> (13.7M parameters), followed by DistilBERTurk (67M).\nBase-scale Turkish encoders, such as ConvBERTurk (cased and mC4 variants), ELECTRA<sub class=\"ltx_sub\">base</sub> (cased and mC4), and BERTurk (cased/uncased), cluster between 106M and 111M parameters with 32k vocabularies.\nRoBERTurk, another RoBERTa-style encoder with a 50k vocabulary, is slightly larger at 125M parameters.\nSindBERT<sub class=\"ltx_sub\">base</sub> grows further to 126M owing to its 52k vocabulary and extended RoBERTa design.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation",
                    "its",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pre-training, we monitored perplexity both on the training set (at each optimization step) and on the validation set (after each epoch; see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.F1\" title=\"Figure 1 &#8227; 4.1 Pre-training &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Across all configurations, the curves follow a consistent convergence pattern. An initial plateau phase can be observed, which is relatively brief for the base models but more pronounced for the large ones. Occasional short upward spikes appear in the training curves; if taken in isolation, these might be misread as divergence, yet they quickly subside as training progresses.</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "across",
                    "validation",
                    "base",
                    "after",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The base models typically stabilize after 20k&#8211;30k steps, while the large models require slightly longer but consistently converge by around 40k steps. By the end of training, both configurations achieve comparably low perplexity, underscoring the efficiency of the pre-training setup. This trend is mirrored in the validation perplexity, which shows steady improvements after each epoch. Overall, training perplexity decreased from about 54.5k to 3.93 for the base models and from about 52.2k to 3.24 for the large models, reflecting robust and reliable convergence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "validation",
                    "base",
                    "large",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across base-scale models, performance on the Turkish Universal Dependencies treebank is consistently high, with micro-F1 values exceeding 93% for nearly all encoders.\nThe strongest overall results are achieved by ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (94.57), closely followed by SindBERT<sub class=\"ltx_sub\">base</sub> (94.47) and BERTurk<sub class=\"ltx_sub\">128k</sub> (94.44).\nInterestingly, both ConvBERTurk variants, trained with different corpora, maintain a narrow margin over ELECTRA-based and RoBERTa-style encoders, suggesting that architectural innovations like dynamic convolution offer slight but consistent gains in token-level syntactic tagging.\nThe relatively low score of RoBERTurk (87.99) indicates the limitations of early RoBERTa replications for Turkish, likely due to smaller corpora and shorter training schedules.\nSindBERT<sub class=\"ltx_sub\">base</sub> performs competitively within this saturated range, demonstrating strong generalization across tasks despite a larger 52k BPE vocabulary.</p>\n\n",
                "matched_terms": [
                    "over",
                    "models",
                    "microf1",
                    "across",
                    "turkish",
                    "tasks",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among large-scale encoders, SindBERT<sub class=\"ltx_sub\">large</sub> attains the highest F1 (94.63), marginally outperforming XLM-R<sub class=\"ltx_sub\">large</sub> (94.39).\nThis indicates that SindBERT&#8217;s pre-training on modern Turkish data contributes positively to syntactic coverage, even when compared to substantially larger multilingual models.\nThe weaker performance of EuroBERT<sub class=\"ltx_sub\">610M</sub> (93.33) may reflect its more domain-diverse, less Turkish-focused corpus composition.</p>\n\n",
                "matched_terms": [
                    "models",
                    "its",
                    "turkish",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, POS tagging performance appears saturated across both scales, with nearly all base models exceeding 94 F1 and only marginal gains from scaling. SindBERT maintains parity with top-tier baselines, confirming that syntactic coverage in Turkish is largely solved for transformer-based encoders.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "pos",
                    "base",
                    "turkish",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best base-scale performance is reached by BERTurk<sub class=\"ltx_sub\">32k</sub> (94.38), confirming its robustness for token-level classification.\nClose behind are ConvBERTurk (94.03) and BERTurk<sub class=\"ltx_sub\">128k</sub> (93.81), while SindBERT<sub class=\"ltx_sub\">base</sub> achieves a solid 93.19, comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> (93.49) and XLM-R<sub class=\"ltx_sub\">base</sub> (92.9).\nThis indicates that SindBERT&#8217;s RoBERTa-like setup neither clearly surpasses nor lags behind the most established Turkish encoders, suggesting that the NER task may already be approaching an upper limit with current dataset size and annotation quality.</p>\n\n",
                "matched_terms": [
                    "its",
                    "ner",
                    "turkish",
                    "best",
                    "classification",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the large scale, XLM-R<sub class=\"ltx_sub\">large</sub> slightly leads (94.44), followed closely by SindBERT<sub class=\"ltx_sub\">large</sub> (93.64).\nGiven that XLM-R was trained on over 2 TB of multilingual text, this narrow margin underscores the efficiency of SindBERT&#8217;s more compact, Turkish-focused pretraining corpus.</p>\n\n",
                "matched_terms": [
                    "over",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "its",
                    "base",
                    "ner",
                    "model",
                    "large",
                    "turkish",
                    "tasks",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For offensive language classification (OffensEval-TR 2020), we observe more pronounced differences between architectures.\nConvBERTurk reaches the highest macro-F1 among base models (81.99), with ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (81.90) and BERTurk<sub class=\"ltx_sub\">128k</sub> (81.77) performing almost identically.\nELECTRA variants and SindBERT<sub class=\"ltx_sub\">base</sub> (81.14) cluster slightly below, while distilled and multilingual models trail more clearly.\nThese results highlight that models trained on monolingual Turkish corpora still offer clear advantages for pragmatic and domain-sensitive tasks.\nSindBERT<sub class=\"ltx_sub\">base</sub> thus performs solidly but not at the very top, suggesting that further pre-training on informal or social-media text could enhance its stylistic robustness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "its",
                    "base",
                    "offensevaltr",
                    "turkish",
                    "classification",
                    "tasks",
                    "macrof1",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the large model group, SindBERT<sub class=\"ltx_sub\">large</sub> again performs best (82.29), surpassing XLM-R<sub class=\"ltx_sub\">large</sub> (81.99) and far exceeding EuroBERT<sub class=\"ltx_sub\">610M</sub> (75.57).\nThis consistent lead across two of four downstream tasks emphasizes SindBERT&#8217;s balanced architecture and effective use of Turkish-specific corpora.</p>\n\n",
                "matched_terms": [
                    "across",
                    "four",
                    "downstream",
                    "model",
                    "large",
                    "best",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.SS2.SSS0.Px4\" title=\"TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> reports the detailed <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> results for all base and large models.\nOverall, SindBERT<sub class=\"ltx_sub\">base</sub> achieves an average score of 90.3, which is comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> and ELECTRA<sub class=\"ltx_sub\">mC4</sub> (both 89.9), while trailing behind the strongest baselines BERTurk<sub class=\"ltx_sub\">32k</sub> (93.8) and BERTurk<sub class=\"ltx_sub\">128k</sub> (95.1).\nA closer look at the per-phenomenon results shows that SindBERT<sub class=\"ltx_sub\">base</sub> is particularly strong on <span class=\"ltx_text ltx_font_italic\">scrambling</span>,\n<span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">subject agreement</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span> (all <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>98), which are central morphosyntactic phenomena of Turkish.\nAt the same time, it struggles with <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (59.0) and <span class=\"ltx_text ltx_font_italic\">island effects</span> (64.0), two categories that remain challenging across most models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "base",
                    "phenomena",
                    "average",
                    "large",
                    "turblimp",
                    "turkish",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the large models, SindBERT<sub class=\"ltx_sub\">large</sub> reaches an average of 89.8, placing it slightly below EuroBERT<sub class=\"ltx_sub\">610M</sub> (90.0) and XLM-R<sub class=\"ltx_sub\">large</sub> (92.7).\nIts strengths mirror the base variant: ceiling-level performance in morphologically rich categories such as <span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">scrambling</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span>.\nHowever, SindBERT<sub class=\"ltx_sub\">large</sub> shows a severe weakness in <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (27.8), which strongly lowers its overall average.</p>\n\n",
                "matched_terms": [
                    "models",
                    "its",
                    "base",
                    "average",
                    "large",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that monolingual models like SindBERT capture Turkish-specific morphosyntax particularly well, while multilingual models such as XLM-R generalize more effectively to harder syntactic phenomena (e.g., ellipsis and binding).\nThis suggests a trade-off between specialization in language-specific structures and broader generalization capacities learned from multilingual corpora.</p>\n\n",
                "matched_terms": [
                    "models",
                    "phenomena"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation shows that SindBERT<sub class=\"ltx_sub\">base</sub> performs competitively with other widely used Turkish encoders, confirming the robustness of its RoBERTa-style pretraining setup.\nAt the same time, SindBERT<sub class=\"ltx_sub\">large</sub> achieves the best overall results in two of four downstream tasks, notably in part-of-speech tagging and offensive language detection, and also performs strongly on several linguistic control tests.\nWhile scaling does not produce uniform gains across all benchmarks, these task-specific improvements suggest that larger contextual capacity primarily benefits pragmatically and syntactically complex settings.\nSimilar saturation effects are visible for EuroBERT and XLM-R, indicating that many Turkish benchmarks may no longer be sufficiently discriminative to reveal consistent scaling trends.\nNonetheless, diagnostic evaluations such as <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> underscore SindBERT&#8217;s strengths in Turkish-specific grammatical phenomena (e.g., scrambling, suspended affixation, subject agreement), highlighting the model&#8217;s linguistic depth beyond aggregate scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "four",
                    "downstream",
                    "its",
                    "linguistic",
                    "phenomena",
                    "turblimp",
                    "turkish",
                    "best",
                    "evaluation",
                    "tasks",
                    "results",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A likely factor explaining the limited scaling gains lies in the training corpus composition.\nSindBERT was trained on 312 GB of text&#8212;dominated by mC4 (242 GB), which provides broad coverage but is considerably noisier than smaller, curated datasets.\nBy contrast, BERTurk, trained on only a fraction of that volume but sourced from cleaner collections (OSCAR, Wikipedia, OPUS, and non-public), achieves excellent results, particularly on linguistically sensitive evaluations.\nThis mirrors trends observed in other monolingual models such as GottBERT, CamemBERT, and GeistBERT, where performance gains stemmed not merely from data size but from an effective balance of quality, domain diversity, and linguistic representativeness.\nOur findings therefore reinforce that corpus curation, not scale alone, is decisive for progress in Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "linguistic",
                    "turkish",
                    "results",
                    "datasets",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further dimension concerns vocabulary design.\nSindBERT employs a 52k BPE vocabulary that balances coverage and efficiency, whereas BERTurk also released a 128k-token variant, which ranks among the strongest performers in our benchmarks, especially on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.\nRecent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> corroborates that vocabulary size has a substantial impact on Turkish models due to the language&#8217;s agglutinative morphology.\nThey report that optimal vocabulary scales differ by tokenization strategy: for BPE or WordPiece, vocabularies around 20% of model parameters tend to be most effective, while morphological or word-level tokenizers may benefit from substantially larger ratios.\nOur results align with this observation: BERTurk<sub class=\"ltx_sub\">128k</sub> profits from an expanded vocabulary despite its smaller corpus, whereas SindBERT&#8217;s 52k vocabulary remains sufficiently expressive to achieve competitive results given its broader but noisier training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "its",
                    "model",
                    "turkish",
                    "turblimp",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "across",
                    "its",
                    "base",
                    "model",
                    "large",
                    "turkish",
                    "tasks",
                    "results",
                    "accuracy",
                    "configuration",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work may extend SindBERT in several directions.\nFirst, while GeistBERT built on the GottBERT checkpoint through continued pre-training on in-domain data <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>, and ChristBERT explored the effects of continued pre-training versus training from scratch using both general and domain-specific vocabularies, a similar ablation study has not yet been conducted for Turkish.\nSindBERT provides a natural starting point for replicating these approaches, enabling systematic comparisons of domain adaptation strategies in Turkish.</p>\n\n",
                "matched_terms": [
                    "checkpoint",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, our findings indicate that many existing benchmarks are already saturated, as they fail to reveal consistent improvements from larger models.\nTo overcome this limitation, future evaluations should adopt more comprehensive and discriminative test suites.\nIn particular, the recently released TrGLUE benchmark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE\" title=\"\">https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE</a></span></span></span> offers a promising step in this direction, providing a diverse collection of tasks. It includes natural language inference, paraphrase detection, sentiment analysis, and question answering, that more closely mirror the breadth of the original GLUE suite.\nIncorporating TrGLUE into future experiments would enable a more fine-grained assessment of SindBERT&#8217;s generalization capabilities across both syntactic and semantic dimensions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "tasks",
                    "test",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, extending evaluation to specialized domains such as biomedical or legal language remains an important frontier for Turkish NLP, where SindBERT could serve as a foundation for targeted domain adaptation, just as GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> and GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite> did for ChristBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "over",
                    "models",
                    "four",
                    "across",
                    "model",
                    "turkish",
                    "best",
                    "evaluation",
                    "tasks",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, we did not perform a systematic error analysis of downstream results. Such an analysis could provide insights into systematic weaknesses (e.g., frequent PoS confusions, NER boundary errors, sentiment misclassifications, or <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> minimal pair failures) and help prioritize future improvements in model design and dataset composition.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "pos",
                    "ner",
                    "model",
                    "turblimp",
                    "results",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fifth, baseline reproducibility introduces some uncertainty. ConvBERTurk and ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> are based on the ELECTRA codebase, but during conversion from the original checkpoints to HuggingFace Transformers the distinction between generator and discriminator is not explicit. While ELECTRA&#8217;s conversion script allows specifying this choice, ConvBERTurk appears to default to the discriminator. This may not invalidate comparisons, but it does leave open the possibility of subtle architectural differences and explains the suboptimal performance on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.</p>\n\n",
                "matched_terms": [
                    "based",
                    "performance",
                    "turblimp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, our evaluation focused on four downstream tasks (PoS tagging, NER, sentiment classification, <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>). While these cover a diverse range of morphosyntactic, semantic, and syntactic phenomena, they do not capture the full scope of Turkish NLP challenges such as question answering, natural language inference, summarization, or long-context understanding. The generalization of SindBERT to these settings remains to be established.</p>\n\n",
                "matched_terms": [
                    "four",
                    "downstream",
                    "pos",
                    "phenomena",
                    "ner",
                    "turblimp",
                    "turkish",
                    "evaluation",
                    "classification",
                    "tasks",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like all large-scale language models, SindBERT may inherit biases from its training data, which can influence downstream tasks such as classification or decision-making. While no deduplication was applied, the corpus may still contain redundancy and noise, as well as deeper societal or representational biases. Furthermore, training on large web-based corpora raises privacy concerns, as models may inadvertently retain sensitive information. Responsible deployment is especially important in high-stakes domains like legal, medical, or financial NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "downstream",
                    "its",
                    "large",
                    "classification",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite optimizations for efficiency, pre-training and evaluating transformer models remain computationally demanding, contributing to energy use and carbon emissions. These environmental costs highlight the need for balancing model performance with sustainable development goals.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n",
                "matched_terms": [
                    "selected",
                    "models",
                    "validation",
                    "downstream",
                    "base",
                    "model",
                    "large",
                    "turkish",
                    "best",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> is not reported, as the pipeline did not record training time.\nSince no hyperparameter search was involved, this omission is minor and corresponds to only a few additional hours.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "turblimp"
                ]
            }
        ]
    },
    "S4.SS2.SSS0.Px4.tab1": {
        "caption": "Table 5: \nDetailed TurBLiMP evaluation across 16 linguistic acceptability phenomena.\nBest results are shown in bold and second-best results are underlined, with rankings reported separately for base and large model groups.\nFor the 13 base models, third-best results are additionally marked with a dotted underline.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ana. Agr.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Arg. Tr.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Arg. Ditr.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Bind.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Det.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ellip.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Irr.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Isl.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Nom.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">NPI</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Pass.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Quant.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">RelCl.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Scramb.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Subj. Agr.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Susp. Aff.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">AVG</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line=</th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\csvcolxviii</span>\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line=</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolviii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolx</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxiii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxiv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxvi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxvii</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxviii</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolviii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolx</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxiii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxiv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxvi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolxvii</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "agr",
            "bind",
            "marked",
            "csvcolvii",
            "phenomena",
            "npi",
            "line",
            "csvcolix",
            "turblimp",
            "evaluation",
            "csvcolxiii",
            "separately",
            "csvcolx",
            "susp",
            "relcl",
            "quant",
            "csvcolxii",
            "csvcolxvi",
            "underline",
            "csvcolxvii",
            "linguistic",
            "base",
            "csvreaderlate",
            "thirdbest",
            "ditr",
            "after",
            "reported",
            "rankings",
            "across",
            "isl",
            "pass",
            "avg",
            "csvcoliv",
            "csvcolxi",
            "acceptability",
            "csvcolii",
            "bold",
            "csvcolviii",
            "dotted",
            "subj",
            "ellip",
            "results",
            "underlined",
            "groups",
            "csvcolxiv",
            "csvcoliii",
            "secondbest",
            "additionally",
            "model",
            "ana",
            "large",
            "irr",
            "best",
            "det",
            "scramb",
            "arg",
            "aff",
            "csvcolxviii",
            "nom",
            "csvcolvi",
            "csvcolv",
            "detailed",
            "csvcolxv"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts.\nWith SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish.\nTrained from scratch on 312&#160;GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish.\nWe evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> linguistic acceptability benchmark.\nOur results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall.\nThis flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated.\nAt the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume.\nTaken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages.\nThe SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.</p>\n\n",
                "matched_terms": [
                    "models",
                    "linguistic",
                    "base",
                    "acceptability",
                    "model",
                    "large",
                    "turblimp",
                    "best",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The advent of transformer-based models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> has reshaped natural language processing (NLP), providing contextualized word representations that generalize across a wide range of tasks. While early efforts focused on English and multilingual approaches, research has consistently shown that monolingual pre-training on large, high-quality corpora yields superior results for the target language&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "large",
                    "models",
                    "across",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish NLP, several transformer-based encoders have been introduced in recent years. Notable examples include BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>, trained on a 35 GB corpus of Turkish OSCAR, Wikipedia, and OPUS data; ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib14\" title=\"\">2021</a>)</cite> models trained on both OSCAR and mC4 (35&#8211;242 GB)<cite class=\"ltx_cite ltx_citemacro_cite\">Jiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite>. While these models provide important milestones, most are relatively small encoder models trained with earlier-generation methods or focus on architectures other than RoBERTa. The only RoBERTa models out there were not computed in its fullest extend, but rather with small batch size for relatively small period <cite class=\"ltx_cite ltx_citemacro_cite\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>); Tas (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite>. Futher, Turkish still lacks a large-scale, high-quality encoder-only model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce SindBERT, a RoBERTa-based encoder model pre-trained specifically for Turkish. SindBERT builds on the design principles of the German model GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> and adapts them to the morphological richness and agglutinative structure of Turkish. We construct a byte-level BPE vocabulary optimized for Turkish, train both base and large variants with fairseq&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ott et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib21\" title=\"\">2019</a>)</cite>, and leverage TPUv4 hardware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jouppi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite> for efficient large-scale pre-training. SindBERT is designed to combine scalability and reproducibility while directly targeting Turkish, resulting in the first large-scale RoBERTa-style encoder model for Turkish.\nOur contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of transformer-based language models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> marked a paradigm shift in NLP, enabling significant improvements across a wide range of tasks. Building on these foundations, multilingual extensions such as mBERT and in particular XLM-RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib5\" title=\"\">2020</a>)</cite> became widely used as strong general-purpose baselines across more than 100 languages. At the same time, a wave of monolingual adaptations demonstrated that language-specific pre-training often outperforms multilingual alternatives when sufficient high-quality data is available&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib9\" title=\"\">2020b</a>; Martin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib20\" title=\"\">2020</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib6\" title=\"\">2020</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "marked"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, multilingual encoder-only models have seen a revival. EuroBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boizard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib3\" title=\"\">2025</a>)</cite> revisits the encoder paradigm with innovations from decoder-only models, introducing a family of multilingual encoders for European and global languages with native support for sequences up to 8,192 tokens. Similarly, mmBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Marone et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib19\" title=\"\">2025</a>)</cite> scales encoder pretraining to 3T tokens across 1,800+ languages, introducing novel sampling schedules and showing strong performance on both high- and low-resource languages. These developments highlight that encoder-based architectures remain competitive even in an era dominated by large decoder models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Turkish, the first widely adopted transformer encoder was BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib27\" title=\"\">2020</a>)</cite>, trained on a 35&#160;GB mixture of OSCAR, Wikipedia, OPUS, and additional resources. Variants included cased/uncased models and vocabularies of 32k or 128k tokens. Distilled versions (DistilBERTurk)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite> and subsequent models such as ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERTurk expanded the model zoo, with some trained on the Turkish portion of mC4 (up to 242&#160;GB) <cite class=\"ltx_cite ltx_citemacro_cite\">Schweter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>. These provided important baselines but generally followed smaller encoder configurations or explored alternative pre-training architectures rather than scaling RoBERTa.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line of work, RoBERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tas, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite> introduced a RoBERTa-style encoder specifically adapted for Turkish, showing that refined pre-training objectives and tokenizer design can yield competitive results.\nIn parallel, research has underscored the critical role of tokenization in morphologically rich languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> systematically analyzed the impact of vocabulary size and segmentation strategy, showing that larger vocabularies can notably improve performance in morphosyntactic evaluations.\nHowever, all these RoBERTa-based models were not extensively trained, typically using moderate batch sizes and relatively few update steps, resulting in comparatively shallow pretraining regimes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "line",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, these contributions highlight steady progress in Turkish NLP. However, despite the availability of increasingly large corpora and modern training infrastructure, Turkish has lacked a RoBERTa-based encoder model trained from scratch at scale. SindBERT addresses this gap by providing the first large-scale RoBERTa encoder dedicated to Turkish, trained on modern corpora and released openly to the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the setup of GottBERT, we pre-trained both SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub> using the fairseq framework on a 128-core TPUv4 pod <cite class=\"ltx_cite ltx_citemacro_cite\">Jouppi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite>. Mixed-precision training (fp16/bfloat16) was not employed, so both models were trained entirely in full precision (fp32). This ensures that training dynamics can be attributed directly to model size, without numerical precision optimizations acting as additional factors.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "base",
                    "large",
                    "model",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
                "matched_terms": [
                    "linguistic",
                    "models",
                    "acceptability",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For NER, we fine-tuned on the Turkish NER dataset introduced in the WikiANN corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib22\" title=\"\">2017</a>)</cite> and widely used for multilingual evaluation. We used the splits from <cite class=\"ltx_cite ltx_citemacro_citet\">Rahimi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib24\" title=\"\">2019</a>)</cite> and report micro F1 across all entity types.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess fine-grained grammatical knowledge, we include evaluation on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#351;ar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib2\" title=\"\">2025</a>)</cite>, a benchmark of 16 core linguistic phenomena ranging from anaphor agreement and argument structure to scrambling and suspended affixation.\nEach phenomenon is represented by 1,000 minimal pairs, and models are scored following the BLiMP protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Warstadt et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib31\" title=\"\">2020</a>)</cite>, i.e., assigning higher probability to the grammatical sentence of each pair.\nFor each model we compute the accuracy within every phenomenon and report the average across all 16 categories as the overall <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> score.\nThis measure complements PoS tagging, NER, and sentiment classification by probing deeper syntactic and morphosyntactic competence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "linguistic",
                    "phenomena",
                    "model",
                    "turblimp",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "base",
                    "model",
                    "large",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.SS6\" title=\"3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">3.6</span></a> summarizes the vocabulary sizes and parameter counts of the Turkish and multilingual models included in our evaluation.\nThe smallest encoder is ELECTRA<sub class=\"ltx_sub\">small</sub> (13.7M parameters), followed by DistilBERTurk (67M).\nBase-scale Turkish encoders, such as ConvBERTurk (cased and mC4 variants), ELECTRA<sub class=\"ltx_sub\">base</sub> (cased and mC4), and BERTurk (cased/uncased), cluster between 106M and 111M parameters with 32k vocabularies.\nRoBERTurk, another RoBERTa-style encoder with a 50k vocabulary, is slightly larger at 125M parameters.\nSindBERT<sub class=\"ltx_sub\">base</sub> grows further to 126M owing to its 52k vocabulary and extended RoBERTa design.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pre-training, we monitored perplexity both on the training set (at each optimization step) and on the validation set (after each epoch; see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.F1\" title=\"Figure 1 &#8227; 4.1 Pre-training &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Across all configurations, the curves follow a consistent convergence pattern. An initial plateau phase can be observed, which is relatively brief for the base models but more pronounced for the large ones. Occasional short upward spikes appear in the training curves; if taken in isolation, these might be misread as divergence, yet they quickly subside as training progresses.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "base",
                    "large",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The base models typically stabilize after 20k&#8211;30k steps, while the large models require slightly longer but consistently converge by around 40k steps. By the end of training, both configurations achieve comparably low perplexity, underscoring the efficiency of the pre-training setup. This trend is mirrored in the validation perplexity, which shows steady improvements after each epoch. Overall, training perplexity decreased from about 54.5k to 3.93 for the base models and from about 52.2k to 3.24 for the large models, reflecting robust and reliable convergence.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "large",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across base-scale models, performance on the Turkish Universal Dependencies treebank is consistently high, with micro-F1 values exceeding 93% for nearly all encoders.\nThe strongest overall results are achieved by ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (94.57), closely followed by SindBERT<sub class=\"ltx_sub\">base</sub> (94.47) and BERTurk<sub class=\"ltx_sub\">128k</sub> (94.44).\nInterestingly, both ConvBERTurk variants, trained with different corpora, maintain a narrow margin over ELECTRA-based and RoBERTa-style encoders, suggesting that architectural innovations like dynamic convolution offer slight but consistent gains in token-level syntactic tagging.\nThe relatively low score of RoBERTurk (87.99) indicates the limitations of early RoBERTa replications for Turkish, likely due to smaller corpora and shorter training schedules.\nSindBERT<sub class=\"ltx_sub\">base</sub> performs competitively within this saturated range, demonstrating strong generalization across tasks despite a larger 52k BPE vocabulary.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, POS tagging performance appears saturated across both scales, with nearly all base models exceeding 94 F1 and only marginal gains from scaling. SindBERT maintains parity with top-tier baselines, confirming that syntactic coverage in Turkish is largely solved for transformer-based encoders.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "base",
                    "model",
                    "large",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For offensive language classification (OffensEval-TR 2020), we observe more pronounced differences between architectures.\nConvBERTurk reaches the highest macro-F1 among base models (81.99), with ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (81.90) and BERTurk<sub class=\"ltx_sub\">128k</sub> (81.77) performing almost identically.\nELECTRA variants and SindBERT<sub class=\"ltx_sub\">base</sub> (81.14) cluster slightly below, while distilled and multilingual models trail more clearly.\nThese results highlight that models trained on monolingual Turkish corpora still offer clear advantages for pragmatic and domain-sensitive tasks.\nSindBERT<sub class=\"ltx_sub\">base</sub> thus performs solidly but not at the very top, suggesting that further pre-training on informal or social-media text could enhance its stylistic robustness.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the large model group, SindBERT<sub class=\"ltx_sub\">large</sub> again performs best (82.29), surpassing XLM-R<sub class=\"ltx_sub\">large</sub> (81.99) and far exceeding EuroBERT<sub class=\"ltx_sub\">610M</sub> (75.57).\nThis consistent lead across two of four downstream tasks emphasizes SindBERT&#8217;s balanced architecture and effective use of Turkish-specific corpora.</p>\n\n",
                "matched_terms": [
                    "best",
                    "across",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.SS2.SSS0.Px4\" title=\"TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> reports the detailed <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> results for all base and large models.\nOverall, SindBERT<sub class=\"ltx_sub\">base</sub> achieves an average score of 90.3, which is comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> and ELECTRA<sub class=\"ltx_sub\">mC4</sub> (both 89.9), while trailing behind the strongest baselines BERTurk<sub class=\"ltx_sub\">32k</sub> (93.8) and BERTurk<sub class=\"ltx_sub\">128k</sub> (95.1).\nA closer look at the per-phenomenon results shows that SindBERT<sub class=\"ltx_sub\">base</sub> is particularly strong on <span class=\"ltx_text ltx_font_italic\">scrambling</span>,\n<span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">subject agreement</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span> (all <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>98), which are central morphosyntactic phenomena of Turkish.\nAt the same time, it struggles with <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (59.0) and <span class=\"ltx_text ltx_font_italic\">island effects</span> (64.0), two categories that remain challenging across most models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "base",
                    "phenomena",
                    "large",
                    "turblimp",
                    "results",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the large models, SindBERT<sub class=\"ltx_sub\">large</sub> reaches an average of 89.8, placing it slightly below EuroBERT<sub class=\"ltx_sub\">610M</sub> (90.0) and XLM-R<sub class=\"ltx_sub\">large</sub> (92.7).\nIts strengths mirror the base variant: ceiling-level performance in morphologically rich categories such as <span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">scrambling</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span>.\nHowever, SindBERT<sub class=\"ltx_sub\">large</sub> shows a severe weakness in <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (27.8), which strongly lowers its overall average.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight that monolingual models like SindBERT capture Turkish-specific morphosyntax particularly well, while multilingual models such as XLM-R generalize more effectively to harder syntactic phenomena (e.g., ellipsis and binding).\nThis suggests a trade-off between specialization in language-specific structures and broader generalization capacities learned from multilingual corpora.</p>\n\n",
                "matched_terms": [
                    "models",
                    "phenomena"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation shows that SindBERT<sub class=\"ltx_sub\">base</sub> performs competitively with other widely used Turkish encoders, confirming the robustness of its RoBERTa-style pretraining setup.\nAt the same time, SindBERT<sub class=\"ltx_sub\">large</sub> achieves the best overall results in two of four downstream tasks, notably in part-of-speech tagging and offensive language detection, and also performs strongly on several linguistic control tests.\nWhile scaling does not produce uniform gains across all benchmarks, these task-specific improvements suggest that larger contextual capacity primarily benefits pragmatically and syntactically complex settings.\nSimilar saturation effects are visible for EuroBERT and XLM-R, indicating that many Turkish benchmarks may no longer be sufficiently discriminative to reveal consistent scaling trends.\nNonetheless, diagnostic evaluations such as <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> underscore SindBERT&#8217;s strengths in Turkish-specific grammatical phenomena (e.g., scrambling, suspended affixation, subject agreement), highlighting the model&#8217;s linguistic depth beyond aggregate scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "linguistic",
                    "phenomena",
                    "turblimp",
                    "best",
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A likely factor explaining the limited scaling gains lies in the training corpus composition.\nSindBERT was trained on 312 GB of text&#8212;dominated by mC4 (242 GB), which provides broad coverage but is considerably noisier than smaller, curated datasets.\nBy contrast, BERTurk, trained on only a fraction of that volume but sourced from cleaner collections (OSCAR, Wikipedia, OPUS, and non-public), achieves excellent results, particularly on linguistically sensitive evaluations.\nThis mirrors trends observed in other monolingual models such as GottBERT, CamemBERT, and GeistBERT, where performance gains stemmed not merely from data size but from an effective balance of quality, domain diversity, and linguistic representativeness.\nOur findings therefore reinforce that corpus curation, not scale alone, is decisive for progress in Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "linguistic",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further dimension concerns vocabulary design.\nSindBERT employs a 52k BPE vocabulary that balances coverage and efficiency, whereas BERTurk also released a 128k-token variant, which ranks among the strongest performers in our benchmarks, especially on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.\nRecent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> corroborates that vocabulary size has a substantial impact on Turkish models due to the language&#8217;s agglutinative morphology.\nThey report that optimal vocabulary scales differ by tokenization strategy: for BPE or WordPiece, vocabularies around 20% of model parameters tend to be most effective, while morphological or word-level tokenizers may benefit from substantially larger ratios.\nOur results align with this observation: BERTurk<sub class=\"ltx_sub\">128k</sub> profits from an expanded vocabulary despite its smaller corpus, whereas SindBERT&#8217;s 52k vocabulary remains sufficiently expressive to achieve competitive results given its broader but noisier training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "results",
                    "turblimp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "across",
                    "base",
                    "model",
                    "large",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, our findings indicate that many existing benchmarks are already saturated, as they fail to reveal consistent improvements from larger models.\nTo overcome this limitation, future evaluations should adopt more comprehensive and discriminative test suites.\nIn particular, the recently released TrGLUE benchmark<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE\" title=\"\">https://huggingface.co/datasets/turkish-nlp-suite/TrGLUE</a></span></span></span> offers a promising step in this direction, providing a diverse collection of tasks. It includes natural language inference, paraphrase detection, sentiment analysis, and question answering, that more closely mirror the breadth of the original GLUE suite.\nIncorporating TrGLUE into future experiments would enable a more fine-grained assessment of SindBERT&#8217;s generalization capabilities across both syntactic and semantic dimensions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "model",
                    "best",
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, we did not perform a systematic error analysis of downstream results. Such an analysis could provide insights into systematic weaknesses (e.g., frequent PoS confusions, NER boundary errors, sentiment misclassifications, or <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> minimal pair failures) and help prioritize future improvements in model design and dataset composition.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "turblimp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, our evaluation focused on four downstream tasks (PoS tagging, NER, sentiment classification, <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>). While these cover a diverse range of morphosyntactic, semantic, and syntactic phenomena, they do not capture the full scope of Turkish NLP challenges such as question answering, natural language inference, summarization, or long-context understanding. The generalization of SindBERT to these settings remains to be established.</p>\n\n",
                "matched_terms": [
                    "phenomena",
                    "evaluation",
                    "turblimp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like all large-scale language models, SindBERT may inherit biases from its training data, which can influence downstream tasks such as classification or decision-making. While no deduplication was applied, the corpus may still contain redundancy and noise, as well as deeper societal or representational biases. Furthermore, training on large web-based corpora raises privacy concerns, as models may inadvertently retain sensitive information. Responsible deployment is especially important in high-stakes domains like legal, medical, or financial NLP.</p>\n\n",
                "matched_terms": [
                    "models",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite optimizations for efficiency, pre-training and evaluating transformer models remain computationally demanding, contributing to energy use and carbon emissions. These environmental costs highlight the need for balancing model performance with sustainable development goals.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "best",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> is not reported, as the pipeline did not record training time.\nSince no hyperparameter search was involved, this omission is minor and corresponds to only a few additional hours.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "turblimp"
                ]
            }
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Computation time in hours and minutes for the Turkish downstream tasks, summing to about 425 hours and 1 minute (approximately 17.7 days).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Task</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Computation Time</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">PoS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">200:21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">WikiANN</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">131:02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">OffensEval-TR 2020</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">93:37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Total</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">425:01</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "time",
            "downstream",
            "hours",
            "pos",
            "task",
            "total",
            "minute",
            "minutes",
            "computation",
            "offensevaltr",
            "turkish",
            "days",
            "tasks",
            "approximately",
            "summing",
            "wikiann",
            "about"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts.\nWith SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish.\nTrained from scratch on 312&#160;GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish.\nWe evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> linguistic acceptability benchmark.\nOur results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall.\nThis flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated.\nAt the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume.\nTaken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages.\nThe SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.</p>\n\n",
                "matched_terms": [
                    "time",
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of transformer-based language models such as BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib10\" title=\"\">2019</a>)</cite> and RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib18\" title=\"\">2019</a>)</cite> marked a paradigm shift in NLP, enabling significant improvements across a wide range of tasks. Building on these foundations, multilingual extensions such as mBERT and in particular XLM-RoBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chan, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib5\" title=\"\">2020</a>)</cite> became widely used as strong general-purpose baselines across more than 100 languages. At the same time, a wave of monolingual adaptations demonstrated that language-specific pre-training often outperforms multilingual alternatives when sufficient high-quality data is available&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delobelle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib9\" title=\"\">2020b</a>; Martin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib20\" title=\"\">2020</a>; Chan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib6\" title=\"\">2020</a>; Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>; Scheible-Schmitt and Frei, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "time",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT was trained on three Turkish corpora: Wikipedia, OSCAR23 <cite class=\"ltx_cite ltx_citemacro_cite\">Jansen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib13\" title=\"\">2022</a>)</cite>, and mC4.\nThe corpus was shuffled and lightly filtered, restricted to the removal of documents containing invalid character encodings.\nThe extracted sizes are approximately 242&#160;GB for mC4, 69&#160;GB for OSCAR, and 0.6&#160;GB for Wikipedia, resulting in a combined pre-training corpus of about 312&#160;GB of Turkish text.</p>\n\n",
                "matched_terms": [
                    "about",
                    "approximately",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to RoBERTa, SindBERT relies on byte pair encoding (BPE) <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib23\" title=\"\">2019</a>)</cite> for subword segmentation, which directly operates on raw text without the need for pre-tokenization or auxiliary tools such as Moses <cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib17\" title=\"\">2007</a>)</cite>. Since the original GPT-2 tokenizer was designed for English, we instead constructed a tokenizer tailored for Turkish. Following the strategy applied in GottBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we trained a dedicated vocabulary using 40 GB of randomly sampled Turkish text, resulting in a 52k subword inventory optimized for the language. In our experience, sampling around 40 GB of text is already enough for the subword statistics to stabilize, while scaling vocabulary training to the entire corpus would primarily increase computational cost without offering substantial gains. While we did not separately evaluate the effect of this adaptation on storage size or downstream accuracy, previous work in Dutch <cite class=\"ltx_cite ltx_citemacro_cite\">Delobelle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>)</cite> and German <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> indicates that language-specific tokenizers can yield improvements in both efficiency and performance.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "days",
                    "approximately",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For NER, we fine-tuned on the Turkish NER dataset introduced in the WikiANN corpus&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib22\" title=\"\">2017</a>)</cite> and widely used for multilingual evaluation. We used the splits from <cite class=\"ltx_cite ltx_citemacro_citet\">Rahimi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib24\" title=\"\">2019</a>)</cite> and report micro F1 across all entity types.</p>\n\n",
                "matched_terms": [
                    "wikiann",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness on user-generated content, we employed the OffensEval-TR 2020 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(&#199;&#246;ltekin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib4\" title=\"\">2020</a>)</cite>, a corpus of Turkish tweets annotated for the presence of offensive language. The dataset contains over 31k training and 3.5k test instances, labeled in a binary fashion as either <span class=\"ltx_text ltx_font_italic\">NOT</span> (not offensive) or <span class=\"ltx_text ltx_font_italic\">OFF</span> (offensive). Mentions and URLs were anonymized during preprocessing (e.g., replaced by @USER or URL), while the tweets otherwise preserve the linguistic and pragmatic properties of social media text.\nWe report performance using macro F1.</p>\n\n",
                "matched_terms": [
                    "offensevaltr",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "total",
                    "downstream",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across base-scale models, performance on the Turkish Universal Dependencies treebank is consistently high, with micro-F1 values exceeding 93% for nearly all encoders.\nThe strongest overall results are achieved by ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (94.57), closely followed by SindBERT<sub class=\"ltx_sub\">base</sub> (94.47) and BERTurk<sub class=\"ltx_sub\">128k</sub> (94.44).\nInterestingly, both ConvBERTurk variants, trained with different corpora, maintain a narrow margin over ELECTRA-based and RoBERTa-style encoders, suggesting that architectural innovations like dynamic convolution offer slight but consistent gains in token-level syntactic tagging.\nThe relatively low score of RoBERTurk (87.99) indicates the limitations of early RoBERTa replications for Turkish, likely due to smaller corpora and shorter training schedules.\nSindBERT<sub class=\"ltx_sub\">base</sub> performs competitively within this saturated range, demonstrating strong generalization across tasks despite a larger 52k BPE vocabulary.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, POS tagging performance appears saturated across both scales, with nearly all base models exceeding 94 F1 and only marginal gains from scaling. SindBERT maintains parity with top-tier baselines, confirming that syntactic coverage in Turkish is largely solved for transformer-based encoders.</p>\n\n",
                "matched_terms": [
                    "turkish",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best base-scale performance is reached by BERTurk<sub class=\"ltx_sub\">32k</sub> (94.38), confirming its robustness for token-level classification.\nClose behind are ConvBERTurk (94.03) and BERTurk<sub class=\"ltx_sub\">128k</sub> (93.81), while SindBERT<sub class=\"ltx_sub\">base</sub> achieves a solid 93.19, comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> (93.49) and XLM-R<sub class=\"ltx_sub\">base</sub> (92.9).\nThis indicates that SindBERT&#8217;s RoBERTa-like setup neither clearly surpasses nor lags behind the most established Turkish encoders, suggesting that the NER task may already be approaching an upper limit with current dataset size and annotation quality.</p>\n\n",
                "matched_terms": [
                    "task",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For offensive language classification (OffensEval-TR 2020), we observe more pronounced differences between architectures.\nConvBERTurk reaches the highest macro-F1 among base models (81.99), with ConvBERTurk<sub class=\"ltx_sub\">mC4</sub> (81.90) and BERTurk<sub class=\"ltx_sub\">128k</sub> (81.77) performing almost identically.\nELECTRA variants and SindBERT<sub class=\"ltx_sub\">base</sub> (81.14) cluster slightly below, while distilled and multilingual models trail more clearly.\nThese results highlight that models trained on monolingual Turkish corpora still offer clear advantages for pragmatic and domain-sensitive tasks.\nSindBERT<sub class=\"ltx_sub\">base</sub> thus performs solidly but not at the very top, suggesting that further pre-training on informal or social-media text could enhance its stylistic robustness.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "offensevaltr",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the large model group, SindBERT<sub class=\"ltx_sub\">large</sub> again performs best (82.29), surpassing XLM-R<sub class=\"ltx_sub\">large</sub> (81.99) and far exceeding EuroBERT<sub class=\"ltx_sub\">610M</sub> (75.57).\nThis consistent lead across two of four downstream tasks emphasizes SindBERT&#8217;s balanced architecture and effective use of Turkish-specific corpora.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.SS2.SSS0.Px4\" title=\"TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a> reports the detailed <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> results for all base and large models.\nOverall, SindBERT<sub class=\"ltx_sub\">base</sub> achieves an average score of 90.3, which is comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> and ELECTRA<sub class=\"ltx_sub\">mC4</sub> (both 89.9), while trailing behind the strongest baselines BERTurk<sub class=\"ltx_sub\">32k</sub> (93.8) and BERTurk<sub class=\"ltx_sub\">128k</sub> (95.1).\nA closer look at the per-phenomenon results shows that SindBERT<sub class=\"ltx_sub\">base</sub> is particularly strong on <span class=\"ltx_text ltx_font_italic\">scrambling</span>,\n<span class=\"ltx_text ltx_font_italic\">suspended affixation</span>, <span class=\"ltx_text ltx_font_italic\">subject agreement</span>, and <span class=\"ltx_text ltx_font_italic\">irregular forms</span> (all <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>98), which are central morphosyntactic phenomena of Turkish.\nAt the same time, it struggles with <span class=\"ltx_text ltx_font_italic\">ellipsis</span> (59.0) and <span class=\"ltx_text ltx_font_italic\">island effects</span> (64.0), two categories that remain challenging across most models.</p>\n\n",
                "matched_terms": [
                    "time",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation shows that SindBERT<sub class=\"ltx_sub\">base</sub> performs competitively with other widely used Turkish encoders, confirming the robustness of its RoBERTa-style pretraining setup.\nAt the same time, SindBERT<sub class=\"ltx_sub\">large</sub> achieves the best overall results in two of four downstream tasks, notably in part-of-speech tagging and offensive language detection, and also performs strongly on several linguistic control tests.\nWhile scaling does not produce uniform gains across all benchmarks, these task-specific improvements suggest that larger contextual capacity primarily benefits pragmatically and syntactically complex settings.\nSimilar saturation effects are visible for EuroBERT and XLM-R, indicating that many Turkish benchmarks may no longer be sufficiently discriminative to reveal consistent scaling trends.\nNonetheless, diagnostic evaluations such as <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> underscore SindBERT&#8217;s strengths in Turkish-specific grammatical phenomena (e.g., scrambling, suspended affixation, subject agreement), highlighting the model&#8217;s linguistic depth beyond aggregate scores.</p>\n\n",
                "matched_terms": [
                    "time",
                    "downstream",
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "turkish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, we did not perform a systematic error analysis of downstream results. Such an analysis could provide insights into systematic weaknesses (e.g., frequent PoS confusions, NER boundary errors, sentiment misclassifications, or <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> minimal pair failures) and help prioritize future improvements in model design and dataset composition.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, our evaluation focused on four downstream tasks (PoS tagging, NER, sentiment classification, <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>). While these cover a diverse range of morphosyntactic, semantic, and syntactic phenomena, they do not capture the full scope of Turkish NLP challenges such as question answering, natural language inference, summarization, or long-context understanding. The generalization of SindBERT to these settings remains to be established.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "tasks",
                    "turkish",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Like all large-scale language models, SindBERT may inherit biases from its training data, which can influence downstream tasks such as classification or decision-making. While no deduplication was applied, the corpus may still contain redundancy and noise, as well as deeper societal or representational biases. Furthermore, training on large web-based corpora raises privacy concerns, as models may inadvertently retain sensitive information. Responsible deployment is especially important in high-stakes domains like legal, medical, or financial NLP.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> is not reported, as the pipeline did not record training time.\nSince no hyperparameter search was involved, this omission is minor and corresponds to only a few additional hours.</p>\n\n",
                "matched_terms": [
                    "time",
                    "hours"
                ]
            }
        ]
    },
    "A1.tab1": {
        "caption": "Table 7: \nHyperparameters of the best-performing downstream task model for each pre-trained model.\nBF denotes the batch size, LR the learning rate.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">PoS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">NER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Sentiment</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">BF</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">BF</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">BF</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line =</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\csvcolvii</span>\n<span class=\"ltx_ERROR undefined\">\\csvreader</span>[late after line =</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvi</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvii</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliii</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcoliv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_ERROR undefined\">\\csvcolvi</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pos",
            "line",
            "size",
            "denotes",
            "each",
            "downstream",
            "rate",
            "csvreaderlate",
            "after",
            "batch",
            "learning",
            "hyperparameters",
            "sentiment",
            "csvcoliv",
            "csvcolii",
            "pretrained",
            "csvcoliii",
            "bestperforming",
            "task",
            "ner",
            "model",
            "csvcolvi",
            "csvcolvii",
            "csvcolv"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">For Turkish NLP, several transformer-based encoders have been introduced in recent years. Notable examples include BERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schweter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib28\" title=\"\">2025</a>)</cite>, trained on a 35 GB corpus of Turkish OSCAR, Wikipedia, and OPUS data; ELECTRA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib7\" title=\"\">2020</a>)</cite> and ConvBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib14\" title=\"\">2021</a>)</cite> models trained on both OSCAR and mC4 (35&#8211;242 GB)<cite class=\"ltx_cite ltx_citemacro_cite\">Jiao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib15\" title=\"\">2020</a>)</cite>. While these models provide important milestones, most are relatively small encoder models trained with earlier-generation methods or focus on architectures other than RoBERTa. The only RoBERTa models out there were not computed in its fullest extend, but rather with small batch size for relatively small period <cite class=\"ltx_cite ltx_citemacro_cite\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>); Tas (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite>. Futher, Turkish still lacks a large-scale, high-quality encoder-only model.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce SindBERT, a RoBERTa-based encoder model pre-trained specifically for Turkish. SindBERT builds on the design principles of the German model GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Scheible et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> and adapts them to the morphological richness and agglutinative structure of Turkish. We construct a byte-level BPE vocabulary optimized for Turkish, train both base and large variants with fairseq&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ott et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib21\" title=\"\">2019</a>)</cite>, and leverage TPUv4 hardware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jouppi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite> for efficient large-scale pre-training. SindBERT is designed to combine scalability and reproducibility while directly targeting Turkish, resulting in the first large-scale RoBERTa-style encoder model for Turkish.\nOur contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line of work, RoBERTurk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tas, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib29\" title=\"\">2024</a>)</cite> introduced a RoBERTa-style encoder specifically adapted for Turkish, showing that refined pre-training objectives and tokenizer design can yield competitive results.\nIn parallel, research has underscored the critical role of tokenization in morphologically rich languages. <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> systematically analyzed the impact of vocabulary size and segmentation strategy, showing that larger vocabularies can notably improve performance in morphosyntactic evaluations.\nHowever, all these RoBERTa-based models were not extensively trained, typically using moderate batch sizes and relatively few update steps, resulting in comparatively shallow pretraining regimes.</p>\n\n",
                "matched_terms": [
                    "size",
                    "line",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similar to RoBERTa, SindBERT relies on byte pair encoding (BPE) <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib23\" title=\"\">2019</a>)</cite> for subword segmentation, which directly operates on raw text without the need for pre-tokenization or auxiliary tools such as Moses <cite class=\"ltx_cite ltx_citemacro_cite\">Koehn et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib17\" title=\"\">2007</a>)</cite>. Since the original GPT-2 tokenizer was designed for English, we instead constructed a tokenizer tailored for Turkish. Following the strategy applied in GottBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we trained a dedicated vocabulary using 40 GB of randomly sampled Turkish text, resulting in a 52k subword inventory optimized for the language. In our experience, sampling around 40 GB of text is already enough for the subword statistics to stabilize, while scaling vocabulary training to the entire corpus would primarily increase computational cost without offering substantial gains. While we did not separately evaluate the effect of this adaptation on storage size or downstream accuracy, previous work in Dutch <cite class=\"ltx_cite ltx_citemacro_cite\">Delobelle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib8\" title=\"\">2020a</a>)</cite> and German <cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite> indicates that language-specific tokenizers can yield improvements in both efficiency and performance.</p>\n\n",
                "matched_terms": [
                    "size",
                    "downstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the setup of GottBERT, we pre-trained both SindBERT<sub class=\"ltx_sub\">base</sub> and SindBERT<sub class=\"ltx_sub\">large</sub> using the fairseq framework on a 128-core TPUv4 pod <cite class=\"ltx_cite ltx_citemacro_cite\">Jouppi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib16\" title=\"\">2023</a>)</cite>. Mixed-precision training (fp16/bfloat16) was not employed, so both models were trained entirely in full precision (fp32). This ensures that training dynamics can be attributed directly to model size, without numerical precision optimizations acting as additional factors.</p>\n\n",
                "matched_terms": [
                    "size",
                    "pretrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SindBERT<sub class=\"ltx_sub\">base</sub> completed training in approximately 29.2 hours, while SindBERT<sub class=\"ltx_sub\">large</sub> required around 6.0 days. We followed the standard RoBERTa pretraining schedule with 100k update steps, a global batch size of 8k, a 10k-step warmup, and polynomial learning rate decay. The base model used a peak learning rate of 0.0004, and the large model 0.00015. Similar to GottBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib25\" title=\"\">2024</a>)</cite>, we evaluated after each epoch and stored checkpoints throughout training. Since the dataset size only permitted roughly four epochs, the final checkpoint coincided with the best-performing one.</p>\n\n",
                "matched_terms": [
                    "bestperforming",
                    "rate",
                    "model",
                    "after",
                    "size",
                    "batch",
                    "learning",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the capabilities of SindBERT, we fine-tuned the model on a diverse suite of Turkish downstream benchmarks covering sequence labeling, text classification, and linguistic acceptability.\nTraining was performed with the Flair framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Akbik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib1\" title=\"\">2019</a>)</cite> v0.15.1, using standardized experiment configurations provided in the repository.\nHyperparameter optimization was carried out over batch size and learning rate (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), with training capped at a maximum of 30 epochs and early stopping applied (patience = 3).\nAll models employed a linear learning rate schedule with a 10% warmup phase.\nWe evaluated SindBERT on the following tasks:</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "rate",
                    "model",
                    "size",
                    "learning",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess fine-grained grammatical knowledge, we include evaluation on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#351;ar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib2\" title=\"\">2025</a>)</cite>, a benchmark of 16 core linguistic phenomena ranging from anaphor agreement and argument structure to scrambling and suspended affixation.\nEach phenomenon is represented by 1,000 minimal pairs, and models are scored following the BLiMP protocol&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Warstadt et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib31\" title=\"\">2020</a>)</cite>, i.e., assigning higher probability to the grammatical sentence of each pair.\nFor each model we compute the accuracy within every phenomenon and report the average across all 16 categories as the overall <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> score.\nThis measure complements PoS tagging, NER, and sentiment classification by probing deeper syntactic and morphosyntactic competence.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "ner",
                    "model",
                    "each",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused our grid search on batch sizes and learning rates, selected based on the most frequent best-performing values in prior experiments (GottBERT, GeistBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Scheible-Schmitt and Frei (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib26\" title=\"\">2025</a>)</cite>; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S3.T2\" title=\"Table 2 &#8227; 3.5 Hyperparameters &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Training was applied to PoS, NER and classification and capped at a maximum of 30 epochs, with early stopping applied using a patience of three epochs. All models employed a linear learning rate schedule with a warmup phase of 10% of the total training steps. All downstream fine-tuning experiments were conducted with a fixed random seed of 1 for the base models and 42 for the large models.\nThis setup ensures reproducibility and consistency within each scale while maintaining overall comparability across model groups; nonetheless, minor deviations may still arise from seed-related variance <cite class=\"ltx_cite ltx_citemacro_cite\">Dodge et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib11\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "bestperforming",
                    "pos",
                    "rate",
                    "ner",
                    "model",
                    "batch",
                    "learning",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pre-training, we monitored perplexity both on the training set (at each optimization step) and on the validation set (after each epoch; see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#S4.F1\" title=\"Figure 1 &#8227; 4.1 Pre-training &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Across all configurations, the curves follow a consistent convergence pattern. An initial plateau phase can be observed, which is relatively brief for the base models but more pronounced for the large ones. Occasional short upward spikes appear in the training curves; if taken in isolation, these might be misread as divergence, yet they quickly subside as training progresses.</p>\n\n",
                "matched_terms": [
                    "each",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The base models typically stabilize after 20k&#8211;30k steps, while the large models require slightly longer but consistently converge by around 40k steps. By the end of training, both configurations achieve comparably low perplexity, underscoring the efficiency of the pre-training setup. This trend is mirrored in the validation perplexity, which shows steady improvements after each epoch. Overall, training perplexity decreased from about 54.5k to 3.93 for the base models and from about 52.2k to 3.24 for the large models, reflecting robust and reliable convergence.</p>\n\n",
                "matched_terms": [
                    "each",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best base-scale performance is reached by BERTurk<sub class=\"ltx_sub\">32k</sub> (94.38), confirming its robustness for token-level classification.\nClose behind are ConvBERTurk (94.03) and BERTurk<sub class=\"ltx_sub\">128k</sub> (93.81), while SindBERT<sub class=\"ltx_sub\">base</sub> achieves a solid 93.19, comparable to ELECTRA<sub class=\"ltx_sub\">base</sub> (93.49) and XLM-R<sub class=\"ltx_sub\">base</sub> (92.9).\nThis indicates that SindBERT&#8217;s RoBERTa-like setup neither clearly surpasses nor lags behind the most established Turkish encoders, suggesting that the NER task may already be approaching an upper limit with current dataset size and annotation quality.</p>\n\n",
                "matched_terms": [
                    "size",
                    "ner",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, NER results reveal minimal separation between base and large encoders, indicating that model size has limited impact once sufficient Turkish data are used. SindBERT performs on par with the strongest monolingual models, underscoring the stability of its representations across token-level semantic tasks.</p>\n\n",
                "matched_terms": [
                    "size",
                    "ner",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the large model group, SindBERT<sub class=\"ltx_sub\">large</sub> again performs best (82.29), surpassing XLM-R<sub class=\"ltx_sub\">large</sub> (81.99) and far exceeding EuroBERT<sub class=\"ltx_sub\">610M</sub> (75.57).\nThis consistent lead across two of four downstream tasks emphasizes SindBERT&#8217;s balanced architecture and effective use of Turkish-specific corpora.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further dimension concerns vocabulary design.\nSindBERT employs a 52k BPE vocabulary that balances coverage and efficiency, whereas BERTurk also released a 128k-token variant, which ranks among the strongest performers in our benchmarks, especially on <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>.\nRecent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Toraman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#bib.bib30\" title=\"\">2023</a>)</cite> corroborates that vocabulary size has a substantial impact on Turkish models due to the language&#8217;s agglutinative morphology.\nThey report that optimal vocabulary scales differ by tokenization strategy: for BPE or WordPiece, vocabularies around 20% of model parameters tend to be most effective, while morphological or word-level tokenizers may benefit from substantially larger ratios.\nOur results align with this observation: BERTurk<sub class=\"ltx_sub\">128k</sub> profits from an expanded vocabulary despite its smaller corpus, whereas SindBERT&#8217;s 52k vocabulary remains sufficiently expressive to achieve competitive results given its broader but noisier training data.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From an efficiency perspective, our findings highlight a favorable trade-off between scale and performance.\nWhile SindBERT<sub class=\"ltx_sub\">base</sub> achieves results comparable to its larger counterpart at a fraction of the computational cost, SindBERT<sub class=\"ltx_sub\">large</sub> still demonstrates measurable advantages on more demanding or pragmatically complex tasks.\nThis indicates that the large model&#8217;s additional capacity is not wasted, but rather contributes selectively where richer contextual representations are required.\nNevertheless, for most real-world scenarios, the base configuration offers an excellent balance between efficiency and accuracy.\nTaken together, the flat scaling behavior across multiple Turkish model families suggests that future progress will hinge less on parameter growth and more on corpus quality, tokenization, and task design.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced SindBERT, the first large-scale RoBERTa encoder trained from scratch on 312 GB of Turkish text.\nAcross four benchmarks, it performs competitively with existing models, with SindBERT<sub class=\"ltx_sub\">large</sub> achieving the best results in two tasks.\nWhile scaling brings only selective gains, this mirrors trends in XLM-R and EuroBERT, suggesting that Turkish benchmarks are nearing saturation.\nThe contrast with BERTurk highlights the decisive role of corpus quality and variance over size.\nTogether, these findings show that progress in Turkish NLP will depend less on scaling and more on curated data, adaptive tokenization, and challenging evaluation suites.\nAs the first openly released large-scale RoBERTa model for Turkish, SindBERT establishes a solid foundation for future Turkish NLP.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, SindBERT was pre-trained with conservative hyperparameter settings and without extensive exploration of alternative masking strategies (e.g., Whole Word Masking) or longer training schedules. Pre-training was also conducted without mixed precision, which increased computational cost and limited the feasibility of scaling to larger model sizes or more training steps.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fourth, we did not perform a systematic error analysis of downstream results. Such an analysis could provide insights into systematic weaknesses (e.g., frequent PoS confusions, NER boundary errors, sentiment misclassifications, or <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span> minimal pair failures) and help prioritize future improvements in model design and dataset composition.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "pos",
                    "ner",
                    "model",
                    "sentiment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, our evaluation focused on four downstream tasks (PoS tagging, NER, sentiment classification, <span class=\"ltx_text ltx_font_smallcaps\">TurBLiMP</span>). While these cover a diverse range of morphosyntactic, semantic, and syntactic phenomena, they do not capture the full scope of Turkish NLP challenges such as question answering, natural language inference, summarization, or long-context understanding. The generalization of SindBERT to these settings remains to be established.</p>\n\n",
                "matched_terms": [
                    "ner",
                    "downstream",
                    "sentiment",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1\" title=\"Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> lists the hyperparameters of the best SindBERT models (selected by validation performance) for each benchmark, supporting reproducibility of our results.\nFor transparency, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21364v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Runtime &#8227; Acknowledgments &#8227; Ethical Considerations &#8227; Limitations &#8227; 7 Conclusion &#8227; 6 Future Directions &#8227; 5.3 Efficiency &#8227; 5 Discussion &#8227; TurBLiMP &#8227; Offensive Language Detection &#8227; 4.2 Downstream Tasks &#8227; 4 Results &#8227; 3.6 Model Properties &#8227; 3 Methods &#8227; SindBERT, the Sailor: Charting the Seas of Turkish NLP\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> reports the total computation time per task, showing that all Turkish downstream experiments together required roughly 425 GPU hours (about 17.7&#160;days). All base model experiments were run on an NVIDIA RTX 3090, and large model experiments on an NVIDIA H100 GPU.</p>\n\n",
                "matched_terms": [
                    "downstream",
                    "task",
                    "model",
                    "each",
                    "hyperparameters"
                ]
            }
        ]
    }
}