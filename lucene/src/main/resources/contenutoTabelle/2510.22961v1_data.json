{
    "S4.T1": {
        "caption": "Table 1:  Word error rate (WER) (%) on LRS3 test set using low-resource (30h) labeled audio-visual data. ”Shared” indicates whether the same model is used across ASR, VSR, and AVSR tasks. (N) denotes results on our constructed noisy test set.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Methods</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Unlab.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Lab.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Shared</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR (N)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR (N)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AV-HuBERT-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"11\">433h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"11\">30h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">51.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-HuBERT-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">44.8</td>\n<td class=\"ltx_td ltx_align_center\">4.5</td>\n<td class=\"ltx_td ltx_align_center\">4.2</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VATLM-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib41\" title=\"\">41</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">48.0</td>\n<td class=\"ltx_td ltx_align_center\">3.6</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RAVEn-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">47.0</td>\n<td class=\"ltx_td ltx_align_center\">4.7</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">BRAVEn-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">43.4</td>\n<td class=\"ltx_td ltx_align_center\">4.0</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-data2vec-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">45.2</td>\n<td class=\"ltx_td ltx_align_center\">4.4</td>\n<td class=\"ltx_td ltx_align_center\">4.2</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">39.4</td>\n<td class=\"ltx_td ltx_align_center\">5.6</td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">36.0</td>\n<td class=\"ltx_td ltx_align_center\">3.2</td>\n<td class=\"ltx_td ltx_align_center\">3.0</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">37.2</td>\n<td class=\"ltx_td ltx_align_center\">3.0</td>\n<td class=\"ltx_td ltx_align_center\">2.8</td>\n<td class=\"ltx_td ltx_align_center\">50.5</td>\n<td class=\"ltx_td ltx_align_center\">11.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">33.2</td>\n<td class=\"ltx_td ltx_align_center\">2.4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center\">45.3</td>\n<td class=\"ltx_td ltx_align_center\">11.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-B (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">31.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.2</span></td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n<td class=\"ltx_td ltx_align_center\">4.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-L (Ours)</em></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">30.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.2</span></td>\n<td class=\"ltx_td ltx_align_center\">2.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AV-HuBERT-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"16\">1759h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"16\">30h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-HuBERT-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">32.5</td>\n<td class=\"ltx_td ltx_align_center\">2.9</td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VATLM-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib41\" title=\"\">41</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">42.6</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">3.4</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VATLM-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib41\" title=\"\">41</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">31.6</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">BRAVEn-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">35.1</td>\n<td class=\"ltx_td ltx_align_center\">3.0</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">BRAVEn-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">30.8</td>\n<td class=\"ltx_td ltx_align_center\">2.3</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-data2vec-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">37.8</td>\n<td class=\"ltx_td ltx_align_center\">3.7</td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-data2vec-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">30.8</td>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n<td class=\"ltx_td ltx_align_center\">2.5</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">26.9</td>\n<td class=\"ltx_td ltx_align_center\">2.4</td>\n<td class=\"ltx_td ltx_align_center\">2.4</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">36.9</td>\n<td class=\"ltx_td ltx_align_center\">2.5</td>\n<td class=\"ltx_td ltx_align_center\">2.2</td>\n<td class=\"ltx_td ltx_align_center\">44.6</td>\n<td class=\"ltx_td ltx_align_center\">9.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">30.2</td>\n<td class=\"ltx_td ltx_align_center\">2.2</td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">41.4</td>\n<td class=\"ltx_td ltx_align_center\">8.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LLama-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-B (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">27.3</td>\n<td class=\"ltx_td ltx_align_center\">2.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-B* (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\">2.4</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">39.8</td>\n<td class=\"ltx_td ltx_align_center\">3.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-L (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">25.3</span></td>\n<td class=\"ltx_td ltx_align_center\">2.5</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\">*Visual encoder pretraining used 1759 hours of data, visual injection pretraining used 433 hours.</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "avsr",
            "word",
            "wer",
            "usrl",
            "usrb",
            "visual",
            "denotes",
            "vatlmb",
            "av2vecmlm",
            "30h",
            "uasrllml",
            "llamaavsr",
            "error",
            "our",
            "unlab",
            "hours",
            "rate",
            "bravenb",
            "distillavb",
            "pretraining",
            "tasks",
            "encoder",
            "audiovisual",
            "test",
            "methods",
            "used",
            "noisy",
            "avdata2vecb",
            "vatlml",
            "uasrllmb",
            "across",
            "usr",
            "shared",
            "constructed",
            "asr",
            "indicates",
            "results",
            "avhubertb",
            "distillavl",
            "labeled",
            "set",
            "ours",
            "same",
            "avhubertl",
            "1759h",
            "bravenl",
            "model",
            "lrs3",
            "lab",
            "avdata2vecl",
            "data",
            "433h",
            "whether",
            "ravenb",
            "lowresource",
            "injection",
            "vsr",
            "”shared”"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> present experimental results for models finetuned with low-resource (30h) and high-resource (433h or 1759h) labeled data, respectively. The unlabeled data refers to audio-visual data used for pretraining the DistillAV-based visual encoder and visual injection pretraining, while the labeled data denotes audio-visual data employed for speech recognition finetuning.</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms most baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Although underperforming Llama-AVSR in ASR, our approach provides a unified framework effective across all three tasks. The superior VSR performance particularly demonstrates the effectiveness of &#8221;reprogramming&#8221; WavLM for lipreading with LLMs, consistent with previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWith 433h labeled data, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method achieves significant accuracy improvements. Compared to previous studies using comparable audio-visual data, our approach demonstrates superior performance across all tasks under both clean and noisy conditions. With 1759h labeled data, our method achieves WERs of 20.9%, 0.84%, and 0.69% on the clean LRS3 test set, rivaling or surpassing state-of-the-art methods. However, our VSR results still lag behind LP-Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib43\" title=\"\">43</a>]</cite>, which uses approximately 100k hours of audio-visual data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework. While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.\nThis paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified visual speech recognition (VSR), automatic speech recognition (ASR), and audio-visual speech recognition (AVSR) tasks by leveraging large language models (LLMs) as text decoders. Our approach introduces visual representations into multiple SFM layers through visual injection modules, enabling multimodal input processing and unified hidden representations. The augmented SFMs connect with decoder-only LLMs via a feed-forward adaptor, where concatenated representations and instruction prompts guide speech transcription.\nWe propose a two-stage training strategy: visual injection pretraining followed by speech recognition finetuning. SFM parameters remain frozen throughout training, with only visual injection modules optimized initially, and LLMs finetuned using LoRA parameters subsequently.\nExperimental results demonstrate superior performance over state-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Ablation studies confirm generalization across various SFMs and LLMs, validating the proposed training strategy.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "audiovisual",
                    "asr",
                    "results",
                    "noisy",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech recognition represents a fundamental challenge in pattern recognition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib1\" title=\"\">1</a>]</cite>, with recent advances driven by speech foundation models (SFMs) such as WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> and Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite>.\nThese large-scale models, trained on extensive speech datasets, have demonstrated remarkable performance in automatic speech recognition (ASR). However, environmental noise remains a critical limitation that can significantly compromise recognition accuracy, particularly in real-world deployment scenarios.\nTo overcome this challenge, researchers have developed complementary approaches including visual speech recognition (VSR, also known as lipreading)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib6\" title=\"\">6</a>]</cite> and audio-visual speech recognition (AVSR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib8\" title=\"\">8</a>]</cite>, which leverage lip movement information either independently or in combination with audio signals. AVSR systems demonstrate substantial improvements in robustness under noisy conditions, making them increasingly valuable for practical applications such as intelligent home systems, online conferencing, and service robotics.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "asr",
                    "audiovisual",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AVSR models typically adopt a sequence-to-sequence architecture nowadays, including recurrent neural network transducer (RNN-T)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite> or hybrid connectionist temporal classification (CTC) and attention architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib11\" title=\"\">11</a>]</cite>.\nRecently, audio-visual self-supervised learning has also been proposed to leverage\nunlabeled audio-visual data for pretraining, such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, REVAn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>, and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>.\nWith a further finetuning stage on few labeled data, these methods achieve a strong VSR and AVSR performance.\nBuilding on these progresses, Haliassos et al. introduced unified speech recognition (USR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>, which consolidates ASR, VSR, and AVSR tasks within a single model framework. This unified approach offers compelling advantages by eliminating the need to construct separate models for each modality, thereby significantly reducing training and deployment costs. The conceptual foundation is particularly appealing, as audio and visual lip movements represent complementary signal views of the same articulatory process during speech production.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "same",
                    "usr",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "data",
                    "audiovisual",
                    "methods",
                    "asr",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have explored utilizing SFMs for VSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib16\" title=\"\">16</a>]</cite> and AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>. Our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> proposed using SFMs for knowledge distillation in audio-visual self-supervised pretraining, achieving competitive performance across VSR, ASR, and AVSR benchmarks.\nHowever, the knowledge distillation approach may inherently limit performance compared to direct utilization of SFMs, particularly in high signal-to-noise ratio (SNR) scenarios where the original models excel.\nSeveral approaches have focused on the direct utilization of SFMs for audio-visual tasks. Lip2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite> proposed reprogramming a wav2vec 2.0 model for lipreading by feeding transformed visual representations to the audio model. Whisper-flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib19\" title=\"\">19</a>]</cite> adapted the Whisper model for AVSR by incorporating an additional visual encoder to process visual information alongside audio inputs.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its success, most previous works address VSR, AVSR, and ASR tasks separately.\nIn this study, we explore tackling these tasks within a single unified model.\nFurthermore, we aim to leverage the rich knowledge embedded in both speech foundation models (SFMs) and large language models (LLMs) to design a general framework that achieves robust USR performance.\nWe propose <span class=\"ltx_text ltx_font_bold\">UASR-LLM</span> (<span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">R</span>ecognition with <span class=\"ltx_text ltx_font_bold\">L</span>arge <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">M</span>odels), a novel framework that adapts frozen speech foundation models to unified VSR, ASR, and AVSR tasks by leveraging LLMs as text decoders.\nOur approach employs a visual encoder from our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> to extract visual representations, which are then integrated into each block of the SFMs through visual injection modules. The SFM outputs are processed by an adapter to obtain hidden embeddings, which are subsequently fed into LLMs for conducting speech recognition across all modalities.\nTo enable SFMs to process visual and audio-visual inputs, we introduce a visual injection pretraining stage based on cross-modal knowledge distillation loss. The model then undergoes a second training stage for speech recognition, incorporating random modality dropout guided by corresponding instructions. Throughout the entire training process, both the SFM and visual encoder parameters remain frozen, while the LLM is finetuned using a small set of Low-Rank Adaptation (LoRA) weights&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib20\" title=\"\">20</a>]</cite>. This approach enables memory-efficient finetuning while preserving the generalization capabilities of the original models.\nBy utilizing the SFM as a unified backbone for processing multimodal audio and visual inputs, our framework facilitates knowledge transfer from audio to video data and unifies the hidden representations of both modalities. This provides a well-aligned output space for LLMs to generate transcriptions, seamlessly achieving VSR, ASR, and AVSR within a single model.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "across",
                    "usr",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "data",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates that UASR-LLM consistently outperforms state-of-the-art baselines across all three tasks when trained with comparable amounts of audio-visual speech data. On the LRS3 test set, UASR-LLM achieves word error rates of 20.9%, 0.84%, and 0.69% for VSR, ASR, and AVSR, respectively. Under noisy conditions, our method exhibits substantial robustness, significantly reducing WER compared to baseline approaches. Furthermore, we demonstrate that our approach generalizes effectively across different speech foundation models and large language models, with all configurations outperforming baseline methods. Finally, we validate our training strategy and confirm the effectiveness of performing unified speech recognition within a single model architecture.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "across",
                    "word",
                    "wer",
                    "model",
                    "methods",
                    "lrs3",
                    "data",
                    "tasks",
                    "audiovisual",
                    "test",
                    "asr",
                    "noisy",
                    "error",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent years have witnessed the emergence of various speech foundation models, which can be broadly categorized into two paradigms: self-supervised and supervised approaches.\nAmong SSL-based speech foundation models,\nHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> utilizes k-means clustering to generate pseudo-class labels for model training using a mask prediction loss,\nwith iterative refinement through subsequent clustering and mask prediction steps.\nWavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> improves HuBERT by using a more diverse pretraining dataset and performs speech denoising modeling during pretraining.\nOn the other hand, Whisper family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> exemplifies supervised learning-based speech foundation model. Trained on large-scale multilingual and multitask labeled data, it demonstrates robust performance in ASR and speech translation tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining",
                    "tasks",
                    "data",
                    "asr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Visual representation learning is crucial for the performance of VSR and AVSR tasks, which can be broadly categorized into supervised and self-supervised approaches.\nSupervised methods leverage labeled&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib22\" title=\"\">22</a>]</cite> or weakly labeled audio-visual speech data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite> to optimize representation extraction and text decoding in an end-to-end manner through label prediction. Several auxiliary tasks have been proposed to enhance visual encoder learning, including articulation feature prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>]</cite> and distillation from acoustic models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib26\" title=\"\">26</a>]</cite>.\nSelf-supervised methods utilize unlabeled audio-visual data with pretext tasks such as masked prediction loss functions. Representative approaches include AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, RAVen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>, AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and AV-data2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>. Our previous work proposed a joint audio-visual representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> that leverages cross-modal knowledge distillation from pretrained SFMs.\nThis work adopts our previously pretrained model for visual representation extraction, which can be viewed as an extension of our previous study. Here, we further leverage SFMs directly for processing audio-visual data, moving beyond the distillation approach to direct utilization of foundation models.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "model",
                    "visual",
                    "data",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "methods",
                    "labeled",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have attracted significant attention for speech recognition applications, with research broadly falling into two categories: multimodal LLMs for universal AI assistants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib31\" title=\"\">31</a>]</cite> and LLMs for enhancing traditional speech recognition tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib32\" title=\"\">32</a>]</cite>. Our work belongs to the latter category.\nRecent approaches for integrating LLMs with speech recognition typically employ lightweight adapters to connect frozen speech encoders with LLMs. Representative works include joint speech-language models with small adapters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib32\" title=\"\">32</a>]</cite> and Q-Former-based connections&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib33\" title=\"\">33</a>]</cite>. Some studies explore discrete speech representations as LLM inputs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib34\" title=\"\">34</a>]</cite>.\nLLMs have also been adopted for visual and audio-visual speech recognition. VSP-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib35\" title=\"\">35</a>]</cite> focuses on visual speech recognition, while Llama-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> uses separate audio and visual encoders with temporal concatenation as LLM input. MMS-Llama&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite> employs audio-visual fusion within encoders and Q-former adapters for improved efficiency.\nOur work differs by proposing a unified speech recognition (USR) approach that processes both audio and visual modalities within a single model architecture, rather than targeting specific modalities separately.</p>\n\n",
                "matched_terms": [
                    "usr",
                    "model",
                    "visual",
                    "tasks",
                    "audiovisual",
                    "llamaavsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our UASR-LLM method comprises two main components: an audio-visual adapted speech foundation model (SFM-AV)\nthat processes both audio and visual modalities and encodes them into a unified representational space,\nand a large language model that decodes the encoded representations into text.\nThe SFM-AV consists of a visual encoder and a speech foundation model enhanced with visual injection modules.\nOur model architecture is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe training of our model follows a two-stage process: in the first pretraining stage,\nthe SFM is adapted to encode both audio and visual information into a unified hidden space;\nin the second finetuning stage, the adapted SFM is connected to the LLM to enable unified speech recognition.\nThe detailed model architecture and training strategy are described in the following sections.</p>\n\n",
                "matched_terms": [
                    "model",
                    "visual",
                    "pretraining",
                    "encoder",
                    "audiovisual",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let lip video be <math alttext=\"V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}</annotation></semantics></math>, where <math alttext=\"T_{v},H,W,\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">T_{v},H,W,</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> represent the temporal, height, width, and channel dimensions, respectively. The visual encoder <math alttext=\"\\text{E}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mtext>E</mtext><mi>v</mi></msub><annotation encoding=\"application/x-tex\">\\text{E}_{v}</annotation></semantics></math> processes the visual input into hidden representations:\n<math alttext=\"H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>v</mi></msub><mo>=</mo><mrow><msub><mtext>E</mtext><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}</annotation></semantics></math>,\nwhere <math alttext=\"D_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">D_{v}</annotation></semantics></math> is the feature dimension of the encoded representations.\nIn this work, we adopt the visual component of the pretrained DistillAV model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> as our visual encoder. DistillAV is designed to distill knowledge from speech foundation models (SFMs) for joint audio-visual representation learning. Unlike SFMs, which are trained on large-scale speech-only corpora, DistillAV is trained on relatively smaller audio-visual datasets. As a result, its performance in audio-only tasks is inferior to that of its teacher models, as shown in our previous ASR evaluations. However, DistillAV exhibits strong visual modeling capabilities and consistently outperforms state-of-the-art lipreading models such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite> and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>. Therefore, we use the pretrained DistillAV model to extract representations from lip video inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "visual",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual injection module is introduced to incorporate visual information into the pretrained SFM. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block of the SFM, we employ the audio representations <math alttext=\"H_{a}^{i}\\in\\mathbb{R}^{T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>H</mi><mi>a</mi><mi>i</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{a}^{i}\\in\\mathbb{R}^{T\\times D}</annotation></semantics></math> as the query and attend to visual representations through a cross-attention mechanism. To enable the model to exploit relative position information between the two modalities, we incorporate relative position embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib38\" title=\"\">38</a>]</cite>.\nThe output is subsequently processed through a feed-forward network and then fed into the next SFM block. Both the cross-attention and feed-forward network are augmented with residual connections and tanh gating, following the approach in Whisper-Flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "model",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, our visual injection module operates as follows:</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{MHA}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mtext>MHA</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{MHA}()</annotation></semantics></math> and <math alttext=\"\\text{FFN}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mtext>FFN</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{FFN}()</annotation></semantics></math> represent the multi-head attention and feed-forward network modules, respectively. <math alttext=\"D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>T</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}</annotation></semantics></math>, where <math alttext=\"d_{m,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{m,n}</annotation></semantics></math> represents the relative position encoding between the <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>-th audio frame and the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-th video frame. <math alttext=\"g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m7\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">g_{i}</annotation></semantics></math> and <math alttext=\"g_{i}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>i</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">g_{i}^{\\prime}</annotation></semantics></math> are trainable gating parameters, which are initialized to zero.\nIt provides an initialization state identical to the original SFMs, which helps stabilize the training.\nThe visual injection modules are inserted before each block of the SFM, while the SFM parameters remain frozen throughout training.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The output representations from the SFM-AV are transformed through a two-layer feed-forward adaptor to match the embedding dimension of large language models, serving as continuous audio-visual tokens for prompting the LLM to generate transcribed text. We employ task-specific instructions that prepend the audio-visual tokens according to the unified speech recognition task. The instruction prompt follows the template &#8220;Transcribe the [].&#8221;, where the placeholder [] is filled with &#8220;audio&#8221;, &#8220;video&#8221;, or &#8220;audio and video&#8221; for ASR, VSR, and AVSR tasks, respectively.\nFor efficient LLM finetuning, we adopt the low-rank adaptation (LoRA) method. The low-rank parameters are applied to adapt all blocks&#8217; self-attention components as well as the output projection layer for predicting text logits. Unlike previous approaches such as Llama-AVSR&#8217;s frame-compression&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> or MMS-Llama&#8217;s Q-former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite>, our method does not employ frame reduction techniques. This design choice reflects our primary focus on improving recognition performance rather than computational efficiency, with the latter left for future investigation.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "tasks",
                    "asr",
                    "audiovisual",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage of our pretraining strategy focuses on integrating visual information into the speech foundation model to enable SFM-AV to process multimodal inputs and generate unified audio-visual representations. We employ a cross-modal knowledge distillation approach where clean audio representations from the original SFM serve as supervision signals for training SFM-AV with corrupted audio-visual inputs, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.3 Prompting the LLM for USR &#8227; 3.1 Model structure &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "visual",
                    "pretraining",
                    "audiovisual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFM-AV pretraining, we apply several data corruption strategies to promote effective audio-visual fusion when predicting clean audio representations. First, the input audio is corrupted by randomly mixing it with background noise, resulting in <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math>. Both the noisy audio <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m2\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math> and the video <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m3\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> are then corrupted using span-based masking. Specifically, for each modality, we randomly select a starting frame and replace a consecutive span of frames with zeros starting from that point.\nIn addition, we adopt modality dropout during training. With probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math>, only the video input is provided to the model, while the audio input is replaced with zeros. With probability <math alttext=\"1-p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msub><mi>p</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">1-p_{v}</annotation></semantics></math>, both audio and video inputs are used.\nAfter applying noise mixing, span masking, and modality dropout, SFM-AV processes the corrupted inputs to generate output representations <math alttext=\"H^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m6\" intent=\":literal\"><semantics><msup><mi>H</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">H^{o}</annotation></semantics></math>, which is summarized as:</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "model",
                    "pretraining",
                    "data",
                    "audiovisual",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> represents learnable projection parameters.\nThis first-stage training leverages unlabeled audio-visual speech data for unsupervised pretraining. And the large language model are not involved at this stage.\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "pretraining",
                    "model",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pretraining stage, we connect the SFM-AV encoder with an LLM to perform unified speech recognition across multiple modalities,\nas shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F3\" title=\"Figure 3 &#8227; 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The model pathway varies depending on the target task: for VSR and AVSR, we utilize SFM representations enhanced with visual information through the injection module. For ASR, we bypass the visual injection module and employ the original SFM to process audio-only inputs.\nTo enhance model robustness, we employ a modality dropout strategy with probabilities <math alttext=\"p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{v}^{\\prime}</annotation></semantics></math>, <math alttext=\"p_{a}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{a}^{\\prime}</annotation></semantics></math>, and <math alttext=\"1-p_{a}^{\\prime}-p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>&#8722;</mo><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">1-p_{a}^{\\prime}-p_{v}^{\\prime}</annotation></semantics></math> for visual-only, audio-only, and audio-visual combined inputs, respectively. The instruction prompts are selected accordingly to match the input modality configuration.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "model",
                    "visual",
                    "pretraining",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary training loss is cross-entropy <math alttext=\"L_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{CE}</annotation></semantics></math> for text prediction by the LLM. Additionally, we incorporate an auxiliary CTC loss to optimize the encoder directly. This requires connecting the encoder to an additional feed-forward projection layer for logit prediction. Initial experiments used the same vocabulary as the LLM; however, convergence proved challenging due to the large vocabulary size (e.g., &#160;152k tokens for Qwen 2.5). Therefore, we adopted a separate, smaller 1k vocabulary specifically for the CTC loss, which significantly improved training stability.\nThe total loss combines both objectives:</p>\n\n",
                "matched_terms": [
                    "used",
                    "encoder",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> controls the relative importance of the auxiliary CTC loss.\nDuring finetuning, we jointly optimize the LoRA parameters of the LLM, the feed-forward adaptor, and the visual injection module, while keeping the core SFM parameters frozen.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nFor pretraining, we utilized the LRS3 dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mmai.io/datasets/lip_reading/\" title=\"\">https://mmai.io/datasets/lip_reading/</a></span></span></span>, which comprises approximately 433 hours of English audio-visual speech data. We also employed a combination dataset of LRS3 and the English-only version of VoxCeleb2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html\" title=\"\">https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html</a></span></span></span>, curated by Shi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, totaling approximately 1759 hours of audio-visual speech data. The VoxCeleb data was transcribed using the Whisper-large-v2 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span>.\nFor finetuning, we adopted both a 30-hour subset and the full 433-hour training set of LRS3. Additionally, the model can be finetuned on the complete 1759-hour audio-visual speech dataset. A validation set of 1000 utterances was reserved, and results were reported on the LRS3 test set. We used the facial images provided in the original datasets without cropping to the lip region-of-interest (ROI)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "hours",
                    "model",
                    "lrs3",
                    "pretraining",
                    "data",
                    "audiovisual",
                    "test",
                    "results",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Noise augmentation.</span>\nThe MUSAN dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/17/\" title=\"\">https://www.openslr.org/17/</a></span></span></span> was employed for noise augmentation. Following the protocol outlined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib40\" title=\"\">40</a>]</cite>, we sampled and added noise to the audio input of the student model during training. To evaluate our proposed method for AVSR under noisy conditions, we constructed test sets with various noise types, including music, natural sounds, speech, and babble, at signal-to-noise ratios (SNRs) of -10, -5, 0, 5, and 10 dB.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "model",
                    "constructed",
                    "test",
                    "noisy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model configurations.</span>\nOur experiments employed a default configuration consisting of three main components: the DistillAV model as the visual encoder, WavLM-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> (317M parameters) as the speech foundation model (SFM), and Qwen 2.5-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2.5-7B</a></span></span></span> as the large language model. The visual encoder offered two variants: Base (103M parameters) and Large (325M parameters). Following our previous study, we denote the proposed method as UASR-LLM-B and UASR-LLM-L accordingly.\nThe visual injection modules were configured with 8 heads, 512 hidden dimensions, and 256 dimensions for both cross-attention and feed-forward networks, totaling approximately 58M parameters. The two-layer feed-forward network adapter that connects the SFM and LLMs contains around 10M parameters. LoRA weights were configured with a rank of 16, resulting in 12M total parameters.\nTo demonstrate the generalization capability of our method, we explored additional speech foundation models: the open-source Whisper-medium&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> and an internal iFLYTEK-Speech model, both containing approximately 300M parameters comparable to WavLM-large. We also evaluated ChatGLM v3-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/THUDM/chatglm3-6b\" title=\"\">https://huggingface.co/THUDM/chatglm3-6b</a></span></span></span> as an alternative large language model.</p>\n\n",
                "matched_terms": [
                    "uasrllmb",
                    "model",
                    "visual",
                    "encoder",
                    "uasrllml",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WavLM model generates representations at 50 fps, double the frame rate of visual representations. We set relative positional encoding according to the real-time interval between audio and visual frames using sinusoidal positional embedding. The parameter <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E4\" title=\"In 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> was set to 8 for WavLM-based SFM and 1 for Whisper and iFLYTEK-Speech-based SFM, following our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "rate",
                    "model",
                    "visual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pretraining, 80% of audio features and 30% of video features were masked for the student model, with modality dropout probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math> set to 0.5. We employed the Adam optimizer with a learning rate schedule that linearly warmed up to 0.0005 over the initial 5% of updates, maintained this rate for 85% of updates, then exponentially decayed over the remaining 10%. The total number of updates was 100k and 200k for 433h and 1759h pretraining data, respectively. During finetuning, modality dropout probabilities <math alttext=\"p^{\\prime}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{v}</annotation></semantics></math> and <math alttext=\"p^{\\prime}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{a}</annotation></semantics></math> were set to 0.5 and 0.25, respectively. For the auxiliary CTC loss, we utilized subword units with a vocabulary size of 1000 as targets, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E7\" title=\"In 3.2.2 Speech recognition finetuning &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> set to 0.25. The learning rate warmed up to 0.0002 over the initial one-third of updates, followed by exponential decay. The total number of updates was 30k, 60k, and 120k for 30h, 433h, and 1759h finetuning data, respectively.</p>\n\n",
                "matched_terms": [
                    "set",
                    "rate",
                    "1759h",
                    "model",
                    "pretraining",
                    "data",
                    "433h",
                    "30h"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing ASR and AVSR performance on our noisy test set reveals that incorporating visual information significantly improves speech recognition robustness.\nThis substantial WER reduction demonstrates that our method successfully fuses speech foundation models with visual representations to enhance noise-invariant capabilities.\nThe next section provides detailed AVSR results under noisy conditions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "visual",
                    "asr",
                    "test",
                    "results",
                    "noisy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness under noisy conditions, we constructed four test sets by mixing the clean audio from the LRS3 test set with different noise types: speech, babble, natural, and music, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.SS1\" title=\"4.1 Implementation details &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. We compared our approach against multiple baselines, including two audio-only SFM models (WavLM and iFLYTEK-Speech) and three AVSR models (AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and DistillAV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>). All audio-only models were finetuned on the LRS3 dataset. The experiments utilized 433h data for pretraining, with all models adopting the Base configuration. We used either 30h or 433h of LRS3 data for finetuning.\nThe experimental results are summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.F4\" title=\"Figure 4 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "noisy",
                    "test",
                    "constructed",
                    "lrs3",
                    "pretraining",
                    "data",
                    "433h",
                    "av2vecmlm",
                    "30h",
                    "results",
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed UASR-LLM method (red line) consistently outperforms all baselines across different noise types and SNR levels, with particularly pronounced advantages in severe noise conditions below 0 dB. Among the four noise conditions, babble noise proves most challenging, with all methods exhibiting higher WER at low SNRs, particularly below 0 dB. However, the proposed approach maintains substantially lower error rates than competing methods even in these challenging low-SNR scenarios. The results highlight the effectiveness of incorporating visual information, as audio-only models (WavLM and iFLYTEK-Speech) degrade rapidly in noisy conditions compared to AVSR approaches.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "wer",
                    "visual",
                    "results",
                    "methods",
                    "noisy",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization capability of our approach, we conducted experiments with different speech feature models (SFMs) and large language models (LLMs). Specifically, we replaced the WavLM-based SFM with two supervised learning alternatives: Whisper and iFLYTEK-Speech. Additionally, we compared ChatGLM v3-6B with Qwen 2.5-7B as the backbone LLM.\nFor training, we used 433 hours of data for the visual injection pretraining stage and finetune the models on both 30 hours and 433 hours of the LRS3 dataset. We adopted a pretrained visual encoder (Base version) trained on 1759 hours of data. The experimental results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T3\" title=\"Table 3 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "lrs3",
                    "visual",
                    "pretraining",
                    "data",
                    "encoder",
                    "results",
                    "used",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first analyze the influence of different SFMs using the same LLM and identical amounts of finetuning data. The results show that iFLYTEK-Speech achieves the best ASR performance under both clean and noisy conditions, while WavLM performs best on the VSR task. For AVSR, WavLM achieves superior results on the noisy testset, whereas the clean AVSR results do not exhibit a clear pattern across different SFMs.\nWhen examining the influence of LLMs with the same SFM, we observe that ChatGLM v3-6B slightly outperforms Qwen 2.5-7B when using low-resource data (30 hours) for finetuning. However, this trend reverses with higher-resource data (433 hours), where Qwen 2.5-7B demonstrates better performance.\nDespite performance variations across different SFM and LLM combinations, our UASR-LLM consistently achieves significant improvements in recognition accuracy compared to baselines, demonstrating the strong generalization ability of our proposed method.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "same",
                    "hours",
                    "lowresource",
                    "data",
                    "asr",
                    "results",
                    "noisy",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our ablation studies, we employed the Base version of the visual encoder pretrained on 1759h of audio-visual data. We used either 30h or 433h of LRS3 data for finetuning. Our ablation studies examined several key design components of our method, including visual injection pretraining, parameter sharing for USR, and the integration of LLM.</p>\n\n",
                "matched_terms": [
                    "usr",
                    "1759h",
                    "lrs3",
                    "visual",
                    "pretraining",
                    "data",
                    "encoder",
                    "audiovisual",
                    "30h",
                    "433h",
                    "used",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T4\" title=\"Table 4 &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the experimental results of our visual injection pretraining ablation study. The results demonstrate that omitting the proposed visual injection pretraining stage (<span class=\"ltx_text ltx_font_bold\">0h</span> for VIPT data) leads to degraded performance in both VSR and AVSR tasks. Furthermore, increasing the amount of pretraining data during the visual injection pretraining stage yields additional improvements in VSR and AVSR performance. These superior results for VSR and AVSR tasks validate the effectiveness of our pretraining stage in processing visual information.\nInterestingly, for the ASR task, the model without the pretraining stage achieves the best results. Since we froze all SFM parameters throughout training, the audio processing capability of SFM itself remained unaffected by the visual injection pretraining stage. This phenomenon may be attributed to our parameter sharing strategy for USR in LLMs, where enhanced AVSR and VSR performance appears to cause a slight degradation in ASR performance&#8212;a trade-off we explore in the following section.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "usr",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "data",
                    "asr",
                    "results",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our parameter sharing approach for realizing USR in a single model with the traditional method of training separate models for each task. The results (<span class=\"ltx_text ltx_font_bold\">w/o shared parameters</span>) are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table shows that training separate models leads to better ASR performance but worse VSR performance. Overall, the proposed USR method achieves comparable performance to the traditional approach that optimizes individual models for each task. These results demonstrate the effectiveness of our method for realizing USR, significantly reducing both training and deployment resources while maintaining strong performance across all tasks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "usr",
                    "shared",
                    "model",
                    "tasks",
                    "results",
                    "asr",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our proposed method based on a decoder-only LLM with the traditional approach using a 6-layer Transformer-based decoder with cross-attention. The results (<span class=\"ltx_text ltx_font_bold\">w/o LLM</span>) are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table demonstrates that our proposed method employing LLMs significantly outperforms the traditional Transformer decoder. This validates that the strong contextual and language modeling capabilities of LLMs, learned from large text corpora, can be effectively transferred to enhance speech recognition performance, which has also been demonstrated in previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UASR-LLM, a novel approach that adapts speech foundation models (SFMs) for unified speech recognition, encompassing ASR, VSR, and AVSR within a single framework using a decoder-only LLM for text prediction.\nOur method augments the SFM with a visual encoder and inserts visual injection modules into each SFM block to fuse visual representations with audio features. We introduce a visual injection pretraining stage where corrupted audio-visual inputs are processed to predict clean audio representations, followed by speech recognition finetuning with modality dropout. The approach leverages the complementary strengths of SFMs pretrained on large-scale audio corpora and LLMs pretrained on extensive text data.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "pretraining",
                    "data",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrated the effectiveness of UASR-LLM, achieving superior performance compared to state-of-the-art baselines across ASR, VSR, and AVSR tasks under both clean and noisy conditions. Our method significantly improved recognition robustness across varying signal-to-noise ratios and noise types. Experiments with different SFM and LLM combinations show consistent improvements over baselines, demonstrating the generalization capability of our framework. Ablation studies validated the importance of visual injection pretraining, parameter sharing, and LLM integration.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "visual",
                    "pretraining",
                    "tasks",
                    "results",
                    "asr",
                    "noisy",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite strong performance, our method has several limitations that warrant future investigation. First, while LLM integration leads to substantial speech recognition improvements, it also requires significant computational resources and memory footprint. This computational burden may limit practical deployment, suggesting the need for more efficient approaches such as dynamic pooling of adjacent audio-visual representations to reduce input tokens or exploring lightweight LLM architectures. Second, our method still lags behind previous lipreading studies that employ much larger audio-visual datasets, indicating that the combination of SFMs and LLMs cannot fully compensate for the shortage of large-scale audio-visual training corpora. This limitation fundamentally affects the visual representation extraction capacity of the visual encoder. We anticipate that employing visual encoders pretrained on large-scale visual datasets could further improve our results. Future work will focus on addressing these challenges to enhance both computational efficiency and performance of unified speech recognition systems.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "encoder",
                    "results",
                    "audiovisual",
                    "our"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Word error rate (WER) (%) on LRS3 test set using high-resource (433h or 1759h) labeled audio-visual data. ”Shared” indicates whether the same model is used across ASR, VSR, and AVSR tasks. (N) denotes results on our constructed noisy test set.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Methods</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Unlab.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Lab.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Shared</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR (N)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR (N)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Makino et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib42\" title=\"\">42</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Auto-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">1.9kh</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">23.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.0</span></td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Auto-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">3.5kh</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">19.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.9</span></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LP-Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib43\" title=\"\">43</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">100k</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">12.8</span></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.9</span></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AV-HuBERT-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"9\">433h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"9\">433h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">BRAVEn-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">36.0</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-data2vec-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">39.0</td>\n<td class=\"ltx_td ltx_align_center\">2.0</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">34.4</td>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n<td class=\"ltx_td ltx_align_center\">2.5</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">34.3</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">1.6</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">34.3</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">44.8</td>\n<td class=\"ltx_td ltx_align_center\">9.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">31.5</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">1.6</td>\n<td class=\"ltx_td ltx_align_center\">41.8</td>\n<td class=\"ltx_td ltx_align_center\">8.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-B (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">28.6</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">38.1</td>\n<td class=\"ltx_td ltx_align_center\">3.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-L (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">26.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">36.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AV-HuBERT-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"13\">1759h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"13\">433h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VATLM-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib41\" title=\"\">41</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">BRAVEn-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">26.6</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-data2vec-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">32.7</td>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AV-data2vec-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">28.5</td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">1.3</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">26.5</td>\n<td class=\"ltx_td ltx_align_center\">1.6</td>\n<td class=\"ltx_td ltx_align_center\">1.3</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">22.3</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">31.4</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\">42.2</td>\n<td class=\"ltx_td ltx_align_center\">7.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DistillAV-L&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">26.2</td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">1.3</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n<td class=\"ltx_td ltx_align_center\">6.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Llama-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">25.3</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-B (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">23.4</td>\n<td class=\"ltx_td ltx_align_center\">0.91</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.73</span></td>\n<td class=\"ltx_td ltx_align_center\">37.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-B* (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">23.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">37.0</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-L (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">21.4</span></td>\n<td class=\"ltx_td ltx_align_center\">0.87</td>\n<td class=\"ltx_td ltx_align_center\">0.80</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">35.4</span></td>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Llama-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\">1759h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\">1759h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RAVEn w/ ST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">23.1</td>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">USR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">21.5</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM-L (Ours)</em></td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">20.9</span></td>\n<td class=\"ltx_td ltx_align_center\">0.84</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">37.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"9\">*Visual encoder pretraining used 1759 hours of data, visual injection pretraining used 433 hours.</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "avsr",
            "word",
            "wer",
            "100k",
            "usrl",
            "usrb",
            "visual",
            "denotes",
            "31k",
            "av2vecmlm",
            "uasrllml",
            "highresource",
            "llamaavsr",
            "error",
            "our",
            "unlab",
            "raven",
            "hours",
            "rate",
            "bravenb",
            "distillavb",
            "19kh",
            "tasks",
            "encoder",
            "audiovisual",
            "test",
            "methods",
            "pretraining",
            "used",
            "noisy",
            "makino",
            "35kh",
            "avdata2vecb",
            "vatlml",
            "uasrllmb",
            "across",
            "usr",
            "shared",
            "lpconformer",
            "constructed",
            "asr",
            "indicates",
            "results",
            "distillavl",
            "labeled",
            "set",
            "ours",
            "same",
            "avhubertl",
            "autoavsr",
            "1759h",
            "bravenl",
            "model",
            "lrs3",
            "lab",
            "avdata2vecl",
            "433h",
            "data",
            "whether",
            "”shared”",
            "injection",
            "vsr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> present experimental results for models finetuned with low-resource (30h) and high-resource (433h or 1759h) labeled data, respectively. The unlabeled data refers to audio-visual data used for pretraining the DistillAV-based visual encoder and visual injection pretraining, while the labeled data denotes audio-visual data employed for speech recognition finetuning.</p>\n\n",
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms most baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Although underperforming Llama-AVSR in ASR, our approach provides a unified framework effective across all three tasks. The superior VSR performance particularly demonstrates the effectiveness of &#8221;reprogramming&#8221; WavLM for lipreading with LLMs, consistent with previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWith 433h labeled data, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method achieves significant accuracy improvements. Compared to previous studies using comparable audio-visual data, our approach demonstrates superior performance across all tasks under both clean and noisy conditions. With 1759h labeled data, our method achieves WERs of 20.9%, 0.84%, and 0.69% on the clean LRS3 test set, rivaling or surpassing state-of-the-art methods. However, our VSR results still lag behind LP-Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib43\" title=\"\">43</a>]</cite>, which uses approximately 100k hours of audio-visual data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework. While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.\nThis paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified visual speech recognition (VSR), automatic speech recognition (ASR), and audio-visual speech recognition (AVSR) tasks by leveraging large language models (LLMs) as text decoders. Our approach introduces visual representations into multiple SFM layers through visual injection modules, enabling multimodal input processing and unified hidden representations. The augmented SFMs connect with decoder-only LLMs via a feed-forward adaptor, where concatenated representations and instruction prompts guide speech transcription.\nWe propose a two-stage training strategy: visual injection pretraining followed by speech recognition finetuning. SFM parameters remain frozen throughout training, with only visual injection modules optimized initially, and LLMs finetuned using LoRA parameters subsequently.\nExperimental results demonstrate superior performance over state-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Ablation studies confirm generalization across various SFMs and LLMs, validating the proposed training strategy.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "audiovisual",
                    "asr",
                    "results",
                    "noisy",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech recognition represents a fundamental challenge in pattern recognition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib1\" title=\"\">1</a>]</cite>, with recent advances driven by speech foundation models (SFMs) such as WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> and Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite>.\nThese large-scale models, trained on extensive speech datasets, have demonstrated remarkable performance in automatic speech recognition (ASR). However, environmental noise remains a critical limitation that can significantly compromise recognition accuracy, particularly in real-world deployment scenarios.\nTo overcome this challenge, researchers have developed complementary approaches including visual speech recognition (VSR, also known as lipreading)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib6\" title=\"\">6</a>]</cite> and audio-visual speech recognition (AVSR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib8\" title=\"\">8</a>]</cite>, which leverage lip movement information either independently or in combination with audio signals. AVSR systems demonstrate substantial improvements in robustness under noisy conditions, making them increasingly valuable for practical applications such as intelligent home systems, online conferencing, and service robotics.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "asr",
                    "audiovisual",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AVSR models typically adopt a sequence-to-sequence architecture nowadays, including recurrent neural network transducer (RNN-T)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite> or hybrid connectionist temporal classification (CTC) and attention architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib11\" title=\"\">11</a>]</cite>.\nRecently, audio-visual self-supervised learning has also been proposed to leverage\nunlabeled audio-visual data for pretraining, such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, REVAn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>, and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>.\nWith a further finetuning stage on few labeled data, these methods achieve a strong VSR and AVSR performance.\nBuilding on these progresses, Haliassos et al. introduced unified speech recognition (USR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>, which consolidates ASR, VSR, and AVSR tasks within a single model framework. This unified approach offers compelling advantages by eliminating the need to construct separate models for each modality, thereby significantly reducing training and deployment costs. The conceptual foundation is particularly appealing, as audio and visual lip movements represent complementary signal views of the same articulatory process during speech production.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "same",
                    "usr",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "data",
                    "audiovisual",
                    "methods",
                    "asr",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have explored utilizing SFMs for VSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib16\" title=\"\">16</a>]</cite> and AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>. Our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> proposed using SFMs for knowledge distillation in audio-visual self-supervised pretraining, achieving competitive performance across VSR, ASR, and AVSR benchmarks.\nHowever, the knowledge distillation approach may inherently limit performance compared to direct utilization of SFMs, particularly in high signal-to-noise ratio (SNR) scenarios where the original models excel.\nSeveral approaches have focused on the direct utilization of SFMs for audio-visual tasks. Lip2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite> proposed reprogramming a wav2vec 2.0 model for lipreading by feeding transformed visual representations to the audio model. Whisper-flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib19\" title=\"\">19</a>]</cite> adapted the Whisper model for AVSR by incorporating an additional visual encoder to process visual information alongside audio inputs.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its success, most previous works address VSR, AVSR, and ASR tasks separately.\nIn this study, we explore tackling these tasks within a single unified model.\nFurthermore, we aim to leverage the rich knowledge embedded in both speech foundation models (SFMs) and large language models (LLMs) to design a general framework that achieves robust USR performance.\nWe propose <span class=\"ltx_text ltx_font_bold\">UASR-LLM</span> (<span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">R</span>ecognition with <span class=\"ltx_text ltx_font_bold\">L</span>arge <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">M</span>odels), a novel framework that adapts frozen speech foundation models to unified VSR, ASR, and AVSR tasks by leveraging LLMs as text decoders.\nOur approach employs a visual encoder from our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> to extract visual representations, which are then integrated into each block of the SFMs through visual injection modules. The SFM outputs are processed by an adapter to obtain hidden embeddings, which are subsequently fed into LLMs for conducting speech recognition across all modalities.\nTo enable SFMs to process visual and audio-visual inputs, we introduce a visual injection pretraining stage based on cross-modal knowledge distillation loss. The model then undergoes a second training stage for speech recognition, incorporating random modality dropout guided by corresponding instructions. Throughout the entire training process, both the SFM and visual encoder parameters remain frozen, while the LLM is finetuned using a small set of Low-Rank Adaptation (LoRA) weights&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib20\" title=\"\">20</a>]</cite>. This approach enables memory-efficient finetuning while preserving the generalization capabilities of the original models.\nBy utilizing the SFM as a unified backbone for processing multimodal audio and visual inputs, our framework facilitates knowledge transfer from audio to video data and unifies the hidden representations of both modalities. This provides a well-aligned output space for LLMs to generate transcriptions, seamlessly achieving VSR, ASR, and AVSR within a single model.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "across",
                    "usr",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "data",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates that UASR-LLM consistently outperforms state-of-the-art baselines across all three tasks when trained with comparable amounts of audio-visual speech data. On the LRS3 test set, UASR-LLM achieves word error rates of 20.9%, 0.84%, and 0.69% for VSR, ASR, and AVSR, respectively. Under noisy conditions, our method exhibits substantial robustness, significantly reducing WER compared to baseline approaches. Furthermore, we demonstrate that our approach generalizes effectively across different speech foundation models and large language models, with all configurations outperforming baseline methods. Finally, we validate our training strategy and confirm the effectiveness of performing unified speech recognition within a single model architecture.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "across",
                    "word",
                    "wer",
                    "model",
                    "methods",
                    "lrs3",
                    "data",
                    "tasks",
                    "audiovisual",
                    "test",
                    "asr",
                    "noisy",
                    "error",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent years have witnessed the emergence of various speech foundation models, which can be broadly categorized into two paradigms: self-supervised and supervised approaches.\nAmong SSL-based speech foundation models,\nHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> utilizes k-means clustering to generate pseudo-class labels for model training using a mask prediction loss,\nwith iterative refinement through subsequent clustering and mask prediction steps.\nWavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> improves HuBERT by using a more diverse pretraining dataset and performs speech denoising modeling during pretraining.\nOn the other hand, Whisper family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> exemplifies supervised learning-based speech foundation model. Trained on large-scale multilingual and multitask labeled data, it demonstrates robust performance in ASR and speech translation tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pretraining",
                    "tasks",
                    "data",
                    "asr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Visual representation learning is crucial for the performance of VSR and AVSR tasks, which can be broadly categorized into supervised and self-supervised approaches.\nSupervised methods leverage labeled&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib22\" title=\"\">22</a>]</cite> or weakly labeled audio-visual speech data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite> to optimize representation extraction and text decoding in an end-to-end manner through label prediction. Several auxiliary tasks have been proposed to enhance visual encoder learning, including articulation feature prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>]</cite> and distillation from acoustic models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib26\" title=\"\">26</a>]</cite>.\nSelf-supervised methods utilize unlabeled audio-visual data with pretext tasks such as masked prediction loss functions. Representative approaches include AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, RAVen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>, AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and AV-data2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>. Our previous work proposed a joint audio-visual representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> that leverages cross-modal knowledge distillation from pretrained SFMs.\nThis work adopts our previously pretrained model for visual representation extraction, which can be viewed as an extension of our previous study. Here, we further leverage SFMs directly for processing audio-visual data, moving beyond the distillation approach to direct utilization of foundation models.</p>\n\n",
                "matched_terms": [
                    "vsr",
                    "avsr",
                    "model",
                    "visual",
                    "data",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "methods",
                    "labeled",
                    "raven",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have attracted significant attention for speech recognition applications, with research broadly falling into two categories: multimodal LLMs for universal AI assistants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib31\" title=\"\">31</a>]</cite> and LLMs for enhancing traditional speech recognition tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib32\" title=\"\">32</a>]</cite>. Our work belongs to the latter category.\nRecent approaches for integrating LLMs with speech recognition typically employ lightweight adapters to connect frozen speech encoders with LLMs. Representative works include joint speech-language models with small adapters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib32\" title=\"\">32</a>]</cite> and Q-Former-based connections&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib33\" title=\"\">33</a>]</cite>. Some studies explore discrete speech representations as LLM inputs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib34\" title=\"\">34</a>]</cite>.\nLLMs have also been adopted for visual and audio-visual speech recognition. VSP-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib35\" title=\"\">35</a>]</cite> focuses on visual speech recognition, while Llama-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> uses separate audio and visual encoders with temporal concatenation as LLM input. MMS-Llama&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite> employs audio-visual fusion within encoders and Q-former adapters for improved efficiency.\nOur work differs by proposing a unified speech recognition (USR) approach that processes both audio and visual modalities within a single model architecture, rather than targeting specific modalities separately.</p>\n\n",
                "matched_terms": [
                    "usr",
                    "model",
                    "visual",
                    "tasks",
                    "audiovisual",
                    "llamaavsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our UASR-LLM method comprises two main components: an audio-visual adapted speech foundation model (SFM-AV)\nthat processes both audio and visual modalities and encodes them into a unified representational space,\nand a large language model that decodes the encoded representations into text.\nThe SFM-AV consists of a visual encoder and a speech foundation model enhanced with visual injection modules.\nOur model architecture is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe training of our model follows a two-stage process: in the first pretraining stage,\nthe SFM is adapted to encode both audio and visual information into a unified hidden space;\nin the second finetuning stage, the adapted SFM is connected to the LLM to enable unified speech recognition.\nThe detailed model architecture and training strategy are described in the following sections.</p>\n\n",
                "matched_terms": [
                    "model",
                    "visual",
                    "pretraining",
                    "encoder",
                    "audiovisual",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let lip video be <math alttext=\"V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}</annotation></semantics></math>, where <math alttext=\"T_{v},H,W,\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">T_{v},H,W,</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> represent the temporal, height, width, and channel dimensions, respectively. The visual encoder <math alttext=\"\\text{E}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mtext>E</mtext><mi>v</mi></msub><annotation encoding=\"application/x-tex\">\\text{E}_{v}</annotation></semantics></math> processes the visual input into hidden representations:\n<math alttext=\"H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>v</mi></msub><mo>=</mo><mrow><msub><mtext>E</mtext><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}</annotation></semantics></math>,\nwhere <math alttext=\"D_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">D_{v}</annotation></semantics></math> is the feature dimension of the encoded representations.\nIn this work, we adopt the visual component of the pretrained DistillAV model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> as our visual encoder. DistillAV is designed to distill knowledge from speech foundation models (SFMs) for joint audio-visual representation learning. Unlike SFMs, which are trained on large-scale speech-only corpora, DistillAV is trained on relatively smaller audio-visual datasets. As a result, its performance in audio-only tasks is inferior to that of its teacher models, as shown in our previous ASR evaluations. However, DistillAV exhibits strong visual modeling capabilities and consistently outperforms state-of-the-art lipreading models such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite> and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>. Therefore, we use the pretrained DistillAV model to extract representations from lip video inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "visual",
                    "tasks",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual injection module is introduced to incorporate visual information into the pretrained SFM. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block of the SFM, we employ the audio representations <math alttext=\"H_{a}^{i}\\in\\mathbb{R}^{T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>H</mi><mi>a</mi><mi>i</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{a}^{i}\\in\\mathbb{R}^{T\\times D}</annotation></semantics></math> as the query and attend to visual representations through a cross-attention mechanism. To enable the model to exploit relative position information between the two modalities, we incorporate relative position embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib38\" title=\"\">38</a>]</cite>.\nThe output is subsequently processed through a feed-forward network and then fed into the next SFM block. Both the cross-attention and feed-forward network are augmented with residual connections and tanh gating, following the approach in Whisper-Flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "model",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, our visual injection module operates as follows:</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{MHA}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mtext>MHA</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{MHA}()</annotation></semantics></math> and <math alttext=\"\\text{FFN}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mtext>FFN</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{FFN}()</annotation></semantics></math> represent the multi-head attention and feed-forward network modules, respectively. <math alttext=\"D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>T</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}</annotation></semantics></math>, where <math alttext=\"d_{m,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{m,n}</annotation></semantics></math> represents the relative position encoding between the <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>-th audio frame and the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-th video frame. <math alttext=\"g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m7\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">g_{i}</annotation></semantics></math> and <math alttext=\"g_{i}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>i</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">g_{i}^{\\prime}</annotation></semantics></math> are trainable gating parameters, which are initialized to zero.\nIt provides an initialization state identical to the original SFMs, which helps stabilize the training.\nThe visual injection modules are inserted before each block of the SFM, while the SFM parameters remain frozen throughout training.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The output representations from the SFM-AV are transformed through a two-layer feed-forward adaptor to match the embedding dimension of large language models, serving as continuous audio-visual tokens for prompting the LLM to generate transcribed text. We employ task-specific instructions that prepend the audio-visual tokens according to the unified speech recognition task. The instruction prompt follows the template &#8220;Transcribe the [].&#8221;, where the placeholder [] is filled with &#8220;audio&#8221;, &#8220;video&#8221;, or &#8220;audio and video&#8221; for ASR, VSR, and AVSR tasks, respectively.\nFor efficient LLM finetuning, we adopt the low-rank adaptation (LoRA) method. The low-rank parameters are applied to adapt all blocks&#8217; self-attention components as well as the output projection layer for predicting text logits. Unlike previous approaches such as Llama-AVSR&#8217;s frame-compression&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> or MMS-Llama&#8217;s Q-former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite>, our method does not employ frame reduction techniques. This design choice reflects our primary focus on improving recognition performance rather than computational efficiency, with the latter left for future investigation.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "tasks",
                    "asr",
                    "audiovisual",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage of our pretraining strategy focuses on integrating visual information into the speech foundation model to enable SFM-AV to process multimodal inputs and generate unified audio-visual representations. We employ a cross-modal knowledge distillation approach where clean audio representations from the original SFM serve as supervision signals for training SFM-AV with corrupted audio-visual inputs, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.3 Prompting the LLM for USR &#8227; 3.1 Model structure &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "visual",
                    "pretraining",
                    "audiovisual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFM-AV pretraining, we apply several data corruption strategies to promote effective audio-visual fusion when predicting clean audio representations. First, the input audio is corrupted by randomly mixing it with background noise, resulting in <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math>. Both the noisy audio <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m2\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math> and the video <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m3\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> are then corrupted using span-based masking. Specifically, for each modality, we randomly select a starting frame and replace a consecutive span of frames with zeros starting from that point.\nIn addition, we adopt modality dropout during training. With probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math>, only the video input is provided to the model, while the audio input is replaced with zeros. With probability <math alttext=\"1-p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msub><mi>p</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">1-p_{v}</annotation></semantics></math>, both audio and video inputs are used.\nAfter applying noise mixing, span masking, and modality dropout, SFM-AV processes the corrupted inputs to generate output representations <math alttext=\"H^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m6\" intent=\":literal\"><semantics><msup><mi>H</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">H^{o}</annotation></semantics></math>, which is summarized as:</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "model",
                    "pretraining",
                    "data",
                    "audiovisual",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> represents learnable projection parameters.\nThis first-stage training leverages unlabeled audio-visual speech data for unsupervised pretraining. And the large language model are not involved at this stage.\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "pretraining",
                    "model",
                    "audiovisual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pretraining stage, we connect the SFM-AV encoder with an LLM to perform unified speech recognition across multiple modalities,\nas shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F3\" title=\"Figure 3 &#8227; 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The model pathway varies depending on the target task: for VSR and AVSR, we utilize SFM representations enhanced with visual information through the injection module. For ASR, we bypass the visual injection module and employ the original SFM to process audio-only inputs.\nTo enhance model robustness, we employ a modality dropout strategy with probabilities <math alttext=\"p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{v}^{\\prime}</annotation></semantics></math>, <math alttext=\"p_{a}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{a}^{\\prime}</annotation></semantics></math>, and <math alttext=\"1-p_{a}^{\\prime}-p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>&#8722;</mo><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">1-p_{a}^{\\prime}-p_{v}^{\\prime}</annotation></semantics></math> for visual-only, audio-only, and audio-visual combined inputs, respectively. The instruction prompts are selected accordingly to match the input modality configuration.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "model",
                    "visual",
                    "pretraining",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary training loss is cross-entropy <math alttext=\"L_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{CE}</annotation></semantics></math> for text prediction by the LLM. Additionally, we incorporate an auxiliary CTC loss to optimize the encoder directly. This requires connecting the encoder to an additional feed-forward projection layer for logit prediction. Initial experiments used the same vocabulary as the LLM; however, convergence proved challenging due to the large vocabulary size (e.g., &#160;152k tokens for Qwen 2.5). Therefore, we adopted a separate, smaller 1k vocabulary specifically for the CTC loss, which significantly improved training stability.\nThe total loss combines both objectives:</p>\n\n",
                "matched_terms": [
                    "used",
                    "encoder",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> controls the relative importance of the auxiliary CTC loss.\nDuring finetuning, we jointly optimize the LoRA parameters of the LLM, the feed-forward adaptor, and the visual injection module, while keeping the core SFM parameters frozen.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nFor pretraining, we utilized the LRS3 dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mmai.io/datasets/lip_reading/\" title=\"\">https://mmai.io/datasets/lip_reading/</a></span></span></span>, which comprises approximately 433 hours of English audio-visual speech data. We also employed a combination dataset of LRS3 and the English-only version of VoxCeleb2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html\" title=\"\">https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html</a></span></span></span>, curated by Shi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, totaling approximately 1759 hours of audio-visual speech data. The VoxCeleb data was transcribed using the Whisper-large-v2 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span>.\nFor finetuning, we adopted both a 30-hour subset and the full 433-hour training set of LRS3. Additionally, the model can be finetuned on the complete 1759-hour audio-visual speech dataset. A validation set of 1000 utterances was reserved, and results were reported on the LRS3 test set. We used the facial images provided in the original datasets without cropping to the lip region-of-interest (ROI)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "hours",
                    "model",
                    "lrs3",
                    "pretraining",
                    "data",
                    "audiovisual",
                    "test",
                    "results",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Noise augmentation.</span>\nThe MUSAN dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/17/\" title=\"\">https://www.openslr.org/17/</a></span></span></span> was employed for noise augmentation. Following the protocol outlined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib40\" title=\"\">40</a>]</cite>, we sampled and added noise to the audio input of the student model during training. To evaluate our proposed method for AVSR under noisy conditions, we constructed test sets with various noise types, including music, natural sounds, speech, and babble, at signal-to-noise ratios (SNRs) of -10, -5, 0, 5, and 10 dB.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "model",
                    "constructed",
                    "test",
                    "noisy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model configurations.</span>\nOur experiments employed a default configuration consisting of three main components: the DistillAV model as the visual encoder, WavLM-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> (317M parameters) as the speech foundation model (SFM), and Qwen 2.5-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2.5-7B</a></span></span></span> as the large language model. The visual encoder offered two variants: Base (103M parameters) and Large (325M parameters). Following our previous study, we denote the proposed method as UASR-LLM-B and UASR-LLM-L accordingly.\nThe visual injection modules were configured with 8 heads, 512 hidden dimensions, and 256 dimensions for both cross-attention and feed-forward networks, totaling approximately 58M parameters. The two-layer feed-forward network adapter that connects the SFM and LLMs contains around 10M parameters. LoRA weights were configured with a rank of 16, resulting in 12M total parameters.\nTo demonstrate the generalization capability of our method, we explored additional speech foundation models: the open-source Whisper-medium&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> and an internal iFLYTEK-Speech model, both containing approximately 300M parameters comparable to WavLM-large. We also evaluated ChatGLM v3-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/THUDM/chatglm3-6b\" title=\"\">https://huggingface.co/THUDM/chatglm3-6b</a></span></span></span> as an alternative large language model.</p>\n\n",
                "matched_terms": [
                    "uasrllmb",
                    "model",
                    "visual",
                    "encoder",
                    "uasrllml",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WavLM model generates representations at 50 fps, double the frame rate of visual representations. We set relative positional encoding according to the real-time interval between audio and visual frames using sinusoidal positional embedding. The parameter <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E4\" title=\"In 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> was set to 8 for WavLM-based SFM and 1 for Whisper and iFLYTEK-Speech-based SFM, following our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "rate",
                    "model",
                    "visual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pretraining, 80% of audio features and 30% of video features were masked for the student model, with modality dropout probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math> set to 0.5. We employed the Adam optimizer with a learning rate schedule that linearly warmed up to 0.0005 over the initial 5% of updates, maintained this rate for 85% of updates, then exponentially decayed over the remaining 10%. The total number of updates was 100k and 200k for 433h and 1759h pretraining data, respectively. During finetuning, modality dropout probabilities <math alttext=\"p^{\\prime}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{v}</annotation></semantics></math> and <math alttext=\"p^{\\prime}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{a}</annotation></semantics></math> were set to 0.5 and 0.25, respectively. For the auxiliary CTC loss, we utilized subword units with a vocabulary size of 1000 as targets, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E7\" title=\"In 3.2.2 Speech recognition finetuning &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> set to 0.25. The learning rate warmed up to 0.0002 over the initial one-third of updates, followed by exponential decay. The total number of updates was 30k, 60k, and 120k for 30h, 433h, and 1759h finetuning data, respectively.</p>\n\n",
                "matched_terms": [
                    "set",
                    "100k",
                    "rate",
                    "1759h",
                    "model",
                    "pretraining",
                    "433h",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing ASR and AVSR performance on our noisy test set reveals that incorporating visual information significantly improves speech recognition robustness.\nThis substantial WER reduction demonstrates that our method successfully fuses speech foundation models with visual representations to enhance noise-invariant capabilities.\nThe next section provides detailed AVSR results under noisy conditions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "visual",
                    "asr",
                    "test",
                    "results",
                    "noisy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness under noisy conditions, we constructed four test sets by mixing the clean audio from the LRS3 test set with different noise types: speech, babble, natural, and music, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.SS1\" title=\"4.1 Implementation details &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. We compared our approach against multiple baselines, including two audio-only SFM models (WavLM and iFLYTEK-Speech) and three AVSR models (AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and DistillAV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>). All audio-only models were finetuned on the LRS3 dataset. The experiments utilized 433h data for pretraining, with all models adopting the Base configuration. We used either 30h or 433h of LRS3 data for finetuning.\nThe experimental results are summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.F4\" title=\"Figure 4 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "noisy",
                    "lrs3",
                    "constructed",
                    "pretraining",
                    "433h",
                    "data",
                    "av2vecmlm",
                    "test",
                    "results",
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed UASR-LLM method (red line) consistently outperforms all baselines across different noise types and SNR levels, with particularly pronounced advantages in severe noise conditions below 0 dB. Among the four noise conditions, babble noise proves most challenging, with all methods exhibiting higher WER at low SNRs, particularly below 0 dB. However, the proposed approach maintains substantially lower error rates than competing methods even in these challenging low-SNR scenarios. The results highlight the effectiveness of incorporating visual information, as audio-only models (WavLM and iFLYTEK-Speech) degrade rapidly in noisy conditions compared to AVSR approaches.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "wer",
                    "visual",
                    "results",
                    "methods",
                    "noisy",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization capability of our approach, we conducted experiments with different speech feature models (SFMs) and large language models (LLMs). Specifically, we replaced the WavLM-based SFM with two supervised learning alternatives: Whisper and iFLYTEK-Speech. Additionally, we compared ChatGLM v3-6B with Qwen 2.5-7B as the backbone LLM.\nFor training, we used 433 hours of data for the visual injection pretraining stage and finetune the models on both 30 hours and 433 hours of the LRS3 dataset. We adopted a pretrained visual encoder (Base version) trained on 1759 hours of data. The experimental results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T3\" title=\"Table 3 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "lrs3",
                    "visual",
                    "pretraining",
                    "data",
                    "encoder",
                    "results",
                    "used",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first analyze the influence of different SFMs using the same LLM and identical amounts of finetuning data. The results show that iFLYTEK-Speech achieves the best ASR performance under both clean and noisy conditions, while WavLM performs best on the VSR task. For AVSR, WavLM achieves superior results on the noisy testset, whereas the clean AVSR results do not exhibit a clear pattern across different SFMs.\nWhen examining the influence of LLMs with the same SFM, we observe that ChatGLM v3-6B slightly outperforms Qwen 2.5-7B when using low-resource data (30 hours) for finetuning. However, this trend reverses with higher-resource data (433 hours), where Qwen 2.5-7B demonstrates better performance.\nDespite performance variations across different SFM and LLM combinations, our UASR-LLM consistently achieves significant improvements in recognition accuracy compared to baselines, demonstrating the strong generalization ability of our proposed method.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "same",
                    "hours",
                    "data",
                    "asr",
                    "results",
                    "noisy",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our ablation studies, we employed the Base version of the visual encoder pretrained on 1759h of audio-visual data. We used either 30h or 433h of LRS3 data for finetuning. Our ablation studies examined several key design components of our method, including visual injection pretraining, parameter sharing for USR, and the integration of LLM.</p>\n\n",
                "matched_terms": [
                    "usr",
                    "1759h",
                    "lrs3",
                    "visual",
                    "pretraining",
                    "data",
                    "encoder",
                    "audiovisual",
                    "433h",
                    "used",
                    "injection",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T4\" title=\"Table 4 &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the experimental results of our visual injection pretraining ablation study. The results demonstrate that omitting the proposed visual injection pretraining stage (<span class=\"ltx_text ltx_font_bold\">0h</span> for VIPT data) leads to degraded performance in both VSR and AVSR tasks. Furthermore, increasing the amount of pretraining data during the visual injection pretraining stage yields additional improvements in VSR and AVSR performance. These superior results for VSR and AVSR tasks validate the effectiveness of our pretraining stage in processing visual information.\nInterestingly, for the ASR task, the model without the pretraining stage achieves the best results. Since we froze all SFM parameters throughout training, the audio processing capability of SFM itself remained unaffected by the visual injection pretraining stage. This phenomenon may be attributed to our parameter sharing strategy for USR in LLMs, where enhanced AVSR and VSR performance appears to cause a slight degradation in ASR performance&#8212;a trade-off we explore in the following section.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "usr",
                    "model",
                    "visual",
                    "pretraining",
                    "tasks",
                    "data",
                    "asr",
                    "results",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our parameter sharing approach for realizing USR in a single model with the traditional method of training separate models for each task. The results (<span class=\"ltx_text ltx_font_bold\">w/o shared parameters</span>) are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table shows that training separate models leads to better ASR performance but worse VSR performance. Overall, the proposed USR method achieves comparable performance to the traditional approach that optimizes individual models for each task. These results demonstrate the effectiveness of our method for realizing USR, significantly reducing both training and deployment resources while maintaining strong performance across all tasks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "usr",
                    "shared",
                    "model",
                    "tasks",
                    "results",
                    "asr",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our proposed method based on a decoder-only LLM with the traditional approach using a 6-layer Transformer-based decoder with cross-attention. The results (<span class=\"ltx_text ltx_font_bold\">w/o LLM</span>) are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table demonstrates that our proposed method employing LLMs significantly outperforms the traditional Transformer decoder. This validates that the strong contextual and language modeling capabilities of LLMs, learned from large text corpora, can be effectively transferred to enhance speech recognition performance, which has also been demonstrated in previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UASR-LLM, a novel approach that adapts speech foundation models (SFMs) for unified speech recognition, encompassing ASR, VSR, and AVSR within a single framework using a decoder-only LLM for text prediction.\nOur method augments the SFM with a visual encoder and inserts visual injection modules into each SFM block to fuse visual representations with audio features. We introduce a visual injection pretraining stage where corrupted audio-visual inputs are processed to predict clean audio representations, followed by speech recognition finetuning with modality dropout. The approach leverages the complementary strengths of SFMs pretrained on large-scale audio corpora and LLMs pretrained on extensive text data.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "pretraining",
                    "data",
                    "encoder",
                    "audiovisual",
                    "asr",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrated the effectiveness of UASR-LLM, achieving superior performance compared to state-of-the-art baselines across ASR, VSR, and AVSR tasks under both clean and noisy conditions. Our method significantly improved recognition robustness across varying signal-to-noise ratios and noise types. Experiments with different SFM and LLM combinations show consistent improvements over baselines, demonstrating the generalization capability of our framework. Ablation studies validated the importance of visual injection pretraining, parameter sharing, and LLM integration.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "across",
                    "visual",
                    "pretraining",
                    "tasks",
                    "results",
                    "asr",
                    "noisy",
                    "injection",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite strong performance, our method has several limitations that warrant future investigation. First, while LLM integration leads to substantial speech recognition improvements, it also requires significant computational resources and memory footprint. This computational burden may limit practical deployment, suggesting the need for more efficient approaches such as dynamic pooling of adjacent audio-visual representations to reduce input tokens or exploring lightweight LLM architectures. Second, our method still lags behind previous lipreading studies that employ much larger audio-visual datasets, indicating that the combination of SFMs and LLMs cannot fully compensate for the shortage of large-scale audio-visual training corpora. This limitation fundamentally affects the visual representation extraction capacity of the visual encoder. We anticipate that employing visual encoders pretrained on large-scale visual datasets could further improve our results. Future work will focus on addressing these challenges to enhance both computational efficiency and performance of unified speech recognition systems.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "encoder",
                    "results",
                    "audiovisual",
                    "our"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Word error rate (WER) (%) on the testset of LRS3 with different LLMs and SFMs using the proposed UASR-LLM method. (N) denotes results on our constructed noisy test set.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Lab.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SFM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR (N)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR (N)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" colspan=\"8\"><em class=\"ltx_emph ltx_font_italic\">DistillAV</em></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">30h</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">36.9</td>\n<td class=\"ltx_td ltx_align_center\">2.5</td>\n<td class=\"ltx_td ltx_align_center\">2.2</td>\n<td class=\"ltx_td ltx_align_center\">44.7</td>\n<td class=\"ltx_td ltx_align_center\">9.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" colspan=\"8\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM (Ours)</em></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"3\">ChatGLM v3-6B</td>\n<td class=\"ltx_td ltx_align_center\">iFLYTEK-Speech</td>\n<td class=\"ltx_td ltx_align_center\">30.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.4</span></td>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">33.6</span></td>\n<td class=\"ltx_td ltx_align_center\">4.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Whisper</td>\n<td class=\"ltx_td ltx_align_center\">31.8</td>\n<td class=\"ltx_td ltx_align_center\">2.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\">39.5</td>\n<td class=\"ltx_td ltx_align_center\">4.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">27.8</span></td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\">40.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"3\">Qwen 2.5-7B</td>\n<td class=\"ltx_td ltx_align_center\">iFLYTEK-Speech</td>\n<td class=\"ltx_td ltx_align_center\">29.3</td>\n<td class=\"ltx_td ltx_align_center\">2.0</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">33.9</td>\n<td class=\"ltx_td ltx_align_center\">4.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Whisper</td>\n<td class=\"ltx_td ltx_align_center\">32.4</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">34.8</td>\n<td class=\"ltx_td ltx_align_center\">5.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\">2.4</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">39.8</td>\n<td class=\"ltx_td ltx_align_center\">3.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\"><em class=\"ltx_emph ltx_font_italic\">DistillAV</em></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">433h</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">31.4</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\">42.2</td>\n<td class=\"ltx_td ltx_align_center\">7.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" colspan=\"8\"><em class=\"ltx_emph ltx_font_italic\">UASR-LLM (Ours)</em></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\" rowspan=\"3\">ChatGLM v3-6B</td>\n<td class=\"ltx_td ltx_align_center\">iFLYTEK-Speech</td>\n<td class=\"ltx_td ltx_align_center\">24.8</td>\n<td class=\"ltx_td ltx_align_center\">0.87</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Whisper</td>\n<td class=\"ltx_td ltx_align_center\">26.1</td>\n<td class=\"ltx_td ltx_align_center\">1.3</td>\n<td class=\"ltx_td ltx_align_center\">0.81</td>\n<td class=\"ltx_td ltx_align_center\">35.1</td>\n<td class=\"ltx_td ltx_align_center\">3.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">WavLM</td>\n<td class=\"ltx_td ltx_align_center\">24.3</td>\n<td class=\"ltx_td ltx_align_center\">1.1</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n<td class=\"ltx_td ltx_align_center\">36.5</td>\n<td class=\"ltx_td ltx_align_center\">2.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" rowspan=\"3\">Qwen 2.5-7B</td>\n<td class=\"ltx_td ltx_align_center\">iFLYTEK-Speech</td>\n<td class=\"ltx_td ltx_align_center\">24.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center\">0.89</td>\n<td class=\"ltx_td ltx_align_center\">32.0</td>\n<td class=\"ltx_td ltx_align_center\">3.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Whisper</td>\n<td class=\"ltx_td ltx_align_center\">24.6</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center\">35.3</td>\n<td class=\"ltx_td ltx_align_center\">3.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_b\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">WavLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">23.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">37.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">2.6</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "distillav",
            "avsr",
            "iflytekspeech",
            "word",
            "wer",
            "v36b",
            "denotes",
            "sfms",
            "30h",
            "our",
            "error",
            "sfm",
            "rate",
            "testset",
            "llm",
            "qwen",
            "test",
            "wavlm",
            "constructed",
            "results",
            "asr",
            "llms",
            "set",
            "ours",
            "proposed",
            "257b",
            "lrs3",
            "chatglm",
            "lab",
            "433h",
            "whisper",
            "method",
            "uasrllm",
            "noisy",
            "different",
            "vsr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the generalization capability of our approach, we conducted experiments with different speech feature models (SFMs) and large language models (LLMs). Specifically, we replaced the WavLM-based SFM with two supervised learning alternatives: Whisper and iFLYTEK-Speech. Additionally, we compared ChatGLM v3-6B with Qwen 2.5-7B as the backbone LLM.\nFor training, we used 433 hours of data for the visual injection pretraining stage and finetune the models on both 30 hours and 433 hours of the LRS3 dataset. We adopted a pretrained visual encoder (Base version) trained on 1759 hours of data. The experimental results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T3\" title=\"Table 3 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework. While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.\nThis paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified visual speech recognition (VSR), automatic speech recognition (ASR), and audio-visual speech recognition (AVSR) tasks by leveraging large language models (LLMs) as text decoders. Our approach introduces visual representations into multiple SFM layers through visual injection modules, enabling multimodal input processing and unified hidden representations. The augmented SFMs connect with decoder-only LLMs via a feed-forward adaptor, where concatenated representations and instruction prompts guide speech transcription.\nWe propose a two-stage training strategy: visual injection pretraining followed by speech recognition finetuning. SFM parameters remain frozen throughout training, with only visual injection modules optimized initially, and LLMs finetuned using LoRA parameters subsequently.\nExperimental results demonstrate superior performance over state-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Ablation studies confirm generalization across various SFMs and LLMs, validating the proposed training strategy.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "llms",
                    "avsr",
                    "proposed",
                    "results",
                    "sfms",
                    "asr",
                    "uasrllm",
                    "noisy",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech recognition represents a fundamental challenge in pattern recognition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib1\" title=\"\">1</a>]</cite>, with recent advances driven by speech foundation models (SFMs) such as WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> and Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite>.\nThese large-scale models, trained on extensive speech datasets, have demonstrated remarkable performance in automatic speech recognition (ASR). However, environmental noise remains a critical limitation that can significantly compromise recognition accuracy, particularly in real-world deployment scenarios.\nTo overcome this challenge, researchers have developed complementary approaches including visual speech recognition (VSR, also known as lipreading)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib6\" title=\"\">6</a>]</cite> and audio-visual speech recognition (AVSR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib8\" title=\"\">8</a>]</cite>, which leverage lip movement information either independently or in combination with audio signals. AVSR systems demonstrate substantial improvements in robustness under noisy conditions, making them increasingly valuable for practical applications such as intelligent home systems, online conferencing, and service robotics.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "noisy",
                    "whisper",
                    "asr",
                    "sfms",
                    "wavlm",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AVSR models typically adopt a sequence-to-sequence architecture nowadays, including recurrent neural network transducer (RNN-T)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite> or hybrid connectionist temporal classification (CTC) and attention architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib11\" title=\"\">11</a>]</cite>.\nRecently, audio-visual self-supervised learning has also been proposed to leverage\nunlabeled audio-visual data for pretraining, such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, REVAn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>, and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>.\nWith a further finetuning stage on few labeled data, these methods achieve a strong VSR and AVSR performance.\nBuilding on these progresses, Haliassos et al. introduced unified speech recognition (USR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>, which consolidates ASR, VSR, and AVSR tasks within a single model framework. This unified approach offers compelling advantages by eliminating the need to construct separate models for each modality, thereby significantly reducing training and deployment costs. The conceptual foundation is particularly appealing, as audio and visual lip movements represent complementary signal views of the same articulatory process during speech production.</p>\n\n",
                "matched_terms": [
                    "vsr",
                    "avsr",
                    "proposed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have explored utilizing SFMs for VSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib16\" title=\"\">16</a>]</cite> and AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>. Our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> proposed using SFMs for knowledge distillation in audio-visual self-supervised pretraining, achieving competitive performance across VSR, ASR, and AVSR benchmarks.\nHowever, the knowledge distillation approach may inherently limit performance compared to direct utilization of SFMs, particularly in high signal-to-noise ratio (SNR) scenarios where the original models excel.\nSeveral approaches have focused on the direct utilization of SFMs for audio-visual tasks. Lip2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite> proposed reprogramming a wav2vec 2.0 model for lipreading by feeding transformed visual representations to the audio model. Whisper-flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib19\" title=\"\">19</a>]</cite> adapted the Whisper model for AVSR by incorporating an additional visual encoder to process visual information alongside audio inputs.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "proposed",
                    "whisper",
                    "asr",
                    "sfms",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its success, most previous works address VSR, AVSR, and ASR tasks separately.\nIn this study, we explore tackling these tasks within a single unified model.\nFurthermore, we aim to leverage the rich knowledge embedded in both speech foundation models (SFMs) and large language models (LLMs) to design a general framework that achieves robust USR performance.\nWe propose <span class=\"ltx_text ltx_font_bold\">UASR-LLM</span> (<span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">R</span>ecognition with <span class=\"ltx_text ltx_font_bold\">L</span>arge <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">M</span>odels), a novel framework that adapts frozen speech foundation models to unified VSR, ASR, and AVSR tasks by leveraging LLMs as text decoders.\nOur approach employs a visual encoder from our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> to extract visual representations, which are then integrated into each block of the SFMs through visual injection modules. The SFM outputs are processed by an adapter to obtain hidden embeddings, which are subsequently fed into LLMs for conducting speech recognition across all modalities.\nTo enable SFMs to process visual and audio-visual inputs, we introduce a visual injection pretraining stage based on cross-modal knowledge distillation loss. The model then undergoes a second training stage for speech recognition, incorporating random modality dropout guided by corresponding instructions. Throughout the entire training process, both the SFM and visual encoder parameters remain frozen, while the LLM is finetuned using a small set of Low-Rank Adaptation (LoRA) weights&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib20\" title=\"\">20</a>]</cite>. This approach enables memory-efficient finetuning while preserving the generalization capabilities of the original models.\nBy utilizing the SFM as a unified backbone for processing multimodal audio and visual inputs, our framework facilitates knowledge transfer from audio to video data and unifies the hidden representations of both modalities. This provides a well-aligned output space for LLMs to generate transcriptions, seamlessly achieving VSR, ASR, and AVSR within a single model.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "llms",
                    "avsr",
                    "set",
                    "llm",
                    "asr",
                    "sfms",
                    "uasrllm",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates that UASR-LLM consistently outperforms state-of-the-art baselines across all three tasks when trained with comparable amounts of audio-visual speech data. On the LRS3 test set, UASR-LLM achieves word error rates of 20.9%, 0.84%, and 0.69% for VSR, ASR, and AVSR, respectively. Under noisy conditions, our method exhibits substantial robustness, significantly reducing WER compared to baseline approaches. Furthermore, we demonstrate that our approach generalizes effectively across different speech foundation models and large language models, with all configurations outperforming baseline methods. Finally, we validate our training strategy and confirm the effectiveness of performing unified speech recognition within a single model architecture.</p>\n\n",
                "matched_terms": [
                    "set",
                    "different",
                    "avsr",
                    "word",
                    "wer",
                    "lrs3",
                    "asr",
                    "test",
                    "method",
                    "uasrllm",
                    "noisy",
                    "error",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent years have witnessed the emergence of various speech foundation models, which can be broadly categorized into two paradigms: self-supervised and supervised approaches.\nAmong SSL-based speech foundation models,\nHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> utilizes k-means clustering to generate pseudo-class labels for model training using a mask prediction loss,\nwith iterative refinement through subsequent clustering and mask prediction steps.\nWavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> improves HuBERT by using a more diverse pretraining dataset and performs speech denoising modeling during pretraining.\nOn the other hand, Whisper family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> exemplifies supervised learning-based speech foundation model. Trained on large-scale multilingual and multitask labeled data, it demonstrates robust performance in ASR and speech translation tasks.</p>\n\n",
                "matched_terms": [
                    "wavlm",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Visual representation learning is crucial for the performance of VSR and AVSR tasks, which can be broadly categorized into supervised and self-supervised approaches.\nSupervised methods leverage labeled&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib22\" title=\"\">22</a>]</cite> or weakly labeled audio-visual speech data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite> to optimize representation extraction and text decoding in an end-to-end manner through label prediction. Several auxiliary tasks have been proposed to enhance visual encoder learning, including articulation feature prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>]</cite> and distillation from acoustic models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib26\" title=\"\">26</a>]</cite>.\nSelf-supervised methods utilize unlabeled audio-visual data with pretext tasks such as masked prediction loss functions. Representative approaches include AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, RAVen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>, AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and AV-data2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>. Our previous work proposed a joint audio-visual representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> that leverages cross-modal knowledge distillation from pretrained SFMs.\nThis work adopts our previously pretrained model for visual representation extraction, which can be viewed as an extension of our previous study. Here, we further leverage SFMs directly for processing audio-visual data, moving beyond the distillation approach to direct utilization of foundation models.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "proposed",
                    "method",
                    "sfms",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) have attracted significant attention for speech recognition applications, with research broadly falling into two categories: multimodal LLMs for universal AI assistants&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib31\" title=\"\">31</a>]</cite> and LLMs for enhancing traditional speech recognition tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib32\" title=\"\">32</a>]</cite>. Our work belongs to the latter category.\nRecent approaches for integrating LLMs with speech recognition typically employ lightweight adapters to connect frozen speech encoders with LLMs. Representative works include joint speech-language models with small adapters&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib32\" title=\"\">32</a>]</cite> and Q-Former-based connections&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib33\" title=\"\">33</a>]</cite>. Some studies explore discrete speech representations as LLM inputs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib34\" title=\"\">34</a>]</cite>.\nLLMs have also been adopted for visual and audio-visual speech recognition. VSP-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib35\" title=\"\">35</a>]</cite> focuses on visual speech recognition, while Llama-AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> uses separate audio and visual encoders with temporal concatenation as LLM input. MMS-Llama&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite> employs audio-visual fusion within encoders and Q-former adapters for improved efficiency.\nOur work differs by proposing a unified speech recognition (USR) approach that processes both audio and visual modalities within a single model architecture, rather than targeting specific modalities separately.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "llm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our UASR-LLM method comprises two main components: an audio-visual adapted speech foundation model (SFM-AV)\nthat processes both audio and visual modalities and encodes them into a unified representational space,\nand a large language model that decodes the encoded representations into text.\nThe SFM-AV consists of a visual encoder and a speech foundation model enhanced with visual injection modules.\nOur model architecture is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe training of our model follows a two-stage process: in the first pretraining stage,\nthe SFM is adapted to encode both audio and visual information into a unified hidden space;\nin the second finetuning stage, the adapted SFM is connected to the LLM to enable unified speech recognition.\nThe detailed model architecture and training strategy are described in the following sections.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "llm",
                    "method",
                    "uasrllm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let lip video be <math alttext=\"V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}</annotation></semantics></math>, where <math alttext=\"T_{v},H,W,\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">T_{v},H,W,</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> represent the temporal, height, width, and channel dimensions, respectively. The visual encoder <math alttext=\"\\text{E}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mtext>E</mtext><mi>v</mi></msub><annotation encoding=\"application/x-tex\">\\text{E}_{v}</annotation></semantics></math> processes the visual input into hidden representations:\n<math alttext=\"H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>v</mi></msub><mo>=</mo><mrow><msub><mtext>E</mtext><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}</annotation></semantics></math>,\nwhere <math alttext=\"D_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">D_{v}</annotation></semantics></math> is the feature dimension of the encoded representations.\nIn this work, we adopt the visual component of the pretrained DistillAV model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> as our visual encoder. DistillAV is designed to distill knowledge from speech foundation models (SFMs) for joint audio-visual representation learning. Unlike SFMs, which are trained on large-scale speech-only corpora, DistillAV is trained on relatively smaller audio-visual datasets. As a result, its performance in audio-only tasks is inferior to that of its teacher models, as shown in our previous ASR evaluations. However, DistillAV exhibits strong visual modeling capabilities and consistently outperforms state-of-the-art lipreading models such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite> and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>. Therefore, we use the pretrained DistillAV model to extract representations from lip video inputs.</p>\n\n",
                "matched_terms": [
                    "distillav",
                    "our",
                    "asr",
                    "sfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{MHA}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mtext>MHA</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{MHA}()</annotation></semantics></math> and <math alttext=\"\\text{FFN}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mtext>FFN</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{FFN}()</annotation></semantics></math> represent the multi-head attention and feed-forward network modules, respectively. <math alttext=\"D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>T</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}</annotation></semantics></math>, where <math alttext=\"d_{m,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{m,n}</annotation></semantics></math> represents the relative position encoding between the <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>-th audio frame and the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-th video frame. <math alttext=\"g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m7\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">g_{i}</annotation></semantics></math> and <math alttext=\"g_{i}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>i</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">g_{i}^{\\prime}</annotation></semantics></math> are trainable gating parameters, which are initialized to zero.\nIt provides an initialization state identical to the original SFMs, which helps stabilize the training.\nThe visual injection modules are inserted before each block of the SFM, while the SFM parameters remain frozen throughout training.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "sfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The output representations from the SFM-AV are transformed through a two-layer feed-forward adaptor to match the embedding dimension of large language models, serving as continuous audio-visual tokens for prompting the LLM to generate transcribed text. We employ task-specific instructions that prepend the audio-visual tokens according to the unified speech recognition task. The instruction prompt follows the template &#8220;Transcribe the [].&#8221;, where the placeholder [] is filled with &#8220;audio&#8221;, &#8220;video&#8221;, or &#8220;audio and video&#8221; for ASR, VSR, and AVSR tasks, respectively.\nFor efficient LLM finetuning, we adopt the low-rank adaptation (LoRA) method. The low-rank parameters are applied to adapt all blocks&#8217; self-attention components as well as the output projection layer for predicting text logits. Unlike previous approaches such as Llama-AVSR&#8217;s frame-compression&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> or MMS-Llama&#8217;s Q-former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite>, our method does not employ frame reduction techniques. This design choice reflects our primary focus on improving recognition performance rather than computational efficiency, with the latter left for future investigation.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "llm",
                    "method",
                    "asr",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage of our pretraining strategy focuses on integrating visual information into the speech foundation model to enable SFM-AV to process multimodal inputs and generate unified audio-visual representations. We employ a cross-modal knowledge distillation approach where clean audio representations from the original SFM serve as supervision signals for training SFM-AV with corrupted audio-visual inputs, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.3 Prompting the LLM for USR &#8227; 3.1 Model structure &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> denotes the total number of blocks in the SFM. To obtain the final teacher representation <math alttext=\"H^{\\mathcal{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m4\" intent=\":literal\"><semantics><msup><mi>H</mi><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></msup><annotation encoding=\"application/x-tex\">H^{\\mathcal{T}}</annotation></semantics></math>, we apply instance normalization to each block&#8217;s output and compute their average:</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pretraining stage, we connect the SFM-AV encoder with an LLM to perform unified speech recognition across multiple modalities,\nas shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F3\" title=\"Figure 3 &#8227; 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The model pathway varies depending on the target task: for VSR and AVSR, we utilize SFM representations enhanced with visual information through the injection module. For ASR, we bypass the visual injection module and employ the original SFM to process audio-only inputs.\nTo enhance model robustness, we employ a modality dropout strategy with probabilities <math alttext=\"p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{v}^{\\prime}</annotation></semantics></math>, <math alttext=\"p_{a}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{a}^{\\prime}</annotation></semantics></math>, and <math alttext=\"1-p_{a}^{\\prime}-p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>&#8722;</mo><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">1-p_{a}^{\\prime}-p_{v}^{\\prime}</annotation></semantics></math> for visual-only, audio-only, and audio-visual combined inputs, respectively. The instruction prompts are selected accordingly to match the input modality configuration.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "avsr",
                    "llm",
                    "asr",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary training loss is cross-entropy <math alttext=\"L_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{CE}</annotation></semantics></math> for text prediction by the LLM. Additionally, we incorporate an auxiliary CTC loss to optimize the encoder directly. This requires connecting the encoder to an additional feed-forward projection layer for logit prediction. Initial experiments used the same vocabulary as the LLM; however, convergence proved challenging due to the large vocabulary size (e.g., &#160;152k tokens for Qwen 2.5). Therefore, we adopted a separate, smaller 1k vocabulary specifically for the CTC loss, which significantly improved training stability.\nThe total loss combines both objectives:</p>\n\n",
                "matched_terms": [
                    "llm",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> controls the relative importance of the auxiliary CTC loss.\nDuring finetuning, we jointly optimize the LoRA parameters of the LLM, the feed-forward adaptor, and the visual injection module, while keeping the core SFM parameters frozen.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nFor pretraining, we utilized the LRS3 dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mmai.io/datasets/lip_reading/\" title=\"\">https://mmai.io/datasets/lip_reading/</a></span></span></span>, which comprises approximately 433 hours of English audio-visual speech data. We also employed a combination dataset of LRS3 and the English-only version of VoxCeleb2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html\" title=\"\">https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html</a></span></span></span>, curated by Shi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, totaling approximately 1759 hours of audio-visual speech data. The VoxCeleb data was transcribed using the Whisper-large-v2 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span>.\nFor finetuning, we adopted both a 30-hour subset and the full 433-hour training set of LRS3. Additionally, the model can be finetuned on the complete 1759-hour audio-visual speech dataset. A validation set of 1000 utterances was reserved, and results were reported on the LRS3 test set. We used the facial images provided in the original datasets without cropping to the lip region-of-interest (ROI)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "lrs3",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Noise augmentation.</span>\nThe MUSAN dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/17/\" title=\"\">https://www.openslr.org/17/</a></span></span></span> was employed for noise augmentation. Following the protocol outlined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib40\" title=\"\">40</a>]</cite>, we sampled and added noise to the audio input of the student model during training. To evaluate our proposed method for AVSR under noisy conditions, we constructed test sets with various noise types, including music, natural sounds, speech, and babble, at signal-to-noise ratios (SNRs) of -10, -5, 0, 5, and 10 dB.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "proposed",
                    "constructed",
                    "method",
                    "test",
                    "noisy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model configurations.</span>\nOur experiments employed a default configuration consisting of three main components: the DistillAV model as the visual encoder, WavLM-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> (317M parameters) as the speech foundation model (SFM), and Qwen 2.5-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2.5-7B</a></span></span></span> as the large language model. The visual encoder offered two variants: Base (103M parameters) and Large (325M parameters). Following our previous study, we denote the proposed method as UASR-LLM-B and UASR-LLM-L accordingly.\nThe visual injection modules were configured with 8 heads, 512 hidden dimensions, and 256 dimensions for both cross-attention and feed-forward networks, totaling approximately 58M parameters. The two-layer feed-forward network adapter that connects the SFM and LLMs contains around 10M parameters. LoRA weights were configured with a rank of 16, resulting in 12M total parameters.\nTo demonstrate the generalization capability of our method, we explored additional speech foundation models: the open-source Whisper-medium&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> and an internal iFLYTEK-Speech model, both containing approximately 300M parameters comparable to WavLM-large. We also evaluated ChatGLM v3-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/THUDM/chatglm3-6b\" title=\"\">https://huggingface.co/THUDM/chatglm3-6b</a></span></span></span> as an alternative large language model.</p>\n\n",
                "matched_terms": [
                    "distillav",
                    "sfm",
                    "llms",
                    "iflytekspeech",
                    "proposed",
                    "chatglm",
                    "qwen",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WavLM model generates representations at 50 fps, double the frame rate of visual representations. We set relative positional encoding according to the real-time interval between audio and visual frames using sinusoidal positional embedding. The parameter <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E4\" title=\"In 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> was set to 8 for WavLM-based SFM and 1 for Whisper and iFLYTEK-Speech-based SFM, following our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "set",
                    "rate",
                    "whisper",
                    "wavlm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pretraining, 80% of audio features and 30% of video features were masked for the student model, with modality dropout probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math> set to 0.5. We employed the Adam optimizer with a learning rate schedule that linearly warmed up to 0.0005 over the initial 5% of updates, maintained this rate for 85% of updates, then exponentially decayed over the remaining 10%. The total number of updates was 100k and 200k for 433h and 1759h pretraining data, respectively. During finetuning, modality dropout probabilities <math alttext=\"p^{\\prime}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{v}</annotation></semantics></math> and <math alttext=\"p^{\\prime}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{a}</annotation></semantics></math> were set to 0.5 and 0.25, respectively. For the auxiliary CTC loss, we utilized subword units with a vocabulary size of 1000 as targets, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E7\" title=\"In 3.2.2 Speech recognition finetuning &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> set to 0.25. The learning rate warmed up to 0.0002 over the initial one-third of updates, followed by exponential decay. The total number of updates was 30k, 60k, and 120k for 30h, 433h, and 1759h finetuning data, respectively.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "433h",
                    "set",
                    "30h"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> present experimental results for models finetuned with low-resource (30h) and high-resource (433h or 1759h) labeled data, respectively. The unlabeled data refers to audio-visual data used for pretraining the DistillAV-based visual encoder and visual injection pretraining, while the labeled data denotes audio-visual data employed for speech recognition finetuning.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "433h",
                    "results",
                    "30h"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms most baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Although underperforming Llama-AVSR in ASR, our approach provides a unified framework effective across all three tasks. The superior VSR performance particularly demonstrates the effectiveness of &#8221;reprogramming&#8221; WavLM for lipreading with LLMs, consistent with previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWith 433h labeled data, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method achieves significant accuracy improvements. Compared to previous studies using comparable audio-visual data, our approach demonstrates superior performance across all tasks under both clean and noisy conditions. With 1759h labeled data, our method achieves WERs of 20.9%, 0.84%, and 0.69% on the clean LRS3 test set, rivaling or surpassing state-of-the-art methods. However, our VSR results still lag behind LP-Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib43\" title=\"\">43</a>]</cite>, which uses approximately 100k hours of audio-visual data.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "set",
                    "avsr",
                    "noisy",
                    "lrs3",
                    "433h",
                    "results",
                    "test",
                    "asr",
                    "method",
                    "wavlm",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing ASR and AVSR performance on our noisy test set reveals that incorporating visual information significantly improves speech recognition robustness.\nThis substantial WER reduction demonstrates that our method successfully fuses speech foundation models with visual representations to enhance noise-invariant capabilities.\nThe next section provides detailed AVSR results under noisy conditions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "asr",
                    "test",
                    "results",
                    "method",
                    "noisy",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness under noisy conditions, we constructed four test sets by mixing the clean audio from the LRS3 test set with different noise types: speech, babble, natural, and music, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.SS1\" title=\"4.1 Implementation details &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. We compared our approach against multiple baselines, including two audio-only SFM models (WavLM and iFLYTEK-Speech) and three AVSR models (AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and DistillAV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>). All audio-only models were finetuned on the LRS3 dataset. The experiments utilized 433h data for pretraining, with all models adopting the Base configuration. We used either 30h or 433h of LRS3 data for finetuning.\nThe experimental results are summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.F4\" title=\"Figure 4 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "distillav",
                    "sfm",
                    "avsr",
                    "set",
                    "iflytekspeech",
                    "noisy",
                    "test",
                    "constructed",
                    "lrs3",
                    "433h",
                    "results",
                    "30h",
                    "wavlm",
                    "different",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed UASR-LLM method (red line) consistently outperforms all baselines across different noise types and SNR levels, with particularly pronounced advantages in severe noise conditions below 0 dB. Among the four noise conditions, babble noise proves most challenging, with all methods exhibiting higher WER at low SNRs, particularly below 0 dB. However, the proposed approach maintains substantially lower error rates than competing methods even in these challenging low-SNR scenarios. The results highlight the effectiveness of incorporating visual information, as audio-only models (WavLM and iFLYTEK-Speech) degrade rapidly in noisy conditions compared to AVSR approaches.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "iflytekspeech",
                    "proposed",
                    "noisy",
                    "wer",
                    "results",
                    "method",
                    "different",
                    "uasrllm",
                    "wavlm",
                    "error",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first analyze the influence of different SFMs using the same LLM and identical amounts of finetuning data. The results show that iFLYTEK-Speech achieves the best ASR performance under both clean and noisy conditions, while WavLM performs best on the VSR task. For AVSR, WavLM achieves superior results on the noisy testset, whereas the clean AVSR results do not exhibit a clear pattern across different SFMs.\nWhen examining the influence of LLMs with the same SFM, we observe that ChatGLM v3-6B slightly outperforms Qwen 2.5-7B when using low-resource data (30 hours) for finetuning. However, this trend reverses with higher-resource data (433 hours), where Qwen 2.5-7B demonstrates better performance.\nDespite performance variations across different SFM and LLM combinations, our UASR-LLM consistently achieves significant improvements in recognition accuracy compared to baselines, demonstrating the strong generalization ability of our proposed method.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "iflytekspeech",
                    "v36b",
                    "sfms",
                    "our",
                    "sfm",
                    "testset",
                    "llm",
                    "qwen",
                    "wavlm",
                    "results",
                    "asr",
                    "llms",
                    "proposed",
                    "257b",
                    "chatglm",
                    "method",
                    "uasrllm",
                    "noisy",
                    "different",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our ablation studies, we employed the Base version of the visual encoder pretrained on 1759h of audio-visual data. We used either 30h or 433h of LRS3 data for finetuning. Our ablation studies examined several key design components of our method, including visual injection pretraining, parameter sharing for USR, and the integration of LLM.</p>\n\n",
                "matched_terms": [
                    "lrs3",
                    "llm",
                    "433h",
                    "method",
                    "30h",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T4\" title=\"Table 4 &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the experimental results of our visual injection pretraining ablation study. The results demonstrate that omitting the proposed visual injection pretraining stage (<span class=\"ltx_text ltx_font_bold\">0h</span> for VIPT data) leads to degraded performance in both VSR and AVSR tasks. Furthermore, increasing the amount of pretraining data during the visual injection pretraining stage yields additional improvements in VSR and AVSR performance. These superior results for VSR and AVSR tasks validate the effectiveness of our pretraining stage in processing visual information.\nInterestingly, for the ASR task, the model without the pretraining stage achieves the best results. Since we froze all SFM parameters throughout training, the audio processing capability of SFM itself remained unaffected by the visual injection pretraining stage. This phenomenon may be attributed to our parameter sharing strategy for USR in LLMs, where enhanced AVSR and VSR performance appears to cause a slight degradation in ASR performance&#8212;a trade-off we explore in the following section.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "llms",
                    "avsr",
                    "proposed",
                    "asr",
                    "results",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our parameter sharing approach for realizing USR in a single model with the traditional method of training separate models for each task. The results (<span class=\"ltx_text ltx_font_bold\">w/o shared parameters</span>) are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table shows that training separate models leads to better ASR performance but worse VSR performance. Overall, the proposed USR method achieves comparable performance to the traditional approach that optimizes individual models for each task. These results demonstrate the effectiveness of our method for realizing USR, significantly reducing both training and deployment resources while maintaining strong performance across all tasks.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "asr",
                    "method",
                    "results",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our proposed method based on a decoder-only LLM with the traditional approach using a 6-layer Transformer-based decoder with cross-attention. The results (<span class=\"ltx_text ltx_font_bold\">w/o LLM</span>) are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table demonstrates that our proposed method employing LLMs significantly outperforms the traditional Transformer decoder. This validates that the strong contextual and language modeling capabilities of LLMs, learned from large text corpora, can be effectively transferred to enhance speech recognition performance, which has also been demonstrated in previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "proposed",
                    "llm",
                    "results",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UASR-LLM, a novel approach that adapts speech foundation models (SFMs) for unified speech recognition, encompassing ASR, VSR, and AVSR within a single framework using a decoder-only LLM for text prediction.\nOur method augments the SFM with a visual encoder and inserts visual injection modules into each SFM block to fuse visual representations with audio features. We introduce a visual injection pretraining stage where corrupted audio-visual inputs are processed to predict clean audio representations, followed by speech recognition finetuning with modality dropout. The approach leverages the complementary strengths of SFMs pretrained on large-scale audio corpora and LLMs pretrained on extensive text data.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "llms",
                    "avsr",
                    "llm",
                    "asr",
                    "sfms",
                    "method",
                    "uasrllm",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrated the effectiveness of UASR-LLM, achieving superior performance compared to state-of-the-art baselines across ASR, VSR, and AVSR tasks under both clean and noisy conditions. Our method significantly improved recognition robustness across varying signal-to-noise ratios and noise types. Experiments with different SFM and LLM combinations show consistent improvements over baselines, demonstrating the generalization capability of our framework. Ablation studies validated the importance of visual injection pretraining, parameter sharing, and LLM integration.</p>\n\n",
                "matched_terms": [
                    "sfm",
                    "avsr",
                    "llm",
                    "results",
                    "asr",
                    "method",
                    "uasrllm",
                    "noisy",
                    "different",
                    "vsr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite strong performance, our method has several limitations that warrant future investigation. First, while LLM integration leads to substantial speech recognition improvements, it also requires significant computational resources and memory footprint. This computational burden may limit practical deployment, suggesting the need for more efficient approaches such as dynamic pooling of adjacent audio-visual representations to reduce input tokens or exploring lightweight LLM architectures. Second, our method still lags behind previous lipreading studies that employ much larger audio-visual datasets, indicating that the combination of SFMs and LLMs cannot fully compensate for the shortage of large-scale audio-visual training corpora. This limitation fundamentally affects the visual representation extraction capacity of the visual encoder. We anticipate that employing visual encoders pretrained on large-scale visual datasets could further improve our results. Future work will focus on addressing these challenges to enhance both computational efficiency and performance of unified speech recognition systems.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "llm",
                    "results",
                    "sfms",
                    "method",
                    "our"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: WER (%) results on LRS3 test set for ablation study of visual injection pretraining (VIPT) data amount. ”0h” indicates omitting the pretraining stage. (N) denotes results on the constructed noisy test set.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Labeled data</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VIPT data</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR (N)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR (N)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\">30h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">433h</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\">2.4</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">39.8</td>\n<td class=\"ltx_td ltx_align_center\">3.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">1759h</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">27.3</span></td>\n<td class=\"ltx_td ltx_align_center\">2.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"3\">433h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">36.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">433h</td>\n<td class=\"ltx_td ltx_align_center\">23.9</td>\n<td class=\"ltx_td ltx_align_center\">0.86</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">37.0</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1759h</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">23.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">37.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">2.4</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "avsr",
            "vipt",
            "wer",
            "visual",
            "denotes",
            "30h",
            "amount",
            "study",
            "stage",
            "ablation",
            "pretraining",
            "test",
            "constructed",
            "results",
            "indicates",
            "asr",
            "labeled",
            "set",
            "omitting",
            "1759h",
            "lrs3",
            "”0h”",
            "data",
            "433h",
            "noisy",
            "injection",
            "vsr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T4\" title=\"Table 4 &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the experimental results of our visual injection pretraining ablation study. The results demonstrate that omitting the proposed visual injection pretraining stage (<span class=\"ltx_text ltx_font_bold\">0h</span> for VIPT data) leads to degraded performance in both VSR and AVSR tasks. Furthermore, increasing the amount of pretraining data during the visual injection pretraining stage yields additional improvements in VSR and AVSR performance. These superior results for VSR and AVSR tasks validate the effectiveness of our pretraining stage in processing visual information.\nInterestingly, for the ASR task, the model without the pretraining stage achieves the best results. Since we froze all SFM parameters throughout training, the audio processing capability of SFM itself remained unaffected by the visual injection pretraining stage. This phenomenon may be attributed to our parameter sharing strategy for USR in LLMs, where enhanced AVSR and VSR performance appears to cause a slight degradation in ASR performance&#8212;a trade-off we explore in the following section.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework. While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.\nThis paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified visual speech recognition (VSR), automatic speech recognition (ASR), and audio-visual speech recognition (AVSR) tasks by leveraging large language models (LLMs) as text decoders. Our approach introduces visual representations into multiple SFM layers through visual injection modules, enabling multimodal input processing and unified hidden representations. The augmented SFMs connect with decoder-only LLMs via a feed-forward adaptor, where concatenated representations and instruction prompts guide speech transcription.\nWe propose a two-stage training strategy: visual injection pretraining followed by speech recognition finetuning. SFM parameters remain frozen throughout training, with only visual injection modules optimized initially, and LLMs finetuned using LoRA parameters subsequently.\nExperimental results demonstrate superior performance over state-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Ablation studies confirm generalization across various SFMs and LLMs, validating the proposed training strategy.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "ablation",
                    "pretraining",
                    "results",
                    "asr",
                    "noisy",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech recognition represents a fundamental challenge in pattern recognition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib1\" title=\"\">1</a>]</cite>, with recent advances driven by speech foundation models (SFMs) such as WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> and Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite>.\nThese large-scale models, trained on extensive speech datasets, have demonstrated remarkable performance in automatic speech recognition (ASR). However, environmental noise remains a critical limitation that can significantly compromise recognition accuracy, particularly in real-world deployment scenarios.\nTo overcome this challenge, researchers have developed complementary approaches including visual speech recognition (VSR, also known as lipreading)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib6\" title=\"\">6</a>]</cite> and audio-visual speech recognition (AVSR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib8\" title=\"\">8</a>]</cite>, which leverage lip movement information either independently or in combination with audio signals. AVSR systems demonstrate substantial improvements in robustness under noisy conditions, making them increasingly valuable for practical applications such as intelligent home systems, online conferencing, and service robotics.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "asr",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AVSR models typically adopt a sequence-to-sequence architecture nowadays, including recurrent neural network transducer (RNN-T)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite> or hybrid connectionist temporal classification (CTC) and attention architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib11\" title=\"\">11</a>]</cite>.\nRecently, audio-visual self-supervised learning has also been proposed to leverage\nunlabeled audio-visual data for pretraining, such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, REVAn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>, and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>.\nWith a further finetuning stage on few labeled data, these methods achieve a strong VSR and AVSR performance.\nBuilding on these progresses, Haliassos et al. introduced unified speech recognition (USR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>, which consolidates ASR, VSR, and AVSR tasks within a single model framework. This unified approach offers compelling advantages by eliminating the need to construct separate models for each modality, thereby significantly reducing training and deployment costs. The conceptual foundation is particularly appealing, as audio and visual lip movements represent complementary signal views of the same articulatory process during speech production.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "stage",
                    "visual",
                    "pretraining",
                    "data",
                    "asr",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have explored utilizing SFMs for VSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib16\" title=\"\">16</a>]</cite> and AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>. Our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> proposed using SFMs for knowledge distillation in audio-visual self-supervised pretraining, achieving competitive performance across VSR, ASR, and AVSR benchmarks.\nHowever, the knowledge distillation approach may inherently limit performance compared to direct utilization of SFMs, particularly in high signal-to-noise ratio (SNR) scenarios where the original models excel.\nSeveral approaches have focused on the direct utilization of SFMs for audio-visual tasks. Lip2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite> proposed reprogramming a wav2vec 2.0 model for lipreading by feeding transformed visual representations to the audio model. Whisper-flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib19\" title=\"\">19</a>]</cite> adapted the Whisper model for AVSR by incorporating an additional visual encoder to process visual information alongside audio inputs.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "study",
                    "visual",
                    "pretraining",
                    "asr",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its success, most previous works address VSR, AVSR, and ASR tasks separately.\nIn this study, we explore tackling these tasks within a single unified model.\nFurthermore, we aim to leverage the rich knowledge embedded in both speech foundation models (SFMs) and large language models (LLMs) to design a general framework that achieves robust USR performance.\nWe propose <span class=\"ltx_text ltx_font_bold\">UASR-LLM</span> (<span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">R</span>ecognition with <span class=\"ltx_text ltx_font_bold\">L</span>arge <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">M</span>odels), a novel framework that adapts frozen speech foundation models to unified VSR, ASR, and AVSR tasks by leveraging LLMs as text decoders.\nOur approach employs a visual encoder from our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> to extract visual representations, which are then integrated into each block of the SFMs through visual injection modules. The SFM outputs are processed by an adapter to obtain hidden embeddings, which are subsequently fed into LLMs for conducting speech recognition across all modalities.\nTo enable SFMs to process visual and audio-visual inputs, we introduce a visual injection pretraining stage based on cross-modal knowledge distillation loss. The model then undergoes a second training stage for speech recognition, incorporating random modality dropout guided by corresponding instructions. Throughout the entire training process, both the SFM and visual encoder parameters remain frozen, while the LLM is finetuned using a small set of Low-Rank Adaptation (LoRA) weights&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib20\" title=\"\">20</a>]</cite>. This approach enables memory-efficient finetuning while preserving the generalization capabilities of the original models.\nBy utilizing the SFM as a unified backbone for processing multimodal audio and visual inputs, our framework facilitates knowledge transfer from audio to video data and unifies the hidden representations of both modalities. This provides a well-aligned output space for LLMs to generate transcriptions, seamlessly achieving VSR, ASR, and AVSR within a single model.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "study",
                    "stage",
                    "visual",
                    "pretraining",
                    "data",
                    "asr",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates that UASR-LLM consistently outperforms state-of-the-art baselines across all three tasks when trained with comparable amounts of audio-visual speech data. On the LRS3 test set, UASR-LLM achieves word error rates of 20.9%, 0.84%, and 0.69% for VSR, ASR, and AVSR, respectively. Under noisy conditions, our method exhibits substantial robustness, significantly reducing WER compared to baseline approaches. Furthermore, we demonstrate that our approach generalizes effectively across different speech foundation models and large language models, with all configurations outperforming baseline methods. Finally, we validate our training strategy and confirm the effectiveness of performing unified speech recognition within a single model architecture.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "lrs3",
                    "data",
                    "asr",
                    "test",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent years have witnessed the emergence of various speech foundation models, which can be broadly categorized into two paradigms: self-supervised and supervised approaches.\nAmong SSL-based speech foundation models,\nHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> utilizes k-means clustering to generate pseudo-class labels for model training using a mask prediction loss,\nwith iterative refinement through subsequent clustering and mask prediction steps.\nWavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> improves HuBERT by using a more diverse pretraining dataset and performs speech denoising modeling during pretraining.\nOn the other hand, Whisper family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> exemplifies supervised learning-based speech foundation model. Trained on large-scale multilingual and multitask labeled data, it demonstrates robust performance in ASR and speech translation tasks.</p>\n\n",
                "matched_terms": [
                    "data",
                    "pretraining",
                    "asr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Visual representation learning is crucial for the performance of VSR and AVSR tasks, which can be broadly categorized into supervised and self-supervised approaches.\nSupervised methods leverage labeled&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib22\" title=\"\">22</a>]</cite> or weakly labeled audio-visual speech data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite> to optimize representation extraction and text decoding in an end-to-end manner through label prediction. Several auxiliary tasks have been proposed to enhance visual encoder learning, including articulation feature prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>]</cite> and distillation from acoustic models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib26\" title=\"\">26</a>]</cite>.\nSelf-supervised methods utilize unlabeled audio-visual data with pretext tasks such as masked prediction loss functions. Representative approaches include AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, RAVen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>, AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and AV-data2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>. Our previous work proposed a joint audio-visual representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> that leverages cross-modal knowledge distillation from pretrained SFMs.\nThis work adopts our previously pretrained model for visual representation extraction, which can be viewed as an extension of our previous study. Here, we further leverage SFMs directly for processing audio-visual data, moving beyond the distillation approach to direct utilization of foundation models.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "study",
                    "visual",
                    "data",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our UASR-LLM method comprises two main components: an audio-visual adapted speech foundation model (SFM-AV)\nthat processes both audio and visual modalities and encodes them into a unified representational space,\nand a large language model that decodes the encoded representations into text.\nThe SFM-AV consists of a visual encoder and a speech foundation model enhanced with visual injection modules.\nOur model architecture is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe training of our model follows a two-stage process: in the first pretraining stage,\nthe SFM is adapted to encode both audio and visual information into a unified hidden space;\nin the second finetuning stage, the adapted SFM is connected to the LLM to enable unified speech recognition.\nThe detailed model architecture and training strategy are described in the following sections.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "pretraining",
                    "visual",
                    "injection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let lip video be <math alttext=\"V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>H</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V\\in\\mathbb{R}^{T_{v}\\times H\\times W\\times C}</annotation></semantics></math>, where <math alttext=\"T_{v},H,W,\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>v</mi></msub><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">T_{v},H,W,</annotation></semantics></math> and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> represent the temporal, height, width, and channel dimensions, respectively. The visual encoder <math alttext=\"\\text{E}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mtext>E</mtext><mi>v</mi></msub><annotation encoding=\"application/x-tex\">\\text{E}_{v}</annotation></semantics></math> processes the visual input into hidden representations:\n<math alttext=\"H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>v</mi></msub><mo>=</mo><mrow><msub><mtext>E</mtext><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>T</mi><mi>v</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{v}=\\text{E}_{v}(V)\\in\\mathbb{R}^{T_{v}\\times D_{v}}</annotation></semantics></math>,\nwhere <math alttext=\"D_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">D_{v}</annotation></semantics></math> is the feature dimension of the encoded representations.\nIn this work, we adopt the visual component of the pretrained DistillAV model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> as our visual encoder. DistillAV is designed to distill knowledge from speech foundation models (SFMs) for joint audio-visual representation learning. Unlike SFMs, which are trained on large-scale speech-only corpora, DistillAV is trained on relatively smaller audio-visual datasets. As a result, its performance in audio-only tasks is inferior to that of its teacher models, as shown in our previous ASR evaluations. However, DistillAV exhibits strong visual modeling capabilities and consistently outperforms state-of-the-art lipreading models such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite> and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>. Therefore, we use the pretrained DistillAV model to extract representations from lip video inputs.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual injection module is introduced to incorporate visual information into the pretrained SFM. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block of the SFM, we employ the audio representations <math alttext=\"H_{a}^{i}\\in\\mathbb{R}^{T\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>H</mi><mi>a</mi><mi>i</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">H_{a}^{i}\\in\\mathbb{R}^{T\\times D}</annotation></semantics></math> as the query and attend to visual representations through a cross-attention mechanism. To enable the model to exploit relative position information between the two modalities, we incorporate relative position embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib38\" title=\"\">38</a>]</cite>.\nThe output is subsequently processed through a feed-forward network and then fed into the next SFM block. Both the cross-attention and feed-forward network are augmented with residual connections and tanh gating, following the approach in Whisper-Flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, our visual injection module operates as follows:</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{MHA}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mtext>MHA</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{MHA}()</annotation></semantics></math> and <math alttext=\"\\text{FFN}()\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mtext>FFN</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{FFN}()</annotation></semantics></math> represent the multi-head attention and feed-forward network modules, respectively. <math alttext=\"D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>T</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">D_{av}=[d_{m,n}]\\in\\mathbb{R}^{T\\times T_{v}}</annotation></semantics></math>, where <math alttext=\"d_{m,n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>d</mi><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><annotation encoding=\"application/x-tex\">d_{m,n}</annotation></semantics></math> represents the relative position encoding between the <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>-th audio frame and the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-th video frame. <math alttext=\"g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m7\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">g_{i}</annotation></semantics></math> and <math alttext=\"g_{i}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>i</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">g_{i}^{\\prime}</annotation></semantics></math> are trainable gating parameters, which are initialized to zero.\nIt provides an initialization state identical to the original SFMs, which helps stabilize the training.\nThe visual injection modules are inserted before each block of the SFM, while the SFM parameters remain frozen throughout training.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The output representations from the SFM-AV are transformed through a two-layer feed-forward adaptor to match the embedding dimension of large language models, serving as continuous audio-visual tokens for prompting the LLM to generate transcribed text. We employ task-specific instructions that prepend the audio-visual tokens according to the unified speech recognition task. The instruction prompt follows the template &#8220;Transcribe the [].&#8221;, where the placeholder [] is filled with &#8220;audio&#8221;, &#8220;video&#8221;, or &#8220;audio and video&#8221; for ASR, VSR, and AVSR tasks, respectively.\nFor efficient LLM finetuning, we adopt the low-rank adaptation (LoRA) method. The low-rank parameters are applied to adapt all blocks&#8217; self-attention components as well as the output projection layer for predicting text logits. Unlike previous approaches such as Llama-AVSR&#8217;s frame-compression&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> or MMS-Llama&#8217;s Q-former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite>, our method does not employ frame reduction techniques. This design choice reflects our primary focus on improving recognition performance rather than computational efficiency, with the latter left for future investigation.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "vsr",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage of our pretraining strategy focuses on integrating visual information into the speech foundation model to enable SFM-AV to process multimodal inputs and generate unified audio-visual representations. We employ a cross-modal knowledge distillation approach where clean audio representations from the original SFM serve as supervision signals for training SFM-AV with corrupted audio-visual inputs, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F2\" title=\"Figure 2 &#8227; 3.1.3 Prompting the LLM for USR &#8227; 3.1 Model structure &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "pretraining",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFM-AV pretraining, we apply several data corruption strategies to promote effective audio-visual fusion when predicting clean audio representations. First, the input audio is corrupted by randomly mixing it with background noise, resulting in <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math>. Both the noisy audio <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m2\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math> and the video <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m3\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> are then corrupted using span-based masking. Specifically, for each modality, we randomly select a starting frame and replace a consecutive span of frames with zeros starting from that point.\nIn addition, we adopt modality dropout during training. With probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math>, only the video input is provided to the model, while the audio input is replaced with zeros. With probability <math alttext=\"1-p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msub><mi>p</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">1-p_{v}</annotation></semantics></math>, both audio and video inputs are used.\nAfter applying noise mixing, span masking, and modality dropout, SFM-AV processes the corrupted inputs to generate output representations <math alttext=\"H^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m6\" intent=\":literal\"><semantics><msup><mi>H</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">H^{o}</annotation></semantics></math>, which is summarized as:</p>\n\n",
                "matched_terms": [
                    "data",
                    "noisy",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> represents learnable projection parameters.\nThis first-stage training leverages unlabeled audio-visual speech data for unsupervised pretraining. And the large language model are not involved at this stage.\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "stage",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pretraining stage, we connect the SFM-AV encoder with an LLM to perform unified speech recognition across multiple modalities,\nas shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F3\" title=\"Figure 3 &#8227; 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The model pathway varies depending on the target task: for VSR and AVSR, we utilize SFM representations enhanced with visual information through the injection module. For ASR, we bypass the visual injection module and employ the original SFM to process audio-only inputs.\nTo enhance model robustness, we employ a modality dropout strategy with probabilities <math alttext=\"p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{v}^{\\prime}</annotation></semantics></math>, <math alttext=\"p_{a}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{a}^{\\prime}</annotation></semantics></math>, and <math alttext=\"1-p_{a}^{\\prime}-p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>&#8722;</mo><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">1-p_{a}^{\\prime}-p_{v}^{\\prime}</annotation></semantics></math> for visual-only, audio-only, and audio-visual combined inputs, respectively. The instruction prompts are selected accordingly to match the input modality configuration.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "stage",
                    "visual",
                    "pretraining",
                    "asr",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> controls the relative importance of the auxiliary CTC loss.\nDuring finetuning, we jointly optimize the LoRA parameters of the LLM, the feed-forward adaptor, and the visual injection module, while keeping the core SFM parameters frozen.</p>\n\n",
                "matched_terms": [
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nFor pretraining, we utilized the LRS3 dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mmai.io/datasets/lip_reading/\" title=\"\">https://mmai.io/datasets/lip_reading/</a></span></span></span>, which comprises approximately 433 hours of English audio-visual speech data. We also employed a combination dataset of LRS3 and the English-only version of VoxCeleb2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html\" title=\"\">https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html</a></span></span></span>, curated by Shi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, totaling approximately 1759 hours of audio-visual speech data. The VoxCeleb data was transcribed using the Whisper-large-v2 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span>.\nFor finetuning, we adopted both a 30-hour subset and the full 433-hour training set of LRS3. Additionally, the model can be finetuned on the complete 1759-hour audio-visual speech dataset. A validation set of 1000 utterances was reserved, and results were reported on the LRS3 test set. We used the facial images provided in the original datasets without cropping to the lip region-of-interest (ROI)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "lrs3",
                    "pretraining",
                    "data",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Noise augmentation.</span>\nThe MUSAN dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/17/\" title=\"\">https://www.openslr.org/17/</a></span></span></span> was employed for noise augmentation. Following the protocol outlined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib40\" title=\"\">40</a>]</cite>, we sampled and added noise to the audio input of the student model during training. To evaluate our proposed method for AVSR under noisy conditions, we constructed test sets with various noise types, including music, natural sounds, speech, and babble, at signal-to-noise ratios (SNRs) of -10, -5, 0, 5, and 10 dB.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "noisy",
                    "constructed",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model configurations.</span>\nOur experiments employed a default configuration consisting of three main components: the DistillAV model as the visual encoder, WavLM-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> (317M parameters) as the speech foundation model (SFM), and Qwen 2.5-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2.5-7B</a></span></span></span> as the large language model. The visual encoder offered two variants: Base (103M parameters) and Large (325M parameters). Following our previous study, we denote the proposed method as UASR-LLM-B and UASR-LLM-L accordingly.\nThe visual injection modules were configured with 8 heads, 512 hidden dimensions, and 256 dimensions for both cross-attention and feed-forward networks, totaling approximately 58M parameters. The two-layer feed-forward network adapter that connects the SFM and LLMs contains around 10M parameters. LoRA weights were configured with a rank of 16, resulting in 12M total parameters.\nTo demonstrate the generalization capability of our method, we explored additional speech foundation models: the open-source Whisper-medium&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> and an internal iFLYTEK-Speech model, both containing approximately 300M parameters comparable to WavLM-large. We also evaluated ChatGLM v3-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/THUDM/chatglm3-6b\" title=\"\">https://huggingface.co/THUDM/chatglm3-6b</a></span></span></span> as an alternative large language model.</p>\n\n",
                "matched_terms": [
                    "study",
                    "injection",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WavLM model generates representations at 50 fps, double the frame rate of visual representations. We set relative positional encoding according to the real-time interval between audio and visual frames using sinusoidal positional embedding. The parameter <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E4\" title=\"In 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> was set to 8 for WavLM-based SFM and 1 for Whisper and iFLYTEK-Speech-based SFM, following our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "study",
                    "set",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pretraining, 80% of audio features and 30% of video features were masked for the student model, with modality dropout probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math> set to 0.5. We employed the Adam optimizer with a learning rate schedule that linearly warmed up to 0.0005 over the initial 5% of updates, maintained this rate for 85% of updates, then exponentially decayed over the remaining 10%. The total number of updates was 100k and 200k for 433h and 1759h pretraining data, respectively. During finetuning, modality dropout probabilities <math alttext=\"p^{\\prime}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{v}</annotation></semantics></math> and <math alttext=\"p^{\\prime}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{a}</annotation></semantics></math> were set to 0.5 and 0.25, respectively. For the auxiliary CTC loss, we utilized subword units with a vocabulary size of 1000 as targets, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E7\" title=\"In 3.2.2 Speech recognition finetuning &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> set to 0.25. The learning rate warmed up to 0.0002 over the initial one-third of updates, followed by exponential decay. The total number of updates was 30k, 60k, and 120k for 30h, 433h, and 1759h finetuning data, respectively.</p>\n\n",
                "matched_terms": [
                    "set",
                    "1759h",
                    "data",
                    "pretraining",
                    "433h",
                    "30h"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> present experimental results for models finetuned with low-resource (30h) and high-resource (433h or 1759h) labeled data, respectively. The unlabeled data refers to audio-visual data used for pretraining the DistillAV-based visual encoder and visual injection pretraining, while the labeled data denotes audio-visual data employed for speech recognition finetuning.</p>\n\n",
                "matched_terms": [
                    "1759h",
                    "visual",
                    "denotes",
                    "pretraining",
                    "data",
                    "433h",
                    "results",
                    "30h",
                    "injection",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms most baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Although underperforming Llama-AVSR in ASR, our approach provides a unified framework effective across all three tasks. The superior VSR performance particularly demonstrates the effectiveness of &#8221;reprogramming&#8221; WavLM for lipreading with LLMs, consistent with previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWith 433h labeled data, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method achieves significant accuracy improvements. Compared to previous studies using comparable audio-visual data, our approach demonstrates superior performance across all tasks under both clean and noisy conditions. With 1759h labeled data, our method achieves WERs of 20.9%, 0.84%, and 0.69% on the clean LRS3 test set, rivaling or surpassing state-of-the-art methods. However, our VSR results still lag behind LP-Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib43\" title=\"\">43</a>]</cite>, which uses approximately 100k hours of audio-visual data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "1759h",
                    "test",
                    "lrs3",
                    "data",
                    "433h",
                    "asr",
                    "results",
                    "noisy",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing ASR and AVSR performance on our noisy test set reveals that incorporating visual information significantly improves speech recognition robustness.\nThis substantial WER reduction demonstrates that our method successfully fuses speech foundation models with visual representations to enhance noise-invariant capabilities.\nThe next section provides detailed AVSR results under noisy conditions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "visual",
                    "results",
                    "test",
                    "asr",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness under noisy conditions, we constructed four test sets by mixing the clean audio from the LRS3 test set with different noise types: speech, babble, natural, and music, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.SS1\" title=\"4.1 Implementation details &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. We compared our approach against multiple baselines, including two audio-only SFM models (WavLM and iFLYTEK-Speech) and three AVSR models (AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and DistillAV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>). All audio-only models were finetuned on the LRS3 dataset. The experiments utilized 433h data for pretraining, with all models adopting the Base configuration. We used either 30h or 433h of LRS3 data for finetuning.\nThe experimental results are summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.F4\" title=\"Figure 4 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "30h",
                    "lrs3",
                    "constructed",
                    "pretraining",
                    "data",
                    "433h",
                    "results",
                    "test",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed UASR-LLM method (red line) consistently outperforms all baselines across different noise types and SNR levels, with particularly pronounced advantages in severe noise conditions below 0 dB. Among the four noise conditions, babble noise proves most challenging, with all methods exhibiting higher WER at low SNRs, particularly below 0 dB. However, the proposed approach maintains substantially lower error rates than competing methods even in these challenging low-SNR scenarios. The results highlight the effectiveness of incorporating visual information, as audio-only models (WavLM and iFLYTEK-Speech) degrade rapidly in noisy conditions compared to AVSR approaches.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "wer",
                    "visual",
                    "results",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization capability of our approach, we conducted experiments with different speech feature models (SFMs) and large language models (LLMs). Specifically, we replaced the WavLM-based SFM with two supervised learning alternatives: Whisper and iFLYTEK-Speech. Additionally, we compared ChatGLM v3-6B with Qwen 2.5-7B as the backbone LLM.\nFor training, we used 433 hours of data for the visual injection pretraining stage and finetune the models on both 30 hours and 433 hours of the LRS3 dataset. We adopted a pretrained visual encoder (Base version) trained on 1759 hours of data. The experimental results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T3\" title=\"Table 3 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "lrs3",
                    "visual",
                    "pretraining",
                    "data",
                    "results",
                    "injection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first analyze the influence of different SFMs using the same LLM and identical amounts of finetuning data. The results show that iFLYTEK-Speech achieves the best ASR performance under both clean and noisy conditions, while WavLM performs best on the VSR task. For AVSR, WavLM achieves superior results on the noisy testset, whereas the clean AVSR results do not exhibit a clear pattern across different SFMs.\nWhen examining the influence of LLMs with the same SFM, we observe that ChatGLM v3-6B slightly outperforms Qwen 2.5-7B when using low-resource data (30 hours) for finetuning. However, this trend reverses with higher-resource data (433 hours), where Qwen 2.5-7B demonstrates better performance.\nDespite performance variations across different SFM and LLM combinations, our UASR-LLM consistently achieves significant improvements in recognition accuracy compared to baselines, demonstrating the strong generalization ability of our proposed method.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "data",
                    "asr",
                    "results",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our ablation studies, we employed the Base version of the visual encoder pretrained on 1759h of audio-visual data. We used either 30h or 433h of LRS3 data for finetuning. Our ablation studies examined several key design components of our method, including visual injection pretraining, parameter sharing for USR, and the integration of LLM.</p>\n\n",
                "matched_terms": [
                    "1759h",
                    "lrs3",
                    "visual",
                    "ablation",
                    "pretraining",
                    "data",
                    "433h",
                    "30h",
                    "injection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section compares our parameter sharing approach for realizing USR in a single model with the traditional method of training separate models for each task. The results (<span class=\"ltx_text ltx_font_bold\">w/o shared parameters</span>) are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table shows that training separate models leads to better ASR performance but worse VSR performance. Overall, the proposed USR method achieves comparable performance to the traditional approach that optimizes individual models for each task. These results demonstrate the effectiveness of our method for realizing USR, significantly reducing both training and deployment resources while maintaining strong performance across all tasks.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "vsr",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UASR-LLM, a novel approach that adapts speech foundation models (SFMs) for unified speech recognition, encompassing ASR, VSR, and AVSR within a single framework using a decoder-only LLM for text prediction.\nOur method augments the SFM with a visual encoder and inserts visual injection modules into each SFM block to fuse visual representations with audio features. We introduce a visual injection pretraining stage where corrupted audio-visual inputs are processed to predict clean audio representations, followed by speech recognition finetuning with modality dropout. The approach leverages the complementary strengths of SFMs pretrained on large-scale audio corpora and LLMs pretrained on extensive text data.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "stage",
                    "visual",
                    "pretraining",
                    "data",
                    "asr",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrated the effectiveness of UASR-LLM, achieving superior performance compared to state-of-the-art baselines across ASR, VSR, and AVSR tasks under both clean and noisy conditions. Our method significantly improved recognition robustness across varying signal-to-noise ratios and noise types. Experiments with different SFM and LLM combinations show consistent improvements over baselines, demonstrating the generalization capability of our framework. Ablation studies validated the importance of visual injection pretraining, parameter sharing, and LLM integration.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "visual",
                    "ablation",
                    "pretraining",
                    "results",
                    "asr",
                    "noisy",
                    "injection",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite strong performance, our method has several limitations that warrant future investigation. First, while LLM integration leads to substantial speech recognition improvements, it also requires significant computational resources and memory footprint. This computational burden may limit practical deployment, suggesting the need for more efficient approaches such as dynamic pooling of adjacent audio-visual representations to reduce input tokens or exploring lightweight LLM architectures. Second, our method still lags behind previous lipreading studies that employ much larger audio-visual datasets, indicating that the combination of SFMs and LLMs cannot fully compensate for the shortage of large-scale audio-visual training corpora. This limitation fundamentally affects the visual representation extraction capacity of the visual encoder. We anticipate that employing visual encoders pretrained on large-scale visual datasets could further improve our results. Future work will focus on addressing these challenges to enhance both computational efficiency and performance of unified speech recognition systems.</p>\n\n",
                "matched_terms": [
                    "results",
                    "visual"
                ]
            }
        ]
    },
    "S4.T5": {
        "caption": "Table 5: WER (%) results on LRS3 test set for ablation study of parameter sharing and LLM integration. (N) denotes results on the constructed noisy test set.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Labeled data</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ASR (N)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AVSR (N)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\">30h</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">UASR-LLM (Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">28.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;w/o shared parameters</td>\n<td class=\"ltx_td ltx_align_center\">30.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;w/o LLM</td>\n<td class=\"ltx_td ltx_align_center\">32.0</td>\n<td class=\"ltx_td ltx_align_center\">5.3</td>\n<td class=\"ltx_td ltx_align_center\">2.8</td>\n<td class=\"ltx_td ltx_align_center\">40.1</td>\n<td class=\"ltx_td ltx_align_center\">6.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\">433h</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">UASR-LLM (Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">23.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;w/o shared parameters</td>\n<td class=\"ltx_td ltx_align_center\">24.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center\">0.91</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">32.8</span></td>\n<td class=\"ltx_td ltx_align_center\">2.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_b\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">&#8194;&#8202;&#160;&#160;&#160;w/o LLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">28.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">2.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">36.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">4.9</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sharing",
            "avsr",
            "wer",
            "denotes",
            "integration",
            "30h",
            "parameter",
            "study",
            "llm",
            "ablation",
            "test",
            "shared",
            "constructed",
            "results",
            "asr",
            "parameters",
            "labeled",
            "set",
            "ours",
            "lrs3",
            "data",
            "433h",
            "method",
            "uasrllm",
            "noisy",
            "vsr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This section compares our parameter sharing approach for realizing USR in a single model with the traditional method of training separate models for each task. The results (<span class=\"ltx_text ltx_font_bold\">w/o shared parameters</span>) are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table shows that training separate models leads to better ASR performance but worse VSR performance. Overall, the proposed USR method achieves comparable performance to the traditional approach that optimizes individual models for each task. These results demonstrate the effectiveness of our method for realizing USR, significantly reducing both training and deployment resources while maintaining strong performance across all tasks.</p>\n\n",
            "<p class=\"ltx_p\">This section compares our proposed method based on a decoder-only LLM with the traditional approach using a 6-layer Transformer-based decoder with cross-attention. The results (<span class=\"ltx_text ltx_font_bold\">w/o LLM</span>) are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T5\" title=\"Table 5 &#8227; 4.5.1 Visual injection pretraining &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The table demonstrates that our proposed method employing LLMs significantly outperforms the traditional Transformer decoder. This validates that the strong contextual and language modeling capabilities of LLMs, learned from large text corpora, can be effectively transferred to enhance speech recognition performance, which has also been demonstrated in previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified speech recognition aims to perform auditory, visual, and audiovisual speech recognition within a single model framework. While speech foundation models (SFMs) have demonstrated remarkable performance in auditory tasks, their adaptation to multimodal scenarios remains underexplored.\nThis paper presents UASR-LLM, a novel framework that adapts frozen SFMs to unified visual speech recognition (VSR), automatic speech recognition (ASR), and audio-visual speech recognition (AVSR) tasks by leveraging large language models (LLMs) as text decoders. Our approach introduces visual representations into multiple SFM layers through visual injection modules, enabling multimodal input processing and unified hidden representations. The augmented SFMs connect with decoder-only LLMs via a feed-forward adaptor, where concatenated representations and instruction prompts guide speech transcription.\nWe propose a two-stage training strategy: visual injection pretraining followed by speech recognition finetuning. SFM parameters remain frozen throughout training, with only visual injection modules optimized initially, and LLMs finetuned using LoRA parameters subsequently.\nExperimental results demonstrate superior performance over state-of-the-art baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Ablation studies confirm generalization across various SFMs and LLMs, validating the proposed training strategy.</p>\n\n",
                "matched_terms": [
                    "vsr",
                    "avsr",
                    "ablation",
                    "asr",
                    "results",
                    "uasrllm",
                    "noisy",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech recognition represents a fundamental challenge in pattern recognition&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib1\" title=\"\">1</a>]</cite>, with recent advances driven by speech foundation models (SFMs) such as WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> and Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite>.\nThese large-scale models, trained on extensive speech datasets, have demonstrated remarkable performance in automatic speech recognition (ASR). However, environmental noise remains a critical limitation that can significantly compromise recognition accuracy, particularly in real-world deployment scenarios.\nTo overcome this challenge, researchers have developed complementary approaches including visual speech recognition (VSR, also known as lipreading)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib6\" title=\"\">6</a>]</cite> and audio-visual speech recognition (AVSR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib8\" title=\"\">8</a>]</cite>, which leverage lip movement information either independently or in combination with audio signals. AVSR systems demonstrate substantial improvements in robustness under noisy conditions, making them increasingly valuable for practical applications such as intelligent home systems, online conferencing, and service robotics.</p>\n\n",
                "matched_terms": [
                    "vsr",
                    "avsr",
                    "noisy",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AVSR models typically adopt a sequence-to-sequence architecture nowadays, including recurrent neural network transducer (RNN-T)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite> or hybrid connectionist temporal classification (CTC) and attention architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib11\" title=\"\">11</a>]</cite>.\nRecently, audio-visual self-supervised learning has also been proposed to leverage\nunlabeled audio-visual data for pretraining, such as AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, REVAn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>, and AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>]</cite>.\nWith a further finetuning stage on few labeled data, these methods achieve a strong VSR and AVSR performance.\nBuilding on these progresses, Haliassos et al. introduced unified speech recognition (USR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib14\" title=\"\">14</a>]</cite>, which consolidates ASR, VSR, and AVSR tasks within a single model framework. This unified approach offers compelling advantages by eliminating the need to construct separate models for each modality, thereby significantly reducing training and deployment costs. The conceptual foundation is particularly appealing, as audio and visual lip movements represent complementary signal views of the same articulatory process during speech production.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "data",
                    "asr",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have explored utilizing SFMs for VSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib16\" title=\"\">16</a>]</cite> and AVSR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib9\" title=\"\">9</a>]</cite>. Our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> proposed using SFMs for knowledge distillation in audio-visual self-supervised pretraining, achieving competitive performance across VSR, ASR, and AVSR benchmarks.\nHowever, the knowledge distillation approach may inherently limit performance compared to direct utilization of SFMs, particularly in high signal-to-noise ratio (SNR) scenarios where the original models excel.\nSeveral approaches have focused on the direct utilization of SFMs for audio-visual tasks. Lip2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite> proposed reprogramming a wav2vec 2.0 model for lipreading by feeding transformed visual representations to the audio model. Whisper-flamingo&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib19\" title=\"\">19</a>]</cite> adapted the Whisper model for AVSR by incorporating an additional visual encoder to process visual information alongside audio inputs.</p>\n\n",
                "matched_terms": [
                    "study",
                    "avsr",
                    "vsr",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its success, most previous works address VSR, AVSR, and ASR tasks separately.\nIn this study, we explore tackling these tasks within a single unified model.\nFurthermore, we aim to leverage the rich knowledge embedded in both speech foundation models (SFMs) and large language models (LLMs) to design a general framework that achieves robust USR performance.\nWe propose <span class=\"ltx_text ltx_font_bold\">UASR-LLM</span> (<span class=\"ltx_text ltx_font_bold\">U</span>nified <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">R</span>ecognition with <span class=\"ltx_text ltx_font_bold\">L</span>arge <span class=\"ltx_text ltx_font_bold\">L</span>anguage <span class=\"ltx_text ltx_font_bold\">M</span>odels), a novel framework that adapts frozen speech foundation models to unified VSR, ASR, and AVSR tasks by leveraging LLMs as text decoders.\nOur approach employs a visual encoder from our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> to extract visual representations, which are then integrated into each block of the SFMs through visual injection modules. The SFM outputs are processed by an adapter to obtain hidden embeddings, which are subsequently fed into LLMs for conducting speech recognition across all modalities.\nTo enable SFMs to process visual and audio-visual inputs, we introduce a visual injection pretraining stage based on cross-modal knowledge distillation loss. The model then undergoes a second training stage for speech recognition, incorporating random modality dropout guided by corresponding instructions. Throughout the entire training process, both the SFM and visual encoder parameters remain frozen, while the LLM is finetuned using a small set of Low-Rank Adaptation (LoRA) weights&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib20\" title=\"\">20</a>]</cite>. This approach enables memory-efficient finetuning while preserving the generalization capabilities of the original models.\nBy utilizing the SFM as a unified backbone for processing multimodal audio and visual inputs, our framework facilitates knowledge transfer from audio to video data and unifies the hidden representations of both modalities. This provides a well-aligned output space for LLMs to generate transcriptions, seamlessly achieving VSR, ASR, and AVSR within a single model.</p>\n\n",
                "matched_terms": [
                    "set",
                    "vsr",
                    "avsr",
                    "study",
                    "llm",
                    "data",
                    "asr",
                    "uasrllm",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates that UASR-LLM consistently outperforms state-of-the-art baselines across all three tasks when trained with comparable amounts of audio-visual speech data. On the LRS3 test set, UASR-LLM achieves word error rates of 20.9%, 0.84%, and 0.69% for VSR, ASR, and AVSR, respectively. Under noisy conditions, our method exhibits substantial robustness, significantly reducing WER compared to baseline approaches. Furthermore, we demonstrate that our approach generalizes effectively across different speech foundation models and large language models, with all configurations outperforming baseline methods. Finally, we validate our training strategy and confirm the effectiveness of performing unified speech recognition within a single model architecture.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "lrs3",
                    "data",
                    "asr",
                    "test",
                    "method",
                    "uasrllm",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent years have witnessed the emergence of various speech foundation models, which can be broadly categorized into two paradigms: self-supervised and supervised approaches.\nAmong SSL-based speech foundation models,\nHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> utilizes k-means clustering to generate pseudo-class labels for model training using a mask prediction loss,\nwith iterative refinement through subsequent clustering and mask prediction steps.\nWavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib21\" title=\"\">21</a>]</cite> improves HuBERT by using a more diverse pretraining dataset and performs speech denoising modeling during pretraining.\nOn the other hand, Whisper family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> exemplifies supervised learning-based speech foundation model. Trained on large-scale multilingual and multitask labeled data, it demonstrates robust performance in ASR and speech translation tasks.</p>\n\n",
                "matched_terms": [
                    "data",
                    "asr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Visual representation learning is crucial for the performance of VSR and AVSR tasks, which can be broadly categorized into supervised and self-supervised approaches.\nSupervised methods leverage labeled&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib22\" title=\"\">22</a>]</cite> or weakly labeled audio-visual speech data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib23\" title=\"\">23</a>]</cite> to optimize representation extraction and text decoding in an end-to-end manner through label prediction. Several auxiliary tasks have been proposed to enhance visual encoder learning, including articulation feature prediction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>]</cite> and distillation from acoustic models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib26\" title=\"\">26</a>]</cite>.\nSelf-supervised methods utilize unlabeled audio-visual data with pretext tasks such as masked prediction loss functions. Representative approaches include AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, RAVen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib27\" title=\"\">27</a>]</cite>, AV2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and AV-data2vec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib29\" title=\"\">29</a>]</cite>. Our previous work proposed a joint audio-visual representation learning method&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite> that leverages cross-modal knowledge distillation from pretrained SFMs.\nThis work adopts our previously pretrained model for visual representation extraction, which can be viewed as an extension of our previous study. Here, we further leverage SFMs directly for processing audio-visual data, moving beyond the distillation approach to direct utilization of foundation models.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "study",
                    "data",
                    "method",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our UASR-LLM method comprises two main components: an audio-visual adapted speech foundation model (SFM-AV)\nthat processes both audio and visual modalities and encodes them into a unified representational space,\nand a large language model that decodes the encoded representations into text.\nThe SFM-AV consists of a visual encoder and a speech foundation model enhanced with visual injection modules.\nOur model architecture is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F1\" title=\"Figure 1 &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThe training of our model follows a two-stage process: in the first pretraining stage,\nthe SFM is adapted to encode both audio and visual information into a unified hidden space;\nin the second finetuning stage, the adapted SFM is connected to the LLM to enable unified speech recognition.\nThe detailed model architecture and training strategy are described in the following sections.</p>\n\n",
                "matched_terms": [
                    "uasrllm",
                    "llm",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The output representations from the SFM-AV are transformed through a two-layer feed-forward adaptor to match the embedding dimension of large language models, serving as continuous audio-visual tokens for prompting the LLM to generate transcribed text. We employ task-specific instructions that prepend the audio-visual tokens according to the unified speech recognition task. The instruction prompt follows the template &#8220;Transcribe the [].&#8221;, where the placeholder [] is filled with &#8220;audio&#8221;, &#8220;video&#8221;, or &#8220;audio and video&#8221; for ASR, VSR, and AVSR tasks, respectively.\nFor efficient LLM finetuning, we adopt the low-rank adaptation (LoRA) method. The low-rank parameters are applied to adapt all blocks&#8217; self-attention components as well as the output projection layer for predicting text logits. Unlike previous approaches such as Llama-AVSR&#8217;s frame-compression&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib36\" title=\"\">36</a>]</cite> or MMS-Llama&#8217;s Q-former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib37\" title=\"\">37</a>]</cite>, our method does not employ frame reduction techniques. This design choice reflects our primary focus on improving recognition performance rather than computational efficiency, with the latter left for future investigation.</p>\n\n",
                "matched_terms": [
                    "vsr",
                    "avsr",
                    "llm",
                    "method",
                    "asr",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For SFM-AV pretraining, we apply several data corruption strategies to promote effective audio-visual fusion when predicting clean audio representations. First, the input audio is corrupted by randomly mixing it with background noise, resulting in <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math>. Both the noisy audio <math alttext=\"A^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m2\" intent=\":literal\"><semantics><msup><mi>A</mi><mi>N</mi></msup><annotation encoding=\"application/x-tex\">A^{N}</annotation></semantics></math> and the video <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m3\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> are then corrupted using span-based masking. Specifically, for each modality, we randomly select a starting frame and replace a consecutive span of frames with zeros starting from that point.\nIn addition, we adopt modality dropout during training. With probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math>, only the video input is provided to the model, while the audio input is replaced with zeros. With probability <math alttext=\"1-p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msub><mi>p</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">1-p_{v}</annotation></semantics></math>, both audio and video inputs are used.\nAfter applying noise mixing, span masking, and modality dropout, SFM-AV processes the corrupted inputs to generate output representations <math alttext=\"H^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m6\" intent=\":literal\"><semantics><msup><mi>H</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">H^{o}</annotation></semantics></math>, which is summarized as:</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m1\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> represents learnable projection parameters.\nThis first-stage training leverages unlabeled audio-visual speech data for unsupervised pretraining. And the large language model are not involved at this stage.\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the pretraining stage, we connect the SFM-AV encoder with an LLM to perform unified speech recognition across multiple modalities,\nas shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.F3\" title=\"Figure 3 &#8227; 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The model pathway varies depending on the target task: for VSR and AVSR, we utilize SFM representations enhanced with visual information through the injection module. For ASR, we bypass the visual injection module and employ the original SFM to process audio-only inputs.\nTo enhance model robustness, we employ a modality dropout strategy with probabilities <math alttext=\"p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{v}^{\\prime}</annotation></semantics></math>, <math alttext=\"p_{a}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p_{a}^{\\prime}</annotation></semantics></math>, and <math alttext=\"1-p_{a}^{\\prime}-p_{v}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>&#8722;</mo><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">1-p_{a}^{\\prime}-p_{v}^{\\prime}</annotation></semantics></math> for visual-only, audio-only, and audio-visual combined inputs, respectively. The instruction prompts are selected accordingly to match the input modality configuration.</p>\n\n",
                "matched_terms": [
                    "vsr",
                    "avsr",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> controls the relative importance of the auxiliary CTC loss.\nDuring finetuning, we jointly optimize the LoRA parameters of the LLM, the feed-forward adaptor, and the visual injection module, while keeping the core SFM parameters frozen.</p>\n\n",
                "matched_terms": [
                    "parameters",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets.</span>\nFor pretraining, we utilized the LRS3 dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mmai.io/datasets/lip_reading/\" title=\"\">https://mmai.io/datasets/lip_reading/</a></span></span></span>, which comprises approximately 433 hours of English audio-visual speech data. We also employed a combination dataset of LRS3 and the English-only version of VoxCeleb2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html\" title=\"\">https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html</a></span></span></span>, curated by Shi et al.&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, totaling approximately 1759 hours of audio-visual speech data. The VoxCeleb data was transcribed using the Whisper-large-v2 model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v2\" title=\"\">https://huggingface.co/openai/whisper-large-v2</a></span></span></span>.\nFor finetuning, we adopted both a 30-hour subset and the full 433-hour training set of LRS3. Additionally, the model can be finetuned on the complete 1759-hour audio-visual speech dataset. A validation set of 1000 utterances was reserved, and results were reported on the LRS3 test set. We used the facial images provided in the original datasets without cropping to the lip region-of-interest (ROI)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib39\" title=\"\">39</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "lrs3",
                    "data",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Noise augmentation.</span>\nThe MUSAN dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/17/\" title=\"\">https://www.openslr.org/17/</a></span></span></span> was employed for noise augmentation. Following the protocol outlined in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib40\" title=\"\">40</a>]</cite>, we sampled and added noise to the audio input of the student model during training. To evaluate our proposed method for AVSR under noisy conditions, we constructed test sets with various noise types, including music, natural sounds, speech, and babble, at signal-to-noise ratios (SNRs) of -10, -5, 0, 5, and 10 dB.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "constructed",
                    "method",
                    "test",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model configurations.</span>\nOur experiments employed a default configuration consisting of three main components: the DistillAV model as the visual encoder, WavLM-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib2\" title=\"\">2</a>]</cite> (317M parameters) as the speech foundation model (SFM), and Qwen 2.5-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2.5-7B</a></span></span></span> as the large language model. The visual encoder offered two variants: Base (103M parameters) and Large (325M parameters). Following our previous study, we denote the proposed method as UASR-LLM-B and UASR-LLM-L accordingly.\nThe visual injection modules were configured with 8 heads, 512 hidden dimensions, and 256 dimensions for both cross-attention and feed-forward networks, totaling approximately 58M parameters. The two-layer feed-forward network adapter that connects the SFM and LLMs contains around 10M parameters. LoRA weights were configured with a rank of 16, resulting in 12M total parameters.\nTo demonstrate the generalization capability of our method, we explored additional speech foundation models: the open-source Whisper-medium&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib3\" title=\"\">3</a>]</cite> and an internal iFLYTEK-Speech model, both containing approximately 300M parameters comparable to WavLM-large. We also evaluated ChatGLM v3-6B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/THUDM/chatglm3-6b\" title=\"\">https://huggingface.co/THUDM/chatglm3-6b</a></span></span></span> as an alternative large language model.</p>\n\n",
                "matched_terms": [
                    "study",
                    "parameters",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WavLM model generates representations at 50 fps, double the frame rate of visual representations. We set relative positional encoding according to the real-time interval between audio and visual frames using sinusoidal positional embedding. The parameter <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E4\" title=\"In 3.2.1 Visual injection pretraining &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> was set to 8 for WavLM-based SFM and 1 for Whisper and iFLYTEK-Speech-based SFM, following our previous study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "study",
                    "parameter",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During pretraining, 80% of audio features and 30% of video features were masked for the student model, with modality dropout probability <math alttext=\"p_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">p_{v}</annotation></semantics></math> set to 0.5. We employed the Adam optimizer with a learning rate schedule that linearly warmed up to 0.0005 over the initial 5% of updates, maintained this rate for 85% of updates, then exponentially decayed over the remaining 10%. The total number of updates was 100k and 200k for 433h and 1759h pretraining data, respectively. During finetuning, modality dropout probabilities <math alttext=\"p^{\\prime}_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>v</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{v}</annotation></semantics></math> and <math alttext=\"p^{\\prime}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><msubsup><mi>p</mi><mi>a</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">p^{\\prime}_{a}</annotation></semantics></math> were set to 0.5 and 0.25, respectively. For the auxiliary CTC loss, we utilized subword units with a vocabulary size of 1000 as targets, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> in Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S3.E7\" title=\"In 3.2.2 Speech recognition finetuning &#8227; 3.2 Training strategy &#8227; 3 Proposed method &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> set to 0.25. The learning rate warmed up to 0.0002 over the initial one-third of updates, followed by exponential decay. The total number of updates was 30k, 60k, and 120k for 30h, 433h, and 1759h finetuning data, respectively.</p>\n\n",
                "matched_terms": [
                    "set",
                    "data",
                    "433h",
                    "30h"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> present experimental results for models finetuned with low-resource (30h) and high-resource (433h or 1759h) labeled data, respectively. The unlabeled data refers to audio-visual data used for pretraining the DistillAV-based visual encoder and visual injection pretraining, while the labeled data denotes audio-visual data employed for speech recognition finetuning.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "data",
                    "433h",
                    "results",
                    "30h",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T1\" title=\"Table 1 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method outperforms most baselines across VSR, ASR, and AVSR tasks under both clean and noisy conditions. Although underperforming Llama-AVSR in ASR, our approach provides a unified framework effective across all three tasks. The superior VSR performance particularly demonstrates the effectiveness of &#8221;reprogramming&#8221; WavLM for lipreading with LLMs, consistent with previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWith 433h labeled data, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Comparison with state-of-the-art baselines &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, our method achieves significant accuracy improvements. Compared to previous studies using comparable audio-visual data, our approach demonstrates superior performance across all tasks under both clean and noisy conditions. With 1759h labeled data, our method achieves WERs of 20.9%, 0.84%, and 0.69% on the clean LRS3 test set, rivaling or surpassing state-of-the-art methods. However, our VSR results still lag behind LP-Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib43\" title=\"\">43</a>]</cite>, which uses approximately 100k hours of audio-visual data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "test",
                    "lrs3",
                    "data",
                    "433h",
                    "asr",
                    "results",
                    "method",
                    "noisy",
                    "vsr",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comparing ASR and AVSR performance on our noisy test set reveals that incorporating visual information significantly improves speech recognition robustness.\nThis substantial WER reduction demonstrates that our method successfully fuses speech foundation models with visual representations to enhance noise-invariant capabilities.\nThe next section provides detailed AVSR results under noisy conditions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "wer",
                    "asr",
                    "test",
                    "results",
                    "method",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate robustness under noisy conditions, we constructed four test sets by mixing the clean audio from the LRS3 test set with different noise types: speech, babble, natural, and music, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.SS1\" title=\"4.1 Implementation details &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. We compared our approach against multiple baselines, including two audio-only SFM models (WavLM and iFLYTEK-Speech) and three AVSR models (AV-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib12\" title=\"\">12</a>]</cite>, AV2vec-MLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib28\" title=\"\">28</a>]</cite>, and DistillAV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#bib.bib18\" title=\"\">18</a>]</cite>). All audio-only models were finetuned on the LRS3 dataset. The experiments utilized 433h data for pretraining, with all models adopting the Base configuration. We used either 30h or 433h of LRS3 data for finetuning.\nThe experimental results are summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.F4\" title=\"Figure 4 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "avsr",
                    "30h",
                    "lrs3",
                    "constructed",
                    "data",
                    "433h",
                    "results",
                    "test",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed UASR-LLM method (red line) consistently outperforms all baselines across different noise types and SNR levels, with particularly pronounced advantages in severe noise conditions below 0 dB. Among the four noise conditions, babble noise proves most challenging, with all methods exhibiting higher WER at low SNRs, particularly below 0 dB. However, the proposed approach maintains substantially lower error rates than competing methods even in these challenging low-SNR scenarios. The results highlight the effectiveness of incorporating visual information, as audio-only models (WavLM and iFLYTEK-Speech) degrade rapidly in noisy conditions compared to AVSR approaches.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "wer",
                    "method",
                    "results",
                    "uasrllm",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the generalization capability of our approach, we conducted experiments with different speech feature models (SFMs) and large language models (LLMs). Specifically, we replaced the WavLM-based SFM with two supervised learning alternatives: Whisper and iFLYTEK-Speech. Additionally, we compared ChatGLM v3-6B with Qwen 2.5-7B as the backbone LLM.\nFor training, we used 433 hours of data for the visual injection pretraining stage and finetune the models on both 30 hours and 433 hours of the LRS3 dataset. We adopted a pretrained visual encoder (Base version) trained on 1759 hours of data. The experimental results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T3\" title=\"Table 3 &#8227; 4.3 AVSR results on noisy test sets &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "lrs3",
                    "data",
                    "llm",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first analyze the influence of different SFMs using the same LLM and identical amounts of finetuning data. The results show that iFLYTEK-Speech achieves the best ASR performance under both clean and noisy conditions, while WavLM performs best on the VSR task. For AVSR, WavLM achieves superior results on the noisy testset, whereas the clean AVSR results do not exhibit a clear pattern across different SFMs.\nWhen examining the influence of LLMs with the same SFM, we observe that ChatGLM v3-6B slightly outperforms Qwen 2.5-7B when using low-resource data (30 hours) for finetuning. However, this trend reverses with higher-resource data (433 hours), where Qwen 2.5-7B demonstrates better performance.\nDespite performance variations across different SFM and LLM combinations, our UASR-LLM consistently achieves significant improvements in recognition accuracy compared to baselines, demonstrating the strong generalization ability of our proposed method.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "llm",
                    "data",
                    "asr",
                    "results",
                    "method",
                    "uasrllm",
                    "noisy",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our ablation studies, we employed the Base version of the visual encoder pretrained on 1759h of audio-visual data. We used either 30h or 433h of LRS3 data for finetuning. Our ablation studies examined several key design components of our method, including visual injection pretraining, parameter sharing for USR, and the integration of LLM.</p>\n\n",
                "matched_terms": [
                    "sharing",
                    "llm",
                    "lrs3",
                    "ablation",
                    "data",
                    "433h",
                    "integration",
                    "method",
                    "30h",
                    "parameter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22961v1#S4.T4\" title=\"Table 4 &#8227; 4.5 Ablation studies &#8227; 4 Experiments and results &#8227; Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the experimental results of our visual injection pretraining ablation study. The results demonstrate that omitting the proposed visual injection pretraining stage (<span class=\"ltx_text ltx_font_bold\">0h</span> for VIPT data) leads to degraded performance in both VSR and AVSR tasks. Furthermore, increasing the amount of pretraining data during the visual injection pretraining stage yields additional improvements in VSR and AVSR performance. These superior results for VSR and AVSR tasks validate the effectiveness of our pretraining stage in processing visual information.\nInterestingly, for the ASR task, the model without the pretraining stage achieves the best results. Since we froze all SFM parameters throughout training, the audio processing capability of SFM itself remained unaffected by the visual injection pretraining stage. This phenomenon may be attributed to our parameter sharing strategy for USR in LLMs, where enhanced AVSR and VSR performance appears to cause a slight degradation in ASR performance&#8212;a trade-off we explore in the following section.</p>\n\n",
                "matched_terms": [
                    "sharing",
                    "vsr",
                    "avsr",
                    "study",
                    "ablation",
                    "data",
                    "results",
                    "asr",
                    "parameter",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UASR-LLM, a novel approach that adapts speech foundation models (SFMs) for unified speech recognition, encompassing ASR, VSR, and AVSR within a single framework using a decoder-only LLM for text prediction.\nOur method augments the SFM with a visual encoder and inserts visual injection modules into each SFM block to fuse visual representations with audio features. We introduce a visual injection pretraining stage where corrupted audio-visual inputs are processed to predict clean audio representations, followed by speech recognition finetuning with modality dropout. The approach leverages the complementary strengths of SFMs pretrained on large-scale audio corpora and LLMs pretrained on extensive text data.</p>\n\n",
                "matched_terms": [
                    "avsr",
                    "llm",
                    "data",
                    "method",
                    "asr",
                    "uasrllm",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrated the effectiveness of UASR-LLM, achieving superior performance compared to state-of-the-art baselines across ASR, VSR, and AVSR tasks under both clean and noisy conditions. Our method significantly improved recognition robustness across varying signal-to-noise ratios and noise types. Experiments with different SFM and LLM combinations show consistent improvements over baselines, demonstrating the generalization capability of our framework. Ablation studies validated the importance of visual injection pretraining, parameter sharing, and LLM integration.</p>\n\n",
                "matched_terms": [
                    "sharing",
                    "avsr",
                    "llm",
                    "ablation",
                    "integration",
                    "results",
                    "asr",
                    "method",
                    "uasrllm",
                    "noisy",
                    "parameter",
                    "vsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite strong performance, our method has several limitations that warrant future investigation. First, while LLM integration leads to substantial speech recognition improvements, it also requires significant computational resources and memory footprint. This computational burden may limit practical deployment, suggesting the need for more efficient approaches such as dynamic pooling of adjacent audio-visual representations to reduce input tokens or exploring lightweight LLM architectures. Second, our method still lags behind previous lipreading studies that employ much larger audio-visual datasets, indicating that the combination of SFMs and LLMs cannot fully compensate for the shortage of large-scale audio-visual training corpora. This limitation fundamentally affects the visual representation extraction capacity of the visual encoder. We anticipate that employing visual encoders pretrained on large-scale visual datasets could further improve our results. Future work will focus on addressing these challenges to enhance both computational efficiency and performance of unified speech recognition systems.</p>\n\n",
                "matched_terms": [
                    "method",
                    "llm",
                    "integration",
                    "results"
                ]
            }
        ]
    }
}