{
    "S3.T1": {
        "source_file": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching",
        "caption": "Table 1: English evaluation results. Bold indicates the best result, and underline indicates the second best.",
        "body": "Model\nSubjective\nObjective\n\n\nSpontaneity (↑\\uparrow)\nCoherence (↑\\uparrow)\nIntelligibility (↑\\uparrow)\nSIM-O (↑\\uparrow)\nWER (↓\\downarrow)\nUTMOS (↑\\uparrow)\n\n\n\n\nCosyVoice2\n3.43\n3.32\n3.88\n0.72\n2.40\n3.516\n\n\nCoVoMix (8 kHz, Fisher)\n3.64\n3.38\n3.02\n0.46\n9.71\n1.735\n\n\nDialoSpeech (ours)\n\n3.71\n3.37\n3.74\n0.67\n8.62\n2.836",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Subjective</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Objective</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Spontaneity (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Coherence (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Intelligibility (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-O (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">UTMOS (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CosyVoice2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.516</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CoVoMix (8 kHz, Fisher)</th>\n<td class=\"ltx_td ltx_align_center\">3.64</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.38</span></td>\n<td class=\"ltx_td ltx_align_center\">3.02</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">9.71</td>\n<td class=\"ltx_td ltx_align_center\">1.735</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">DialoSpeech <span class=\"ltx_text ltx_font_bold\">(ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">8.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.836</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cosyvoice2",
            "fisher",
            "wer",
            "subjective",
            "coherence",
            "utmos",
            "underline",
            "↑uparrow",
            "simo",
            "spontaneity",
            "khz",
            "covomix",
            "evaluation",
            "best",
            "objective",
            "results",
            "dialospeech",
            "bold",
            "indicates",
            "↓downarrow",
            "second",
            "result",
            "english",
            "ours",
            "intelligibility",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the performance of our proposed DialoSpeech system in both objective and subjective metrics, and compare it with two strong baselines: CoVoMix and CosyVoice2. Additionally, we include MoonCast as a topline reference. The evaluation results for Chinese and English dialogue generation are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#S3.T1\" title=\"Table 1 &#8227; 3.3.2 Block-Wise Guided Attention &#8227; 3.3 Streaming Waveform Reconstruction via Chunked Flow Matching &#8227; 3 Methodology &#8227; DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#S3.T2\" title=\"Table 2 &#8227; 3.3.2 Block-Wise Guided Attention &#8227; 3.3 Streaming Waveform Reconstruction via Chunked Flow Matching &#8227; 3 Methodology &#8227; DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-speech (TTS) synthesis, particularly those leveraging large language models (LLMs), have significantly improved expressiveness and naturalness. However, generating human-like, interactive dialogue speech remains challenging. Current systems face limitations due to the scarcity of dual-track data and difficulties in achieving naturalness, contextual coherence, and interactional dynamics, such as turn-taking, overlapping speech, and speaker consistency, in multi-turn conversations. To address these challenges, we propose <span class=\"ltx_text ltx_font_bold\">DialoSpeech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">Codes and checkpoints will be publicly released.</span></span></span></span></span>, a dual-track architecture combining a large language model with Chunked Flow Matching for expressive, human-like dialogue speech synthesis. DialoSpeech generates natural multi-turn conversations with coherent speaker turns and natural overlaps, supporting both Chinese and English and cross-lingual speech synthesis. We introduce a data processing pipeline to construct dual-track dialogue datasets, facilitating scalable training and experimental validation. Experiments show that our model outperforms baselines, offering a solution for generating human-like spoken dialogues. Audio samples are available at\n<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://tiamojames.github.io/DialoSpeech/\" title=\"\">https://tiamojames.github.io/DialoSpeech/</a></p>\n\n",
                "matched_terms": [
                    "coherence",
                    "english",
                    "model",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoVoMix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx9\" title=\"\">9</a>]</cite> introduced a dual-channel architecture to model multi-speaker interactions, marking one of the earliest attempts at zero-shot, human-like, mixed-speech generation. However, CoVoMix is trained on the Fisher dataset, which is limited in scale and audio quality. MoonCast&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx10\" title=\"\">10</a>]</cite> focuses on long-form, spontaneous dialogue generation by leveraging large language models (LLMs) to script podcast-style conversations. Still, its single-stream token representation prevents it from effectively modelling crucial interactional dynamics such as overlapping speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "covomix",
                    "fisher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present DialoSpeech, a Dialogue TTS architecture that combines a Language Model (LM), a dual-track token generation mechanism, and a chunked Flow Matching acoustic model, enabling zero-shot, high-quality, and expressive dialogue synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrate that DialoSpeech consistently outperforms baselines across multiple metrics. We will release the implementation and pretrained checkpoints to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "results",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the T2T stage, we use the S3tokenizer from CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx16\" title=\"\">16</a>]</cite> to convert speech into a dual-track semantic token representation. These tokens then serve as the training target for our T2T model.\nThe T2T model, guided by an LLM, processes the input dialogue scripts to predict these dual-track semantic tokens. This allows for modeling inter-speaker dynamics, including turn-taking and overlaps, crucial for natural dialogue. This LLM interprets conversational context and speaker roles.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike single-speaker zero-shot speech synthesis,\nThe zero-shot Dual-speaker Dialogue generation task aims to synthesize each conversation turn using the corresponding speaker&#8217;s voice, based on the provided reference speech from two speakers. The core component of DialoSpeech is the DiaLM model, which enables zero-shot dual-speaker conversational speech generation. Given dialogue text with alternating speaker turns and reference audio samples from two target speakers, DiaLM learns to synthesize each utterance in the corresponding speaker&#8217;s voice while capturing natural conversational turn transitions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"S^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msup><mi>S</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">S^{c}</annotation></semantics></math> denotes the semantic token sequence for speaker <math alttext=\"c\\in\\{1,2\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{1,2\\}</annotation></semantics></math>, <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the dialogue text, <math alttext=\"S_{p}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>S</mi><mi>p</mi><mi>c</mi></msubsup><annotation encoding=\"application/x-tex\">S_{p}^{c}</annotation></semantics></math> is the reference embedding of speaker <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, and <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> represents model parameters. This objective encourages the model to generate speaker-aware, semantically aligned token sequences in a dual-stream manner.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset includes: (1) 3,000 hours of professionally recorded Chinese dialogues from Biaobei Corp., providing clean, structured conversations; (2) 5,000 hours of spontaneous multi-speaker Chinese podcast data, offering acoustic and stylistic diversity; and (3) 2,000 hours of English telephone dialogues from the Fisher corpus, used to enhance cross-lingual generalization. Fisher captures real-world bilingual interactions despite their noise and was similarly processed into dual-speaker semantic streams.</p>\n\n",
                "matched_terms": [
                    "english",
                    "fisher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx16\" title=\"\">16</a>]</cite>: A multilingual, zero-shot, single-speaker TTS model. We generate each turn independently for dialogue evaluation and concatenate the audio to form complete conversations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "cosyvoice2",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CoVoMix</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx9\" title=\"\">9</a>]</cite>: We re-implement CoVoMix and train it on the English Fisher dataset to benchmark its performance in multi-speaker dialogue settings.</p>\n\n",
                "matched_terms": [
                    "english",
                    "covomix",
                    "fisher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate system performance using both objective and subjective metrics:</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Metrics.</span>\nWe assess the generated speech using several quantitative indicators. First, we compute Word Error Rate (WER) and Character Error Rate (CER) based on transcriptions produced by the FunASR toolkit<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/modelscope/FunASR</span></span></span>, which offers state-of-the-art Mandarin speech recognition performance. These metrics reflect the intelligibility and phonetic accuracy of synthesized outputs.\nSecond, we calculate the cosine similarity (SIM) between generated and reference audio speaker embeddings to evaluate speaker identity preservation.\nFinally, we report UTMOS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/tarepan/SpeechMOS</span></span></span> scores to estimate perceptual quality regarding naturalness and clarity.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "intelligibility",
                    "second",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DialoSpeech achieves the lowest CER in Chinese, outperforming CosyVoice2 and MoonCast. It also attains a speaker similarity score of 0.69, comparable to both baselines. Notably, our system is trained on only &#160;10K hours of data with a lightweight 0.5B-parameter model, and MoonCast and CosyVoice2 rely on over 1M hours, demonstrating the efficiency of our dual-track framework.\nRegarding UTMOS, DialoSpeech matches CosyVoice2 and surpasses MoonCast in short-dialogue scenarios, likely due to the latter&#8217;s focus on long-form conversations. Subjective evaluations show that DialoSpeech performs on par overall and leads in Spontaneity, highlighting its ability to produce natural, lifelike dialogues.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "spontaneity",
                    "subjective",
                    "dialospeech",
                    "utmos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate cross-lingual generalization by using Chinese prompts to generate English dialogue. DialoSpeech is trained on English speech from the Fisher corpus only, yet achieves significantly better speaker similarity and UTMOS than CoVoMix. While CosyVoice2 performs better on WER and UTMOS due to access to larger English datasets, DialoSpeech outperforms it in subjective Spontaneity and Coherence, demonstrating robust cross-lingual generalization under a limited English dataset.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "english",
                    "spontaneity",
                    "fisher",
                    "covomix",
                    "wer",
                    "subjective",
                    "coherence",
                    "dialospeech",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced <em class=\"ltx_emph ltx_font_italic\">DialoSpeech</em>, a dual-speaker dialogue TTS that integrates a language model with dual-track token generation and chunked Flow Matching. To overcome dialogue data scarcity, we designed a pipeline to scale up. Experiments in Chinese and English show that DialoSpeech outperforms strong baselines and achieves competitive objective scores in less training data. In the future, we first need to scale up the dialogue datasets. Due to training the language model with Flow Matching, it becomes difficult on minute-scale data because of memory limitations. Therefore, exploring continuous latent representations for dialogue modeling is a promising future direction and trend.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "dialospeech",
                    "objective"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching",
        "caption": "Table 2: Chinese evaluation results. Bold indicates the best result, and underline indicates the second best.",
        "body": "Model\nSubjective\nObjective\n\n\nSpontaneity (↑\\uparrow)\nCoherence (↑\\uparrow)\nIntelligibility (↑\\uparrow)\nSIM-O (↑\\uparrow)\nCER (↓\\downarrow)\nUTMOS (↑\\uparrow)\n\n\n\n\nCosyVoice2\n3.44\n3.52\n4.18\n0.75\n2.81\n3.499\n\n\nMoonCast\n3.87\n3.98\n4.23\n0.74\n3.61\n2.745\n\n\nDialoSpeech (ours)\n\n3.96\n3.79\n4.12\n0.69\n2.27\n3.410",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Subjective</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Objective</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Spontaneity (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Coherence (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Intelligibility (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM-O (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">UTMOS (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CosyVoice2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.499</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MoonCast</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center\">3.61</td>\n<td class=\"ltx_td ltx_align_center\">2.745</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">DialoSpeech <span class=\"ltx_text ltx_font_bold\">(ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.410</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cosyvoice2",
            "subjective",
            "coherence",
            "utmos",
            "chinese",
            "underline",
            "↑uparrow",
            "simo",
            "spontaneity",
            "evaluation",
            "best",
            "objective",
            "results",
            "dialospeech",
            "bold",
            "indicates",
            "↓downarrow",
            "mooncast",
            "second",
            "result",
            "ours",
            "intelligibility",
            "cer",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the performance of our proposed DialoSpeech system in both objective and subjective metrics, and compare it with two strong baselines: CoVoMix and CosyVoice2. Additionally, we include MoonCast as a topline reference. The evaluation results for Chinese and English dialogue generation are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#S3.T1\" title=\"Table 1 &#8227; 3.3.2 Block-Wise Guided Attention &#8227; 3.3 Streaming Waveform Reconstruction via Chunked Flow Matching &#8227; 3 Methodology &#8227; DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#S3.T2\" title=\"Table 2 &#8227; 3.3.2 Block-Wise Guided Attention &#8227; 3.3 Streaming Waveform Reconstruction via Chunked Flow Matching &#8227; 3 Methodology &#8227; DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in text-to-speech (TTS) synthesis, particularly those leveraging large language models (LLMs), have significantly improved expressiveness and naturalness. However, generating human-like, interactive dialogue speech remains challenging. Current systems face limitations due to the scarcity of dual-track data and difficulties in achieving naturalness, contextual coherence, and interactional dynamics, such as turn-taking, overlapping speech, and speaker consistency, in multi-turn conversations. To address these challenges, we propose <span class=\"ltx_text ltx_font_bold\">DialoSpeech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">1</span></span><span class=\"ltx_text ltx_font_medium\">Codes and checkpoints will be publicly released.</span></span></span></span></span>, a dual-track architecture combining a large language model with Chunked Flow Matching for expressive, human-like dialogue speech synthesis. DialoSpeech generates natural multi-turn conversations with coherent speaker turns and natural overlaps, supporting both Chinese and English and cross-lingual speech synthesis. We introduce a data processing pipeline to construct dual-track dialogue datasets, facilitating scalable training and experimental validation. Experiments show that our model outperforms baselines, offering a solution for generating human-like spoken dialogues. Audio samples are available at\n<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://tiamojames.github.io/DialoSpeech/\" title=\"\">https://tiamojames.github.io/DialoSpeech/</a></p>\n\n",
                "matched_terms": [
                    "coherence",
                    "model",
                    "dialospeech",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CoVoMix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx9\" title=\"\">9</a>]</cite> introduced a dual-channel architecture to model multi-speaker interactions, marking one of the earliest attempts at zero-shot, human-like, mixed-speech generation. However, CoVoMix is trained on the Fisher dataset, which is limited in scale and audio quality. MoonCast&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx10\" title=\"\">10</a>]</cite> focuses on long-form, spontaneous dialogue generation by leveraging large language models (LLMs) to script podcast-style conversations. Still, its single-stream token representation prevents it from effectively modelling crucial interactional dynamics such as overlapping speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present DialoSpeech, a Dialogue TTS architecture that combines a Language Model (LM), a dual-track token generation mechanism, and a chunked Flow Matching acoustic model, enabling zero-shot, high-quality, and expressive dialogue synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experimental results demonstrate that DialoSpeech consistently outperforms baselines across multiple metrics. We will release the implementation and pretrained checkpoints to facilitate further research.</p>\n\n",
                "matched_terms": [
                    "results",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the T2T stage, we use the S3tokenizer from CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx16\" title=\"\">16</a>]</cite> to convert speech into a dual-track semantic token representation. These tokens then serve as the training target for our T2T model.\nThe T2T model, guided by an LLM, processes the input dialogue scripts to predict these dual-track semantic tokens. This allows for modeling inter-speaker dynamics, including turn-taking and overlaps, crucial for natural dialogue. This LLM interprets conversational context and speaker roles.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike single-speaker zero-shot speech synthesis,\nThe zero-shot Dual-speaker Dialogue generation task aims to synthesize each conversation turn using the corresponding speaker&#8217;s voice, based on the provided reference speech from two speakers. The core component of DialoSpeech is the DiaLM model, which enables zero-shot dual-speaker conversational speech generation. Given dialogue text with alternating speaker turns and reference audio samples from two target speakers, DiaLM learns to synthesize each utterance in the corresponding speaker&#8217;s voice while capturing natural conversational turn transitions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dialospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"S^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msup><mi>S</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">S^{c}</annotation></semantics></math> denotes the semantic token sequence for speaker <math alttext=\"c\\in\\{1,2\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c\\in\\{1,2\\}</annotation></semantics></math>, <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the dialogue text, <math alttext=\"S_{p}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><msubsup><mi>S</mi><mi>p</mi><mi>c</mi></msubsup><annotation encoding=\"application/x-tex\">S_{p}^{c}</annotation></semantics></math> is the reference embedding of speaker <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, and <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> represents model parameters. This objective encourages the model to generate speaker-aware, semantically aligned token sequences in a dual-stream manner.</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CosyVoice2</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx16\" title=\"\">16</a>]</cite>: A multilingual, zero-shot, single-speaker TTS model. We generate each turn independently for dialogue evaluation and concatenate the audio to form complete conversations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "cosyvoice2",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MoonCast</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.08373v1#bib.bibx10\" title=\"\">10</a>]</cite>: A state-of-the-art conversational podcast generation system, included for performance comparison on Chinese dialogue.</p>\n\n",
                "matched_terms": [
                    "mooncast",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate system performance using both objective and subjective metrics:</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Metrics.</span>\nWe assess the generated speech using several quantitative indicators. First, we compute Word Error Rate (WER) and Character Error Rate (CER) based on transcriptions produced by the FunASR toolkit<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/modelscope/FunASR</span></span></span>, which offers state-of-the-art Mandarin speech recognition performance. These metrics reflect the intelligibility and phonetic accuracy of synthesized outputs.\nSecond, we calculate the cosine similarity (SIM) between generated and reference audio speaker embeddings to evaluate speaker identity preservation.\nFinally, we report UTMOS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/tarepan/SpeechMOS</span></span></span> scores to estimate perceptual quality regarding naturalness and clarity.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "intelligibility",
                    "second",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DialoSpeech achieves the lowest CER in Chinese, outperforming CosyVoice2 and MoonCast. It also attains a speaker similarity score of 0.69, comparable to both baselines. Notably, our system is trained on only &#160;10K hours of data with a lightweight 0.5B-parameter model, and MoonCast and CosyVoice2 rely on over 1M hours, demonstrating the efficiency of our dual-track framework.\nRegarding UTMOS, DialoSpeech matches CosyVoice2 and surpasses MoonCast in short-dialogue scenarios, likely due to the latter&#8217;s focus on long-form conversations. Subjective evaluations show that DialoSpeech performs on par overall and leads in Spontaneity, highlighting its ability to produce natural, lifelike dialogues.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "spontaneity",
                    "subjective",
                    "dialospeech",
                    "utmos",
                    "chinese",
                    "mooncast",
                    "cer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate cross-lingual generalization by using Chinese prompts to generate English dialogue. DialoSpeech is trained on English speech from the Fisher corpus only, yet achieves significantly better speaker similarity and UTMOS than CoVoMix. While CosyVoice2 performs better on WER and UTMOS due to access to larger English datasets, DialoSpeech outperforms it in subjective Spontaneity and Coherence, demonstrating robust cross-lingual generalization under a limited English dataset.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "spontaneity",
                    "subjective",
                    "coherence",
                    "dialospeech",
                    "utmos",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced <em class=\"ltx_emph ltx_font_italic\">DialoSpeech</em>, a dual-speaker dialogue TTS that integrates a language model with dual-track token generation and chunked Flow Matching. To overcome dialogue data scarcity, we designed a pipeline to scale up. Experiments in Chinese and English show that DialoSpeech outperforms strong baselines and achieves competitive objective scores in less training data. In the future, we first need to scale up the dialogue datasets. Due to training the language model with Flow Matching, it becomes difficult on minute-scale data because of memory limitations. Therefore, exploring continuous latent representations for dialogue modeling is a promising future direction and trend.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model",
                    "dialospeech",
                    "chinese"
                ]
            }
        ]
    }
}