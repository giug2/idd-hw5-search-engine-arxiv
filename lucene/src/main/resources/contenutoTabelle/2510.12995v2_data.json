{
    "S5.T1": {
        "caption": "Table 1: Objective evaluation on LibriSpeech(PC) test-clean. Results are reported for WER (%), speaker similarity (cosine SIM), and UTMOS. † denotes results reproduced by NaturalSpeech3. Our method outperforms larger hybrid AR+NAR baselines while using fewer parameters.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Modeling</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Token</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER(%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Ground Truth</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vocoder</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">2.56</td>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center\">3.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">VALL-E<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> (&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib35\" title=\"\">35</a>]</cite>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AR+NAR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Discrete</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">400M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mega TTS<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> (&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib16\" title=\"\">16</a>]</cite>)</th>\n<td class=\"ltx_td ltx_align_center\">AR+NAR</td>\n<td class=\"ltx_td ltx_align_center\">Continuous</td>\n<td class=\"ltx_td ltx_align_center\">500M</td>\n<td class=\"ltx_td ltx_align_center\">2.32</td>\n<td class=\"ltx_td ltx_align_center\">0.53</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Voicebox<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> (&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib19\" title=\"\">19</a>]</cite>)</th>\n<td class=\"ltx_td ltx_align_center\">NAR</td>\n<td class=\"ltx_td ltx_align_center\">Continuous</td>\n<td class=\"ltx_td ltx_align_center\">400M</td>\n<td class=\"ltx_td ltx_align_center\">2.14</td>\n<td class=\"ltx_td ltx_align_center\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">3.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">StyleTTS2<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> (&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib23\" title=\"\">23</a>]</cite>)</th>\n<td class=\"ltx_td ltx_align_center\">NAR</td>\n<td class=\"ltx_td ltx_align_center\">Continuous</td>\n<td class=\"ltx_td ltx_align_center\">700M</td>\n<td class=\"ltx_td ltx_align_center\">2.49</td>\n<td class=\"ltx_td ltx_align_center\">0.38</td>\n<td class=\"ltx_td ltx_align_center\">3.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Stage-1 Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Continuous</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">160M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Proposed Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">AR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Continuous</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">160M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.00</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer↓downarrow",
            "testclean",
            "utmos",
            "while",
            "modeling",
            "wer",
            "valle†",
            "tts†",
            "baselines",
            "evaluation",
            "denotes",
            "hybrid",
            "outperforms",
            "400m",
            "nar",
            "our",
            "larger",
            "similarity",
            "naturalspeech3",
            "token",
            "reproduced",
            "params",
            "objective",
            "reported",
            "speaker",
            "mega",
            "discrete",
            "arnar",
            "voicebox†",
            "700m",
            "truth",
            "stage1",
            "160m",
            "sim↑uparrow",
            "fewer",
            "results",
            "parameters",
            "cosine",
            "styletts2†",
            "500m",
            "proposed",
            "vocoder",
            "continuous",
            "sim",
            "librispeechpc",
            "method",
            "baseline",
            "utmos↑uparrow",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Main Results &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the comparison between our model and representative hybrid autoregressive baselines.\nVALL-E, which relies on discrete tokens, yields a WER of 6.11% and a speaker similarity of 0.47, illustrating the limitations of quantization in preserving fine-grained acoustic details.\nMegaTTS, a continuous-token model with 500M parameters, achieves stronger results with a WER of 2.32% and a similarity of 0.53.\nBy contrast, our model attains a WER of 1.95% and a similarity of 0.54, while maintaining competitive perceptual quality (UTMOS 4.00 compared with 4.02 for MegaTTS).\nDespite using only 160M parameters, our system consistently outperforms larger models, demonstrating the efficiency of combining an autoregressive backbone with a continuous diffusion head.\nFor additional context, we also include ground-truth speech and vocoder reconstructions in the table for reference.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information.\nIn this work, we investigate the TTS within the MLLM paradigm using continuous speech representations.\nWe design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis.\n(3) Masked training is employed to address exposure bias in autoregressive decoding.\n(4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "utmos",
                    "similarity",
                    "modeling",
                    "wer",
                    "token",
                    "continuous",
                    "speaker",
                    "librispeechpc",
                    "discrete",
                    "results",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in multimodal large language models (MLLMs) have enabled a single model to perform diverse tasks across modalities in an autoregressive manner <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib37\" title=\"\">37</a>]</cite>. In text-to-speech (TTS), the dominant approach converts speech into discrete tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib35\" title=\"\">35</a>]</cite>, allowing TTS to be framed as a sequence prediction problem within the LLM framework. While effective, discrete quantization can discard fine-grained acoustic details, limiting naturalness and speech fidelity.\nIn contrast, continuous speech representations&#8212;often learned by variational autoencoders (VAEs) or other self-supervised encoders <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib13\" title=\"\">13</a>]</cite>&#8212;better preserve the intrinsic properties of speech. Diffusion models, originally successful in high-fidelity image generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib32\" title=\"\">32</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib39\" title=\"\">39</a>]</cite>, have recently achieved state-of-the-art results for TTS by modeling such continuous representations in a non-autoregressive manner <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib28\" title=\"\">28</a>]</cite>. However, the integration of continuous-token diffusion into an autoregressive MLLM framework remains largely unexplored.</p>\n\n",
                "matched_terms": [
                    "while",
                    "modeling",
                    "continuous",
                    "discrete",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line, we propose to integrate diffusion directly into an autoregressive MLLM framework, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Prior autoregressive diffusion methods either rely on intermediate semantic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib34\" title=\"\">34</a>]</cite>, or relax framewise causality in diffusion modules to predict multi-frame blocks per AR step <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib24\" title=\"\">24</a>]</cite>. In contrast, our approach implements a strictly frame-by-frame autoregressive, continuous-representation diffusion head on top of an LLM backbone. This design enables the model to directly generate high-fidelity speech in the continuous speech representation space, avoiding the quantization bottleneck.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain the MLLM&#8217;s multi-task consistency, we designed a dual-head architecture. The diffusion head generates continuous speech embeddings each frame, which are then decoded to synthesize waveforms. The language model (LM) head retained from the original LLM predicts the start and end of speech tokens, enabling variable-length speech synthesis. This token-based control enables seamless integration of speech generation with multimodal generation. Unlike prior TTS methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib26\" title=\"\">26</a>]</cite> introducing an external classifier module, our design preserves a single, unified framework within the MLLM.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We stabilize training with a two-stage strategy, yielding large performance gains and cutting WER by 46%, reaching SOTA AR on LibriSpeech(PC) test-clean.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The models and results described in this paper are intended for research purposes only.</span></span></span></p>\n\n",
                "matched_terms": [
                    "librispeechpc",
                    "results",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot TTS.</span>\nZero-shot text-to-speech (TTS) refers to synthesizing speech for previously unseen speakers by leveraging a short reference utterance as conditioning, thereby enabling speaker generalization without explicit speaker-specific training.\nInspired by advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib4\" title=\"\">4</a>]</cite>, zero-shot TTS is often formulated as a language modeling task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib40\" title=\"\">40</a>]</cite>, where speech waveforms are transformed into sequences of tokens and synthesized via next-token prediction.\nExisting methods can be broadly categorized into multi-stage and single-stage pipelines.\nMulti-stage systems, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib35\" title=\"\">35</a>]</cite> and SALAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib34\" title=\"\">34</a>]</cite>, autoregressively predict coarse units such as semantic&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib3\" title=\"\">3</a>]</cite> or codec tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib38\" title=\"\">38</a>]</cite>, which are then refined into waveforms.\nThis decomposition improves stability but often discards fine-grained acoustic details.\nIn contrast, single-stage approaches, exemplified by MegaTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib16\" title=\"\">16</a>]</cite> and NaturalSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib17\" title=\"\">17</a>]</cite>, directly generate high-information continuous representations, offering higher fidelity while facing greater challenges in robustness.\nOur method follows this single-stage paradigm.</p>\n\n",
                "matched_terms": [
                    "while",
                    "modeling",
                    "continuous",
                    "speaker",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Autoregressive Diffusion.</span>\nAutoregressive language models were originally developed for discrete symbol sequences, whereas diffusion models are particularly effective for continuous data distributions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib12\" title=\"\">12</a>]</cite>.\nRecent work has explored combining these paradigms for sequence generation.\nSeveral studies modify the diffusion process to behave autoregressively, for example by adjusting denoising schedules so earlier tokens are predicted before later ones&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib9\" title=\"\">9</a>]</cite>.\nTransFusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib42\" title=\"\">42</a>]</cite> exemplifies this strategy with a shared transformer that applies causal attention to discrete tokens and bidirectional attention to continuous features, though it still struggles with strictly causal generation of continuous signals.\nOther efforts replace discrete codec units with continuous-valued tokens modeled directly through diffusion losses&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite>.\nBy avoiding quantization, these approaches preserve fine-grained semantic and acoustic detail, positioning diffusion as a compelling alternative to conventional autoregressive modeling.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "discrete",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to autoregressively generate speech in the space of continuous acoustic embeddings. Our framework builds upon a large language model backbone\nand introduces a dual-head architecture. The first part of the method addresses how continuous speech tokens are generated: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> introduces continuous-token generation with a diffusion head, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS2\" title=\"3.2 EOS Control in Dual-Head Architecture &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> presents EOS control in the dual-head architecture, which together form the foundation for continuous speech generation within a multitask unified foundation model setting. The second part focuses on improving robustness and overall performance: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS3\" title=\"3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describes masked autoregressive learning, which exposes the model to imperfect histories, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS4\" title=\"3.4 Two-Stage Scheme &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> details a two-stage optimization scheme, which stabilizes training by mitigating distribution drift.</p>\n\n",
                "matched_terms": [
                    "method",
                    "continuous",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the inherent continuous nature of speech, recent work has begun to adopt continuous representation for speech generation in TTS. Compared to discrete codebook tokens, continuous representations preserve fine-grained characteristics, while decreasing the potential information loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib22\" title=\"\">22</a>]</cite>. Inspired by image&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib21\" title=\"\">21</a>]</cite> and audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite> generation, we introduce a lightweight diffusion head on top of a causal foundation model to generate high-fidelity speech from continuous embeddings.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "while",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Formally, the target is a sequence of continuous speech embeddings at frame level <math alttext=\"x=\\{x_{1},\\ldots,x_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=\\{x_{1},\\ldots,x_{N}\\}</annotation></semantics></math>, where <math alttext=\"x_{i}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><annotation encoding=\"application/x-tex\">x_{i}\\in\\mathbb{R}^{d}</annotation></semantics></math> denotes the speech embedding in frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. An off-the-shelf variational autoencoder <math alttext=\"V=\\{V_{E},V_{D}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>V</mi><mi>E</mi></msub><mo>,</mo><msub><mi>V</mi><mi>D</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{V_{E},V_{D}\\}</annotation></semantics></math> provides the waveform-embedding mapping: the encoder <math alttext=\"V_{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><msub><mi>V</mi><mi>E</mi></msub><annotation encoding=\"application/x-tex\">V_{E}</annotation></semantics></math> extracts embeddings from waveform <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math>, and the decoder <math alttext=\"V_{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><msub><mi>V</mi><mi>D</mi></msub><annotation encoding=\"application/x-tex\">V_{D}</annotation></semantics></math> reconstructs audio from predicted embedding <math alttext=\"\\hat{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{x}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "continuous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> As shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in order to supervise this control process, we extend the vocabulary with a special token <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> in addition to <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span> and <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>. Although <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> is not emitted during inference, its explicit supervision during training provides dense learning signals throughout the speech segment. This design reduces the risk of the LM head prematurely predicting <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span> in variable-length sequences, compared with supervising only the boundary tokens. The LM head is trained with the standard cross-entropy <math alttext=\"\\mathcal{L}_{\\text{LM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LM}}</annotation></semantics></math> over the control tokens, while the diffusion head is trained with the noise-prediction loss <math alttext=\"\\mathcal{L}_{\\text{diff}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>diff</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{diff}}</annotation></semantics></math> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). The overall objective is the sum of the LM cross-entropy loss and the diffusion loss:</p>\n\n",
                "matched_terms": [
                    "objective",
                    "token",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When training the MLLM and diffusion head jointly in an end-to-end manner, we observed instability caused by distribution drift. The MLLM output distribution <math alttext=\"p_{\\theta}(z\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>z</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z\\mid p)</annotation></semantics></math> evolves as the parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> are updated. The diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> is expected to learn a mapping from <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> to continuous speech embeddings <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m5\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>. However, since the source distribution <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> is non-stationary during training, <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> must adapt to a shifting input space, making convergence unreliable and degrading generation quality. We hypothesize that freezing <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to fix <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> yields a stationary input distribution, allowing the diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> to focus on modeling a stable transformation, thereby improving optimization stability and synthesis fidelity.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "parameters",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement this idea, we adopt a two-stage training strategy, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). In <span class=\"ltx_text ltx_font_bold\">Stage 1</span>, we jointly train the causal LM <math alttext=\"\\mathcal{C}\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}\\theta</annotation></semantics></math> and the diffusion head <math alttext=\"\\mathcal{M}\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{M}\\phi</annotation></semantics></math> in an end-to-end manner by minimizing the sum of the cross-entropy loss and the diffusion loss. This stage enables the model to align the LM outputs with the target distribution and to produce coarse speech embeddings. However, we observe that although the overall training objective consistently decreases, autoregressive evaluation metrics exhibit a non-monotonic trend, improving initially but then deteriorating. This is consistent with our hypothesis that distribution drift hinders stable refinement.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our dual-head continuous speech generation framework, we conduct TTS experiments under different settings, comparing intelligibility, speaker identity preservation, and speech naturalness. We first introduce the datasets and evaluation protocols in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS1\" title=\"4.1 Dataset and Metrics &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, then present implementation details in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS2\" title=\"4.2 Implementation Details &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, and finally describe the baselines in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS3\" title=\"4.3 Baseline &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "baselines",
                    "speaker",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span> We adopt the LibriVox corpus as the training source. Specifically, we use a 50k-hour subset (derived from the Libri-Light collection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib18\" title=\"\">18</a>]</cite>), which consists of read English audiobooks from thousands of speakers. For evaluation, we use the Librispeech (PC) test-clean dataset. Following the protocol of NaturalSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib17\" title=\"\">17</a>]</cite>, we randomly select one utterance from each of 40 speakers and use an additional 3-second clip as the speaker reference.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "testclean",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system on three aspects: intelligibility, speaker identity preservation, and speech quality.\n(1) Word error rate (WER) measures intelligibility, about how accurately the synthesized speech conveys the reference text. The generated speech is transcribed by Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib30\" title=\"\">30</a>]</cite>, and WER is calculated from insertions, substitutions, and deletions.\n(2) Speaker similarity is measured as cosine similarity between embeddings extracted by ECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib7\" title=\"\">7</a>]</cite>. We report SIM-R (to reference prompt) and SIM-G (to ground-truth speech).\n(3) Speech quality is estimated with UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib31\" title=\"\">31</a>]</cite>, an objective MOS predictor trained on large-scale human ratings.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "similarity",
                    "wer",
                    "objective",
                    "speaker",
                    "cosine",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Representation.</span>\nOur system relies on two types of speech representations: one for speaker reference prompting and another for generation targets.\nFor speaker reference prompting, we extract a 768-dimensional embedding from a three-second reference clip using LAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib29\" title=\"\">29</a>]</cite>. This embedding encodes speaker identity and is projected into the LLM input space as conditioning information.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Details</span>\nOur architecture builds on an autoregressive LLM backbone and extends it with projection modules for multi modality and a diffusion head for speech generation. We adopt OPT-125M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib41\" title=\"\">41</a>]</cite> as the LLM backbone. For multi-modality, we add two projectors: one maps the 768-dimensional reference embedding extracted from the speaker prompt into the LLM input space, and the other maps the 64-dimensional continuous acoustic tokens into the LLM input space. With these projections, the backbone functions as a multimodal language model (MLLM) rather than a purely text-based LLM.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "continuous",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Hyperparameters</span>\nDuring training, we use a diffusion process with <math alttext=\"T=1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding=\"application/x-tex\">T=1000</annotation></semantics></math> steps and adopt the cosine noise schedule&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib27\" title=\"\">27</a>]</cite>,where derives <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> implicitly from the cumulative product <math alttext=\"\\bar{\\alpha}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}</annotation></semantics></math>.\nThe diffusion head is an MLP with residual blocks; we experimented with 3, 6, and 12 layers, and report 12-layer results unless otherwise noted.\nEach block consists of layer normalization, linear layers, and SiLU activation with adaptive layer normalization modulation, with no dropout.\nDuring inference, we reduce the denoising process to 100 steps and apply a sampling temperature of <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>. CFG is set to 1.</p>\n\n",
                "matched_terms": [
                    "cosine",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the model from the first-stage joint training of all components, including the MLLM backbone, LM head and diffusion head, as our baseline. During training, the loss decreases monotonically, but the evaluation WER first decreases and then increases because of the dynamic condition for the diffusion head. We therefore apply the early stopping and select the checkpoint with the lowest validation WER as the baseline model, from which the stage-2 training is initialized.</p>\n\n",
                "matched_terms": [
                    "our",
                    "evaluation",
                    "baseline",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first present the main results by comparing our model with representative baselines.\nWe then provide ablations and analyses of key design choices and inference settings to understand their impact on intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "baselines",
                    "speaker",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze the effect of our two-stage training strategy.\nThe Stage-1 baseline achieves a WER of 3.61%, a speaker similarity of 0.49, and a UTMOS of 3.21, indicating that the diffusion head initially suffers from unstable input distributions.\nAfter Stage-2 training, the WER is reduced by 46% relative (from 3.61% to 1.95%), while speaker similarity increases from 0.49 to 0.54 and UTMOS rises from 3.21 to 4.00.\nThese improvements show that stabilizing the diffusion head&#8217;s input distribution in Stage-2 not only reduces recognition errors but also enhances both speaker consistency and perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "utmos",
                    "while",
                    "stage1",
                    "wer",
                    "speaker",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contributions of different components and design choices, we conduct a series of ablation and analysis experiments.\nWe first investigate the effect of masked training, which aims to mitigate exposure bias during autoregressive decoding.\nWe then examine the role of diffusion head capacity and the impact of our two-stage training strategy.\nFinally, we analyze the influence of stopping criteria and inference hyperparameters, highlighting how these factors jointly affect intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "similarity",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Ratio.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the impact of different masking rates in the masked training scheme. Without masking (0%), the model suffers from severe exposure bias, leading to a WER of 15.06%. Introducing moderate masking improves robustness, with the best performance at 30% masking (WER 6.17%, UTMOS 3.21). However, excessive masking (50%) degrades both intelligibility and naturalness, as too much corruption disrupts semantic alignment. This confirms that moderate masking helps bridge the gap between training and inference conditions.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Head Depth.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares different diffusion head depths with and without the proposed two-stage training. Increasing the depth from 3 to 12 layers progressively improves WER and speaker similarity, demonstrating the benefit of a stronger decoder. More importantly, enabling two-stage training further reduces WER to 1.95% and boosts similarity and naturalness, highlighting the effectiveness of stabilizing the diffusion head with a fixed input distribution.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "similarity",
                    "proposed",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stopping Criteria.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T4\" title=\"Table 4 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares different stopping strategies. Using ground-truth durations causes unstable outputs (WER 29.36%), while oracle endpoint supervision achieves low WER but requires non-causal labels. Our EOS-token design achieves comparable WER and UTMOS without relying on oracle information, while maintaining stable generation speed, making it a practical choice for unified MLLM-based TTS.</p>\n\n",
                "matched_terms": [
                    "our",
                    "utmos",
                    "while",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference Hyperparameters.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T5\" title=\"Table 5 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> analyzes the influence of diffusion inference parameters. Lower temperatures tend to produce cleaner but truncated outputs, leading to higher WER and lower similarity. Conversely, higher temperatures improve diversity but may reduce naturalness. We find that a temperature of 0.9 with 100 denoising steps achieves the best trade-off, yielding the lowest WER (1.95%), highest similarity (0.54), and best UTMOS (4.00).</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "similarity",
                    "parameters",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a dual-head multimodal language model that integrates a frame-level continuous-token diffusion head with an autoregressive LLM backbone for speaker-referenced TTS. By combining continuous speech representations, our approach avoids the quantization bottleneck and achieves high-fidelity and natural speech. To overcome exposure bias and improve training performance, we adapt masked training and a two-stage optimization scheme, which together substantially improve robustness and quality. Evaluations on LibriSpeech(PC) demonstrate significant gains, including a 46% relative WER reduction over our baseline, along with higher speaker similarity and audio quality. These results highlight the effectiveness of bridging autoregressive modeling with diffusion-based refinement for continuous speech generation.\nLooking ahead, this framework provides a path toward unified foundation models that can support multiple speech and multimodal tasks within a single framework.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "modeling",
                    "wer",
                    "continuous",
                    "speaker",
                    "librispeechpc",
                    "results",
                    "baseline",
                    "our"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Performance with different masking rates, using a 3-layer MLP diffusion head.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Masking Rate(%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-R<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-G<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">UTMOS</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">0</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">15</th>\n<td class=\"ltx_td ltx_align_center\">12.65</td>\n<td class=\"ltx_td ltx_align_center\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.42</td>\n<td class=\"ltx_td ltx_align_center\">1.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">30</th>\n<td class=\"ltx_td ltx_align_center\">6.17</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center\">3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">50</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">8.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.84</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "diffusion",
            "utmos",
            "mlp",
            "↑uparrow",
            "rates",
            "wer",
            "rate",
            "simr↑uparrow",
            "masking",
            "head",
            "simg↑uparrow",
            "↓downarrow",
            "different",
            "3layer",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Ratio.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the impact of different masking rates in the masked training scheme. Without masking (0%), the model suffers from severe exposure bias, leading to a WER of 15.06%. Introducing moderate masking improves robustness, with the best performance at 30% masking (WER 6.17%, UTMOS 3.21). However, excessive masking (50%) degrades both intelligibility and naturalness, as too much corruption disrupts semantic alignment. This confirms that moderate masking helps bridge the gap between training and inference conditions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information.\nIn this work, we investigate the TTS within the MLLM paradigm using continuous speech representations.\nWe design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis.\n(3) Masked training is employed to address exposure bias in autoregressive decoding.\n(4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "wer",
                    "head",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line, we propose to integrate diffusion directly into an autoregressive MLLM framework, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Prior autoregressive diffusion methods either rely on intermediate semantic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib34\" title=\"\">34</a>]</cite>, or relax framewise causality in diffusion modules to predict multi-frame blocks per AR step <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib24\" title=\"\">24</a>]</cite>. In contrast, our approach implements a strictly frame-by-frame autoregressive, continuous-representation diffusion head on top of an LLM backbone. This design enables the model to directly generate high-fidelity speech in the continuous speech representation space, avoiding the quantization bottleneck.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain the MLLM&#8217;s multi-task consistency, we designed a dual-head architecture. The diffusion head generates continuous speech embeddings each frame, which are then decoded to synthesize waveforms. The language model (LM) head retained from the original LLM predicts the start and end of speech tokens, enabling variable-length speech synthesis. This token-based control enables seamless integration of speech generation with multimodal generation. Unlike prior TTS methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib26\" title=\"\">26</a>]</cite> introducing an external classifier module, our design preserves a single, unified framework within the MLLM.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then we observed that jointly optimizing the diffusion head with the LLM is unstable. The LLM&#8217;s output distribution evolves during training, causing instability in the diffusion learning process. To address this, we employ a two-stage training strategy: in stage 1, train the LLM and diffusion head jointly, allowing the LLM to learn speech token prediction and the diffusion head to adapt to evolving inputs. In stage 2, we freeze the entire LLM side, including the backbone, the LM head, and the speech-projection, thereby fixing the input distribution to the diffusion head. We then train only the diffusion head. This allows the diffusion model to focus solely on refining the mapping from the LLM outputs to the target speech space.\nThis separation stabilizes training and significantly improves generation quality.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a frame-by-frame continuous-token diffusion head into an autoregressive MLLM for speaker-referenced TTS, distinguishing it from block-wise multi-frame designs.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We stabilize training with a two-stage strategy, yielding large performance gains and cutting WER by 46%, reaching SOTA AR on LibriSpeech(PC) test-clean.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The models and results described in this paper are intended for research purposes only.</span></span></span></p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to autoregressively generate speech in the space of continuous acoustic embeddings. Our framework builds upon a large language model backbone\nand introduces a dual-head architecture. The first part of the method addresses how continuous speech tokens are generated: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> introduces continuous-token generation with a diffusion head, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS2\" title=\"3.2 EOS Control in Dual-Head Architecture &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> presents EOS control in the dual-head architecture, which together form the foundation for continuous speech generation within a multitask unified foundation model setting. The second part focuses on improving robustness and overall performance: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS3\" title=\"3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describes masked autoregressive learning, which exposes the model to imperfect histories, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS4\" title=\"3.4 Two-Stage Scheme &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> details a two-stage optimization scheme, which stabilizes training by mitigating distribution drift.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the inherent continuous nature of speech, recent work has begun to adopt continuous representation for speech generation in TTS. Compared to discrete codebook tokens, continuous representations preserve fine-grained characteristics, while decreasing the potential information loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib22\" title=\"\">22</a>]</cite>. Inspired by image&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib21\" title=\"\">21</a>]</cite> and audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite> generation, we introduce a lightweight diffusion head on top of a causal foundation model to generate high-fidelity speech from continuous embeddings.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (b) illustrates the framework.\nGiven a prompt <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> with transcription and reference audio, multi-modal causal LLM <math alttext=\"\\mathcal{C_{\\theta}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{C_{\\theta}}</annotation></semantics></math> takes <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> and past predictions <math alttext=\"\\hat{x}_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{&lt;i}</annotation></semantics></math> to autoregressively produce a hidden state as condition <math alttext=\"z_{i}=\\mathcal{C}_{\\theta}(p,\\hat{x}_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{i}=\\mathcal{C}_{\\theta}(p,\\hat{x}_{&lt;i})</annotation></semantics></math>. This vector <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> conditions a diffusion head with MLP denoiser <math alttext=\"M_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">M_{\\phi}</annotation></semantics></math>, which starts from Gaussian noise <math alttext=\"x_{i}^{t}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{i}^{t}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> and iteratively denoises to produce the next embedding <math alttext=\"\\hat{x}_{i}=x_{i}^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mn>0</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_{i}=x_{i}^{0}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "mlp",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">which backpropagates through <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>. This diffusion loss can be jointly optimized with the objective of the language model head. Further details are provided in the following sections.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Generation proceeds under the control of the LM head. The model begins in the textual phase until the LM head emits a special token <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span>, which triggers the speech generation phase.\nAt each subsequent step, the LM head produces a control token. If it emits <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span>, this token is not added to the output sequence; instead, it signals the diffusion head to generate the next speech embeddings. Otherwise when LM head predicts <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>, the speech generation phase terminates.\nThis token-based mechanism provides a unified and modality-agnostic interface for switching between text and speech without additional architectural components.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> As shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in order to supervise this control process, we extend the vocabulary with a special token <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> in addition to <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span> and <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>. Although <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> is not emitted during inference, its explicit supervision during training provides dense learning signals throughout the speech segment. This design reduces the risk of the LM head prematurely predicting <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span> in variable-length sequences, compared with supervising only the boundary tokens. The LM head is trained with the standard cross-entropy <math alttext=\"\\mathcal{L}_{\\text{LM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LM}}</annotation></semantics></math> over the control tokens, while the diffusion head is trained with the noise-prediction loss <math alttext=\"\\mathcal{L}_{\\text{diff}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>diff</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{diff}}</annotation></semantics></math> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). The overall objective is the sum of the LM cross-entropy loss and the diffusion loss:</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When training the MLLM and diffusion head jointly in an end-to-end manner, we observed instability caused by distribution drift. The MLLM output distribution <math alttext=\"p_{\\theta}(z\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>z</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z\\mid p)</annotation></semantics></math> evolves as the parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> are updated. The diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> is expected to learn a mapping from <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> to continuous speech embeddings <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m5\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>. However, since the source distribution <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> is non-stationary during training, <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> must adapt to a shifting input space, making convergence unreliable and degrading generation quality. We hypothesize that freezing <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to fix <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> yields a stationary input distribution, allowing the diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> to focus on modeling a stable transformation, thereby improving optimization stability and synthesis fidelity.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement this idea, we adopt a two-stage training strategy, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). In <span class=\"ltx_text ltx_font_bold\">Stage 1</span>, we jointly train the causal LM <math alttext=\"\\mathcal{C}\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}\\theta</annotation></semantics></math> and the diffusion head <math alttext=\"\\mathcal{M}\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{M}\\phi</annotation></semantics></math> in an end-to-end manner by minimizing the sum of the cross-entropy loss and the diffusion loss. This stage enables the model to align the LM outputs with the target distribution and to produce coarse speech embeddings. However, we observe that although the overall training objective consistently decreases, autoregressive evaluation metrics exhibit a non-monotonic trend, improving initially but then deteriorating. This is consistent with our hypothesis that distribution drift hinders stable refinement.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">Stage 2</span>, we freeze the MLLM and LM head parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> and train only the diffusion<math alttext=\"\\mathcal{M}\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{M}\\phi</annotation></semantics></math>. With <math alttext=\"p\\theta(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p\\theta(z)</annotation></semantics></math> fixed, the input distribution to the diffusion head remains stationary, allowing it to focus on refining LM outputs into high-fidelity acoustic frames.\nThis stage stabilizes optimization, mitigates the instability observed in joint training, and leads to improved synthesis quality.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system on three aspects: intelligibility, speaker identity preservation, and speech quality.\n(1) Word error rate (WER) measures intelligibility, about how accurately the synthesized speech conveys the reference text. The generated speech is transcribed by Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib30\" title=\"\">30</a>]</cite>, and WER is calculated from insertions, substitutions, and deletions.\n(2) Speaker similarity is measured as cosine similarity between embeddings extracted by ECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib7\" title=\"\">7</a>]</cite>. We report SIM-R (to reference prompt) and SIM-G (to ground-truth speech).\n(3) Speech quality is estimated with UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib31\" title=\"\">31</a>]</cite>, an objective MOS predictor trained on large-scale human ratings.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Details</span>\nOur architecture builds on an autoregressive LLM backbone and extends it with projection modules for multi modality and a diffusion head for speech generation. We adopt OPT-125M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib41\" title=\"\">41</a>]</cite> as the LLM backbone. For multi-modality, we add two projectors: one maps the 768-dimensional reference embedding extracted from the speaker prompt into the LLM input space, and the other maps the 64-dimensional continuous acoustic tokens into the LLM input space. With these projections, the backbone functions as a multimodal language model (MLLM) rather than a purely text-based LLM.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the final hidden state from the LLM decoder as the input to the diffusion module. Before entering the diffusion process, this hidden representation is projected through the third linear layer to 768 dimensions as the diffusion condition. The diffusion head is implemented as a stack of MLP layers and operates with a DDPM-based denoising process, similar to &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "mlp",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Hyperparameters</span>\nDuring training, we use a diffusion process with <math alttext=\"T=1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding=\"application/x-tex\">T=1000</annotation></semantics></math> steps and adopt the cosine noise schedule&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib27\" title=\"\">27</a>]</cite>,where derives <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> implicitly from the cumulative product <math alttext=\"\\bar{\\alpha}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}</annotation></semantics></math>.\nThe diffusion head is an MLP with residual blocks; we experimented with 3, 6, and 12 layers, and report 12-layer results unless otherwise noted.\nEach block consists of layer normalization, linear layers, and SiLU activation with adaptive layer normalization modulation, with no dropout.\nDuring inference, we reduce the denoising process to 100 steps and apply a sampling temperature of <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>. CFG is set to 1.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "mlp",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the model from the first-stage joint training of all components, including the MLLM backbone, LM head and diffusion head, as our baseline. During training, the loss decreases monotonically, but the evaluation WER first decreases and then increases because of the dynamic condition for the diffusion head. We therefore apply the early stopping and select the checkpoint with the lowest validation WER as the baseline model, from which the stage-2 training is initialized.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Main Results &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the comparison between our model and representative hybrid autoregressive baselines.\nVALL-E, which relies on discrete tokens, yields a WER of 6.11% and a speaker similarity of 0.47, illustrating the limitations of quantization in preserving fine-grained acoustic details.\nMegaTTS, a continuous-token model with 500M parameters, achieves stronger results with a WER of 2.32% and a similarity of 0.53.\nBy contrast, our model attains a WER of 1.95% and a similarity of 0.54, while maintaining competitive perceptual quality (UTMOS 4.00 compared with 4.02 for MegaTTS).\nDespite using only 160M parameters, our system consistently outperforms larger models, demonstrating the efficiency of combining an autoregressive backbone with a continuous diffusion head.\nFor additional context, we also include ground-truth speech and vocoder reconstructions in the table for reference.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze the effect of our two-stage training strategy.\nThe Stage-1 baseline achieves a WER of 3.61%, a speaker similarity of 0.49, and a UTMOS of 3.21, indicating that the diffusion head initially suffers from unstable input distributions.\nAfter Stage-2 training, the WER is reduced by 46% relative (from 3.61% to 1.95%), while speaker similarity increases from 0.49 to 0.54 and UTMOS rises from 3.21 to 4.00.\nThese improvements show that stabilizing the diffusion head&#8217;s input distribution in Stage-2 not only reduces recognition errors but also enhances both speaker consistency and perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contributions of different components and design choices, we conduct a series of ablation and analysis experiments.\nWe first investigate the effect of masked training, which aims to mitigate exposure bias during autoregressive decoding.\nWe then examine the role of diffusion head capacity and the impact of our two-stage training strategy.\nFinally, we analyze the influence of stopping criteria and inference hyperparameters, highlighting how these factors jointly affect intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "different",
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Head Depth.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares different diffusion head depths with and without the proposed two-stage training. Increasing the depth from 3 to 12 layers progressively improves WER and speaker similarity, demonstrating the benefit of a stronger decoder. More importantly, enabling two-stage training further reduces WER to 1.95% and boosts similarity and naturalness, highlighting the effectiveness of stabilizing the diffusion head with a fixed input distribution.</p>\n\n",
                "matched_terms": [
                    "different",
                    "diffusion",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stopping Criteria.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T4\" title=\"Table 4 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares different stopping strategies. Using ground-truth durations causes unstable outputs (WER 29.36%), while oracle endpoint supervision achieves low WER but requires non-causal labels. Our EOS-token design achieves comparable WER and UTMOS without relying on oracle information, while maintaining stable generation speed, making it a practical choice for unified MLLM-based TTS.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "different",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference Hyperparameters.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T5\" title=\"Table 5 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> analyzes the influence of diffusion inference parameters. Lower temperatures tend to produce cleaner but truncated outputs, leading to higher WER and lower similarity. Conversely, higher temperatures improve diversity but may reduce naturalness. We find that a temperature of 0.9 with 100 denoising steps achieves the best trade-off, yielding the lowest WER (1.95%), highest similarity (0.54), and best UTMOS (4.00).</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a dual-head multimodal language model that integrates a frame-level continuous-token diffusion head with an autoregressive LLM backbone for speaker-referenced TTS. By combining continuous speech representations, our approach avoids the quantization bottleneck and achieves high-fidelity and natural speech. To overcome exposure bias and improve training performance, we adapt masked training and a two-stage optimization scheme, which together substantially improve robustness and quality. Evaluations on LibriSpeech(PC) demonstrate significant gains, including a 46% relative WER reduction over our baseline, along with higher speaker similarity and audio quality. These results highlight the effectiveness of bridging autoregressive modeling with diffusion-based refinement for continuous speech generation.\nLooking ahead, this framework provides a path toward unified foundation models that can support multiple speech and multimodal tasks within a single framework.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head",
                    "performance",
                    "wer"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Comparison of different numbers of MLP layers in the diffusion head, with dropout rate set to 30%. Stage-2 FT indicates whether two-stage fine-tuning is applied.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># MLP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Stage-2 FT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># Params</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-R<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-G<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">UTMOS</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">w/o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">148.7M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">6</th>\n<td class=\"ltx_td ltx_align_center\">w/o</td>\n<td class=\"ltx_td ltx_align_center\">164.4M</td>\n<td class=\"ltx_td ltx_align_center\">5.12</td>\n<td class=\"ltx_td ltx_align_center\">0.50</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">3.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">12</th>\n<td class=\"ltx_td ltx_align_center\">w/o</td>\n<td class=\"ltx_td ltx_align_center\">159.9M</td>\n<td class=\"ltx_td ltx_align_center\">3.61</td>\n<td class=\"ltx_td ltx_align_center\">0.49</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">12</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">w</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">159.9M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.00</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "utmos",
            "wer",
            "twostage",
            "↓downarrow",
            "1599m",
            "applied",
            "comparison",
            "rate",
            "params",
            "simg↑uparrow",
            "1644m",
            "diffusion",
            "finetuning",
            "simr↑uparrow",
            "head",
            "numbers",
            "indicates",
            "layers",
            "set",
            "1487m",
            "↑uparrow",
            "mlp",
            "whether",
            "dropout",
            "different",
            "stage2"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Head Depth.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares different diffusion head depths with and without the proposed two-stage training. Increasing the depth from 3 to 12 layers progressively improves WER and speaker similarity, demonstrating the benefit of a stronger decoder. More importantly, enabling two-stage training further reduces WER to 1.95% and boosts similarity and naturalness, highlighting the effectiveness of stabilizing the diffusion head with a fixed input distribution.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information.\nIn this work, we investigate the TTS within the MLLM paradigm using continuous speech representations.\nWe design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis.\n(3) Masked training is employed to address exposure bias in autoregressive decoding.\n(4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "wer",
                    "twostage",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line, we propose to integrate diffusion directly into an autoregressive MLLM framework, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Prior autoregressive diffusion methods either rely on intermediate semantic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib34\" title=\"\">34</a>]</cite>, or relax framewise causality in diffusion modules to predict multi-frame blocks per AR step <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib24\" title=\"\">24</a>]</cite>. In contrast, our approach implements a strictly frame-by-frame autoregressive, continuous-representation diffusion head on top of an LLM backbone. This design enables the model to directly generate high-fidelity speech in the continuous speech representation space, avoiding the quantization bottleneck.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain the MLLM&#8217;s multi-task consistency, we designed a dual-head architecture. The diffusion head generates continuous speech embeddings each frame, which are then decoded to synthesize waveforms. The language model (LM) head retained from the original LLM predicts the start and end of speech tokens, enabling variable-length speech synthesis. This token-based control enables seamless integration of speech generation with multimodal generation. Unlike prior TTS methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib26\" title=\"\">26</a>]</cite> introducing an external classifier module, our design preserves a single, unified framework within the MLLM.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then we observed that jointly optimizing the diffusion head with the LLM is unstable. The LLM&#8217;s output distribution evolves during training, causing instability in the diffusion learning process. To address this, we employ a two-stage training strategy: in stage 1, train the LLM and diffusion head jointly, allowing the LLM to learn speech token prediction and the diffusion head to adapt to evolving inputs. In stage 2, we freeze the entire LLM side, including the backbone, the LM head, and the speech-projection, thereby fixing the input distribution to the diffusion head. We then train only the diffusion head. This allows the diffusion model to focus solely on refining the mapping from the LLM outputs to the target speech space.\nThis separation stabilizes training and significantly improves generation quality.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "twostage",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a frame-by-frame continuous-token diffusion head into an autoregressive MLLM for speaker-referenced TTS, distinguishing it from block-wise multi-frame designs.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We stabilize training with a two-stage strategy, yielding large performance gains and cutting WER by 46%, reaching SOTA AR on LibriSpeech(PC) test-clean.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The models and results described in this paper are intended for research purposes only.</span></span></span></p>\n\n",
                "matched_terms": [
                    "twostage",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to autoregressively generate speech in the space of continuous acoustic embeddings. Our framework builds upon a large language model backbone\nand introduces a dual-head architecture. The first part of the method addresses how continuous speech tokens are generated: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> introduces continuous-token generation with a diffusion head, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS2\" title=\"3.2 EOS Control in Dual-Head Architecture &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> presents EOS control in the dual-head architecture, which together form the foundation for continuous speech generation within a multitask unified foundation model setting. The second part focuses on improving robustness and overall performance: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS3\" title=\"3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describes masked autoregressive learning, which exposes the model to imperfect histories, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS4\" title=\"3.4 Two-Stage Scheme &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> details a two-stage optimization scheme, which stabilizes training by mitigating distribution drift.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "twostage",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the inherent continuous nature of speech, recent work has begun to adopt continuous representation for speech generation in TTS. Compared to discrete codebook tokens, continuous representations preserve fine-grained characteristics, while decreasing the potential information loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib22\" title=\"\">22</a>]</cite>. Inspired by image&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib21\" title=\"\">21</a>]</cite> and audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite> generation, we introduce a lightweight diffusion head on top of a causal foundation model to generate high-fidelity speech from continuous embeddings.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (b) illustrates the framework.\nGiven a prompt <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> with transcription and reference audio, multi-modal causal LLM <math alttext=\"\\mathcal{C_{\\theta}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{C_{\\theta}}</annotation></semantics></math> takes <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> and past predictions <math alttext=\"\\hat{x}_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{&lt;i}</annotation></semantics></math> to autoregressively produce a hidden state as condition <math alttext=\"z_{i}=\\mathcal{C}_{\\theta}(p,\\hat{x}_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{i}=\\mathcal{C}_{\\theta}(p,\\hat{x}_{&lt;i})</annotation></semantics></math>. This vector <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> conditions a diffusion head with MLP denoiser <math alttext=\"M_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">M_{\\phi}</annotation></semantics></math>, which starts from Gaussian noise <math alttext=\"x_{i}^{t}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{i}^{t}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> and iteratively denoises to produce the next embedding <math alttext=\"\\hat{x}_{i}=x_{i}^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mn>0</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_{i}=x_{i}^{0}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "mlp",
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">which backpropagates through <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>. This diffusion loss can be jointly optimized with the objective of the language model head. Further details are provided in the following sections.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Generation proceeds under the control of the LM head. The model begins in the textual phase until the LM head emits a special token <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span>, which triggers the speech generation phase.\nAt each subsequent step, the LM head produces a control token. If it emits <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span>, this token is not added to the output sequence; instead, it signals the diffusion head to generate the next speech embeddings. Otherwise when LM head predicts <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>, the speech generation phase terminates.\nThis token-based mechanism provides a unified and modality-agnostic interface for switching between text and speech without additional architectural components.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> As shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in order to supervise this control process, we extend the vocabulary with a special token <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> in addition to <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span> and <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>. Although <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> is not emitted during inference, its explicit supervision during training provides dense learning signals throughout the speech segment. This design reduces the risk of the LM head prematurely predicting <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span> in variable-length sequences, compared with supervising only the boundary tokens. The LM head is trained with the standard cross-entropy <math alttext=\"\\mathcal{L}_{\\text{LM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LM}}</annotation></semantics></math> over the control tokens, while the diffusion head is trained with the noise-prediction loss <math alttext=\"\\mathcal{L}_{\\text{diff}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>diff</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{diff}}</annotation></semantics></math> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). The overall objective is the sum of the LM cross-entropy loss and the diffusion loss:</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When training the MLLM and diffusion head jointly in an end-to-end manner, we observed instability caused by distribution drift. The MLLM output distribution <math alttext=\"p_{\\theta}(z\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>z</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z\\mid p)</annotation></semantics></math> evolves as the parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> are updated. The diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> is expected to learn a mapping from <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> to continuous speech embeddings <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m5\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>. However, since the source distribution <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> is non-stationary during training, <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> must adapt to a shifting input space, making convergence unreliable and degrading generation quality. We hypothesize that freezing <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to fix <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> yields a stationary input distribution, allowing the diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> to focus on modeling a stable transformation, thereby improving optimization stability and synthesis fidelity.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement this idea, we adopt a two-stage training strategy, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). In <span class=\"ltx_text ltx_font_bold\">Stage 1</span>, we jointly train the causal LM <math alttext=\"\\mathcal{C}\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}\\theta</annotation></semantics></math> and the diffusion head <math alttext=\"\\mathcal{M}\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{M}\\phi</annotation></semantics></math> in an end-to-end manner by minimizing the sum of the cross-entropy loss and the diffusion loss. This stage enables the model to align the LM outputs with the target distribution and to produce coarse speech embeddings. However, we observe that although the overall training objective consistently decreases, autoregressive evaluation metrics exhibit a non-monotonic trend, improving initially but then deteriorating. This is consistent with our hypothesis that distribution drift hinders stable refinement.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "twostage",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <span class=\"ltx_text ltx_font_bold\">Stage 2</span>, we freeze the MLLM and LM head parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> and train only the diffusion<math alttext=\"\\mathcal{M}\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{M}\\phi</annotation></semantics></math>. With <math alttext=\"p\\theta(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p\\theta(z)</annotation></semantics></math> fixed, the input distribution to the diffusion head remains stationary, allowing it to focus on refining LM outputs into high-fidelity acoustic frames.\nThis stage stabilizes optimization, mitigates the instability observed in joint training, and leads to improved synthesis quality.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system on three aspects: intelligibility, speaker identity preservation, and speech quality.\n(1) Word error rate (WER) measures intelligibility, about how accurately the synthesized speech conveys the reference text. The generated speech is transcribed by Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib30\" title=\"\">30</a>]</cite>, and WER is calculated from insertions, substitutions, and deletions.\n(2) Speaker similarity is measured as cosine similarity between embeddings extracted by ECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib7\" title=\"\">7</a>]</cite>. We report SIM-R (to reference prompt) and SIM-G (to ground-truth speech).\n(3) Speech quality is estimated with UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib31\" title=\"\">31</a>]</cite>, an objective MOS predictor trained on large-scale human ratings.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Details</span>\nOur architecture builds on an autoregressive LLM backbone and extends it with projection modules for multi modality and a diffusion head for speech generation. We adopt OPT-125M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib41\" title=\"\">41</a>]</cite> as the LLM backbone. For multi-modality, we add two projectors: one maps the 768-dimensional reference embedding extracted from the speaker prompt into the LLM input space, and the other maps the 64-dimensional continuous acoustic tokens into the LLM input space. With these projections, the backbone functions as a multimodal language model (MLLM) rather than a purely text-based LLM.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the final hidden state from the LLM decoder as the input to the diffusion module. Before entering the diffusion process, this hidden representation is projected through the third linear layer to 768 dimensions as the diffusion condition. The diffusion head is implemented as a stack of MLP layers and operates with a DDPM-based denoising process, similar to &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "mlp",
                    "diffusion",
                    "head",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Hyperparameters</span>\nDuring training, we use a diffusion process with <math alttext=\"T=1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding=\"application/x-tex\">T=1000</annotation></semantics></math> steps and adopt the cosine noise schedule&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib27\" title=\"\">27</a>]</cite>,where derives <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> implicitly from the cumulative product <math alttext=\"\\bar{\\alpha}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}</annotation></semantics></math>.\nThe diffusion head is an MLP with residual blocks; we experimented with 3, 6, and 12 layers, and report 12-layer results unless otherwise noted.\nEach block consists of layer normalization, linear layers, and SiLU activation with adaptive layer normalization modulation, with no dropout.\nDuring inference, we reduce the denoising process to 100 steps and apply a sampling temperature of <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>. CFG is set to 1.</p>\n\n",
                "matched_terms": [
                    "set",
                    "diffusion",
                    "mlp",
                    "head",
                    "dropout",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the model from the first-stage joint training of all components, including the MLLM backbone, LM head and diffusion head, as our baseline. During training, the loss decreases monotonically, but the evaluation WER first decreases and then increases because of the dynamic condition for the diffusion head. We therefore apply the early stopping and select the checkpoint with the lowest validation WER as the baseline model, from which the stage-2 training is initialized.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "head",
                    "stage2",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Main Results &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the comparison between our model and representative hybrid autoregressive baselines.\nVALL-E, which relies on discrete tokens, yields a WER of 6.11% and a speaker similarity of 0.47, illustrating the limitations of quantization in preserving fine-grained acoustic details.\nMegaTTS, a continuous-token model with 500M parameters, achieves stronger results with a WER of 2.32% and a similarity of 0.53.\nBy contrast, our model attains a WER of 1.95% and a similarity of 0.54, while maintaining competitive perceptual quality (UTMOS 4.00 compared with 4.02 for MegaTTS).\nDespite using only 160M parameters, our system consistently outperforms larger models, demonstrating the efficiency of combining an autoregressive backbone with a continuous diffusion head.\nFor additional context, we also include ground-truth speech and vocoder reconstructions in the table for reference.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "wer",
                    "head",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze the effect of our two-stage training strategy.\nThe Stage-1 baseline achieves a WER of 3.61%, a speaker similarity of 0.49, and a UTMOS of 3.21, indicating that the diffusion head initially suffers from unstable input distributions.\nAfter Stage-2 training, the WER is reduced by 46% relative (from 3.61% to 1.95%), while speaker similarity increases from 0.49 to 0.54 and UTMOS rises from 3.21 to 4.00.\nThese improvements show that stabilizing the diffusion head&#8217;s input distribution in Stage-2 not only reduces recognition errors but also enhances both speaker consistency and perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "wer",
                    "twostage",
                    "head",
                    "stage2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contributions of different components and design choices, we conduct a series of ablation and analysis experiments.\nWe first investigate the effect of masked training, which aims to mitigate exposure bias during autoregressive decoding.\nWe then examine the role of diffusion head capacity and the impact of our two-stage training strategy.\nFinally, we analyze the influence of stopping criteria and inference hyperparameters, highlighting how these factors jointly affect intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "different",
                    "diffusion",
                    "twostage",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Ratio.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the impact of different masking rates in the masked training scheme. Without masking (0%), the model suffers from severe exposure bias, leading to a WER of 15.06%. Introducing moderate masking improves robustness, with the best performance at 30% masking (WER 6.17%, UTMOS 3.21). However, excessive masking (50%) degrades both intelligibility and naturalness, as too much corruption disrupts semantic alignment. This confirms that moderate masking helps bridge the gap between training and inference conditions.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "different",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stopping Criteria.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T4\" title=\"Table 4 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares different stopping strategies. Using ground-truth durations causes unstable outputs (WER 29.36%), while oracle endpoint supervision achieves low WER but requires non-causal labels. Our EOS-token design achieves comparable WER and UTMOS without relying on oracle information, while maintaining stable generation speed, making it a practical choice for unified MLLM-based TTS.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "different",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference Hyperparameters.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T5\" title=\"Table 5 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> analyzes the influence of diffusion inference parameters. Lower temperatures tend to produce cleaner but truncated outputs, leading to higher WER and lower similarity. Conversely, higher temperatures improve diversity but may reduce naturalness. We find that a temperature of 0.9 with 100 denoising steps achieves the best trade-off, yielding the lowest WER (1.95%), highest similarity (0.54), and best UTMOS (4.00).</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a dual-head multimodal language model that integrates a frame-level continuous-token diffusion head with an autoregressive LLM backbone for speaker-referenced TTS. By combining continuous speech representations, our approach avoids the quantization bottleneck and achieves high-fidelity and natural speech. To overcome exposure bias and improve training performance, we adapt masked training and a two-stage optimization scheme, which together substantially improve robustness and quality. Evaluations on LibriSpeech(PC) demonstrate significant gains, including a 46% relative WER reduction over our baseline, along with higher speaker similarity and audio quality. These results highlight the effectiveness of bridging autoregressive modeling with diffusion-based refinement for continuous speech generation.\nLooking ahead, this framework provides a path toward unified foundation models that can support multiple speech and multimodal tasks within a single framework.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "twostage",
                    "head",
                    "wer"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Performance with different stopping criteria.\nGT-Dur.: using oracle duration;\nGT-EP.: using oracle end-of-speech (oracle stopping point);\nEOS Token: our method, where the LM head predicts an end-of-sequence token during inference.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Stopping Criteria</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-R<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-G<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">GT-Dur.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GT-EP.</th>\n<td class=\"ltx_td ltx_align_center\">3.46</td>\n<td class=\"ltx_td ltx_align_center\">0.49</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">EOS Token</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.21</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "stopping",
            "gtdur",
            "inference",
            "utmos↑uparrow",
            "↓downarrow",
            "eos",
            "our",
            "oracle",
            "token",
            "criteria",
            "simg↑uparrow",
            "predicts",
            "endofspeech",
            "performance",
            "simr↑uparrow",
            "head",
            "where",
            "gtep",
            "during",
            "point",
            "duration",
            "method",
            "endofsequence",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stopping Criteria.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T4\" title=\"Table 4 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares different stopping strategies. Using ground-truth durations causes unstable outputs (WER 29.36%), while oracle endpoint supervision achieves low WER but requires non-causal labels. Our EOS-token design achieves comparable WER and UTMOS without relying on oracle information, while maintaining stable generation speed, making it a practical choice for unified MLLM-based TTS.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information.\nIn this work, we investigate the TTS within the MLLM paradigm using continuous speech representations.\nWe design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis.\n(3) Masked training is employed to address exposure bias in autoregressive decoding.\n(4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "token",
                    "head",
                    "performance",
                    "where",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this line, we propose to integrate diffusion directly into an autoregressive MLLM framework, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Prior autoregressive diffusion methods either rely on intermediate semantic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib34\" title=\"\">34</a>]</cite>, or relax framewise causality in diffusion modules to predict multi-frame blocks per AR step <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib24\" title=\"\">24</a>]</cite>. In contrast, our approach implements a strictly frame-by-frame autoregressive, continuous-representation diffusion head on top of an LLM backbone. This design enables the model to directly generate high-fidelity speech in the continuous speech representation space, avoiding the quantization bottleneck.</p>\n\n",
                "matched_terms": [
                    "head",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maintain the MLLM&#8217;s multi-task consistency, we designed a dual-head architecture. The diffusion head generates continuous speech embeddings each frame, which are then decoded to synthesize waveforms. The language model (LM) head retained from the original LLM predicts the start and end of speech tokens, enabling variable-length speech synthesis. This token-based control enables seamless integration of speech generation with multimodal generation. Unlike prior TTS methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib26\" title=\"\">26</a>]</cite> introducing an external classifier module, our design preserves a single, unified framework within the MLLM.</p>\n\n",
                "matched_terms": [
                    "predicts",
                    "head",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation into continuous-token autoregressive TTS revealed two major training challenges: exposure bias and joint optimization instability. First, the mismatch between teacher-forced training and free-running inference causes small frame-level deviations to accumulate over long sequences. We mitigate this exposure bias <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib2\" title=\"\">2</a>]</cite> by introducing a masked training scheme inspired by audio generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite>, where a proportion of ground-truth frames is masked during training. This bridges the gap between training and inference conditions, improving robustness and temporal consistency in generated speech.</p>\n\n",
                "matched_terms": [
                    "during",
                    "inference",
                    "where",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then we observed that jointly optimizing the diffusion head with the LLM is unstable. The LLM&#8217;s output distribution evolves during training, causing instability in the diffusion learning process. To address this, we employ a two-stage training strategy: in stage 1, train the LLM and diffusion head jointly, allowing the LLM to learn speech token prediction and the diffusion head to adapt to evolving inputs. In stage 2, we freeze the entire LLM side, including the backbone, the LM head, and the speech-projection, thereby fixing the input distribution to the diffusion head. We then train only the diffusion head. This allows the diffusion model to focus solely on refining the mapping from the LLM outputs to the target speech space.\nThis separation stabilizes training and significantly improves generation quality.</p>\n\n",
                "matched_terms": [
                    "token",
                    "head",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a dual-head architecture where LM head supports variable-length speech and keeps unified multimodal framework.</p>\n\n",
                "matched_terms": [
                    "head",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We stabilize training with a two-stage strategy, yielding large performance gains and cutting WER by 46%, reaching SOTA AR on LibriSpeech(PC) test-clean.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The models and results described in this paper are intended for research purposes only.</span></span></span></p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot TTS.</span>\nZero-shot text-to-speech (TTS) refers to synthesizing speech for previously unseen speakers by leveraging a short reference utterance as conditioning, thereby enabling speaker generalization without explicit speaker-specific training.\nInspired by advances in large language models (LLMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib4\" title=\"\">4</a>]</cite>, zero-shot TTS is often formulated as a language modeling task&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib40\" title=\"\">40</a>]</cite>, where speech waveforms are transformed into sequences of tokens and synthesized via next-token prediction.\nExisting methods can be broadly categorized into multi-stage and single-stage pipelines.\nMulti-stage systems, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib35\" title=\"\">35</a>]</cite> and SALAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib34\" title=\"\">34</a>]</cite>, autoregressively predict coarse units such as semantic&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib3\" title=\"\">3</a>]</cite> or codec tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib38\" title=\"\">38</a>]</cite>, which are then refined into waveforms.\nThis decomposition improves stability but often discards fine-grained acoustic details.\nIn contrast, single-stage approaches, exemplified by MegaTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib16\" title=\"\">16</a>]</cite> and NaturalSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib17\" title=\"\">17</a>]</cite>, directly generate high-information continuous representations, offering higher fidelity while facing greater challenges in robustness.\nOur method follows this single-stage paradigm.</p>\n\n",
                "matched_terms": [
                    "where",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We aim to autoregressively generate speech in the space of continuous acoustic embeddings. Our framework builds upon a large language model backbone\nand introduces a dual-head architecture. The first part of the method addresses how continuous speech tokens are generated: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> introduces continuous-token generation with a diffusion head, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS2\" title=\"3.2 EOS Control in Dual-Head Architecture &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> presents EOS control in the dual-head architecture, which together form the foundation for continuous speech generation within a multitask unified foundation model setting. The second part focuses on improving robustness and overall performance: Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS3\" title=\"3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a> describes masked autoregressive learning, which exposes the model to imperfect histories, and Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS4\" title=\"3.4 Two-Stage Scheme &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a> details a two-stage optimization scheme, which stabilizes training by mitigating distribution drift.</p>\n\n",
                "matched_terms": [
                    "our",
                    "head",
                    "method",
                    "eos",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (b) illustrates the framework.\nGiven a prompt <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> with transcription and reference audio, multi-modal causal LLM <math alttext=\"\\mathcal{C_{\\theta}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{C_{\\theta}}</annotation></semantics></math> takes <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> and past predictions <math alttext=\"\\hat{x}_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{&lt;i}</annotation></semantics></math> to autoregressively produce a hidden state as condition <math alttext=\"z_{i}=\\mathcal{C}_{\\theta}(p,\\hat{x}_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z_{i}=\\mathcal{C}_{\\theta}(p,\\hat{x}_{&lt;i})</annotation></semantics></math>. This vector <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> conditions a diffusion head with MLP denoiser <math alttext=\"M_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">M_{\\phi}</annotation></semantics></math>, which starts from Gaussian noise <math alttext=\"x_{i}^{t}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{i}^{t}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> and iteratively denoises to produce the next embedding <math alttext=\"\\hat{x}_{i}=x_{i}^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><msubsup><mi>x</mi><mi>i</mi><mn>0</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_{i}=x_{i}^{0}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> During training,\nwe sample a total timestep <math alttext=\"t\\!\\sim\\!\\mathit{U}\\{1,\\dots,T\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8764;</mo><mrow><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>T</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t\\!\\sim\\!\\mathit{U}\\{1,\\dots,T\\}</annotation></semantics></math> for adding noise and noise <math alttext=\"\\varepsilon\\!\\sim\\!\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>&#949;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\varepsilon\\!\\sim\\!\\mathcal{N}(0,I)</annotation></semantics></math>, and form a noised target <math alttext=\"x_{i}^{t}=\\sqrt{\\bar{\\alpha}_{t}}x_{i}+\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mrow><msqrt><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub></msqrt><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msqrt><mrow><mn>1</mn><mo>&#8722;</mo><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub></mrow></msqrt><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#949;</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x_{i}^{t}=\\sqrt{\\bar{\\alpha}_{t}}x_{i}+\\sqrt{1-\\bar{\\alpha}_{t}}\\varepsilon</annotation></semantics></math>, where <math alttext=\"\\bar{\\alpha}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}</annotation></semantics></math> defines a noise schedule <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib27\" title=\"\">27</a>]</cite>.\nA small MLP denoiser <math alttext=\"M_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>M</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">M_{\\phi}</annotation></semantics></math> predicts the noise <math alttext=\"\\hat{\\varepsilon}=M_{\\phi}\\!\\big(x_{i}^{t},\\,t,\\,z_{i}\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#949;</mi><mo>^</mo></mover><mo>=</mo><mrow><msub><mi>M</mi><mi>&#981;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo rspace=\"0.337em\">,</mo><mi>t</mi><mo rspace=\"0.337em\">,</mo><msub><mi>z</mi><mi>i</mi></msub><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{\\varepsilon}=M_{\\phi}\\!\\big(x_{i}^{t},\\,t,\\,z_{i}\\big)</annotation></semantics></math>, where <math alttext=\"x_{i}^{t},t,z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}^{t},t,z_{i}</annotation></semantics></math> denotes the current state, the timestep, and the LLM condition <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>. We minimize the standard noise-prediction loss</p>\n\n",
                "matched_terms": [
                    "predicts",
                    "during",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior speech generation approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib35\" title=\"\">35</a>]</cite> typically rely on an auxiliary classifier or a fixed-length constraint to determine the endpoint of speech output. In contrast, our framework delegates boundary control to the LM head, enabling seamless integration into a unified multi-task backbone.</p>\n\n",
                "matched_terms": [
                    "head",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Generation proceeds under the control of the LM head. The model begins in the textual phase until the LM head emits a special token <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span>, which triggers the speech generation phase.\nAt each subsequent step, the LM head produces a control token. If it emits <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span>, this token is not added to the output sequence; instead, it signals the diffusion head to generate the next speech embeddings. Otherwise when LM head predicts <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>, the speech generation phase terminates.\nThis token-based mechanism provides a unified and modality-agnostic interface for switching between text and speech without additional architectural components.</p>\n\n",
                "matched_terms": [
                    "token",
                    "inference",
                    "head",
                    "predicts",
                    "eos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span> As shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, in order to supervise this control process, we extend the vocabulary with a special token <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> in addition to <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span> and <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>. Although <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span> is not emitted during inference, its explicit supervision during training provides dense learning signals throughout the speech segment. This design reduces the risk of the LM head prematurely predicting <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span> in variable-length sequences, compared with supervising only the boundary tokens. The LM head is trained with the standard cross-entropy <math alttext=\"\\mathcal{L}_{\\text{LM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>LM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{LM}}</annotation></semantics></math> over the control tokens, while the diffusion head is trained with the noise-prediction loss <math alttext=\"\\mathcal{L}_{\\text{diff}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>diff</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{diff}}</annotation></semantics></math> (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.SS1\" title=\"3.1 Continuous-token Generation with Diffusion Head &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). The overall objective is the sum of the LM cross-entropy loss and the diffusion loss:</p>\n\n",
                "matched_terms": [
                    "during",
                    "token",
                    "inference",
                    "head",
                    "eos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive generation is affected by exposure bias <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib2\" title=\"\">2</a>]</cite>, where models are trained on ground-truth histories but must rely on its own potentially erroneous predictions during inference, resulting in error accumulation.\nTo mitigate this issue, various strategies such as masking have been explored in text generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib10\" title=\"\">10</a>]</cite> and audio generation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib36\" title=\"\">36</a>]</cite>. Motivated by these advances, we adapt masking to continuous frame-level speech generation.</p>\n\n",
                "matched_terms": [
                    "during",
                    "inference",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, before feeding the acoustic embedding sequence <math alttext=\"x=\\{x^{1},\\ldots,x^{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>x</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>x</mi><mi>N</mi></msup><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=\\{x^{1},\\ldots,x^{N}\\}</annotation></semantics></math> into the causal predictor <math alttext=\"C_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>C</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">C_{\\theta}</annotation></semantics></math>, we apply <span class=\"ltx_text ltx_font_italic\">zero embedding masking</span> to simulate imperfect histories. We define a binary mask <math alttext=\"v=\\{v^{1},\\ldots,v^{N}\\},\\;v^{t}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>v</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>v</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>v</mi><mi>N</mi></msup><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"0.447em\">,</mo><mrow><msup><mi>v</mi><mi>t</mi></msup><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v=\\{v^{1},\\ldots,v^{N}\\},\\;v^{t}\\in\\{0,1\\}</annotation></semantics></math>, where each entry is sampled independently as <math alttext=\"v^{t}\\sim\\mathrm{Bernoulli}(1-p_{\\mathrm{mask}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>v</mi><mi>t</mi></msup><mo>&#8764;</mo><mrow><mi>Bernoulli</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><msub><mi>p</mi><mi>mask</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v^{t}\\sim\\mathrm{Bernoulli}(1-p_{\\mathrm{mask}})</annotation></semantics></math>. Thus, with probability <math alttext=\"p_{\\mathrm{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>mask</mi></msub><annotation encoding=\"application/x-tex\">p_{\\mathrm{mask}}</annotation></semantics></math>, the corresponding frame is masked and replaced by the zero vector. The corrupted sequence is then <math alttext=\"\\tilde{x}=x\\odot v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>x</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>v</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}=x\\odot v</annotation></semantics></math>, where <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes element-wise multiplication, so masked positions are replaced by the zero vector.\nThis zero embedding masking strategy can be viewed as input-level masking, where the masking ratio <math alttext=\"p_{\\mathrm{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>mask</mi></msub><annotation encoding=\"application/x-tex\">p_{\\mathrm{mask}}</annotation></semantics></math> explicitly controls the level of corruption in the autoregressive history. The training masking is shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (b).\nAt inference time, no masking is applied. The model operates autoregressively on its own predictions.\nBy training under such corrupted contexts, the model is encouraged to handle imperfect histories more robustly, thereby mitigating exposure bias and improving stability in long-form speech synthesis.</p>\n\n",
                "matched_terms": [
                    "during",
                    "inference",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When training the MLLM and diffusion head jointly in an end-to-end manner, we observed instability caused by distribution drift. The MLLM output distribution <math alttext=\"p_{\\theta}(z\\mid p)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>z</mi><mo>&#8739;</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z\\mid p)</annotation></semantics></math> evolves as the parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> are updated. The diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> is expected to learn a mapping from <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> to continuous speech embeddings <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m5\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>. However, since the source distribution <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> is non-stationary during training, <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> must adapt to a shifting input space, making convergence unreliable and degrading generation quality. We hypothesize that freezing <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> to fix <math alttext=\"p_{\\theta}(z)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(z)</annotation></semantics></math> yields a stationary input distribution, allowing the diffusion head <math alttext=\"\\mathcal{M}_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{\\phi}</annotation></semantics></math> to focus on modeling a stable transformation, thereby improving optimization stability and synthesis fidelity.</p>\n\n",
                "matched_terms": [
                    "head",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement this idea, we adopt a two-stage training strategy, as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>(c). In <span class=\"ltx_text ltx_font_bold\">Stage 1</span>, we jointly train the causal LM <math alttext=\"\\mathcal{C}\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}\\theta</annotation></semantics></math> and the diffusion head <math alttext=\"\\mathcal{M}\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{M}\\phi</annotation></semantics></math> in an end-to-end manner by minimizing the sum of the cross-entropy loss and the diffusion loss. This stage enables the model to align the LM outputs with the target distribution and to produce coarse speech embeddings. However, we observe that although the overall training objective consistently decreases, autoregressive evaluation metrics exhibit a non-monotonic trend, improving initially but then deteriorating. This is consistent with our hypothesis that distribution drift hinders stable refinement.</p>\n\n",
                "matched_terms": [
                    "head",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our dual-head continuous speech generation framework, we conduct TTS experiments under different settings, comparing intelligibility, speaker identity preservation, and speech naturalness. We first introduce the datasets and evaluation protocols in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS1\" title=\"4.1 Dataset and Metrics &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, then present implementation details in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS2\" title=\"4.2 Implementation Details &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, and finally describe the baselines in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS3\" title=\"4.3 Baseline &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system on three aspects: intelligibility, speaker identity preservation, and speech quality.\n(1) Word error rate (WER) measures intelligibility, about how accurately the synthesized speech conveys the reference text. The generated speech is transcribed by Whisper-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib30\" title=\"\">30</a>]</cite>, and WER is calculated from insertions, substitutions, and deletions.\n(2) Speaker similarity is measured as cosine similarity between embeddings extracted by ECAPA-TDNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib7\" title=\"\">7</a>]</cite>. We report SIM-R (to reference prompt) and SIM-G (to ground-truth speech).\n(3) Speech quality is estimated with UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib31\" title=\"\">31</a>]</cite>, an objective MOS predictor trained on large-scale human ratings.</p>\n\n",
                "matched_terms": [
                    "our",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Details</span>\nOur architecture builds on an autoregressive LLM backbone and extends it with projection modules for multi modality and a diffusion head for speech generation. We adopt OPT-125M&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib41\" title=\"\">41</a>]</cite> as the LLM backbone. For multi-modality, we add two projectors: one maps the 768-dimensional reference embedding extracted from the speaker prompt into the LLM input space, and the other maps the 64-dimensional continuous acoustic tokens into the LLM input space. With these projections, the backbone functions as a multimodal language model (MLLM) rather than a purely text-based LLM.</p>\n\n",
                "matched_terms": [
                    "head",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Hyperparameters</span>\nDuring training, we use a diffusion process with <math alttext=\"T=1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding=\"application/x-tex\">T=1000</annotation></semantics></math> steps and adopt the cosine noise schedule&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib27\" title=\"\">27</a>]</cite>,where derives <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> implicitly from the cumulative product <math alttext=\"\\bar{\\alpha}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}</annotation></semantics></math>.\nThe diffusion head is an MLP with residual blocks; we experimented with 3, 6, and 12 layers, and report 12-layer results unless otherwise noted.\nEach block consists of layer normalization, linear layers, and SiLU activation with adaptive layer normalization modulation, with no dropout.\nDuring inference, we reduce the denoising process to 100 steps and apply a sampling temperature of <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>. CFG is set to 1.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "during",
                    "head"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the model from the first-stage joint training of all components, including the MLLM backbone, LM head and diffusion head, as our baseline. During training, the loss decreases monotonically, but the evaluation WER first decreases and then increases because of the dynamic condition for the diffusion head. We therefore apply the early stopping and select the checkpoint with the lowest validation WER as the baseline model, from which the stage-2 training is initialized.</p>\n\n",
                "matched_terms": [
                    "during",
                    "wer",
                    "stopping",
                    "head",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first present the main results by comparing our model with representative baselines.\nWe then provide ablations and analyses of key design choices and inference settings to understand their impact on intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Main Results &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the comparison between our model and representative hybrid autoregressive baselines.\nVALL-E, which relies on discrete tokens, yields a WER of 6.11% and a speaker similarity of 0.47, illustrating the limitations of quantization in preserving fine-grained acoustic details.\nMegaTTS, a continuous-token model with 500M parameters, achieves stronger results with a WER of 2.32% and a similarity of 0.53.\nBy contrast, our model attains a WER of 1.95% and a similarity of 0.54, while maintaining competitive perceptual quality (UTMOS 4.00 compared with 4.02 for MegaTTS).\nDespite using only 160M parameters, our system consistently outperforms larger models, demonstrating the efficiency of combining an autoregressive backbone with a continuous diffusion head.\nFor additional context, we also include ground-truth speech and vocoder reconstructions in the table for reference.</p>\n\n",
                "matched_terms": [
                    "our",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further analyze the effect of our two-stage training strategy.\nThe Stage-1 baseline achieves a WER of 3.61%, a speaker similarity of 0.49, and a UTMOS of 3.21, indicating that the diffusion head initially suffers from unstable input distributions.\nAfter Stage-2 training, the WER is reduced by 46% relative (from 3.61% to 1.95%), while speaker similarity increases from 0.49 to 0.54 and UTMOS rises from 3.21 to 4.00.\nThese improvements show that stabilizing the diffusion head&#8217;s input distribution in Stage-2 not only reduces recognition errors but also enhances both speaker consistency and perceived naturalness.</p>\n\n",
                "matched_terms": [
                    "our",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contributions of different components and design choices, we conduct a series of ablation and analysis experiments.\nWe first investigate the effect of masked training, which aims to mitigate exposure bias during autoregressive decoding.\nWe then examine the role of diffusion head capacity and the impact of our two-stage training strategy.\nFinally, we analyze the influence of stopping criteria and inference hyperparameters, highlighting how these factors jointly affect intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stopping",
                    "criteria",
                    "head",
                    "inference",
                    "different",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Ratio.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the impact of different masking rates in the masked training scheme. Without masking (0%), the model suffers from severe exposure bias, leading to a WER of 15.06%. Introducing moderate masking improves robustness, with the best performance at 30% masking (WER 6.17%, UTMOS 3.21). However, excessive masking (50%) degrades both intelligibility and naturalness, as too much corruption disrupts semantic alignment. This confirms that moderate masking helps bridge the gap between training and inference conditions.</p>\n\n",
                "matched_terms": [
                    "different",
                    "inference",
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Head Depth.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares different diffusion head depths with and without the proposed two-stage training. Increasing the depth from 3 to 12 layers progressively improves WER and speaker similarity, demonstrating the benefit of a stronger decoder. More importantly, enabling two-stage training further reduces WER to 1.95% and boosts similarity and naturalness, highlighting the effectiveness of stabilizing the diffusion head with a fixed input distribution.</p>\n\n",
                "matched_terms": [
                    "different",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference Hyperparameters.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T5\" title=\"Table 5 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> analyzes the influence of diffusion inference parameters. Lower temperatures tend to produce cleaner but truncated outputs, leading to higher WER and lower similarity. Conversely, higher temperatures improve diversity but may reduce naturalness. We find that a temperature of 0.9 with 100 denoising steps achieves the best trade-off, yielding the lowest WER (1.95%), highest similarity (0.54), and best UTMOS (4.00).</p>\n\n",
                "matched_terms": [
                    "inference",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a dual-head multimodal language model that integrates a frame-level continuous-token diffusion head with an autoregressive LLM backbone for speaker-referenced TTS. By combining continuous speech representations, our approach avoids the quantization bottleneck and achieves high-fidelity and natural speech. To overcome exposure bias and improve training performance, we adapt masked training and a two-stage optimization scheme, which together substantially improve robustness and quality. Evaluations on LibriSpeech(PC) demonstrate significant gains, including a 46% relative WER reduction over our baseline, along with higher speaker similarity and audio quality. These results highlight the effectiveness of bridging autoregressive modeling with diffusion-based refinement for continuous speech generation.\nLooking ahead, this framework provides a path toward unified foundation models that can support multiple speech and multimodal tasks within a single framework.</p>\n\n",
                "matched_terms": [
                    "our",
                    "head",
                    "performance",
                    "wer"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Performance under different inference parameters.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Temperature</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Inference Steps</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER(%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-R<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SIM-G<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">200</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">100</th>\n<td class=\"ltx_td ltx_align_center\">7.53</td>\n<td class=\"ltx_td ltx_align_center\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">0.44</td>\n<td class=\"ltx_td ltx_align_center\">3.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">0.9</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">100</th>\n<td class=\"ltx_td ltx_align_center\">1.95</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">0.50</td>\n<td class=\"ltx_td ltx_align_center\">4.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">0.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">100</th>\n<td class=\"ltx_td ltx_align_center\">16.11</td>\n<td class=\"ltx_td ltx_align_center\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">3.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">0.8</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">80</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">19.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.07</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer↓downarrow",
            "simr↑uparrow",
            "temperature",
            "steps",
            "under",
            "inference",
            "simg↑uparrow",
            "utmos↑uparrow",
            "different",
            "parameters",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference Hyperparameters.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T5\" title=\"Table 5 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> analyzes the influence of diffusion inference parameters. Lower temperatures tend to produce cleaner but truncated outputs, leading to higher WER and lower similarity. Conversely, higher temperatures improve diversity but may reduce naturalness. We find that a temperature of 0.9 with 100 denoising steps achieves the best trade-off, yielding the lowest WER (1.95%), highest similarity (0.54), and best UTMOS (4.00).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Inference.</span> Generation proceeds under the control of the LM head. The model begins in the textual phase until the LM head emits a special token <span class=\"ltx_text ltx_font_italic\">&lt;speech_bos&gt;</span>, which triggers the speech generation phase.\nAt each subsequent step, the LM head produces a control token. If it emits <span class=\"ltx_text ltx_font_italic\">&lt;cont_speech_gen&gt;</span>, this token is not added to the output sequence; instead, it signals the diffusion head to generate the next speech embeddings. Otherwise when LM head predicts <span class=\"ltx_text ltx_font_italic\">&lt;eos&gt;</span>, the speech generation phase terminates.\nThis token-based mechanism provides a unified and modality-agnostic interface for switching between text and speech without additional architectural components.</p>\n\n",
                "matched_terms": [
                    "under",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, before feeding the acoustic embedding sequence <math alttext=\"x=\\{x^{1},\\ldots,x^{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>x</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>x</mi><mi>N</mi></msup><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=\\{x^{1},\\ldots,x^{N}\\}</annotation></semantics></math> into the causal predictor <math alttext=\"C_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>C</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">C_{\\theta}</annotation></semantics></math>, we apply <span class=\"ltx_text ltx_font_italic\">zero embedding masking</span> to simulate imperfect histories. We define a binary mask <math alttext=\"v=\\{v^{1},\\ldots,v^{N}\\},\\;v^{t}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>v</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>v</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msup><mi>v</mi><mi>N</mi></msup><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"0.447em\">,</mo><mrow><msup><mi>v</mi><mi>t</mi></msup><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v=\\{v^{1},\\ldots,v^{N}\\},\\;v^{t}\\in\\{0,1\\}</annotation></semantics></math>, where each entry is sampled independently as <math alttext=\"v^{t}\\sim\\mathrm{Bernoulli}(1-p_{\\mathrm{mask}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msup><mi>v</mi><mi>t</mi></msup><mo>&#8764;</mo><mrow><mi>Bernoulli</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>&#8722;</mo><msub><mi>p</mi><mi>mask</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v^{t}\\sim\\mathrm{Bernoulli}(1-p_{\\mathrm{mask}})</annotation></semantics></math>. Thus, with probability <math alttext=\"p_{\\mathrm{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>mask</mi></msub><annotation encoding=\"application/x-tex\">p_{\\mathrm{mask}}</annotation></semantics></math>, the corresponding frame is masked and replaced by the zero vector. The corrupted sequence is then <math alttext=\"\\tilde{x}=x\\odot v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>x</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>v</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{x}=x\\odot v</annotation></semantics></math>, where <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes element-wise multiplication, so masked positions are replaced by the zero vector.\nThis zero embedding masking strategy can be viewed as input-level masking, where the masking ratio <math alttext=\"p_{\\mathrm{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>mask</mi></msub><annotation encoding=\"application/x-tex\">p_{\\mathrm{mask}}</annotation></semantics></math> explicitly controls the level of corruption in the autoregressive history. The training masking is shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S3.F2\" title=\"Figure 2 &#8227; 3.3 Masked Autoregressive Learning &#8227; 3 Proposed Method &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (b).\nAt inference time, no masking is applied. The model operates autoregressively on its own predictions.\nBy training under such corrupted contexts, the model is encouraged to handle imperfect histories more robustly, thereby mitigating exposure bias and improving stability in long-form speech synthesis.</p>\n\n",
                "matched_terms": [
                    "under",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our dual-head continuous speech generation framework, we conduct TTS experiments under different settings, comparing intelligibility, speaker identity preservation, and speech naturalness. We first introduce the datasets and evaluation protocols in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS1\" title=\"4.1 Dataset and Metrics &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, then present implementation details in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS2\" title=\"4.2 Implementation Details &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, and finally describe the baselines in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S4.SS3\" title=\"4.3 Baseline &#8227; 4 Experiments &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "under",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Diffusion Hyperparameters</span>\nDuring training, we use a diffusion process with <math alttext=\"T=1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding=\"application/x-tex\">T=1000</annotation></semantics></math> steps and adopt the cosine noise schedule&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#bib.bib27\" title=\"\">27</a>]</cite>,where derives <math alttext=\"\\beta_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#946;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\beta_{t}</annotation></semantics></math> implicitly from the cumulative product <math alttext=\"\\bar{\\alpha}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#945;</mi><mo>&#175;</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\bar{\\alpha}_{t}</annotation></semantics></math>.\nThe diffusion head is an MLP with residual blocks; we experimented with 3, 6, and 12 layers, and report 12-layer results unless otherwise noted.\nEach block consists of layer normalization, linear layers, and SiLU activation with adaptive layer normalization modulation, with no dropout.\nDuring inference, we reduce the denoising process to 100 steps and apply a sampling temperature of <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math>. CFG is set to 1.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "inference",
                    "temperature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the contributions of different components and design choices, we conduct a series of ablation and analysis experiments.\nWe first investigate the effect of masked training, which aims to mitigate exposure bias during autoregressive decoding.\nWe then examine the role of diffusion head capacity and the impact of our two-stage training strategy.\nFinally, we analyze the influence of stopping criteria and inference hyperparameters, highlighting how these factors jointly affect intelligibility, speaker similarity, and naturalness.</p>\n\n",
                "matched_terms": [
                    "different",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Masked Ratio.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12995v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Ablation and Analysis &#8227; 5 Results &#8227; Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the impact of different masking rates in the masked training scheme. Without masking (0%), the model suffers from severe exposure bias, leading to a WER of 15.06%. Introducing moderate masking improves robustness, with the best performance at 30% masking (WER 6.17%, UTMOS 3.21). However, excessive masking (50%) degrades both intelligibility and naturalness, as too much corruption disrupts semantic alignment. This confirms that moderate masking helps bridge the gap between training and inference conditions.</p>\n\n",
                "matched_terms": [
                    "different",
                    "inference",
                    "performance"
                ]
            }
        ]
    }
}