{
    "S3.T1": {
        "source_file": "Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation",
        "caption": "Table 1: Results in xcomet 3 (↑\\uparrow) and Blaser 2.0 (↑\\uparrow) on the FLEURS test set (en→\\rightarrowxx). Reporting the three model variants (Base, Dual, Noisy), across two generation strategies (Cascade, CoT). Best results across model and generation strategies are in bold, and second best results are underlined.",
        "body": "Blaser 2.0 (↑\\uparrow)\n\n\nxcomet (↑\\uparrow)\n\n\n\n\nca\nde\nes\nfr\nit\npt\navg.\nca\nde\nes\nfr\nit\npt\navg.\n\n\n\n\n\nBase-Cascade\n\n4.17\n4.44\n4.17\n4.15\n4.14\n4.35\n4.24\n86.39\n92.42\n87.27\n81.02\n87.16\n87.95\n87.04\n\n\n\nBase-CoT\n\n4.16\n4.44\n4.22\n4.24\n4.22\n4.41\n4.28\n85.41\n92.40\n86.86\n81.47\n87.00\n87.20\n86.72\n\n\n\nDual-Cascade\n\n4.19\n4.47\n4.17\n4.17\n4.13\n4.39\n4.25\n88.48\n93.78\n89.71\n83.21\n89.10\n89.80\n89.01\n\n\n\nDual-CoT\n\n4.21\n4.46\n4.21\n4.24\n4.24\n4.44\n4.30\n87.97\n93.56\n89.42\n83.89\n89.77\n89.50\n89.02\n\n\n\nNoisy-Cascade\n\n4.16\n4.44\n4.18\n4.14\n4.14\n4.36\n4.24\n86.48\n92.55\n88.03\n81.25\n88.05\n88.47\n87.47\n\n\n\nNoisy-CoT\n\n4.15\n4.47\n4.21\n4.27\n4.21\n4.40\n4.28\n86.94\n93.08\n87.76\n83.29\n87.98\n87.99\n87.84",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:0.4pt 3.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"7\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Blaser&#160;2.0</span><span class=\"ltx_text\" style=\"font-size:70%;\"> (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\">)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"7\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">x</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">comet</span><span class=\"ltx_text\" style=\"font-size:70%;\"> (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\">)</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding:0.4pt 3.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">ca</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">de</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">es</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">fr</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">pt</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">avg.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">ca</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">de</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">es</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">fr</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">pt</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Base</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cascade</span>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.17</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.44</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.17</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.15</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.14</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.35</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.24</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.39</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">92.42</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.27</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">81.02</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.16</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.95</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Base</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">CoT</span>\n</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.16</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.44</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.22</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.24</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.22</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.41</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.28</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">85.41</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">92.40</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.86</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">81.47</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.00</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.20</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.72</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Dual</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cascade</span>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.19</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.47</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.17</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.17</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.39</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.25</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">88.48</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">93.78</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">89.71</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">83.21</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">89.10</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">89.80</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">89.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Dual</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">CoT</span>\n</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.46</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.24</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.24</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.44</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.30</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">87.97</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">93.56</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">89.42</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">83.89</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">89.77</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">89.50</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">89.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Noisy</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cascade</span>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.16</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.44</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.18</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.14</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.14</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.36</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.24</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.48</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">92.55</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">88.03</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">81.25</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">88.05</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">88.47</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Noisy</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">CoT</span>\n</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.15</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.47</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.27</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.40</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">4.28</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.94</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">93.08</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.76</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">83.29</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.98</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.99</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "strategies",
            "en→rightarrowxx",
            "basecot",
            "second",
            "dualcot",
            "blaser",
            "avg",
            "basecascade",
            "three",
            "test",
            "across",
            "noisycot",
            "reporting",
            "base",
            "noisy",
            "results",
            "generation",
            "cot",
            "model",
            "cascade",
            "noisycascade",
            "variants",
            "bold",
            "↑uparrow",
            "set",
            "underlined",
            "dualcascade",
            "xcomet",
            "two",
            "best",
            "fleurs",
            "dual"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> performs slightly below </span>Cascade<span class=\"ltx_text ltx_font_upright\">.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Model Details &#8227; 3 Experimental Setup &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, training only with CoT data (</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) limits the performance of </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compared with </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Incorporating Direct-formatted data (</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) improves both strategies, most noticeably in x</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">comet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and brings </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to competitive levels. Adding noisy data (</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) also yields gains over </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although performance remains below </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Overall, these results indicate that our interventions do not harm performance and can even provide improvements, establishing a solid basis for the detailed analyses that follow.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-to-Text Translation (S2TT) systems built from Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major limitations: error propagation and the inability to exploit prosodic or other acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced, with the expectation that jointly accessing speech and transcription will overcome these issues. Analyzing CoT through attribution methods, robustness evaluations with corrupted transcripts, and prosody-awareness, we find that it largely mirrors cascaded behavior, relying mainly on transcripts while barely leveraging speech. Simple training interventions, such as adding Direct S2TT data or noisy transcript injection, enhance robustness and increase speech attribution. These findings challenge the assumed advantages of CoT and highlight the need for architectures that explicitly integrate acoustic information into translation.</span>\n</p>\n\n",
                "matched_terms": [
                    "cot",
                    "two",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nSpeech-to-Text Translation, Speech Large Language Model, Chain-of-Thought, Cascade, Prosody</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-to-Text Translation (S2TT) refers to converting spoken utterances in one language into written text in another.\nHistorically, S2TT systems were built by cascading an Automatic Speech Recognition (ASR) module, which first transcribes the speech, and a Text-to-Text Translation (T2TT) module which translates it into the target language&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSuch </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">cascade</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> systems are conceptually simple but have two main limitations. First, they can suffer from error propagation, as the T2TT stage cannot correct transcription errors. Second, it cannot preserve information in speech that can be relevant for translation, such as prosody, including emphasis, or breaks that convey a certain meaning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "two",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these limitations, alternative approaches have been proposed. In </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Direct</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> S2TT models, speech is directly translated without intermediate text representations&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, these systems tend to underperform cascade models, mostly due to the scarcity of large-scale parallel S2TT data.\nMore recently, research has shifted toward Speech Large Language Models (SLLMs), introducing a new paradigm of </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Chain-of-Thought</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (CoT) or </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">multi-turn</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> S2TT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Similar to cascade systems, these models first generate the transcription and then produce the translation. The key difference is that they retain access to both the speech input and the transcription during translation. This design is expected to overcome the limitations of cascade models by improving robustness to error propagation and enabling the use of acoustic cues&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, this assumption has not yet been thoroughly validated, and to the best of our knowledge, no prior work has systematically examined whether CoT provides such benefits over cascade S2TT.</span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "cot",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we investigate whether CoT models deliver the hypothesized benefits. We examine this from three complementary perspectives. First, we analyze internal mechanisms of a model with an interpretability method, quantifying the amount of information the model uses from the source speech utterance during translation (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Attribution scores &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Second, we evaluate robustness to error propagation by simulating noisy ASR outputs. Measuring translation quality under such conditions provides indirect evidence of whether the model exploits speech information to compensate for degraded transcripts (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Third, we assess prosody awareness of the model using the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS4\" style=\"font-size:90%;\" title=\"2.4 Prosody awareness &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which directly tests whether the model leverages acoustic cues beyond the transcript to disambiguate meaning when prosody affects translation. We complement these analyses with simple training strategies to mitigate the weaknesses of standard CoT setups and to encourage greater reliance on speech information during translation (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Models &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "three",
                    "strategies",
                    "cot",
                    "model",
                    "second",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This section introduces the methods we use to analyze the relevance of speech cues in CoT S2TT. We assess their direct contribution to translation through attribution analyses, their indirect role via robustness and prosody evaluations, and compare different model variants to understand how training modalities shape CoT behavior.</span>\n</p>\n\n",
                "matched_terms": [
                    "cot",
                    "model",
                    "variants"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Architecture</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We carry out our study on models consisting on a Speech LLM architecture. Specifically, we extend an LLM-based translation model to accept spoken inputs and perform S2TT. Following recent approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, speech is discretized and incorporated into the model as additional tokens in its vocabulary. Each utterance is first encoded with a self-supervised speech model to obtain continuous representations, which are then quantized with k-means clustering into </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">discrete speech units</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (DSUs). The embedding layer of the base LLM is expanded to accommodate these tokens. After this discretization, training proceeds as in a text-only LLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference Strategies</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We compare the behavior of models under the </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> strategies. Models are trained on a mixture of ASR, T2TT, and S2TT data (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Data &#8227; 3 Experimental Setup &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which enables them to operate in both modes. In the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> setup, the model generates a transcription followed by a translation while retaining the original speech input in the context. In the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> setup, we adopt a </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">self-cascade</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> approach: the model first generates a transcription from the speech input and then, conditioned solely on this transcription, produces the translation.</span>\n</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "cot",
                    "model",
                    "cascade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Configurations</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We first consider a </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model trained with S2TT data exclusively in the CoT format. This serves as the baseline for comparing the two inference strategies. We hypothesize that there will be minimal differences between them, implying that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides no meaningful gains over </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To address this limitation, we introduce two interventions. The first is </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a model trained on a mixture of CoT (25%) and Direct (75%) formats. Including samples where the model relies only on the DSUs (Direct) encourages greater awareness of the spoken signal during </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generation. The second is </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which replaces the transcript in a subset of CoT samples with a noisy variant. This simulates error propagation during training and discourages over-reliance on transcriptions. We follow the same corruption strategy used in the analysis of robustness to transcription errors (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and we modify 25% of the training samples (selected by manual tuning). For these samples, we omit the transcription loss to avoid degrading ASR performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "strategies",
                    "cot",
                    "model",
                    "second",
                    "cascade",
                    "two",
                    "base",
                    "noisy",
                    "dual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">inseq</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> library&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract token-level contribution scores, represented as token&#8211;token interaction matrices. To analyze modality use, we aggregate these scores over spans of tokens. We focus on three input regions: the speech input (DSUs), the transcription, and the previously translated tokens. For each region, we compute its contribution by summing the scores of all tokens in the region and averaging across all output tokens. This yields a normalized distribution over input regions, where contributions sum to 1 (with a small portion attributed to special tokens). We then analyze these aggregated scores across layers of the model to trace how reliance on each modality evolves along the model depth (see Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "three",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We analyze robustness to transcription errors as an indirect way of testing whether CoT models exploit speech cues. If the model relies only on the transcript, its performance should degrade quickly once the transcript is corrupted. Conversely, meaningful use of the audio input would mitigate this effect.</span>\n</p>\n\n",
                "matched_terms": [
                    "cot",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the suggestion of </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who propose exploring how CoT behaves when fed with lower-quality ASR transcripts, our controlled corruption procedure provides a reproducible way to simulate such conditions and evaluate robustness systematically. For each transcription in a test set, we replace a contiguous fragment of the transcript with an unrelated one. The replacement preserves the length and grammaticality of the sentence but alters its content entirely, ensuring that the corrupted transcript remains fluent while diverging semantically. We generated the corrupted fragment with Gemini-2.0-Flash&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The start position is sampled uniformly, and the fragment length is determined by the target corruption ratio (e.g., 2.5&#8211;30% of the words). An example of an alteration of the transcript is shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cot",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The corrupted transcripts are then paired with the original speech and inserted into a CoT prompt. Results are evaluated with x</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">comet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a well-known translation metric that predicts human judgments from multilingual encoder representations. By measuring the translation quality drop as corrupted percentages increase, we can quantify the robustness of the model to error propagation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "xcomet",
                    "cot",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We aim to analyze whether CoT models make use of speech inputs to perform prosody-aware S2TT or instead behave like cascade models, which are effectively blind to prosody. To this end, we evaluate prosody awareness using </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a benchmark that provides paired utterances differing only in prosodic emphasis. These differences alter meaning and therefore require distinct translations, so low prosody awareness would indicate that the model is unable to leverage speech information. For each pair, the model translates both utterances, and each output is scored with x</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">comet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> against both references, yielding a </span>\n  <math alttext=\"2{\\times}2\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2{\\times}2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> matrix of scores. From this, two metrics are derived: the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Directional score (D)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which checks whether each translation is closer to its intended reference than to the alternative, and the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Global score (G)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which requires this condition to hold for both members of the pair simultaneously. The benchmark covers multiple prosodic phenomena, and results are averaged across them to obtain final scores.</span>\n</p>\n\n",
                "matched_terms": [
                    "cot",
                    "model",
                    "across",
                    "cascade",
                    "xcomet",
                    "two",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Prompt Format</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We follow the strategy of the base translation model and format training samples as instructions using OpenAI&#8217;s </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">chatml</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> template, which casts them into user&#8211;assistant interactions. Spoken inputs (DSUs) are represented as strings by mapping them to private-use Unicode characters (PUA), following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For ASR and T2TT tasks, the user turn contains the input (DSUs or text) followed by a short instruction (e.g., &#8220;Transcribe in English&#8221; or &#8220;Translate from English into Catalan&#8221;), and the assistant turn contains the expected output. For S2TT, we mainly use the CoT strategy, formatting the prompt as multiple turns: the model is first asked to transcribe and then to translate. Unlike a cascade approach, the transcription remains in context during the translation step. In the Direct setup (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Models &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), S2TT samples are formatted as a single turn that requests the translation directly from the speech input. Only assistant outputs are optimized during training, and in the CoT setup both assistant turns are used.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "cascade",
                    "cot",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We combine several datasets for each task. For ASR, we use the English splits of Common Voice 21.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VoxPopuli&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an </span>\n  <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">8</mn>\n      <annotation encoding=\"application/x-tex\">8</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">k-hour subset of Multilingual LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For S2TT, we include CoVoST 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Catalan and German) and Europarl-ST&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (all languages except Catalan). For T2TT, we reuse the same datasets as for S2TT to ensure comparability across tasks. We deliberately avoid using external T2TT corpora, prioritizing a fair comparison between CoT and cascade approaches and preventing potential quality mismatches between S2TT and T2TT data. We restrict the amount of T2TT to </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">k sentence pairs per language, as the goal is to just keep the initial performance on this task. All evaluations and analyses are conducted on FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "cot",
                    "fleurs",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DSU Embeddings Adaptation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;To accommodate the new DSU tokens, the base LLM expands its embedding layer (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Models &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). This layer then combines pretrained text embeddings from the base model with newly initialized embeddings for the DSUs. Prior work suggests adapting these new embeddings to the LLM&#8217;s representation space before full training&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which accelerates convergence and improves performance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For this adaptation, we train the model with a next-token prediction objective on discretized speech-only data, keeping the base LLM frozen except for the embedding layer and the output projection.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Hyperparameters</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;\nWe train our model with the AdamW optimizer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"16.7k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">16.7</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">16.7k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps with a maximum learning rate (LR) of </span>\n  <math alttext=\"1\\cdot 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8901;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\cdot 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The LR follows a cosine schedule, starting with a warmup for the </span>\n  <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">10\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the training and decaying until </span>\n  <math alttext=\"1\\cdot 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8901;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">6</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\cdot 10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The effective batch size is </span>\n  <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">256</mn>\n      <annotation encoding=\"application/x-tex\">256</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, result of training on 16 GPUs with a per-device batch size of </span>\n  <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">16</mn>\n      <annotation encoding=\"application/x-tex\">16</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We set a maximum length of </span>\n  <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2048</mn>\n      <annotation encoding=\"application/x-tex\">2048</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens, and we clip the gradients to a maximum norm of </span>\n  <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1.0</mn>\n      <annotation encoding=\"application/x-tex\">1.0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For generation, we use beam-search multinomial sampling, setting 5 beams, a temperature of </span>\n  <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.2</mn>\n      <annotation encoding=\"application/x-tex\">0.2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a top-</span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.95</mn>\n      <annotation encoding=\"application/x-tex\">0.95</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a top-</span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m12\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m13\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> tends to overlook speech inputs.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;The interpretability analysis with Value Zeroing (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Attribution scores &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) shows that during translation generation, </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> models rely primarily on the transcription and the previously translated tokens. Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents a layer-wise view of the contributions. In </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the contribution of speech tokens is close to zero across all layers, whereas </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and especially </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> exhibit increased speech attribution in mid&#8211;late layers. Additionally, we consistently observe a peak at layer&#160;4 across models, leaving detailed analyses to future work. Averaging the speech contribution across layers clarifies the trend:\n</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: </span>\n  <math alttext=\"0.0228\\pm 0.0005\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.0228</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.0005</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.0228\\pm 0.0005</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: </span>\n  <math alttext=\"0.035\\pm 0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.035</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.001</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.035\\pm 0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: </span>\n  <math alttext=\"0.051\\pm 0.002\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.051</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.002</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.051\\pm 0.002</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Relative to </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is </span>\n  <math alttext=\"\\sim\\!1.54\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\" rspace=\"0.108em\">&#8764;</mo>\n        <mn mathsize=\"0.900em\">1.54</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\sim\\!1.54\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> higher and </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"\\sim\\!2.24\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\" rspace=\"0.108em\">&#8764;</mo>\n        <mn mathsize=\"0.900em\">2.24</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\sim\\!2.24\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In summary, we find that in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, models internally behave close to a </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> system, and our interventions mitigate this tendency.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "cot",
                    "across",
                    "cascade",
                    "base",
                    "noisy",
                    "dual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> is vulnerable to transcription errors.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;The robustness analysis against error propagation (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) shows that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generation follows a behavior similar to </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates that in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, performance decreases at almost the same rate for both strategies as noise increases in the transcript. This similarity suggests that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, like </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, relies on the transcript and largely disregards speech tokens at inference, which is consistent with the findings of the interpretability analysis. </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is slightly more robust in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding a shallower slope. The most notable improvement comes with </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which displays an almost flat curve, indicating small degradation even when up to 30% of the words in the transcript are corrupted. At first glance, this might suggest that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> behaves as a </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Direct</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> system. However, the interpretability results in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that the model still relies primarily on the transcription rather than the speech input. We therefore conclude that the inclusion of noisy transcripts during training seems to resolve the error propagation problem in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "strategies",
                    "cot",
                    "model",
                    "cascade",
                    "base",
                    "noisy",
                    "dual",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> hardly leverages prosodic information.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;As seen in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, compared to the results reported in the original </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our scores remain consistently lower, at a level similar to cascaded systems. We hypothesize that our controlled setup, with limited training data, may not provide sufficient coverage for the model to fully exploit prosodic cues at the level achieved by state-of-the-art systems. This indicates that, without enough data, the model does not learn to incorporate prosodic cues implicitly. Still, the interventions bring some gains: the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> variant shows a modest improvement, and the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> variant achieves the highest scores, suggesting that exposure to corrupted transcripts encourages a greater reliance on acoustic input.</span>\n</p>\n\n",
                "matched_terms": [
                    "cot",
                    "model",
                    "noisy",
                    "dual",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compared Chain-of-Thought (CoT) and Self-Cascade architectures for S2TT beyond translation quality, analyzing modality attributions, robustness to transcription noise, and prosody-awareness. Our results show that CoT resembles a cascade system in practice: it relies primarily on transcripts, is vulnerable to error propagation, and barely leverages prosodic cues.</span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "cot",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Simple training interventions, such as mixing Direct S2TT data or injecting noisy transcripts, improve CoT. However, the same gains also appear in Cascade, confirming the strong parallelism between both approaches. Training only on CoT samples, by contrast, harms performance and limits potential benefits.</span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "cot",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While real-world Cascade systems can exploit high-quality ASR and T2TT modules, our controlled setting ensures a fair comparison. Overall, the low performance of both CoT and Cascade on </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProST</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> highlights the need for methods that preserve and integrate acoustic information throughout the translation pipeline.</span>\n</p>\n\n",
                "matched_terms": [
                    "cascade",
                    "cot"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation",
        "caption": "Table 2: Contrastive quality results on ContraProst. Reporting the three model variants, across two generation strategies. Best results are in bold, and second best results are underlined.",
        "body": "Directional (↑\\uparrow)\n\n\nGlobal (↑\\uparrow)\n\n\n\n\nde\nes\navg.\nde\nes\navg.\n\n\n\n\n\nBase-Cascade\n\n33.9\n27.7\n30.80\n1.3\n1.6\n1.45\n\n\n\nBase-CoT\n\n35.0\n30.2\n32.60\n2.4\n2.7\n2.55\n\n\n\nDual-Cascade\n\n34.1\n26.4\n30.25\n2.2\n2.0\n2.10\n\n\n\nDual-CoT\n\n35.3\n30.0\n32.65\n3.7\n2.4\n3.05\n\n\n\nNoisy-Cascade\n\n34.2\n27.9\n31.05\n2.3\n1.9\n2.10\n\n\n\nNoisy-CoT\n\n37.2\n33.3\n35.25\n3.3\n5.2\n4.25",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:0.4pt 3.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Directional (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\">)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Global (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\">)</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding:0.4pt 3.0pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">de</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">es</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">avg.</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">de</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">es</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Base</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cascade</span>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">33.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">27.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">30.80</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Base</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">CoT</span>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">35.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">30.2</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">32.60</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.4</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">2.7</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.55</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Dual</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cascade</span>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">34.1</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">26.4</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">30.25</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Dual</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">CoT</span>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">35.3</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">30.0</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">32.65</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.7</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.4</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">3.05</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Noisy</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cascade</span>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">34.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">27.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">31.05</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Noisy</span><span class=\"ltx_text\" style=\"font-size:70%;\">-</span><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">CoT</span>\n</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">37.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">33.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">35.25</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">3.3</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.4pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.25</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "global",
            "strategies",
            "quality",
            "basecot",
            "second",
            "dualcot",
            "avg",
            "basecascade",
            "three",
            "across",
            "noisycot",
            "reporting",
            "results",
            "generation",
            "model",
            "contrastive",
            "contraprost",
            "directional",
            "noisycascade",
            "variants",
            "bold",
            "↑uparrow",
            "underlined",
            "dualcascade",
            "two",
            "best"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> hardly leverages prosodic information.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;As seen in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, compared to the results reported in the original </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our scores remain consistently lower, at a level similar to cascaded systems. We hypothesize that our controlled setup, with limited training data, may not provide sufficient coverage for the model to fully exploit prosodic cues at the level achieved by state-of-the-art systems. This indicates that, without enough data, the model does not learn to incorporate prosodic cues implicitly. Still, the interventions bring some gains: the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> variant shows a modest improvement, and the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> variant achieves the highest scores, suggesting that exposure to corrupted transcripts encourages a greater reliance on acoustic input.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech-to-Text Translation (S2TT) refers to converting spoken utterances in one language into written text in another.\nHistorically, S2TT systems were built by cascading an Automatic Speech Recognition (ASR) module, which first transcribes the speech, and a Text-to-Text Translation (T2TT) module which translates it into the target language&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSuch </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">cascade</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> systems are conceptually simple but have two main limitations. First, they can suffer from error propagation, as the T2TT stage cannot correct transcription errors. Second, it cannot preserve information in speech that can be relevant for translation, such as prosody, including emphasis, or breaks that convey a certain meaning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "two",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we investigate whether CoT models deliver the hypothesized benefits. We examine this from three complementary perspectives. First, we analyze internal mechanisms of a model with an interpretability method, quantifying the amount of information the model uses from the source speech utterance during translation (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Attribution scores &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Second, we evaluate robustness to error propagation by simulating noisy ASR outputs. Measuring translation quality under such conditions provides indirect evidence of whether the model exploits speech information to compensate for degraded transcripts (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Third, we assess prosody awareness of the model using the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> benchmark (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS4\" style=\"font-size:90%;\" title=\"2.4 Prosody awareness &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which directly tests whether the model leverages acoustic cues beyond the transcript to disambiguate meaning when prosody affects translation. We complement these analyses with simple training strategies to mitigate the weaknesses of standard CoT setups and to encourage greater reliance on speech information during translation (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Models &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "three",
                    "strategies",
                    "model",
                    "quality",
                    "second",
                    "contraprost"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This section introduces the methods we use to analyze the relevance of speech cues in CoT S2TT. We assess their direct contribution to translation through attribution analyses, their indirect role via robustness and prosody evaluations, and compare different model variants to understand how training modalities shape CoT behavior.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "variants"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference Strategies</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We compare the behavior of models under the </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> strategies. Models are trained on a mixture of ASR, T2TT, and S2TT data (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Data &#8227; 3 Experimental Setup &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which enables them to operate in both modes. In the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> setup, the model generates a transcription followed by a translation while retaining the original speech input in the context. In the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> setup, we adopt a </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">self-cascade</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> approach: the model first generates a transcription from the speech input and then, conditioned solely on this transcription, produces the translation.</span>\n</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Configurations</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We first consider a </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model trained with S2TT data exclusively in the CoT format. This serves as the baseline for comparing the two inference strategies. We hypothesize that there will be minimal differences between them, implying that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides no meaningful gains over </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To address this limitation, we introduce two interventions. The first is </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a model trained on a mixture of CoT (25%) and Direct (75%) formats. Including samples where the model relies only on the DSUs (Direct) encourages greater awareness of the spoken signal during </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generation. The second is </span>\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which replaces the transcript in a subset of CoT samples with a noisy variant. This simulates error propagation during training and discourages over-reliance on transcriptions. We follow the same corruption strategy used in the analysis of robustness to transcription errors (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), and we modify 25% of the training samples (selected by manual tuning). For these samples, we omit the transcription loss to avoid degrading ASR performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "strategies",
                    "model",
                    "second",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">inseq</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> library&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract token-level contribution scores, represented as token&#8211;token interaction matrices. To analyze modality use, we aggregate these scores over spans of tokens. We focus on three input regions: the speech input (DSUs), the transcription, and the previously translated tokens. For each region, we compute its contribution by summing the scores of all tokens in the region and averaging across all output tokens. This yields a normalized distribution over input regions, where contributions sum to 1 (with a small portion attributed to special tokens). We then analyze these aggregated scores across layers of the model to trace how reliance on each modality evolves along the model depth (see Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "three",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The corrupted transcripts are then paired with the original speech and inserted into a CoT prompt. Results are evaluated with x</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">comet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a well-known translation metric that predicts human judgments from multilingual encoder representations. By measuring the translation quality drop as corrupted percentages increase, we can quantify the robustness of the model to error propagation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We aim to analyze whether CoT models make use of speech inputs to perform prosody-aware S2TT or instead behave like cascade models, which are effectively blind to prosody. To this end, we evaluate prosody awareness using </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a benchmark that provides paired utterances differing only in prosodic emphasis. These differences alter meaning and therefore require distinct translations, so low prosody awareness would indicate that the model is unable to leverage speech information. For each pair, the model translates both utterances, and each output is scored with x</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">comet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> against both references, yielding a </span>\n  <math alttext=\"2{\\times}2\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2{\\times}2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> matrix of scores. From this, two metrics are derived: the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Directional score (D)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which checks whether each translation is closer to its intended reference than to the alternative, and the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Global score (G)</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which requires this condition to hold for both members of the pair simultaneously. The benchmark covers multiple prosodic phenomena, and results are averaged across them to obtain final scores.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "model",
                    "across",
                    "contraprost",
                    "directional",
                    "two",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;We combine several datasets for each task. For ASR, we use the English splits of Common Voice 21.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VoxPopuli&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an </span>\n  <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">8</mn>\n      <annotation encoding=\"application/x-tex\">8</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">k-hour subset of Multilingual LibriSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For S2TT, we include CoVoST 2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (Catalan and German) and Europarl-ST&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (all languages except Catalan). For T2TT, we reuse the same datasets as for S2TT to ensure comparability across tasks. We deliberately avoid using external T2TT corpora, prioritizing a fair comparison between CoT and cascade approaches and preventing potential quality mismatches between S2TT and T2TT data. We restrict the amount of T2TT to </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">k sentence pairs per language, as the goal is to just keep the initial performance on this task. All evaluations and analyses are conducted on FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ContraProst</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "contraprost",
                    "quality",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Hyperparameters</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;\nWe train our model with the AdamW optimizer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"16.7k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">16.7</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">16.7k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps with a maximum learning rate (LR) of </span>\n  <math alttext=\"1\\cdot 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8901;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\cdot 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The LR follows a cosine schedule, starting with a warmup for the </span>\n  <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">10\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the training and decaying until </span>\n  <math alttext=\"1\\cdot 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8901;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">6</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\cdot 10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The effective batch size is </span>\n  <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">256</mn>\n      <annotation encoding=\"application/x-tex\">256</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, result of training on 16 GPUs with a per-device batch size of </span>\n  <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">16</mn>\n      <annotation encoding=\"application/x-tex\">16</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We set a maximum length of </span>\n  <math alttext=\"2048\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2048</mn>\n      <annotation encoding=\"application/x-tex\">2048</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens, and we clip the gradients to a maximum norm of </span>\n  <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1.0</mn>\n      <annotation encoding=\"application/x-tex\">1.0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For generation, we use beam-search multinomial sampling, setting 5 beams, a temperature of </span>\n  <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m9\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.2</mn>\n      <annotation encoding=\"application/x-tex\">0.2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a top-</span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.95</mn>\n      <annotation encoding=\"application/x-tex\">0.95</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a top-</span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m12\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m13\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> performs slightly below </span>Cascade<span class=\"ltx_text ltx_font_upright\">.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Model Details &#8227; 3 Experimental Setup &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, training only with CoT data (</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) limits the performance of </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compared with </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Incorporating Direct-formatted data (</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) improves both strategies, most noticeably in x</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">comet</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and brings </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to competitive levels. Adding noisy data (</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) also yields gains over </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although performance remains below </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Overall, these results indicate that our interventions do not harm performance and can even provide improvements, establishing a solid basis for the detailed analyses that follow.</span>\n</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> tends to overlook speech inputs.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;The interpretability analysis with Value Zeroing (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Attribution scores &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) shows that during translation generation, </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> models rely primarily on the transcription and the previously translated tokens. Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents a layer-wise view of the contributions. In </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the contribution of speech tokens is close to zero across all layers, whereas </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and especially </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> exhibit increased speech attribution in mid&#8211;late layers. Additionally, we consistently observe a peak at layer&#160;4 across models, leaving detailed analyses to future work. Averaging the speech contribution across layers clarifies the trend:\n</span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: </span>\n  <math alttext=\"0.0228\\pm 0.0005\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.0228</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.0005</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.0228\\pm 0.0005</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: </span>\n  <math alttext=\"0.035\\pm 0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.035</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.001</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.035\\pm 0.001</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: </span>\n  <math alttext=\"0.051\\pm 0.002\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.051</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.002</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.051\\pm 0.002</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Relative to </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is </span>\n  <math alttext=\"\\sim\\!1.54\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\" rspace=\"0.108em\">&#8764;</mo>\n        <mn mathsize=\"0.900em\">1.54</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\sim\\!1.54\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> higher and </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"\\sim\\!2.24\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\" rspace=\"0.108em\">&#8764;</mo>\n        <mn mathsize=\"0.900em\">2.24</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\">&#215;</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\sim\\!2.24\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In summary, we find that in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, models internally behave close to a </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> system, and our interventions mitigate this tendency.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:90%;\">CoT<span class=\"ltx_text ltx_font_upright\"> is vulnerable to transcription errors.</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196;&#160;The robustness analysis against error propagation (&#167;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S2.SS3\" style=\"font-size:90%;\" title=\"2.3 Robustness to Transcription Errors &#8227; 2 Methodology &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) shows that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generation follows a behavior similar to </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates that in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Base</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, performance decreases at almost the same rate for both strategies as noise increases in the transcript. This similarity suggests that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, like </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Cascade</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, relies on the transcript and largely disregards speech tokens at inference, which is consistent with the findings of the interpretability analysis. </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Dual</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is slightly more robust in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding a shallower slope. The most notable improvement comes with </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which displays an almost flat curve, indicating small degradation even when up to 30% of the words in the transcript are corrupted. At first glance, this might suggest that </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Noisy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> behaves as a </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Direct</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> system. However, the interpretability results in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03115v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Results &#8227; Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that the model still relies primarily on the transcription rather than the speech input. We therefore conclude that the inclusion of noisy transcripts during training seems to resolve the error propagation problem in </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "generation",
                    "strategies",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compared Chain-of-Thought (CoT) and Self-Cascade architectures for S2TT beyond translation quality, analyzing modality attributions, robustness to transcription noise, and prosody-awareness. Our results show that CoT resembles a cascade system in practice: it relies primarily on transcripts, is vulnerable to error propagation, and barely leverages prosodic cues.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "quality"
                ]
            }
        ]
    }
}