{
    "S4.T1": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 1: The model architecture of VoxCPM-0.5B.",
        "body": "Module\nConfiguration\n\n\nLocEnc\n4 layers, 1024 hidden dim, 4096 FFN dim\n\n\nTSLM\n24 layers (MiniCPM-4-0.5B initialized), 1024 hidden dim, 4096 FFN dim\n\n\nFSQ\n256 dimensions, 9 quantization levels\n\n\nRALM\n6 layers, 1024 hidden dim, 4096 FFN dim\n\n\nLocDiT\n4 layers, 1024 hidden dim, 4096 FFN dim\n\n\nStop Predictor\n3-layer MLP, 1024 hidden dim, 2 output dim\n\n\npatch-size\n2 (that is, TSLM and RALM work in 12.5Hz token rate)\n\n\nAudioVAE\n16kHz waveform →\\rightarrow 25Hz latents (downsampling at [2, 5, 8, 8])",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Module</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Configuration</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LocEnc</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">4 layers, 1024 hidden dim, 4096 FFN dim</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">TSLM</td>\n<td class=\"ltx_td ltx_align_left\">24 layers (MiniCPM-4-0.5B initialized), 1024 hidden dim, 4096 FFN dim</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FSQ</td>\n<td class=\"ltx_td ltx_align_left\">256 dimensions, 9 quantization levels</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">RALM</td>\n<td class=\"ltx_td ltx_align_left\">6 layers, 1024 hidden dim, 4096 FFN dim</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LocDiT</td>\n<td class=\"ltx_td ltx_align_left\">4 layers, 1024 hidden dim, 4096 FFN dim</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Stop Predictor</td>\n<td class=\"ltx_td ltx_align_left\">3-layer MLP, 1024 hidden dim, 2 output dim</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">patch-size</td>\n<td class=\"ltx_td ltx_align_left\">2 (that is, TSLM and RALM work in 12.5Hz token rate)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">AudioVAE</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">16kHz waveform <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 25Hz latents (downsampling at [2, 5, 8, 8])</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "fsq",
            "mlp",
            "quantization",
            "patchsize",
            "locenc",
            "output",
            "architecture",
            "rate",
            "dimensions",
            "ffn",
            "→rightarrow",
            "stop",
            "125hz",
            "3layer",
            "locdit",
            "waveform",
            "work",
            "ralm",
            "dim",
            "predictor",
            "layers",
            "16khz",
            "model",
            "latents",
            "module",
            "hidden",
            "configuration",
            "token",
            "audiovae",
            "downsampling",
            "voxcpm05b",
            "levels",
            "tslm",
            "25hz",
            "minicpm405b",
            "initialized"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "voxcpm05b",
                    "model",
                    "quantization",
                    "latents",
                    "architecture",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite>).\nAutoregressively or Non-autoregressively predicting these tokens from text or phonemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> offers excellent scalability and in-context learning capabilities.\nHowever, this approach faces a fundamental \"quantization ceiling\", as the compression process irreversibly discards subtle acoustic details.\nTo mitigate this quality loss, state-of-the-art TTS systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> adopt multi-stage hybrid pipelines.\nHere, an LLM generates discrete tokens which condition a separate diffusion-based decoder.\nWhile improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context.\nThis fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, other approaches directly model continuous speech representations to avoid quantization loss.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> and more recent models such as MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> generate mel-spectrograms autoregressively.\nHowever, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs.\nTo address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> and autoregressive methods<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite>.\nAmong these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.\nThis innovation successfully enhances the detail and diversity of generated continuous representations.\nHowever, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective.\nThe model is forced to simultaneously solve two disparate tasks&#8212;requiring different inductive biases&#8212;in a continuous output space.\nThis entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.\nWe argue that this conflation is a root cause of instability.\nThe model&#8217;s focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences <cite class=\"ltx_cite ltx_citemacro_citep\">(Pasini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib33\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "model",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "voxcpm05b",
                    "model",
                    "quantization",
                    "latents",
                    "work",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech. The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.</p>\n\n",
                "matched_terms": [
                    "voxcpm05b",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The discrete token paradigm has emerged as a dominant approach in modern TTS, leveraging the success of large language models. This method converts speech into discrete representations using neural audio codecs such as EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite> and DAC <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib22\" title=\"\">2023</a>)</cite> through residual vector quantization (RVQ).\nAudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>)</cite> and VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>)</cite> pioneered this direction by framing audio generation and TTS as an autoregressive sequence prediction task over discrete acoustic tokens. Subsequent developments include SoundStorm <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib5\" title=\"\">2023b</a>)</cite>, which introduced non-autoregressive generation for improved efficiency, and Spear-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>)</cite>, which focused on multilingual capabilities with minimum supervision.\nBesides, VoiceCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> and XTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> further advanced zero-shot TTS with in-context learning.</p>\n\n",
                "matched_terms": [
                    "token",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "model",
                    "quantization",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> established the encoder-decoder framework for text-to-mel mapping, while FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib37\" title=\"\">2020</a>)</cite> introduced explicit duration modeling for alignment stability.\nInspired from VALL-E, MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.\nRecent developments have integrated diffusion processes to enhance detail and diversity. Non-autoregressive models like NaturalSpeech 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>)</cite> and VoiceBox <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>)</cite> apply diffusion directly on continuous representations.\nF5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> advanced flow-matching for efficient synthesis.\nAutoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.\nInnovations like ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>)</cite> use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing.\nDiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite> extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement.\nVibeVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite> employed next-token diffusion for long-form multi-speaker synthesis.\nBesides, recent models such as CLEAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib45\" title=\"\">2025</a>)</cite> and FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib42\" title=\"\">2025a</a>)</cite> focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib1\" title=\"\">2025</a>)</cite> and KALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib50\" title=\"\">2024</a>)</cite> combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality. Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "quantization",
                    "latents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike multi-stage TTS systems composed of seperate LM and diffusion that treat quantization as a means to obtain discrete prediction targets, our approach uses quantization solely as a regularization mechanism to constrain the hidden state space.\nThis distinction allows us to avoid the vocabulary explosion problem while still benefiting from the stabilizing effects of discrete representations.</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoxCPM employs a hierarchical autoregressive architecture that generates sequences of continuous speech latents <math alttext=\"\\mathbf{Z}=\\{\\mathbf{z}_{1},...,\\mathbf{z}_{M}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119859;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119859;</mi><mi>M</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=\\{\\mathbf{z}_{1},...,\\mathbf{z}_{M}\\}</annotation></semantics></math> conditioned on input text tokens <math alttext=\"\\mathbf{T}=\\{t_{1},...,t_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119827;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{T}=\\{t_{1},...,t_{N}\\}</annotation></semantics></math>,\nwhere each <math alttext=\"\\mathbf{z}_{i}\\in\\mathbb{R}^{P\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#119859;</mi><mi>i</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>P</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i}\\in\\mathbb{R}^{P\\times D}</annotation></semantics></math> represents a patch of <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> frames with <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>-dimensional VAE latent vectors.\nThe generation process follows:</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "latents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core innovation lies in our hierarchical conditioning mechanism with residual representation learning.\nIt is made up of a local audio encoder (LocEnc), a text-semantic language model (TSLM), a residual acoustic language model (RALM) and a local diffusion transformer decoder (LocDiT). A stop predictor is attached to the output of the TSLM to determine the endpoint of generation.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, each patch generation involves:</p>\n\n",
                "matched_terms": [
                    "model",
                    "predictor",
                    "output",
                    "locenc",
                    "locdit",
                    "tslm",
                    "stop",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{E}_{&lt;i}=\\text{LocEnc}(\\mathbf{Z}_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><mtext>LocEnc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119833;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{&lt;i}=\\text{LocEnc}(\\mathbf{Z}_{&lt;i})</annotation></semantics></math> represents historical audio context aggregated by a lightweight LocEnc that compresses VAE latent patches into compact acoustic embeddings.\nThe hierarchical backbone produces a conditioning signal <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> that encapsulates both semantic content from TSLM (with FSQ) and acoustic details from RALM.\nThis signal guides the LocDiT to generate the current latent patch <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> through a denoising diffusion process.\nThe entire model is trained end-to-end with gradients flowing through all components, including the FSQ bottleneck via straight-through estimation, ensuring coordinated optimization toward holistic speech synthesis.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "locenc",
                    "locdit",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Text-Semantic Language Model forms the main part of our hierarchical architecture, responsible for capturing high-level linguistic structure and generating contextually appropriate speech patterns.\nUnlike conventional TTS systems that typically operate on phoneme sequences, our approach leverages a pre-trained text language model (MiniCPM-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite>) as its initial backbone, enabling richer contextual understanding and more natural prosody prediction directly from raw text.\nSpecifically, we employ character-level segmentation for Chinese BPE Tokenizer to mitigate the vocabulary sparsity issue in TTS tasks.\nBy processing both text tokens and historical audio context, the TSLM learns to generate semantic content and prosodic structure that evolve naturally throughout an utterance, reflecting the underlying linguistic meaning rather than simply mapping phonemes to acoustic features.\nThe TSLM produces continuous semantic-prosodic representations that encode both the content to be spoken and how it should be prosodically realized, serving as input to the subsequent quantization stage.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "model",
                    "quantization",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our approach lies the Finite Scalar Quantization (FSQ) layer, which projects the continuous hidden states from the TSLM onto a structured lattice to create a semi-discrete representation.\nThe FSQ operation transforms each dimension of the continuous vector through a deterministic scalar quantization:</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "fsq",
                    "quantization",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math> is the quantization step size, <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the clipping range, and <span class=\"ltx_text ltx_markedasmath\">round</span> maps values to discrete levels.\nThis transformation creates a structured discrete representation while maintaining differentiability through the straight-through estimator during backward passes.</p>\n\n",
                "matched_terms": [
                    "levels",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ layer acts as a bottleneck, analogous to the first layer of Residual Vector Quantization (RVQ), which captures a coarse semantic-prosodic skeleton (e.g., content, intonation patterns).\nWe term this representation &#8220;semi-discrete\" as it employs a significantly larger dimensionality than standard FSQ to ensure sufficient informational capacity.\nUnlike RVQ, where the first layer is a prediction target and subsequent layers model finer details, our FSQ bottleneck serves as an intermediate, differentiable inductive bias within the continuous data flow.\nIt encourages the model to prioritize modeling stable, high-level components (the semantic-prosodic skeleton) by providing a clear learning signal for what information should be preserved through the bottleneck.\nThis structured approach mitigates error accumulation by reducing the modeling burden on the TSLM, allowing it to focus on the major components of the speech.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "layers",
                    "model",
                    "quantization",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To recover the fine-grained acoustic information attenuated by quantization, we introduce the Residual Acoustic Language Model (RALM).\nThis module specializes in reconstructing those subtle vocal characteristics that conventional discrete methods sacrifice for stability.\nIt processes the quantization residuals along with contextual information to recover speaker identity, spectral fine structure, and micro-prosodic variations:</p>\n\n",
                "matched_terms": [
                    "model",
                    "ralm",
                    "quantization",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, the RALM conditions its predictions on both the TSLM hidden states of the text part <math alttext=\"\\mathbf{H_{\\text{text}}^{\\text{TSLM}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mtext>text</mtext><mtext>TSLM</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H_{\\text{text}}^{\\text{TSLM}}}</annotation></semantics></math>, the semi-discrete representation of speech part <math alttext=\"\\mathbf{H}_{&lt;i}^{\\text{FSQ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow><mtext>FSQ</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}_{&lt;i}^{\\text{FSQ}}</annotation></semantics></math>, and the historical acoustic embeddings <math alttext=\"\\mathbf{E}_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{&lt;i}</annotation></semantics></math>.\nThis residual learning approach creates a natural division of labor: the TSLM+FSQ pathway focuses on content stability and prosodic coherence, while the RALM pathway specializes in acoustic expressivity and speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "ralm",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Local Diffusion Transformer (LocDiT) serves as our high-fidelity synthesis module, generating continuous latent patches conditioned on the hierarchical representation <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> produced by the preceding modules.\nFollowing DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite>, we employ a bidirectional Transformer architecture that enables full receptive field modeling within each patch.\nTo enhance generation consistency, we incorporate the previous patch <math alttext=\"\\mathbf{z}_{i-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i-1}</annotation></semantics></math> as additional conditioning context, which has been empirically validated to significantly improve output quality by framing the task as outpainting rather than independent patch generation.\nBesides, we mask the LM guidance in LocDiT condition with a specific probability ratio, for enabling classifier-free guidance (CFG) during inference.</p>\n\n",
                "matched_terms": [
                    "module",
                    "output",
                    "architecture",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire model is trained end-to-end using a flow-matching objective that directly optimizes the quality of the generated speech latents. We adopt the conditional flow-matching formulation for its training stability and sampling efficiency:</p>\n\n",
                "matched_terms": [
                    "model",
                    "latents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gradients from this loss are backpropagated through the entire autoregressive hierarchy, including the FSQ layer (via straight-through estimation), the TSLM and the LocEnc. This end-to-end optimization\nunder the combined objective <math alttext=\"\\mathcal{L}=\\mathcal{L}_{\\text{FM}}+\\lambda\\mathcal{L}_{\\text{Stop}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FM</mtext></msub><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>Stop</mtext></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{\\text{FM}}+\\lambda\\mathcal{L}_{\\text{Stop}}</annotation></semantics></math>\nallows each component to learn its specialized role&#8212;semantic planning, stabilization, and acoustic refinement&#8212;in a coordinated manner, guided by the unified objective of accurately modeling the continuous speech latents.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "locenc",
                    "tslm",
                    "latents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, the Audio VAE operates continuous speech tokens at a 25 Hz frame rate.\nThe VAE&#8217;s architecture is similar to DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib22\" title=\"\">2023</a>)</cite>, with both its encoder and decoder implemented using stacked Causal Convolutional Networks (Causal CNNs).\nFor 16 kHz single-channel audio, the encoder achieves a 640x downsampling factor through a series of strided convolutions with a stride sequence of [2, 5, 8, 8], compressing the audio into a 25 Hz latent representation.\nThe decoder then reconstructs the original waveform by upsampling from this latent representation.\nThe training objectives consist of an adversarial (GAN) loss, a Mel-spectrogram loss, and a KL divergence loss, with the latter&#8217;s weight set to a very small value <math alttext=\"5e-5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">5e-5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "downsampling",
                    "rate",
                    "architecture",
                    "waveform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "16khz",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rate",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T6\" title=\"Table 6 &#8227; 4.3 Ablation Study: Effect of the Semi-discrete Bottleneck &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the ablation studies on the FSQ bottleneck dimensionality provide critical insights.\nThe catastrophic performance degradation of the purely continuous model (w/o FSQ), especially on hard cases (ZH-CER: 24.92%), validates our core hypothesis: entangling semantic planning and acoustic rendering in a continuous space leads to instability. Without the inductive bias imposed by FSQ, the model struggles to separate these tasks even with a hierarchical design, resulting in error accumulation on complex utterances.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The optimal performance observed at FSQ levels (FSQ-d128/d256) reveals a key trade-off. Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity. Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist. The peak at FSQ-d256 indicates the bottleneck creates an effective &#8220;summary space\": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "fsq",
                    "model",
                    "levels"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "locdit",
                    "hidden",
                    "→rightarrow",
                    "ralm",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n",
                "matched_terms": [
                    "hidden",
                    "model",
                    "ralm",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our core hypothesis of learned implicit semantic-acoustic disentanglement, we conducted a t-SNE visualization of the internal representations in our hierarchical model.\nThe resulting distributions, shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F2\" title=\"Figure 2 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, empirically confirm the specialized roles of the TSLM and the RALM.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F2\" title=\"Figure 2 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the model&#8217;s behavior in a zero-shot voice cloning task, where each color corresponds to a distinct utterance from an unseen speaker.\nThe TSLM-FSQ outputs form semantic-prosodic structure closely tied to text content, while the RALM residuals exhibit strong speaker-related variations for acoustic rendering, confirming their specialized roles in content planning and acoustic refinement.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> further demonstrates the VoxCPM&#8217;s capability to infer appropriate prosody and style directly from text, when not using any speech prompt.\nWhen processing different text genres (news, poetry, conversation), TSLM-FSQ representations cluster by semantic category, showing that the pre-trained language model backbone effectively infers appropriate prosodic patterns directly from text content.\nFor example, embeddings for &#8220;news\" group together, separate from &#8220;story-telling\" or &#8220;rap-lyrics.\"\nThe RALM outputs display greater within-category variation, indicating its role in adding fine-grained acoustic nuances to the semantic-prosodic plan.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ralm",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.\nIt resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations.\nOur approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details.\nThis eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models.\nExtensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity. The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.</p>\n\n",
                "matched_terms": [
                    "work",
                    "model",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations</span>\nDespite these advancements, our work still has several limitations.\nFirst, the model&#8217;s multilingual capability remains limited, as it is primarily optimized for Chinese and English, with uncertain generalization to other languages.\nSecond, the controllability of speech attributes&#8212;such as fine-grained prosody and emotional expression&#8212;is still constrained, lacking both intuitive user guidance and precise adjustment mechanisms.\nFinally, the current AudioVAE only supports 16kHz audio generation, which restricts perceptual quality and falls short of high-fidelity application requirements that typically demand 24kHz or 44.1kHz sampling rates.\nThese limitations point to meaningful directions for future research.</p>\n\n",
                "matched_terms": [
                    "audiovae",
                    "work",
                    "16khz"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 2: Training configurations for VoxCPM variants.",
        "body": "Model\nPhase\nLearning Rate\nTokens/Batch\nIterations\nGPUs\n\n\nVoxCPM\nStable\n1×10−41\\times 10^{-4}\n4,096\n400K\n40 ×\\times H100\n\n\nVoxCPM\nDecay\n1×10−4→5×10−61\\times 10^{-4}\\rightarrow 5\\times 10^{-6}\n8,192\n100K\n40 ×\\times H100\n\n\nVoxCPM-Emilia\nStable\n1×10−41\\times 10^{-4}\n4,096\n150K\n24 ×\\times H100\n\n\nVoxCPM-Emilia\nDecay\n1×10−4→5×10−61\\times 10^{-4}\\rightarrow 5\\times 10^{-6}\n8,192\n50K\n24 ×\\times H100\n\n\nVoxCPM-ablation\nStable\n1×10−41\\times 10^{-4}\n4,096\n200K\n8 ×\\times H100",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Phase</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Learning Rate</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Tokens/Batch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Iterations</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">GPUs</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VoxCPM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Stable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4,096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">400K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> H100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoxCPM</td>\n<td class=\"ltx_td ltx_align_center\">Decay</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1\\times 10^{-4}\\rightarrow 5\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}\\rightarrow 5\\times 10^{-6}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">8,192</td>\n<td class=\"ltx_td ltx_align_center\">100K</td>\n<td class=\"ltx_td ltx_align_center\">40 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> H100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VoxCPM-Emilia</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Stable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4,096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">150K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> H100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VoxCPM-Emilia</td>\n<td class=\"ltx_td ltx_align_center\">Decay</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1\\times 10^{-4}\\rightarrow 5\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}\\rightarrow 5\\times 10^{-6}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">8,192</td>\n<td class=\"ltx_td ltx_align_center\">50K</td>\n<td class=\"ltx_td ltx_align_center\">24 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> H100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">VoxCPM-ablation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Stable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4,096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">200K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">8 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> H100</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "gpus",
            "training",
            "rate",
            "104rightarrow",
            "1×10−41times",
            "iterations",
            "decay",
            "learning",
            "phase",
            "tokensbatch",
            "100k",
            "stable",
            "model",
            "400k",
            "h100",
            "variants",
            "×times",
            "voxcpm",
            "5times",
            "50k",
            "200k",
            "voxcpmemilia",
            "150k",
            "configurations",
            "1×10−4→5×10−61times",
            "voxcpmablation"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "stable",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite>).\nAutoregressively or Non-autoregressively predicting these tokens from text or phonemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> offers excellent scalability and in-context learning capabilities.\nHowever, this approach faces a fundamental \"quantization ceiling\", as the compression process irreversibly discards subtle acoustic details.\nTo mitigate this quality loss, state-of-the-art TTS systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> adopt multi-stage hybrid pipelines.\nHere, an LLM generates discrete tokens which condition a separate diffusion-based decoder.\nWhile improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context.\nThis fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, other approaches directly model continuous speech representations to avoid quantization loss.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> and more recent models such as MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> generate mel-spectrograms autoregressively.\nHowever, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs.\nTo address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> and autoregressive methods<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite>.\nAmong these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.\nThis innovation successfully enhances the detail and diversity of generated continuous representations.\nHowever, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective.\nThe model is forced to simultaneously solve two disparate tasks&#8212;requiring different inductive biases&#8212;in a continuous output space.\nThis entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.\nWe argue that this conflation is a root cause of instability.\nThe model&#8217;s focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences <cite class=\"ltx_cite ltx_citemacro_citep\">(Pasini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib33\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "stable",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a residual learning strategy that, in conjunction with the bottleneck, enables a holistic yet specialized modeling process. Unlike fragmented multi-stage pipelines, our approach achieves functional separation without architectural fragmentation, simplifying the training pipeline and eliminating dependency on external speech tokenizers.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech. The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "learning",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that an effective solution should <span class=\"ltx_text ltx_font_bold\">structurally separate</span> the modeling of stable semantic-prosodic content from fine-grained acoustic details while maintaining differentiability for end-to-end training.\nOur key insight is to introduce a <span class=\"ltx_text ltx_font_bold\">differentiable quantization bottleneck</span> that naturally induces this separation through scalar quantization, splitting information into a discrete-like skeleton for content stability and continuous residual components for detail expressivity.</p>\n\n",
                "matched_terms": [
                    "stable",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core innovation lies in our hierarchical conditioning mechanism with residual representation learning.\nIt is made up of a local audio encoder (LocEnc), a text-semantic language model (TSLM), a residual acoustic language model (RALM) and a local diffusion transformer decoder (LocDiT). A stop predictor is attached to the output of the TSLM to determine the endpoint of generation.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, each patch generation involves:</p>\n\n",
                "matched_terms": [
                    "model",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ layer acts as a bottleneck, analogous to the first layer of Residual Vector Quantization (RVQ), which captures a coarse semantic-prosodic skeleton (e.g., content, intonation patterns).\nWe term this representation &#8220;semi-discrete\" as it employs a significantly larger dimensionality than standard FSQ to ensure sufficient informational capacity.\nUnlike RVQ, where the first layer is a prediction target and subsequent layers model finer details, our FSQ bottleneck serves as an intermediate, differentiable inductive bias within the continuous data flow.\nIt encourages the model to prioritize modeling stable, high-level components (the semantic-prosodic skeleton) by providing a clear learning signal for what information should be preserved through the bottleneck.\nThis structured approach mitigates error accumulation by reducing the modeling burden on the TSLM, allowing it to focus on the major components of the speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "stable",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire model is trained end-to-end using a flow-matching objective that directly optimizes the quality of the generated speech latents. We adopt the conditional flow-matching formulation for its training stability and sampling efficiency:</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, the Audio VAE operates continuous speech tokens at a 25 Hz frame rate.\nThe VAE&#8217;s architecture is similar to DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib22\" title=\"\">2023</a>)</cite>, with both its encoder and decoder implemented using stacked Causal Convolutional Networks (Causal CNNs).\nFor 16 kHz single-channel audio, the encoder achieves a 640x downsampling factor through a series of strided convolutions with a stride sequence of [2, 5, 8, 8], compressing the audio into a 25 Hz latent representation.\nThe decoder then reconstructs the original waveform by upsampling from this latent representation.\nThe training objectives consist of an adversarial (GAN) loss, a Mel-spectrogram loss, and a KL divergence loss, with the latter&#8217;s weight set to a very small value <math alttext=\"5e-5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">5e-5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "gpus",
                    "model",
                    "training",
                    "iterations",
                    "200k",
                    "voxcpmemilia",
                    "decay",
                    "rate",
                    "learning",
                    "h100",
                    "phase",
                    "voxcpm",
                    "1×10−41times"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "learning",
                    "training",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluations (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T5\" title=\"Table 5 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) further validate the objective findings, with VoxCPM achieving competitive performance across both languages. On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.\nFor Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity. This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.\nVoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "training",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "model",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "decay",
                    "model",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability and Efficiency</span>\nThe performance improvement from VoxCPM-Emilia to VoxCPM highlights the architecture&#8217;s scalability with increased data.\nThe hierarchical design allows larger models to effectively utilize increased capacity for learning complex patterns.\nIn terms of inference efficiency, VoxCPM-0.5B achieves a real-time factor (RTF) of 0.17 on a single NVIDIA RTX 4090, confirming practical deployment feasibility.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "learning",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.\nIt resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations.\nOur approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details.\nThis eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models.\nExtensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity. The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model",
                    "learning"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 3: Performance on Seed-TTS-eval Benchmark",
        "body": "Model\nParams\nOpen-Source\nEN\nZH\nHard\n\n\n\nWER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\n\nMegaTTS3 (Jiang et al., 2025)\n0.5B\n✗\n2.79\n77.1\n1.52\n79.0\n-\n-\n\n\nDiTAR (Jia et al., 2025)\n0.6B\n✗\n1.69\n73.5\n1.02\n75.3\n-\n-\n\n\nCosyVoice3 (Du et al., 2025)\n0.5B\n✗\n2.02\n71.8\n1.16\n78.0\n6.08\n75.8\n\n\nCosyVoice3 (Du et al., 2025)\n1.5B\n✗\n2.22\n72.0\n1.12\n78.1\n5.83\n75.8\n\n\nSeed-TTS (Anastassiou et al., 2024)\n-\n✗\n2.25\n76.2\n1.12\n79.6\n7.59\n77.6\n\n\nMiniMax-Speech (Zhang et al., 2025)\n-\n✗\n1.65\n69.2\n0.83\n78.3\n-\n-\n\n\nF5-TTS (Chen et al., 2024)\n\n0.3B\n✓\n2.00\n67.0\n1.53\n76.0\n8.67\n71.3\n\n\nMaskGCT (Wang et al., )\n\n\n✓\n2.62\n71.7\n2.27\n77.4\n-\n-\n\n\nCosyVoice (Du et al., 2024a)\n\n0.3B\n✓\n4.29\n60.9\n3.63\n72.3\n11.75\n70.9\n\n\nCosyVoice2 (Du et al., 2024b)\n\n0.5B\n✓\n3.09\n65.9\n1.38\n75.7\n6.83\n72.4\n\n\nSparkTTS (Wang et al., 2025b)\n\n0.5B\n✓\n3.14\n57.3\n1.54\n66.0\n-\n-\n\n\nFireRedTTS (Guo et al., 2024)\n\n0.5B\n✓\n3.82\n46.0\n1.51\n63.5\n17.45\n62.1\n\n\nFireRedTTS-2 (Xie et al., 2025)\n\n\n✓\n1.95\n66.5\n1.14\n73.6\n-\n-\n\n\nQwen2.5-Omni (Xu et al., 2025)\n\n7B\n✓\n2.72\n63.2\n1.70\n75.2\n7.97\n74.7\n\n\nOpenAudio-s1-mini (OpenAudio, 2024)\n\n0.5B\n✓\n1.94\n55.0\n1.18\n68.5\n23.37\n64.3\n\n\nIndexTTS 2 (Zhou et al., 2025)\n\n1.5B\n✓\n2.23\n70.6\n1.03\n76.5\n7.12\n75.5\n\n\nVibeVoice (Peng et al., 2025)\n\n1.5B\n✓\n3.04\n68.9\n1.16\n74.4\n-\n-\n\n\nHiggsAudio-v2 (BosonAI, 2025)\n\n3B\n✓\n2.44\n67.7\n1.50\n74.0\n55.07\n65.6\n\n\nVoxCPM-Emilia\n0.5B\n✓\n2.34\n68.1\n1.11\n74.0\n12.46\n69.8\n\n\nVoxCPM\n0.5B\n✓\n1.85\n72.9\n0.93\n77.2\n8.87\n73.0",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Open-Source</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Hard</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">MegaTTS3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib20\" title=\"\">2025</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.5B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">2.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">77.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">79.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">DiTAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.6B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.69</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">75.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">CosyVoice3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.5B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">2.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">71.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">78.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">6.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">75.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">CosyVoice3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.5B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">2.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">72.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">78.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">5.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">75.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib2\" title=\"\">2024</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">2.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">76.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">79.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">7.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">77.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">MiniMax-Speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib48\" title=\"\">2025</a>)</cite></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">1.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">69.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">78.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>)</cite>\n</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">2.62</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">71.7</span></td>\n<td class=\"ltx_td ltx_align_center\">2.27</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">77.4</span></td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.3B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">4.29</td>\n<td class=\"ltx_td ltx_align_center\">60.9</td>\n<td class=\"ltx_td ltx_align_center\">3.63</td>\n<td class=\"ltx_td ltx_align_center\">72.3</td>\n<td class=\"ltx_td ltx_align_center\">11.75</td>\n<td class=\"ltx_td ltx_align_center\">70.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.5B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">3.09</td>\n<td class=\"ltx_td ltx_align_center\">65.9</td>\n<td class=\"ltx_td ltx_align_center\">1.38</td>\n<td class=\"ltx_td ltx_align_center\">75.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">6.83</span></td>\n<td class=\"ltx_td ltx_align_center\">72.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SparkTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.5B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">3.14</td>\n<td class=\"ltx_td ltx_align_center\">57.3</td>\n<td class=\"ltx_td ltx_align_center\">1.54</td>\n<td class=\"ltx_td ltx_align_center\">66.0</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.5B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">3.82</td>\n<td class=\"ltx_td ltx_align_center\">46.0</td>\n<td class=\"ltx_td ltx_align_center\">1.51</td>\n<td class=\"ltx_td ltx_align_center\">63.5</td>\n<td class=\"ltx_td ltx_align_center\">17.45</td>\n<td class=\"ltx_td ltx_align_center\">62.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FireRedTTS-2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">1.95</td>\n<td class=\"ltx_td ltx_align_center\">66.5</td>\n<td class=\"ltx_td ltx_align_center\">1.14</td>\n<td class=\"ltx_td ltx_align_center\">73.6</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib47\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">7B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">2.72</td>\n<td class=\"ltx_td ltx_align_center\">63.2</td>\n<td class=\"ltx_td ltx_align_center\">1.70</td>\n<td class=\"ltx_td ltx_align_center\">75.2</td>\n<td class=\"ltx_td ltx_align_center\">7.97</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">74.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OpenAudio-s1-mini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.5B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.94</span></td>\n<td class=\"ltx_td ltx_align_center\">55.0</td>\n<td class=\"ltx_td ltx_align_center\">1.18</td>\n<td class=\"ltx_td ltx_align_center\">68.5</td>\n<td class=\"ltx_td ltx_align_center\">23.37</td>\n<td class=\"ltx_td ltx_align_center\">64.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IndexTTS 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">1.5B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">2.23</td>\n<td class=\"ltx_td ltx_align_center\">70.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.03</span></td>\n<td class=\"ltx_td ltx_align_center\">76.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">75.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VibeVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">1.5B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">3.04</td>\n<td class=\"ltx_td ltx_align_center\">68.9</td>\n<td class=\"ltx_td ltx_align_center\">1.16</td>\n<td class=\"ltx_td ltx_align_center\">74.4</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">HiggsAudio-v2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">3B</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">2.44</td>\n<td class=\"ltx_td ltx_align_center\">67.7</td>\n<td class=\"ltx_td ltx_align_center\">1.50</td>\n<td class=\"ltx_td ltx_align_center\">74.0</td>\n<td class=\"ltx_td ltx_align_center\">55.07</td>\n<td class=\"ltx_td ltx_align_center\">65.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">VoxCPM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">72.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">77.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">8.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">73.0</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "03b",
            "2024a",
            "2025b",
            "2024b",
            "cosyvoice",
            "indextts",
            "peng",
            "seedtts",
            "↓downarrow",
            "jiang",
            "sim",
            "zhang",
            "guo",
            "f5tts",
            "openaudios1mini",
            "higgsaudiov2",
            "xie",
            "cosyvoice3",
            "qwen25omni",
            "benchmark",
            "ditar",
            "zhou",
            "06b",
            "wang",
            "openaudio",
            "megatts3",
            "wer",
            "bosonai",
            "seedttseval",
            "model",
            "params",
            "opensource",
            "15b",
            "voxcpm",
            "vibevoice",
            "cer",
            "↑uparrow",
            "performance",
            "hard",
            "maskgct",
            "anastassiou",
            "chen",
            "cosyvoice2",
            "sparktts",
            "fireredtts2",
            "voxcpmemilia",
            "jia",
            "05b",
            "fireredtts",
            "minimaxspeech"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "performance",
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite>).\nAutoregressively or Non-autoregressively predicting these tokens from text or phonemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> offers excellent scalability and in-context learning capabilities.\nHowever, this approach faces a fundamental \"quantization ceiling\", as the compression process irreversibly discards subtle acoustic details.\nTo mitigate this quality loss, state-of-the-art TTS systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> adopt multi-stage hybrid pipelines.\nHere, an LLM generates discrete tokens which condition a separate diffusion-based decoder.\nWhile improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context.\nThis fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "model",
                    "peng",
                    "chen",
                    "wang",
                    "zhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, other approaches directly model continuous speech representations to avoid quantization loss.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> and more recent models such as MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> generate mel-spectrograms autoregressively.\nHowever, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs.\nTo address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> and autoregressive methods<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite>.\nAmong these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.\nThis innovation successfully enhances the detail and diversity of generated continuous representations.\nHowever, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective.\nThe model is forced to simultaneously solve two disparate tasks&#8212;requiring different inductive biases&#8212;in a continuous output space.\nThis entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.\nWe argue that this conflation is a root cause of instability.\nThe model&#8217;s focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences <cite class=\"ltx_cite ltx_citemacro_citep\">(Pasini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib33\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "peng",
                    "jia",
                    "performance",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech. The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The discrete token paradigm has emerged as a dominant approach in modern TTS, leveraging the success of large language models. This method converts speech into discrete representations using neural audio codecs such as EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite> and DAC <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib22\" title=\"\">2023</a>)</cite> through residual vector quantization (RVQ).\nAudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>)</cite> and VALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>)</cite> pioneered this direction by framing audio generation and TTS as an autoregressive sequence prediction task over discrete acoustic tokens. Subsequent developments include SoundStorm <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib5\" title=\"\">2023b</a>)</cite>, which introduced non-autoregressive generation for improved efficiency, and Spear-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>)</cite>, which focused on multilingual capabilities with minimum supervision.\nBesides, VoiceCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> and XTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> further advanced zero-shot TTS with in-context learning.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "peng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "2025b",
                    "model",
                    "sparktts",
                    "indextts",
                    "2024b",
                    "cosyvoice",
                    "fireredtts2",
                    "fireredtts",
                    "wang",
                    "guo",
                    "openaudio",
                    "bosonai",
                    "xie",
                    "performance",
                    "zhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> established the encoder-decoder framework for text-to-mel mapping, while FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib37\" title=\"\">2020</a>)</cite> introduced explicit duration modeling for alignment stability.\nInspired from VALL-E, MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.\nRecent developments have integrated diffusion processes to enhance detail and diversity. Non-autoregressive models like NaturalSpeech 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>)</cite> and VoiceBox <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>)</cite> apply diffusion directly on continuous representations.\nF5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> advanced flow-matching for efficient synthesis.\nAutoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.\nInnovations like ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>)</cite> use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing.\nDiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite> extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement.\nVibeVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite> employed next-token diffusion for long-form multi-speaker synthesis.\nBesides, recent models such as CLEAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib45\" title=\"\">2025</a>)</cite> and FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib42\" title=\"\">2025a</a>)</cite> focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib1\" title=\"\">2025</a>)</cite> and KALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib50\" title=\"\">2024</a>)</cite> combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality. Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "peng",
                    "jia",
                    "chen",
                    "wang",
                    "f5tts",
                    "vibevoice",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Local Diffusion Transformer (LocDiT) serves as our high-fidelity synthesis module, generating continuous latent patches conditioned on the hierarchical representation <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> produced by the preceding modules.\nFollowing DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite>, we employ a bidirectional Transformer architecture that enables full receptive field modeling within each patch.\nTo enhance generation consistency, we incorporate the previous patch <math alttext=\"\\mathbf{z}_{i-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i-1}</annotation></semantics></math> as additional conditioning context, which has been empirically validated to significantly improve output quality by framing the task as outpainting rather than independent patch generation.\nBesides, we mask the LM guidance in LocDiT condition with a specific probability ratio, for enabling classifier-free guidance (CFG) during inference.</p>\n\n",
                "matched_terms": [
                    "jia",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "model",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "sim",
                    "wer",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines</span>\nWe compared VoxCPM against a wide range of state-of-the-art open-source TTS systems, including CosyVoice series <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>)</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite>, FireRedTTS series <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>; Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite>, IndexTTS 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite>, HiggsAudio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> and so on.\nAll baseline results were obtained using official implementations with default settings, or as reported in their original papers.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "2025b",
                    "sparktts",
                    "indextts",
                    "cosyvoice",
                    "chen",
                    "fireredtts",
                    "wang",
                    "guo",
                    "opensource",
                    "f5tts",
                    "voxcpm",
                    "bosonai",
                    "xie",
                    "maskgct",
                    "zhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the CV3-EVAL benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), designed to evaluate expressive and in-the-wild performance,\nVoxCPM excels with a ZH-CER of 3.40% and an EN-WER of 4.04%.\nIts robustness is further confirmed on the challenging CV3 Hard-Test set, where it achieves an EN-WER of 7.89%, outperforming even close-sourced CosyVoice 3.\nThese results underscore the model&#8217;s capability to handle complex, realistic inputs, a strength attributed to the RALM&#8217;s role in recovering fine-grained acoustic details subsequent to the TSLM-FSQ-based semantic-prosodic modeling.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "benchmark",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluations (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T5\" title=\"Table 5 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) further validate the objective findings, with VoxCPM achieving competitive performance across both languages. On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.\nFor Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity. This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.\nVoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "voxcpmemilia",
                    "indextts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T6\" title=\"Table 6 &#8227; 4.3 Ablation Study: Effect of the Semi-discrete Bottleneck &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the ablation studies on the FSQ bottleneck dimensionality provide critical insights.\nThe catastrophic performance degradation of the purely continuous model (w/o FSQ), especially on hard cases (ZH-CER: 24.92%), validates our core hypothesis: entangling semantic planning and acoustic rendering in a continuous space leads to instability. Without the inductive bias imposed by FSQ, the model struggles to separate these tasks even with a hierarchical design, resulting in error accumulation on complex utterances.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "hard",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The optimal performance observed at FSQ levels (FSQ-d128/d256) reveals a key trade-off. Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity. Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist. The peak at FSQ-d256 indicates the bottleneck creates an effective &#8220;summary space\": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "performance",
                    "model",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability and Efficiency</span>\nThe performance improvement from VoxCPM-Emilia to VoxCPM highlights the architecture&#8217;s scalability with increased data.\nThe hierarchical design allows larger models to effectively utilize increased capacity for learning complex patterns.\nIn terms of inference efficiency, VoxCPM-0.5B achieves a real-time factor (RTF) of 0.17 on a single NVIDIA RTX 4090, confirming practical deployment feasibility.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.\nIt resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations.\nOur approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details.\nThis eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models.\nExtensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity. The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "performance",
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Core Contributors</span> &#8195;Yixuan Zhou, Guoyang Zeng, Xin Liu, Xiang Li, Renjie Yu, Ziyang Wang, Runchuan Ye, Weiyue Sun, Jiancheng Gui, Kehan Li, Zhiyong Wu, Zhiyuan Liu</p>\n\n",
                "matched_terms": [
                    "wang",
                    "zhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Other Contributors (Alphabetical order)</span> &#8195;Biyuan Lin, Chao Jia, Chenzhe Jing, Hongyu Liu, Jie Cai, Jie Zhou, Junshao Guo, Lei Chen, Rongting Tang, Rui Li, Ruiqi Shao, Qundong Shi, Shuo Wang, Siyuan Huang, Shun Lei, Wenxi Yang, Xiaoshuang Wang, Yihang He, Zichao Nie</p>\n\n",
                "matched_terms": [
                    "jia",
                    "wang",
                    "guo",
                    "chen",
                    "zhou"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 4: Performance on CV3-eval Benchmark. *denotes close-sourced systems.",
        "body": "Model\nCV3-EVAL\nCV3-Hard-ZH\nCV3-Hard-EN\n\n\n\nZH-CER ↓\\downarrow\n\n\nEN-WER ↓\\downarrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\nDNSMOS↑\\uparrow\n\nWER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\nDNSMOS↑\\uparrow\n\n\nF5-TTS\n5.47\n8.90\n-\n-\n-\n-\n-\n-\n\n\nSparkTTS\n5.15\n11.0\n-\n-\n-\n-\n-\n-\n\n\nGPT-Sovits\n7.34\n12.5\n-\n-\n-\n-\n-\n-\n\n\nCosyVoice2\n4.08\n6.32\n12.58\n72.6\n3.81\n11.96\n66.7\n3.95\n\n\nOpenAudio-s1-mini\n4.00\n5.54\n18.1\n58.2\n3.77\n12.4\n55.7\n3.89\n\n\nIndexTTS2\n3.58\n4.45\n12.8\n74.6\n3.65\n8.78\n74.5\n3.80\n\n\nHiggsAudio-v2\n9.54\n7.89\n41.0\n60.2\n3.39\n10.3\n61.8\n3.68\n\n\nCosyVoice3-0.5B*\n3.89\n5.24\n14.15\n78.6\n3.75\n9.04\n75.9\n3.92\n\n\nCosyVoice3-1.5B*\n3.91\n4.99\n9.77\n78.5\n3.79\n10.55\n76.1\n3.95\n\n\nVoxCPM-Emilia\n4.47\n5.23\n22.2\n62.6\n3.47\n10.00\n62.6\n3.68\n\n\nVoxCPM\n3.40\n4.04\n12.9\n66.1\n3.59\n7.89\n64.3\n3.74",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">CV3-EVAL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">CV3-Hard-ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">CV3-Hard-EN</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">ZH-CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">EN-WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">DNSMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">DNSMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">F5-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SparkTTS</td>\n<td class=\"ltx_td ltx_align_center\">5.15</td>\n<td class=\"ltx_td ltx_align_center\">11.0</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-Sovits</td>\n<td class=\"ltx_td ltx_align_center\">7.34</td>\n<td class=\"ltx_td ltx_align_center\">12.5</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CosyVoice2</td>\n<td class=\"ltx_td ltx_align_center\">4.08</td>\n<td class=\"ltx_td ltx_align_center\">6.32</td>\n<td class=\"ltx_td ltx_align_center\">12.58</td>\n<td class=\"ltx_td ltx_align_center\">72.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\">11.96</td>\n<td class=\"ltx_td ltx_align_center\">66.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OpenAudio-s1-mini</td>\n<td class=\"ltx_td ltx_align_center\">4.00</td>\n<td class=\"ltx_td ltx_align_center\">5.54</td>\n<td class=\"ltx_td ltx_align_center\">18.1</td>\n<td class=\"ltx_td ltx_align_center\">58.2</td>\n<td class=\"ltx_td ltx_align_center\">3.77</td>\n<td class=\"ltx_td ltx_align_center\">12.4</td>\n<td class=\"ltx_td ltx_align_center\">55.7</td>\n<td class=\"ltx_td ltx_align_center\">3.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IndexTTS2</td>\n<td class=\"ltx_td ltx_align_center\">3.58</td>\n<td class=\"ltx_td ltx_align_center\">4.45</td>\n<td class=\"ltx_td ltx_align_center\">12.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">74.6</span></td>\n<td class=\"ltx_td ltx_align_center\">3.65</td>\n<td class=\"ltx_td ltx_align_center\">8.78</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">74.5</span></td>\n<td class=\"ltx_td ltx_align_center\">3.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">HiggsAudio-v2</td>\n<td class=\"ltx_td ltx_align_center\">9.54</td>\n<td class=\"ltx_td ltx_align_center\">7.89</td>\n<td class=\"ltx_td ltx_align_center\">41.0</td>\n<td class=\"ltx_td ltx_align_center\">60.2</td>\n<td class=\"ltx_td ltx_align_center\">3.39</td>\n<td class=\"ltx_td ltx_align_center\">10.3</td>\n<td class=\"ltx_td ltx_align_center\">61.8</td>\n<td class=\"ltx_td ltx_align_center\">3.68</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">CosyVoice3-0.5B*</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">5.24</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">14.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">78.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">9.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">75.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.92</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">CosyVoice3-1.5B*</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.91</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">4.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">9.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">78.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">10.55</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">76.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">3.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">VoxCPM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">12.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">7.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.74</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "cv3eval",
            "cv3harden",
            "cv3hardzh",
            "↓downarrow",
            "sim",
            "openaudios1mini",
            "f5tts",
            "higgsaudiov2",
            "benchmark",
            "enwer",
            "cosyvoice305b",
            "wer",
            "model",
            "indextts2",
            "zhcer",
            "denotes",
            "dnsmos↑uparrow",
            "gptsovits",
            "voxcpm",
            "cer",
            "↑uparrow",
            "systems",
            "performance",
            "cosyvoice2",
            "sparktts",
            "closesourced",
            "voxcpmemilia",
            "cosyvoice315b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">On the CV3-EVAL benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), designed to evaluate expressive and in-the-wild performance,\nVoxCPM excels with a ZH-CER of 3.40% and an EN-WER of 4.04%.\nIts robustness is further confirmed on the challenging CV3 Hard-Test set, where it achieves an EN-WER of 7.89%, outperforming even close-sourced CosyVoice 3.\nThese results underscore the model&#8217;s capability to handle complex, realistic inputs, a strength attributed to the RALM&#8217;s role in recovering fine-grained acoustic details subsequent to the TSLM-FSQ-based semantic-prosodic modeling.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "voxcpm",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite>).\nAutoregressively or Non-autoregressively predicting these tokens from text or phonemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> offers excellent scalability and in-context learning capabilities.\nHowever, this approach faces a fundamental \"quantization ceiling\", as the compression process irreversibly discards subtle acoustic details.\nTo mitigate this quality loss, state-of-the-art TTS systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> adopt multi-stage hybrid pipelines.\nHere, an LLM generates discrete tokens which condition a separate diffusion-based decoder.\nWhile improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context.\nThis fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, other approaches directly model continuous speech representations to avoid quantization loss.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> and more recent models such as MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> generate mel-spectrograms autoregressively.\nHowever, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs.\nTo address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> and autoregressive methods<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite>.\nAmong these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.\nThis innovation successfully enhances the detail and diversity of generated continuous representations.\nHowever, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective.\nThe model is forced to simultaneously solve two disparate tasks&#8212;requiring different inductive biases&#8212;in a continuous output space.\nThis entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.\nWe argue that this conflation is a root cause of instability.\nThe model&#8217;s focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences <cite class=\"ltx_cite ltx_citemacro_citep\">(Pasini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib33\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech. The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "indextts2",
                    "model",
                    "sparktts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> established the encoder-decoder framework for text-to-mel mapping, while FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib37\" title=\"\">2020</a>)</cite> introduced explicit duration modeling for alignment stability.\nInspired from VALL-E, MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.\nRecent developments have integrated diffusion processes to enhance detail and diversity. Non-autoregressive models like NaturalSpeech 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>)</cite> and VoiceBox <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>)</cite> apply diffusion directly on continuous representations.\nF5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> advanced flow-matching for efficient synthesis.\nAutoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.\nInnovations like ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>)</cite> use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing.\nDiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite> extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement.\nVibeVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite> employed next-token diffusion for long-form multi-speaker synthesis.\nBesides, recent models such as CLEAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib45\" title=\"\">2025</a>)</cite> and FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib42\" title=\"\">2025a</a>)</cite> focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib1\" title=\"\">2025</a>)</cite> and KALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib50\" title=\"\">2024</a>)</cite> combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality. Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "f5tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Text-Semantic Language Model forms the main part of our hierarchical architecture, responsible for capturing high-level linguistic structure and generating contextually appropriate speech patterns.\nUnlike conventional TTS systems that typically operate on phoneme sequences, our approach leverages a pre-trained text language model (MiniCPM-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite>) as its initial backbone, enabling richer contextual understanding and more natural prosody prediction directly from raw text.\nSpecifically, we employ character-level segmentation for Chinese BPE Tokenizer to mitigate the vocabulary sparsity issue in TTS tasks.\nBy processing both text tokens and historical audio context, the TSLM learns to generate semantic content and prosodic structure that evolve naturally throughout an utterance, reflecting the underlying linguistic meaning rather than simply mapping phonemes to acoustic features.\nThe TSLM produces continuous semantic-prosodic representations that encode both the content to be spoken and how it should be prosodically realized, serving as input to the subsequent quantization stage.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "model",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines</span>\nWe compared VoxCPM against a wide range of state-of-the-art open-source TTS systems, including CosyVoice series <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>)</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite>, FireRedTTS series <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>; Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite>, IndexTTS 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite>, HiggsAudio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> and so on.\nAll baseline results were obtained using official implementations with default settings, or as reported in their original papers.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "f5tts",
                    "systems",
                    "sparktts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "enwer",
                    "indextts2",
                    "voxcpmemilia",
                    "zhcer",
                    "sim",
                    "voxcpm",
                    "wer",
                    "cer",
                    "performance",
                    "cosyvoice2",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluations (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T5\" title=\"Table 5 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) further validate the objective findings, with VoxCPM achieving competitive performance across both languages. On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.\nFor Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity. This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.\nVoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T6\" title=\"Table 6 &#8227; 4.3 Ablation Study: Effect of the Semi-discrete Bottleneck &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the ablation studies on the FSQ bottleneck dimensionality provide critical insights.\nThe catastrophic performance degradation of the purely continuous model (w/o FSQ), especially on hard cases (ZH-CER: 24.92%), validates our core hypothesis: entangling semantic planning and acoustic rendering in a continuous space leads to instability. Without the inductive bias imposed by FSQ, the model struggles to separate these tasks even with a hierarchical design, resulting in error accumulation on complex utterances.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "zhcer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The optimal performance observed at FSQ levels (FSQ-d128/d256) reveals a key trade-off. Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity. Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist. The peak at FSQ-d256 indicates the bottleneck creates an effective &#8220;summary space\": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "performance",
                    "model",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability and Efficiency</span>\nThe performance improvement from VoxCPM-Emilia to VoxCPM highlights the architecture&#8217;s scalability with increased data.\nThe hierarchical design allows larger models to effectively utilize increased capacity for learning complex patterns.\nIn terms of inference efficiency, VoxCPM-0.5B achieves a real-time factor (RTF) of 0.17 on a single NVIDIA RTX 4090, confirming practical deployment feasibility.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "voxcpm",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.\nIt resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations.\nOur approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details.\nThis eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models.\nExtensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity. The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "voxcpm",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ethics statement</span>\nSince our zero-shot TTS model achieves high-quality speech synthesis with the ability to closely mimic speaker characteristics, it carries potential risks of misuse. These risks include, but are not limited to, spoofing voice authentication systems or impersonating a specific speaker without their consent. Our experiments were conducted under the assumption that the use of any reference speaker&#8217;s voice is authorized and intended for legitimate synthesis purposes. To mitigate these risks, we strongly advocate for the development of robust synthesized speech detection algorithms. Furthermore, we believe it is crucial to establish clear ethical guidelines and reporting mechanisms for the responsible deployment of such technology.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "model"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 5: Subjective Evaluations in terms of Naturalness and Speaker Similarity.",
        "body": "Model\nZH\nEN\n\n\nN-MOS\nS-MOS\nN-MOS\nS-MOS\n\n\nMaskGCT\n3.20±0.113.20\\pm 0.11\n3.77±0.113.77\\pm 0.11\n3.84±0.113.84\\pm 0.11\n4.00±0.104.00\\pm 0.10\n\n\nCosyVoice 2\n3.38±0.123.38\\pm 0.12\n4.01±0.104.01\\pm 0.10\n4.14±0.09\\mathbf{4.14\\pm 0.09}\n3.97±0.103.97\\pm 0.10\n\n\nIndexTTS 2\n4.25±0.09\\mathbf{4.25\\pm 0.09}\n4.05±0.094.05\\pm 0.09\n4.03±0.104.03\\pm 0.10\n4.16±0.094.16\\pm 0.09\n\n\nVoxCPM-Emilia\n3.79±0.123.79\\pm 0.12\n3.99±0.113.99\\pm 0.11\n3.91±0.103.91\\pm 0.10\n4.10±0.094.10\\pm 0.09\n\n\nVoxCPM\n4.10±0.104.10\\pm 0.10\n4.11±0.10\\mathbf{4.11\\pm 0.10}\n4.11±0.094.11\\pm 0.09\n4.18±0.09\\mathbf{4.18\\pm 0.09}",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">EN</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">N-MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">S-MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">N-MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">S-MOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MaskGCT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.20\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m1\" intent=\":literal\"><semantics><mrow><mn>3.20</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.20\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.77\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m2\" intent=\":literal\"><semantics><mrow><mn>3.77</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.77\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.84\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m3\" intent=\":literal\"><semantics><mrow><mn>3.84</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.84\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.00\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m4\" intent=\":literal\"><semantics><mrow><mn>4.00</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">4.00\\pm 0.10</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">CosyVoice 2</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.38\\pm 0.12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m5\" intent=\":literal\"><semantics><mrow><mn>3.38</mn><mo>&#177;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">3.38\\pm 0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.01\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m6\" intent=\":literal\"><semantics><mrow><mn>4.01</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">4.01\\pm 0.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\mathbf{4.14\\pm 0.09}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m7\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.14</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.09</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.14\\pm 0.09}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.97\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m8\" intent=\":literal\"><semantics><mrow><mn>3.97</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">3.97\\pm 0.10</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IndexTTS 2</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\mathbf{4.25\\pm 0.09}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m9\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.25</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.09</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.25\\pm 0.09}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"4.05\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m10\" intent=\":literal\"><semantics><mrow><mn>4.05</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">4.05\\pm 0.09</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.03\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m11\" intent=\":literal\"><semantics><mrow><mn>4.03</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">4.03\\pm 0.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"4.16\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m12\" intent=\":literal\"><semantics><mrow><mn>4.16</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">4.16\\pm 0.09</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.79\\pm 0.12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m13\" intent=\":literal\"><semantics><mrow><mn>3.79</mn><mo>&#177;</mo><mn>0.12</mn></mrow><annotation encoding=\"application/x-tex\">3.79\\pm 0.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.99\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m14\" intent=\":literal\"><semantics><mrow><mn>3.99</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.99\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.91\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m15\" intent=\":literal\"><semantics><mrow><mn>3.91</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">3.91\\pm 0.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.10\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m16\" intent=\":literal\"><semantics><mrow><mn>4.10</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">4.10\\pm 0.09</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">VoxCPM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"4.10\\pm 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m17\" intent=\":literal\"><semantics><mrow><mn>4.10</mn><mo>&#177;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">4.10\\pm 0.10</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\mathbf{4.11\\pm 0.10}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m18\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.11</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.11\\pm 0.10}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\"><math alttext=\"4.11\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m19\" intent=\":literal\"><semantics><mrow><mn>4.11</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">4.11\\pm 0.09</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\mathbf{4.18\\pm 0.09}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m20\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.18</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.09</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.18\\pm 0.09}</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "indextts",
            "subjective",
            "cosyvoice",
            "418±009mathbf418pm",
            "400±010400pm",
            "evaluations",
            "nmos",
            "338±012338pm",
            "416±009416pm",
            "401±010401pm",
            "377±011377pm",
            "425±009mathbf425pm",
            "414±009mathbf414pm",
            "399±011399pm",
            "speaker",
            "391±010391pm",
            "model",
            "410±009410pm",
            "410±010410pm",
            "411±010mathbf411pm",
            "411±009411pm",
            "similarity",
            "403±010403pm",
            "voxcpm",
            "405±009405pm",
            "397±010397pm",
            "maskgct",
            "320±011320pm",
            "terms",
            "379±012379pm",
            "voxcpmemilia",
            "384±011384pm",
            "smos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Subjective evaluations (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T5\" title=\"Table 5 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) further validate the objective findings, with VoxCPM achieving competitive performance across both languages. On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.\nFor Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity. This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.\nVoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "indextts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> established the encoder-decoder framework for text-to-mel mapping, while FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib37\" title=\"\">2020</a>)</cite> introduced explicit duration modeling for alignment stability.\nInspired from VALL-E, MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.\nRecent developments have integrated diffusion processes to enhance detail and diversity. Non-autoregressive models like NaturalSpeech 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>)</cite> and VoiceBox <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>)</cite> apply diffusion directly on continuous representations.\nF5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> advanced flow-matching for efficient synthesis.\nAutoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.\nInnovations like ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>)</cite> use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing.\nDiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite> extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement.\nVibeVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite> employed next-token diffusion for long-form multi-speaker synthesis.\nBesides, recent models such as CLEAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib45\" title=\"\">2025</a>)</cite> and FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib42\" title=\"\">2025a</a>)</cite> focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib1\" title=\"\">2025</a>)</cite> and KALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib50\" title=\"\">2024</a>)</cite> combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality. Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To recover the fine-grained acoustic information attenuated by quantization, we introduce the Residual Acoustic Language Model (RALM).\nThis module specializes in reconstructing those subtle vocal characteristics that conventional discrete methods sacrifice for stability.\nIt processes the quantization residuals along with contextual information to recover speaker identity, spectral fine structure, and micro-prosodic variations:</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voxcpmemilia",
                    "similarity",
                    "voxcpm",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "smos",
                    "evaluations",
                    "nmos",
                    "cosyvoice",
                    "subjective",
                    "similarity",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines</span>\nWe compared VoxCPM against a wide range of state-of-the-art open-source TTS systems, including CosyVoice series <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>)</cite>, MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>)</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite>, SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite>, FireRedTTS series <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>; Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite>, IndexTTS 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite>, HiggsAudio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> and so on.\nAll baseline results were obtained using official implementations with default settings, or as reported in their original papers.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "maskgct",
                    "indextts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "speaker",
                    "voxcpm",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the CV3-EVAL benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), designed to evaluate expressive and in-the-wild performance,\nVoxCPM excels with a ZH-CER of 3.40% and an EN-WER of 4.04%.\nIts robustness is further confirmed on the challenging CV3 Hard-Test set, where it achieves an EN-WER of 7.89%, outperforming even close-sourced CosyVoice 3.\nThese results underscore the model&#8217;s capability to handle complex, realistic inputs, a strength attributed to the RALM&#8217;s role in recovering fine-grained acoustic details subsequent to the TSLM-FSQ-based semantic-prosodic modeling.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The optimal performance observed at FSQ levels (FSQ-d128/d256) reveals a key trade-off. Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity. Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist. The peak at FSQ-d256 indicates the bottleneck creates an effective &#8220;summary space\": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our core hypothesis of learned implicit semantic-acoustic disentanglement, we conducted a t-SNE visualization of the internal representations in our hierarchical model.\nThe resulting distributions, shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F2\" title=\"Figure 2 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, empirically confirm the specialized roles of the TSLM and the RALM.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F2\" title=\"Figure 2 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the model&#8217;s behavior in a zero-shot voice cloning task, where each color corresponds to a distinct utterance from an unseen speaker.\nThe TSLM-FSQ outputs form semantic-prosodic structure closely tied to text content, while the RALM residuals exhibit strong speaker-related variations for acoustic rendering, confirming their specialized roles in content planning and acoustic refinement.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> further demonstrates the VoxCPM&#8217;s capability to infer appropriate prosody and style directly from text, when not using any speech prompt.\nWhen processing different text genres (news, poetry, conversation), TSLM-FSQ representations cluster by semantic category, showing that the pre-trained language model backbone effectively infers appropriate prosodic patterns directly from text content.\nFor example, embeddings for &#8220;news\" group together, separate from &#8220;story-telling\" or &#8220;rap-lyrics.\"\nThe RALM outputs display greater within-category variation, indicating its role in adding fine-grained acoustic nuances to the semantic-prosodic plan.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability and Efficiency</span>\nThe performance improvement from VoxCPM-Emilia to VoxCPM highlights the architecture&#8217;s scalability with increased data.\nThe hierarchical design allows larger models to effectively utilize increased capacity for learning complex patterns.\nIn terms of inference efficiency, VoxCPM-0.5B achieves a real-time factor (RTF) of 0.17 on a single NVIDIA RTX 4090, confirming practical deployment feasibility.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "voxcpm",
                    "voxcpmemilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.\nIt resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations.\nOur approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details.\nThis eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models.\nExtensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity. The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "speaker",
                    "model",
                    "voxcpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ethics statement</span>\nSince our zero-shot TTS model achieves high-quality speech synthesis with the ability to closely mimic speaker characteristics, it carries potential risks of misuse. These risks include, but are not limited to, spoofing voice authentication systems or impersonating a specific speaker without their consent. Our experiments were conducted under the assumption that the use of any reference speaker&#8217;s voice is authorized and intended for legitimate synthesis purposes. To mitigate these risks, we strongly advocate for the development of robust synthesized speech detection algorithms. Furthermore, we believe it is crucial to establish clear ethical guidelines and reporting mechanisms for the responsible deployment of such technology.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 6: FSQ dimension selection study on the Emilia dataset. Note: The 256-dim was selected for the final VoxCPM configuration, with the understanding that larger training datasets needs more powerful modeling capabilities.",
        "body": "Model Setting\nEN\nZH\nZH-hard case\n\n\n\nWER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\n\nw FSQ: d4s9\n5.18\n59.3\n4.05\n68.0\n19.55\n62.3\n\n\nw FSQ: d16s9\n3.22\n60.4\n1.87\n70.5\n14.42\n66.2\n\n\nw FSQ: d64s9\n3.22\n61.1\n2.14\n69.8\n17.48\n65.1\n\n\nw FSQ: d128s9\n3.43\n62.2\n1.67\n70.7\n16.76\n65.7\n\n\nw FSQ: d256s9\n2.98\n62.6\n1.77\n70.4\n18.19\n64.9\n\n\nw FSQ: d1024s9\n3.07\n62.0\n2.38\n69.8\n20.38\n64.7\n\n\nw/o FSQ: d1024s∞\\infty\n\n3.67\n62.1\n2.30\n69.6\n24.92\n63.5",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH-hard case</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">w FSQ: d4s9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w FSQ: d16s9</td>\n<td class=\"ltx_td ltx_align_center\">3.22</td>\n<td class=\"ltx_td ltx_align_center\">60.4</td>\n<td class=\"ltx_td ltx_align_center\">1.87</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">70.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">14.42</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">66.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w FSQ: d64s9</td>\n<td class=\"ltx_td ltx_align_center\">3.22</td>\n<td class=\"ltx_td ltx_align_center\">61.1</td>\n<td class=\"ltx_td ltx_align_center\">2.14</td>\n<td class=\"ltx_td ltx_align_center\">69.8</td>\n<td class=\"ltx_td ltx_align_center\">17.48</td>\n<td class=\"ltx_td ltx_align_center\">65.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w FSQ: d128s9</td>\n<td class=\"ltx_td ltx_align_center\">3.43</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">62.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">16.76</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">65.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w FSQ: d256s9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.77</span></td>\n<td class=\"ltx_td ltx_align_center\">70.4</td>\n<td class=\"ltx_td ltx_align_center\">18.19</td>\n<td class=\"ltx_td ltx_align_center\">64.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w FSQ: d1024s9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.07</span></td>\n<td class=\"ltx_td ltx_align_center\">62.0</td>\n<td class=\"ltx_td ltx_align_center\">2.38</td>\n<td class=\"ltx_td ltx_align_center\">69.8</td>\n<td class=\"ltx_td ltx_align_center\">20.38</td>\n<td class=\"ltx_td ltx_align_center\">64.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">w/o FSQ: d1024s<math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">62.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">69.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">24.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">63.5</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "fsq",
            "training",
            "256dim",
            "needs",
            "↓downarrow",
            "sim",
            "powerful",
            "datasets",
            "more",
            "d1024s9",
            "d64s9",
            "modeling",
            "larger",
            "wer",
            "model",
            "dataset",
            "voxcpm",
            "configuration",
            "emilia",
            "d16s9",
            "d128s9",
            "cer",
            "↑uparrow",
            "capabilities",
            "d1024s∞infty",
            "setting",
            "study",
            "final",
            "dimension",
            "zhhard",
            "selection",
            "case",
            "understanding",
            "note",
            "selected",
            "d4s9",
            "d256s9"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T6\" title=\"Table 6 &#8227; 4.3 Ablation Study: Effect of the Semi-discrete Bottleneck &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the ablation studies on the FSQ bottleneck dimensionality provide critical insights.\nThe catastrophic performance degradation of the purely continuous model (w/o FSQ), especially on hard cases (ZH-CER: 24.92%), validates our core hypothesis: entangling semantic planning and acoustic rendering in a continuous space leads to instability. Without the inductive bias imposed by FSQ, the model struggles to separate these tasks even with a hierarchical design, resulting in error accumulation on complex utterances.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite>).\nAutoregressively or Non-autoregressively predicting these tokens from text or phonemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> offers excellent scalability and in-context learning capabilities.\nHowever, this approach faces a fundamental \"quantization ceiling\", as the compression process irreversibly discards subtle acoustic details.\nTo mitigate this quality loss, state-of-the-art TTS systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> adopt multi-stage hybrid pipelines.\nHere, an LLM generates discrete tokens which condition a separate diffusion-based decoder.\nWhile improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context.\nThis fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "capabilities",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, other approaches directly model continuous speech representations to avoid quantization loss.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> and more recent models such as MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> generate mel-spectrograms autoregressively.\nHowever, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs.\nTo address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> and autoregressive methods<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>; Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite>.\nAmong these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.\nThis innovation successfully enhances the detail and diversity of generated continuous representations.\nHowever, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective.\nThe model is forced to simultaneously solve two disparate tasks&#8212;requiring different inductive biases&#8212;in a continuous output space.\nThis entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.\nWe argue that this conflation is a root cause of instability.\nThe model&#8217;s focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences <cite class=\"ltx_cite ltx_citemacro_citep\">(Pasini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib33\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "more",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a residual learning strategy that, in conjunction with the bottleneck, enables a holistic yet specialized modeling process. Unlike fragmented multi-stage pipelines, our approach achieves functional separation without architectural fragmentation, simplifying the training pipeline and eliminating dependency on external speech tokenizers.</p>\n\n",
                "matched_terms": [
                    "training",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech. The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "modeling",
                    "powerful",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> established the encoder-decoder framework for text-to-mel mapping, while FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib37\" title=\"\">2020</a>)</cite> introduced explicit duration modeling for alignment stability.\nInspired from VALL-E, MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.\nRecent developments have integrated diffusion processes to enhance detail and diversity. Non-autoregressive models like NaturalSpeech 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>)</cite> and VoiceBox <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>)</cite> apply diffusion directly on continuous representations.\nF5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> advanced flow-matching for efficient synthesis.\nAutoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.\nInnovations like ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>)</cite> use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing.\nDiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite> extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement.\nVibeVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite> employed next-token diffusion for long-form multi-speaker synthesis.\nBesides, recent models such as CLEAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib45\" title=\"\">2025</a>)</cite> and FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib42\" title=\"\">2025a</a>)</cite> focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib1\" title=\"\">2025</a>)</cite> and KALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib50\" title=\"\">2024</a>)</cite> combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality. Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Critically, we identify a key limitation in existing discrete tokenization approaches: methods that directly use FSQ or VQ to obtain discrete codebooks for language modeling face an inherent scalability challenge. As the dimensionality increases to capture richer acoustic information, the codebook size grows exponentially, creating an unmanageably large and sparse vocabulary that language models struggle to predict accurately.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that an effective solution should <span class=\"ltx_text ltx_font_bold\">structurally separate</span> the modeling of stable semantic-prosodic content from fine-grained acoustic details while maintaining differentiability for end-to-end training.\nOur key insight is to introduce a <span class=\"ltx_text ltx_font_bold\">differentiable quantization bottleneck</span> that naturally induces this separation through scalar quantization, splitting information into a discrete-like skeleton for content stability and continuous residual components for detail expressivity.</p>\n\n",
                "matched_terms": [
                    "training",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{E}_{&lt;i}=\\text{LocEnc}(\\mathbf{Z}_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><mtext>LocEnc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119833;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{&lt;i}=\\text{LocEnc}(\\mathbf{Z}_{&lt;i})</annotation></semantics></math> represents historical audio context aggregated by a lightweight LocEnc that compresses VAE latent patches into compact acoustic embeddings.\nThe hierarchical backbone produces a conditioning signal <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> that encapsulates both semantic content from TSLM (with FSQ) and acoustic details from RALM.\nThis signal guides the LocDiT to generate the current latent patch <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> through a denoising diffusion process.\nThe entire model is trained end-to-end with gradients flowing through all components, including the FSQ bottleneck via straight-through estimation, ensuring coordinated optimization toward holistic speech synthesis.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Text-Semantic Language Model forms the main part of our hierarchical architecture, responsible for capturing high-level linguistic structure and generating contextually appropriate speech patterns.\nUnlike conventional TTS systems that typically operate on phoneme sequences, our approach leverages a pre-trained text language model (MiniCPM-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite>) as its initial backbone, enabling richer contextual understanding and more natural prosody prediction directly from raw text.\nSpecifically, we employ character-level segmentation for Chinese BPE Tokenizer to mitigate the vocabulary sparsity issue in TTS tasks.\nBy processing both text tokens and historical audio context, the TSLM learns to generate semantic content and prosodic structure that evolve naturally throughout an utterance, reflecting the underlying linguistic meaning rather than simply mapping phonemes to acoustic features.\nThe TSLM produces continuous semantic-prosodic representations that encode both the content to be spoken and how it should be prosodically realized, serving as input to the subsequent quantization stage.</p>\n\n",
                "matched_terms": [
                    "model",
                    "understanding",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our approach lies the Finite Scalar Quantization (FSQ) layer, which projects the continuous hidden states from the TSLM onto a structured lattice to create a semi-discrete representation.\nThe FSQ operation transforms each dimension of the continuous vector through a deterministic scalar quantization:</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ layer acts as a bottleneck, analogous to the first layer of Residual Vector Quantization (RVQ), which captures a coarse semantic-prosodic skeleton (e.g., content, intonation patterns).\nWe term this representation &#8220;semi-discrete\" as it employs a significantly larger dimensionality than standard FSQ to ensure sufficient informational capacity.\nUnlike RVQ, where the first layer is a prediction target and subsequent layers model finer details, our FSQ bottleneck serves as an intermediate, differentiable inductive bias within the continuous data flow.\nIt encourages the model to prioritize modeling stable, high-level components (the semantic-prosodic skeleton) by providing a clear learning signal for what information should be preserved through the bottleneck.\nThis structured approach mitigates error accumulation by reducing the modeling burden on the TSLM, allowing it to focus on the major components of the speech.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "larger",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The entire model is trained end-to-end using a flow-matching objective that directly optimizes the quality of the generated speech latents. We adopt the conditional flow-matching formulation for its training stability and sampling efficiency:</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gradients from this loss are backpropagated through the entire autoregressive hierarchy, including the FSQ layer (via straight-through estimation), the TSLM and the LocEnc. This end-to-end optimization\nunder the combined objective <math alttext=\"\\mathcal{L}=\\mathcal{L}_{\\text{FM}}+\\lambda\\mathcal{L}_{\\text{Stop}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FM</mtext></msub><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>Stop</mtext></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{\\text{FM}}+\\lambda\\mathcal{L}_{\\text{Stop}}</annotation></semantics></math>\nallows each component to learn its specialized role&#8212;semantic planning, stabilization, and acoustic refinement&#8212;in a coordinated manner, guided by the unified objective of accurately modeling the continuous speech latents.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "dataset",
                    "emilia",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "fsq",
                    "configuration",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "dataset",
                    "voxcpm",
                    "emilia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "training",
                    "dataset",
                    "modeling",
                    "sim",
                    "voxcpm",
                    "understanding",
                    "wer",
                    "cer",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the CV3-EVAL benchmark (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T4\" title=\"Table 4 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), designed to evaluate expressive and in-the-wild performance,\nVoxCPM excels with a ZH-CER of 3.40% and an EN-WER of 4.04%.\nIts robustness is further confirmed on the challenging CV3 Hard-Test set, where it achieves an EN-WER of 7.89%, outperforming even close-sourced CosyVoice 3.\nThese results underscore the model&#8217;s capability to handle complex, realistic inputs, a strength attributed to the RALM&#8217;s role in recovering fine-grained acoustic details subsequent to the TSLM-FSQ-based semantic-prosodic modeling.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluations (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T5\" title=\"Table 5 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) further validate the objective findings, with VoxCPM achieving competitive performance across both languages. On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.\nFor Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity. This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.\nVoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The optimal performance observed at FSQ levels (FSQ-d128/d256) reveals a key trade-off. Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity. Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist. The peak at FSQ-d256 indicates the bottleneck creates an effective &#8220;summary space\": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "setting",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "zhhard",
                    "cer",
                    "model",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n",
                "matched_terms": [
                    "model",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "powerful",
                    "voxcpm",
                    "understanding",
                    "capabilities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scalability and Efficiency</span>\nThe performance improvement from VoxCPM-Emilia to VoxCPM highlights the architecture&#8217;s scalability with increased data.\nThe hierarchical design allows larger models to effectively utilize increased capacity for learning complex patterns.\nIn terms of inference efficiency, VoxCPM-0.5B achieves a real-time factor (RTF) of 0.17 on a single NVIDIA RTX 4090, confirming practical deployment feasibility.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "larger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.\nIt resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations.\nOur approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details.\nThis eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models.\nExtensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity. The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "model",
                    "modeling"
                ]
            }
        ]
    },
    "S4.T7": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 7: Ablation Studies about core architecture designs.",
        "body": "Model Setting\nEN\nZH\nZH-hard case\n\n\n\nWER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\n\ndefault setting\n2.98\n62.6\n1.77\n70.4\n18.19\n64.9\n\n\nw/o RALM: TSLM (24 layers) →\\rightarrow LocDiT\n4.34\n61.8\n3.05\n69.4\n25.00\n63.8\n\n\nw/o RALM: TSLM (30 layers) →\\rightarrow LocDiT\n5.35\n62.6\n3.46\n69.8\n30.40\n63.9\n\n\nw/o E<iE_{<i} in RALM: TSLM →\\rightarrow ALM →\\rightarrow LocDiT\n4.91\n60.9\n4.94\n68.1\n27.17\n61.7\n\n\nw/o hresidualh^{\\text{residual}} in condition: TSLM →\\rightarrow FSQ →\\rightarrow LocDiT\n3.86\n58.3\n3.05\n67.6\n23.65\n61.7",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model Setting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH-hard case</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">default setting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">70.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w/o RALM: TSLM (24 layers) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT</td>\n<td class=\"ltx_td ltx_align_center\">4.34</td>\n<td class=\"ltx_td ltx_align_center\">61.8</td>\n<td class=\"ltx_td ltx_align_center\">3.05</td>\n<td class=\"ltx_td ltx_align_center\">69.4</td>\n<td class=\"ltx_td ltx_align_center\">25.00</td>\n<td class=\"ltx_td ltx_align_center\">63.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w/o RALM: TSLM (30 layers) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT</td>\n<td class=\"ltx_td ltx_align_center\">5.35</td>\n<td class=\"ltx_td ltx_align_center\">62.6</td>\n<td class=\"ltx_td ltx_align_center\">3.46</td>\n<td class=\"ltx_td ltx_align_center\">69.8</td>\n<td class=\"ltx_td ltx_align_center\">30.40</td>\n<td class=\"ltx_td ltx_align_center\">63.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m9\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> ALM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT</td>\n<td class=\"ltx_td ltx_align_center\">4.91</td>\n<td class=\"ltx_td ltx_align_center\">60.9</td>\n<td class=\"ltx_td ltx_align_center\">4.94</td>\n<td class=\"ltx_td ltx_align_center\">68.1</td>\n<td class=\"ltx_td ltx_align_center\">27.17</td>\n<td class=\"ltx_td ltx_align_center\">61.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">w/o <math alttext=\"h^{\\text{residual}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m12\" intent=\":literal\"><semantics><msup><mi>h</mi><mtext>residual</mtext></msup><annotation encoding=\"application/x-tex\">h^{\\text{residual}}</annotation></semantics></math> in condition: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> FSQ <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">58.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">67.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">23.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">61.7</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "alm",
            "fsq",
            "ablation",
            "about",
            "designs",
            "architecture",
            "↓downarrow",
            "sim",
            "core",
            "→rightarrow",
            "studies",
            "locdit",
            "wer",
            "ralm",
            "layers",
            "model",
            "default",
            "cer",
            "↑uparrow",
            "eiei",
            "setting",
            "zhhard",
            "hresidualhtextresidual",
            "case",
            "condition",
            "tslm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ralm",
                    "architecture",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib10\" title=\"\">2022</a>)</cite>).\nAutoregressively or Non-autoregressively predicting these tokens from text or phonemes <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib4\" title=\"\">2023a</a>; Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib21\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib8\" title=\"\">2025</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib44\" title=\"\">Wang et&#160;al., </a>; Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib34\" title=\"\">2024</a>)</cite> offers excellent scalability and in-context learning capabilities.\nHowever, this approach faces a fundamental \"quantization ceiling\", as the compression process irreversibly discards subtle acoustic details.\nTo mitigate this quality loss, state-of-the-art TTS systems <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib7\" title=\"\">2024</a>)</cite> adopt multi-stage hybrid pipelines.\nHere, an LLM generates discrete tokens which condition a separate diffusion-based decoder.\nWhile improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context.\nThis fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "core",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide extensive ablation studies that conclusively validate the semi-discrete residual representations as the crucial component for robust, expressive, and l context-aware synthesis. Besides, we release the codes and models publicly to support community development and future research.</p>\n\n",
                "matched_terms": [
                    "studies",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.\nEarly systems like Tacotron 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib38\" title=\"\">2018</a>)</cite> established the encoder-decoder framework for text-to-mel mapping, while FastSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib37\" title=\"\">2020</a>)</cite> introduced explicit duration modeling for alignment stability.\nInspired from VALL-E, MELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib29\" title=\"\">2024</a>)</cite> autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.\nRecent developments have integrated diffusion processes to enhance detail and diversity. Non-autoregressive models like NaturalSpeech 2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib39\" title=\"\">2023</a>)</cite> and VoiceBox <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib23\" title=\"\">2023</a>)</cite> apply diffusion directly on continuous representations.\nF5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib9\" title=\"\">2024</a>)</cite> advanced flow-matching for efficient synthesis.\nAutoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.\nInnovations like ARDiT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib27\" title=\"\">2024</a>)</cite> use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing.\nDiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite> extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement.\nVibeVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib35\" title=\"\">2025</a>)</cite> employed next-token diffusion for long-form multi-speaker synthesis.\nBesides, recent models such as CLEAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib45\" title=\"\">2025</a>)</cite> and FELLE <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib42\" title=\"\">2025a</a>)</cite> focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib1\" title=\"\">2025</a>)</cite> and KALL-E <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib50\" title=\"\">2024</a>)</cite> combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality. Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The core innovation lies in our hierarchical conditioning mechanism with residual representation learning.\nIt is made up of a local audio encoder (LocEnc), a text-semantic language model (TSLM), a residual acoustic language model (RALM) and a local diffusion transformer decoder (LocDiT). A stop predictor is attached to the output of the TSLM to determine the endpoint of generation.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, each patch generation involves:</p>\n\n",
                "matched_terms": [
                    "model",
                    "locdit",
                    "core",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{E}_{&lt;i}=\\text{LocEnc}(\\mathbf{Z}_{&lt;i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><mtext>LocEnc</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119833;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{&lt;i}=\\text{LocEnc}(\\mathbf{Z}_{&lt;i})</annotation></semantics></math> represents historical audio context aggregated by a lightweight LocEnc that compresses VAE latent patches into compact acoustic embeddings.\nThe hierarchical backbone produces a conditioning signal <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> that encapsulates both semantic content from TSLM (with FSQ) and acoustic details from RALM.\nThis signal guides the LocDiT to generate the current latent patch <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> through a denoising diffusion process.\nThe entire model is trained end-to-end with gradients flowing through all components, including the FSQ bottleneck via straight-through estimation, ensuring coordinated optimization toward holistic speech synthesis.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "locdit",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Text-Semantic Language Model forms the main part of our hierarchical architecture, responsible for capturing high-level linguistic structure and generating contextually appropriate speech patterns.\nUnlike conventional TTS systems that typically operate on phoneme sequences, our approach leverages a pre-trained text language model (MiniCPM-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite>) as its initial backbone, enabling richer contextual understanding and more natural prosody prediction directly from raw text.\nSpecifically, we employ character-level segmentation for Chinese BPE Tokenizer to mitigate the vocabulary sparsity issue in TTS tasks.\nBy processing both text tokens and historical audio context, the TSLM learns to generate semantic content and prosodic structure that evolve naturally throughout an utterance, reflecting the underlying linguistic meaning rather than simply mapping phonemes to acoustic features.\nThe TSLM produces continuous semantic-prosodic representations that encode both the content to be spoken and how it should be prosodically realized, serving as input to the subsequent quantization stage.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of our approach lies the Finite Scalar Quantization (FSQ) layer, which projects the continuous hidden states from the TSLM onto a structured lattice to create a semi-discrete representation.\nThe FSQ operation transforms each dimension of the continuous vector through a deterministic scalar quantization:</p>\n\n",
                "matched_terms": [
                    "core",
                    "fsq",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ layer acts as a bottleneck, analogous to the first layer of Residual Vector Quantization (RVQ), which captures a coarse semantic-prosodic skeleton (e.g., content, intonation patterns).\nWe term this representation &#8220;semi-discrete\" as it employs a significantly larger dimensionality than standard FSQ to ensure sufficient informational capacity.\nUnlike RVQ, where the first layer is a prediction target and subsequent layers model finer details, our FSQ bottleneck serves as an intermediate, differentiable inductive bias within the continuous data flow.\nIt encourages the model to prioritize modeling stable, high-level components (the semantic-prosodic skeleton) by providing a clear learning signal for what information should be preserved through the bottleneck.\nThis structured approach mitigates error accumulation by reducing the modeling burden on the TSLM, allowing it to focus on the major components of the speech.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "layers",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To recover the fine-grained acoustic information attenuated by quantization, we introduce the Residual Acoustic Language Model (RALM).\nThis module specializes in reconstructing those subtle vocal characteristics that conventional discrete methods sacrifice for stability.\nIt processes the quantization residuals along with contextual information to recover speaker identity, spectral fine structure, and micro-prosodic variations:</p>\n\n",
                "matched_terms": [
                    "model",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, the RALM conditions its predictions on both the TSLM hidden states of the text part <math alttext=\"\\mathbf{H_{\\text{text}}^{\\text{TSLM}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mtext>text</mtext><mtext>TSLM</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H_{\\text{text}}^{\\text{TSLM}}}</annotation></semantics></math>, the semi-discrete representation of speech part <math alttext=\"\\mathbf{H}_{&lt;i}^{\\text{FSQ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow><mtext>FSQ</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}_{&lt;i}^{\\text{FSQ}}</annotation></semantics></math>, and the historical acoustic embeddings <math alttext=\"\\mathbf{E}_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{&lt;i}</annotation></semantics></math>.\nThis residual learning approach creates a natural division of labor: the TSLM+FSQ pathway focuses on content stability and prosodic coherence, while the RALM pathway specializes in acoustic expressivity and speaker characteristics.</p>\n\n",
                "matched_terms": [
                    "ralm",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Local Diffusion Transformer (LocDiT) serves as our high-fidelity synthesis module, generating continuous latent patches conditioned on the hierarchical representation <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> produced by the preceding modules.\nFollowing DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite>, we employ a bidirectional Transformer architecture that enables full receptive field modeling within each patch.\nTo enhance generation consistency, we incorporate the previous patch <math alttext=\"\\mathbf{z}_{i-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i-1}</annotation></semantics></math> as additional conditioning context, which has been empirically validated to significantly improve output quality by framing the task as outpainting rather than independent patch generation.\nBesides, we mask the LM guidance in LocDiT condition with a specific probability ratio, for enabling classifier-free guidance (CFG) during inference.</p>\n\n",
                "matched_terms": [
                    "condition",
                    "architecture",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gradients from this loss are backpropagated through the entire autoregressive hierarchy, including the FSQ layer (via straight-through estimation), the TSLM and the LocEnc. This end-to-end optimization\nunder the combined objective <math alttext=\"\\mathcal{L}=\\mathcal{L}_{\\text{FM}}+\\lambda\\mathcal{L}_{\\text{Stop}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FM</mtext></msub><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>Stop</mtext></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}=\\mathcal{L}_{\\text{FM}}+\\lambda\\mathcal{L}_{\\text{Stop}}</annotation></semantics></math>\nallows each component to learn its specialized role&#8212;semantic planning, stabilization, and acoustic refinement&#8212;in a coordinated manner, guided by the unified objective of accurately modeling the continuous speech latents.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "studies",
                    "model",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "layers",
                    "model",
                    "architecture",
                    "locdit",
                    "tslm",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ablation",
                    "locdit",
                    "condition",
                    "studies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "ablation",
                    "sim",
                    "wer",
                    "studies",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T6\" title=\"Table 6 &#8227; 4.3 Ablation Study: Effect of the Semi-discrete Bottleneck &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, the ablation studies on the FSQ bottleneck dimensionality provide critical insights.\nThe catastrophic performance degradation of the purely continuous model (w/o FSQ), especially on hard cases (ZH-CER: 24.92%), validates our core hypothesis: entangling semantic planning and acoustic rendering in a continuous space leads to instability. Without the inductive bias imposed by FSQ, the model struggles to separate these tasks even with a hierarchical design, resulting in error accumulation on complex utterances.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model",
                    "ablation",
                    "core",
                    "studies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The optimal performance observed at FSQ levels (FSQ-d128/d256) reveals a key trade-off. Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity. Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist. The peak at FSQ-d256 indicates the bottleneck creates an effective &#8220;summary space\": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "zhhard",
                    "cer",
                    "model",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n",
                "matched_terms": [
                    "model",
                    "setting",
                    "locdit",
                    "condition",
                    "ralm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our core hypothesis of learned implicit semantic-acoustic disentanglement, we conducted a t-SNE visualization of the internal representations in our hierarchical model.\nThe resulting distributions, shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F2\" title=\"Figure 2 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, empirically confirm the specialized roles of the TSLM and the RALM.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F2\" title=\"Figure 2 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the model&#8217;s behavior in a zero-shot voice cloning task, where each color corresponds to a distinct utterance from an unseen speaker.\nThe TSLM-FSQ outputs form semantic-prosodic structure closely tied to text content, while the RALM residuals exhibit strong speaker-related variations for acoustic rendering, confirming their specialized roles in content planning and acoustic refinement.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> further demonstrates the VoxCPM&#8217;s capability to infer appropriate prosody and style directly from text, when not using any speech prompt.\nWhen processing different text genres (news, poetry, conversation), TSLM-FSQ representations cluster by semantic category, showing that the pre-trained language model backbone effectively infers appropriate prosodic patterns directly from text content.\nFor example, embeddings for &#8220;news\" group together, separate from &#8220;story-telling\" or &#8220;rap-lyrics.\"\nThe RALM outputs display greater within-category variation, indicating its role in adding fine-grained acoustic nuances to the semantic-prosodic plan.</p>\n\n",
                "matched_terms": [
                    "core",
                    "model",
                    "ralm",
                    "tslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "architecture"
                ]
            }
        ]
    },
    "S4.T8": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 8: Performance across training phases.",
        "body": "Phase\nEN\nZH\nZH-Hard Case\n\n\n\nWER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\n\nStable\n2.05\n69.7\n0.99\n75.1\n13.22\n68.6\n\n\nDecay\n1.85\n72.9\n0.93\n77.2\n8.87\n73.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Phase</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH-Hard Case</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Stable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Decay</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">72.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">77.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">8.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">73.0</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "stable",
            "training",
            "across",
            "phases",
            "↑uparrow",
            "decay",
            "↓downarrow",
            "sim",
            "zhhard",
            "phase",
            "case",
            "wer",
            "cer",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
            "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement.\nThis challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model&#8211;VoxCPM.\nOur framework introduces a differentiable quantization bottleneck that induces natural specialization:\na Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.\nThis hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents.\nCritically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nBesides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow.\nTo facilitate community-driven research and development, VoxCPM is publicly accessible under Apache 2.0.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model&#8211;VoxCPM.\nOur key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.\nThe core innovation is a differentiable Finite Scalar Quantization (FSQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib30\" title=\"\">Mentzer et&#160;al., </a>)</cite> bottleneck that induces natural specialization:\n(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;\nand (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.\nThis hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.\nCritically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.\nTrained on a massive 1.8 million hours of bilingual corpus, our VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis.\nOur main contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech. The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation.\nCosyVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib12\" title=\"\">2024a</a>)</cite> proposed supervised semantic tokens for improved zero-shot performance, while its successors,\nCosyVoice 2 and 3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib13\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib14\" title=\"\">2025</a>)</cite>\nincorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.\nIndexTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib11\" title=\"\">2025</a>)</cite> and IndexTTS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib49\" title=\"\">2025</a>)</cite> introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements. SparkTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib43\" title=\"\">2025b</a>)</cite> utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib15\" title=\"\">2024</a>)</cite> along with its update FireRedTTS-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib46\" title=\"\">2025</a>)</cite> established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.\nOpenaudio-s1 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAudio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib32\" title=\"\">2024</a>)</cite> used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.\nHiggs Audio v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(BosonAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib6\" title=\"\">2025</a>)</cite> proposed a unified audio tokenizer captures\nboth semantic and acoustic features, and pretrained on over 10 million hours of audio data, providing a powerful foundation model.\nDespite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that an effective solution should <span class=\"ltx_text ltx_font_bold\">structurally separate</span> the modeling of stable semantic-prosodic content from fine-grained acoustic details while maintaining differentiability for end-to-end training.\nOur key insight is to introduce a <span class=\"ltx_text ltx_font_bold\">differentiable quantization bottleneck</span> that naturally induces this separation through scalar quantization, splitting information into a discrete-like skeleton for content stability and continuous residual components for detail expressivity.</p>\n\n",
                "matched_terms": [
                    "stable",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nWe conducted experiments on two primary datasets:\n(1) <span class=\"ltx_text ltx_font_bold\">Large-scale Bilingual Corpus</span>: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech.\nThe raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas.\nTo enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.\nAll audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.\n(2) <span class=\"ltx_text ltx_font_bold\">Emilia Dataset</span>: For comparisons and ablation studies, we used the publicly available Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib17\" title=\"\">2024</a>)</cite> (95K hours) including Chinese and English utterances.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "training",
                    "sim",
                    "wer",
                    "cer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Subjective evaluations (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T5\" title=\"Table 5 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) further validate the objective findings, with VoxCPM achieving competitive performance across both languages. On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.\nFor Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity. This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.\nVoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "training",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T7\" title=\"Table 7 &#8227; 4.4 Ablation Study: Effect of Residual Acoustic Modeling &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the ablation studies about the residual language modeling validate our core architectural innovations.\nNotably, the purely continuous variant (w/o RALM: TSLM <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> LocDiT) &#8212;analogous to DiTAR&#8217;s approach&#8212;shows significantly degraded performance, particularly on challenging cases.\nThe performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.\nThis conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.\nSecondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o <math alttext=\"E_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{&lt;i}</annotation></semantics></math> in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.\nFinally, the best performance of the default setting demonstrates the effectiveness of the residual connection.\nBy summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance. The initial Stable phase allows the model to converge reliably to a strong baseline. The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "performance",
                    "stable",
                    "decay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive and Context-Aware Synthesis Capabilities</span>\nBeyond quantitative metrics, VoxCPM shows\ngood expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data.\nThe powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above.\nWhen not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.F3\" title=\"Figure 3 &#8227; 4.7 Analysis and Discussion &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nWe strongly recommend readers to listen our demo samples<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openbmb.github.io/VoxCPM-demopage/\" title=\"\">https://openbmb.github.io/VoxCPM-demopage/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "across"
                ]
            }
        ]
    },
    "S4.T9": {
        "source_file": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning",
        "caption": "Table 9: Effect of LM guidance on LocDiT, tested with VoxCPM.",
        "body": "CFG Value\nEN\nZH\nZH-hard case\n\n\n\nWER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\n\n1.0 (w/o CFG)\n16.32\n55.1\n14.47\n61.5\n56.87\n43.0\n\n\n1.5\n1.86\n72.1\n1.16\n77.0\n9.60\n73.9\n\n\n2.0\n1.85\n72.9\n0.93\n77.2\n8.87\n73.0\n\n\n3.0\n2.16\n71.4\n1.12\n74.7\n13.22\n65.0\n\n\n5.0\n12.78\n60.7\n17.23\n59.4\n48.46\n39.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">CFG Value</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ZH-hard case</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">CER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">1.0 (w/o CFG)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">56.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">1.86</td>\n<td class=\"ltx_td ltx_align_center\">72.1</td>\n<td class=\"ltx_td ltx_align_center\">1.16</td>\n<td class=\"ltx_td ltx_align_center\">77.0</td>\n<td class=\"ltx_td ltx_align_center\">9.60</td>\n<td class=\"ltx_td ltx_align_center\">73.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">2.0</td>\n<td class=\"ltx_td ltx_align_center\">1.85</td>\n<td class=\"ltx_td ltx_align_center\">72.9</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">77.2</td>\n<td class=\"ltx_td ltx_align_center\">8.87</td>\n<td class=\"ltx_td ltx_align_center\">73.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">3.0</td>\n<td class=\"ltx_td ltx_align_center\">2.16</td>\n<td class=\"ltx_td ltx_align_center\">71.4</td>\n<td class=\"ltx_td ltx_align_center\">1.12</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">13.22</td>\n<td class=\"ltx_td ltx_align_center\">65.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">5.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">12.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">60.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">17.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">59.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">48.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">39.9</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "cfg",
            "value",
            "↓downarrow",
            "locdit",
            "sim",
            "zhhard",
            "voxcpm",
            "tested",
            "case",
            "wer",
            "cer",
            "↑uparrow",
            "guidance",
            "effect"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.\nAs detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T9\" title=\"Table 9 &#8227; 4.6 Effect of LM Guidance on LocDiT &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity.\nThe absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input.\nEmploying a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values (<math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS6.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math>3.0) degraded intelligibility significantly.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The Local Diffusion Transformer (LocDiT) serves as our high-fidelity synthesis module, generating continuous latent patches conditioned on the hierarchical representation <math alttext=\"\\mathbf{h}_{i}^{\\text{final}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m1\" intent=\":literal\"><semantics><msubsup><mi>&#119841;</mi><mi>i</mi><mtext>final</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{h}_{i}^{\\text{final}}</annotation></semantics></math> produced by the preceding modules.\nFollowing DiTAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib19\" title=\"\">2025</a>)</cite>, we employ a bidirectional Transformer architecture that enables full receptive field modeling within each patch.\nTo enhance generation consistency, we incorporate the previous patch <math alttext=\"\\mathbf{z}_{i-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i-1}</annotation></semantics></math> as additional conditioning context, which has been empirically validated to significantly improve output quality by framing the task as outpainting rather than independent patch generation.\nBesides, we mask the LM guidance in LocDiT condition with a specific probability ratio, for enabling classifier-free guidance (CFG) during inference.</p>\n\n",
                "matched_terms": [
                    "guidance",
                    "cfg",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Architecture Configurations</span>\nWe implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib40\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openbmb/MiniCPM4-0.5B\" title=\"\">https://huggingface.co/openbmb/MiniCPM4-0.5B</a></span></span></span>, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM).\nThe FSQ layer uses 256 dimensions with 9 scalar levels.\nThe LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.\nDetail Configrations are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Experimental Setup &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "voxcpm",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Details</span>\nWe trained two models for comparisons:\n1) <span class=\"ltx_text ltx_font_bold\">VoxCPM</span> was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;\n2) <span class=\"ltx_text ltx_font_bold\">VoxCPM-Emilia</span> was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.\nBoth VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and a Warmup-Stable-Decay (WSD) schedule <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#bib.bib18\" title=\"\">2024</a>)</cite> which we found essential for optimal convergence. Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nAll ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math>\nto ensure a consistent comparison.\nFor LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.</p>\n\n",
                "matched_terms": [
                    "guidance",
                    "voxcpm",
                    "cfg",
                    "locdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics and Benchmarks</span>\nWe employed comprehensive subjective and objective evaluations. Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality.\nSubjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.\nModels were assessed on two challenging benchmarks: 1) <span class=\"ltx_text ltx_font_bold\">SEED-TTS-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span></span>, focusing on general TTS intelligibility and similarity in English and Chinese, including a &#8220;Hard\" set with complex sentences; 2) <span class=\"ltx_text ltx_font_bold\">CV3-EVAL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_medium\" href=\"https://github.com/FunAudioLLM/CV3-Eval\" title=\"\">https://github.com/FunAudioLLM/CV3-Eval</a></span></span></span></span>, derived from CosyVoice 3 competition, emphasizing expressive and in-the-wild voice cloning.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T3\" title=\"Table 3 &#8227; 4.2 Main Results: Comparison with State-of-the-Art TTS &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark.\nIt attains an English WER of 1.85% and a Chinese CER of 0.93%, surpassing strong competitors like IndexTTS2 and CosyVoice2.\nConcurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9% (EN) and 77.2% (ZH).\nThis demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.\nThe VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34%, ZH-CER: 1.11%).\nThis highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.\nNotably, while DiTAR&#8217;s phoneme-based approach shows slightly better stability,\nVoxCPM&#8217;s use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.\nBesides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "voxcpm",
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gains from this two-phase strategy are substantiated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24650v1#S4.T8\" title=\"Table 8 &#8227; 4.5 Effect of Training Phase on Performance &#8227; 4 Experiments and Results &#8227; VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.\nCompared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity. Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22% to 8.87%, alongside a 4.4-point SIM improvement.</p>\n\n",
                "matched_terms": [
                    "zhhard",
                    "cer",
                    "sim"
                ]
            }
        ]
    }
}