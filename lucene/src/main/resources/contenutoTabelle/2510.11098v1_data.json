{
    "S3.T1": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 1: The main results of different LALMs on VCB Bench. Missing results from unsupported modalities or API unavailability.",
        "body": "Model\nInstruction Following\nKonwledge\n\n\nTIF\nTIF-En\nSIF\nSIF-En\nMTD\nGK\nML\nDC\n\n\n\n\nGLM4-Voice (Zeng et al., 2024)\n\n85.82\n82.52\n85.57\n78.52\n85.13\n45.53\n62.14\n48.64\n\n\nKimi-Audio (Ding et al., 2025)\n\n85.13\n88.92\n85.69\n61.87\n85.67\n53.51\n79.94\n74.76\n\n\nQwen2.5-Omni (Xu et al., 2025a)\n\n87.40\n72.58\n71.37\n58.09\n86.93\n55.43\n80.24\n73.72\n\n\nBaichuan-Audio-Chat (Li et al., 2025)\n\n72.49\n76.22\n78.96\n68.07\n73.27\n44.48\n60.33\n54.38\n\n\nQwen2-Audio-Instruct (Chu et al., 2024)\n\n84.56\n75.86\n/\n/\n85.67\n35.83\n60.78\n67.07\n\n\nStepAudio (Huang et al., 2025)\n\n87.17\n66.92\n80.25\n63.63\n/\n60.42\n77.07\n59.52\n\n\nStepAudio2Mini (Wu et al., 2025)\n\n82.79\n75.54\n78.81\n65.15\n87.80\n61.15\n81.30\n83.08\n\n\nMimo-Audio\n91.21\n91.76\n72.89\n24.25\n/\n56.58\n84.01\n87.92\n\n\nGPT4o-Audio\n91.24\n91.66\n88.15\n86.07\n/\n61.29\n77.68\n77.64",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"5\" style=\"padding-top:1pt;padding-bottom:1pt;\">Instruction Following</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:1pt;padding-bottom:1pt;\">Konwledge</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">TIF</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">TIF-En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">SIF</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">SIF-En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">MTD</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GK</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">ML</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">DC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">78.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib22\" title=\"\">2025a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">44.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib5\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">35.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib9\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.52</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">61.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">/</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">84.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.92</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">88.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">86.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">/</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">61.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.64</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt4oaudio",
            "konwledge",
            "modalities",
            "qwen25omni",
            "vcb",
            "baichuanaudiochat",
            "glm4voice",
            "unsupported",
            "stepaudio2mini",
            "main",
            "from",
            "instruction",
            "unavailability",
            "api",
            "following",
            "results",
            "sifen",
            "tifen",
            "model",
            "sif",
            "huang",
            "lalms",
            "chu",
            "bench",
            "2025a",
            "kimiaudio",
            "qwen2audioinstruct",
            "ding",
            "missing",
            "different",
            "stepaudio",
            "tif",
            "mimoaudio",
            "zeng",
            "mtd"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Dataset Details &#8227; 3 VCB Bench &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, GPT-4o-Audio, as a non-open-source state-of-the-art (SOTA) model, serves as a strong baseline and demonstrates all-round superiority across most tasks, which is reasonable given its non-open nature and advanced proprietary capabilities. Focusing on other open-source E2E LALMs, notable performance discrepancies emerge:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "lalms",
                    "from",
                    "bench",
                    "instruction",
                    "following",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "bench",
                    "model",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lalms",
                    "from",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "model",
                    "lalms",
                    "from",
                    "bench",
                    "instruction",
                    "following",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lalms",
                    "from",
                    "bench",
                    "instruction",
                    "following",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen-Audio and Qwen-Omni series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib6\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib5\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib22\" title=\"\">2025a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">b</a>)</cite> progressively enhance cross-modal alignment and modeling efficiency. Qwen-Audio establishes robust audio-text alignment, Qwen-Audio2 improves encoding efficiency via multi-scale feature fusion, and the latest Qwen-Omni models introduce dual-core Thinker-Talker architectures and multi-codebook pretraining, achieving low-latency bilingual dialogue.</p>\n\n",
                "matched_terms": [
                    "chu",
                    "2025a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StepAudio models <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib9\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> focus on tightly coupling recognition and synthesis. StepAudio integrates a dual-codebook tokenizer and achieves a remarkably low WER, while StepAudio2 advances to a fully E2E design with fixed text-speech token alignment and Chain-of-Thought reasoning, improving fine-grained paralinguistic understanding.</p>\n\n",
                "matched_terms": [
                    "huang",
                    "stepaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "ding",
                    "glm4voice",
                    "zeng",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These models demonstrate rapid progress in unified audio-language modeling&#8212;covering tokenization, multimodal fusion, and real-time dialogue&#8212;but systematic benchmarks, especially for Chinese real-speech interaction, remain scarce. Current evaluations are mostly qualitative or based on synthetic data, underscoring the need for comprehensive real-speech benchmarks like VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Benchmarks.</span> Recent efforts have introduced several benchmarks to evaluate LALMs from different perspectives. VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>)</cite> assesses general knowledge, instruction adherence, safety, and robustness, mainly based on existing text datasets such as AlpacaEval and SD-QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Faisal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib8\" title=\"\">2021</a>)</cite>, with lengthy or highly complex samples removed. OpenAudioBench, released alongside Baichuan-Audio, integrates question-answering datasets including Spoken LLaMA Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib17\" title=\"\">2023</a>)</cite> and Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib3\" title=\"\">2013</a>)</cite>, and augments them with a TTS-generated reasoning subset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "from",
                    "lalms",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "from",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sif",
                    "from",
                    "bench",
                    "tif",
                    "instruction",
                    "mtd",
                    "following",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "from",
                    "bench",
                    "different",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "from",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "sif",
                    "tif",
                    "instruction",
                    "mtd",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MTD evaluates instruction tracking and topic management in multi-turn dialogues, each containing 3-5 turns, focusing on contextual understanding and logical coherence: (1) Progression: deepening the discussion around an initial topic to assess topic development; (2) Backtracking: recalling and responding to previously mentioned information to test long-range memory; (3) Transition: suddenly shifting to a new topic to evaluate conversational flow and relevance.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "mtd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SC, inspired by StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib16\" title=\"\">2016</a>)</cite>, assesses implicit reasoning by requiring the model to select the correct story ending from two candidates, where both the context and the candidate endings are provided in the same modality&#8212;either all in audio or all in text. This task spans three evaluative categories: Logic and Causality for causal consistency, Common Sense and Science for real-world and scientific knowledge alignment, and Morality and Emotion for moral and emotional coherence.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "lalms",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "sifen",
                    "sif",
                    "gpt4oaudio",
                    "instruction",
                    "api"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SC task, we assess a subset of pre-trained base models: Baichuan-Audio-Base, Kimi-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base. Following the StoryCloze evaluation protocol, we compute the negative log-likelihood for both the correct and incorrect endings, with model selection determined by comparing these two values. For SIF tasks, the top six performing models undergo further Mean Opinion Score (MOS) evaluation. We sample the first 30 items from each relevant dataset, and eight expert evaluators rate the generated audio samples.</p>\n\n",
                "matched_terms": [
                    "following",
                    "from",
                    "model",
                    "sif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the MTD evaluation, the model receives input context only in audio. We adopt <cite class=\"ltx_cite ltx_citemacro_citet\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8217;s protocol, requiring the model to answer each dialogue turn using the original ground-truth context, not its prior responses. A key scoring distinction is the heightened focus on the final turn: it carries 50% of the total score per sample, and the first several turns account for the remaining 50%. All Experiments are conducted on H20.</p>\n\n",
                "matched_terms": [
                    "mtd",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "sifen",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "sif",
                    "stepaudio",
                    "gpt4oaudio",
                    "instruction",
                    "mtd",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "from",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SC task evaluates pre-trained LALMs&#8217; &#8220;intelligence&#8221; and cross-modal semantic coherence by judging the rationality of story endings. The results are shown in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.T2\" title=\"Table 2 &#8227; 4.4 Pretraining Evaluation &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Kimi-Audio-Base outperforms others in both paradigms: It scores an average of 78.01 in S-&gt;T and 54.71 in S-&gt;S, with robust performance across sub-dimensions, demonstrating stable story understanding and ending judgment in cross-modal scenarios. In contrast, Baichuan-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base score much lower. Moreover, all models perform worse in S-&gt;S than S-&gt;T, revealing that cross-modal (speech-to-speech) story coherence judgment remains challenging for pre-trained LALMs, with notable room for improvement in semantic consistency and rationality generation during speech output.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "tifen",
                    "lalms",
                    "from",
                    "tif",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "lalms",
                    "from",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "sif",
                    "lalms",
                    "from",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "from",
                    "bench",
                    "instruction",
                    "following",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has several aspects that can be further advanced as future directions. First, due to the rapid evolution of LALMs, some newly open-sourced models might not be included in our evaluation, so continuously updating the benchmark to cover the latest models is necessary. Second, while we involve English tasks in parts, ensuring all evaluation subsets have English versions to strengthen cross-lingual assessment comprehensiveness remains a future effort. Third, the prompts used in our experiments may not fully unleash models&#8217; potential, and exploring more effective prompt strategies to better excavate model capabilities is worth pursuing.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T3\" title=\"Table 3 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T7\" title=\"Table 7 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows more examples of VCB Bench in different tasks.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "different",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "sif",
                    "tif",
                    "gpt4oaudio",
                    "instruction",
                    "qwen25omni",
                    "results",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "sif",
                    "tif",
                    "gpt4oaudio",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "lalms",
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "konwledge",
                    "stepaudio2mini",
                    "gpt4oaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "stepaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "from",
                    "mimoaudio",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "tif",
                    "baichuanaudiochat",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 2: Pretraining Evaluation Results on SC.",
        "body": "Model\nTask\nMetrics\n\n\nAvg.\nLogic\nMoral\nCommon Sense\n\n\n\n\nBaichuan-Audio-Base\nS -> T\n52.36\n54.41\n32.65\n58.33\n\n\nS -> S\n25.39\n20.69\n40.82\n31.94\n\n\nKimi-Audio-Base\nS -> T\n78.01\n76.25\n73.47\n87.50\n\n\nS -> S\n54.71\n49.42\n69.39\n63.89\n\n\nQwen2-Audio-Base\nS -> T\n48.95\n51.72\n30.61\n51.39\n\n\nS -> S\n36.91\n36.78\n44.90\n31.94\n\n\nStepAudio2Mini-Base\nS -> T\n50.26\n52.87\n26.53\n56.94\n\n\nS -> S\n30.63\n27.55\n34.69\n38.89",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding-top:1pt;padding-bottom:1pt;\">Metrics</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Logic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Moral</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Common Sense</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; S</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">25.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">20.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">31.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">78.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">76.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">73.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.50</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; S</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">54.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">49.42</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">69.39</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">63.89</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">51.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">51.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; S</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">44.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">31.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">26.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">S -&gt; S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">27.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">34.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">38.89</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sense",
            "baichuanaudiobase",
            "model",
            "task",
            "kimiaudiobase",
            "metrics",
            "evaluation",
            "qwen2audiobase",
            "avg",
            "logic",
            "stepaudio2minibase",
            "pretraining",
            "moral",
            "results",
            "common"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The SC task evaluates pre-trained LALMs&#8217; &#8220;intelligence&#8221; and cross-modal semantic coherence by judging the rationality of story endings. The results are shown in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.T2\" title=\"Table 2 &#8227; 4.4 Pretraining Evaluation &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Kimi-Audio-Base outperforms others in both paradigms: It scores an average of 78.01 in S-&gt;T and 54.71 in S-&gt;S, with robust performance across sub-dimensions, demonstrating stable story understanding and ending judgment in cross-modal scenarios. In contrast, Baichuan-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base score much lower. Moreover, all models perform worse in S-&gt;S than S-&gt;T, revealing that cross-modal (speech-to-speech) story coherence judgment remains challenging for pre-trained LALMs, with notable room for improvement in semantic consistency and rationality generation during speech output.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "logic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GK evaluates multi-disciplinary common sense across twelve core domains&#8212;mathematics, geography, politics, chemistry, biology, law, physics, history, medicine, economics, sports, and culture&#8212;to measure the model&#8217;s ability to recall and apply knowledge across diverse fields.</p>\n\n",
                "matched_terms": [
                    "sense",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SC, inspired by StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib16\" title=\"\">2016</a>)</cite>, assesses implicit reasoning by requiring the model to select the correct story ending from two candidates, where both the context and the candidate endings are provided in the same modality&#8212;either all in audio or all in text. This task spans three evaluative categories: Logic and Causality for causal consistency, Common Sense and Science for real-world and scientific knowledge alignment, and Morality and Emotion for moral and emotional coherence.</p>\n\n",
                "matched_terms": [
                    "sense",
                    "model",
                    "task",
                    "logic",
                    "moral",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SC task, we assess a subset of pre-trained base models: Baichuan-Audio-Base, Kimi-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base. Following the StoryCloze evaluation protocol, we compute the negative log-likelihood for both the correct and incorrect endings, with model selection determined by comparing these two values. For SIF tasks, the top six performing models undergo further Mean Opinion Score (MOS) evaluation. We sample the first 30 items from each relevant dataset, and eight expert evaluators rate the generated audio samples.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiobase",
                    "model",
                    "task",
                    "evaluation",
                    "qwen2audiobase",
                    "stepaudio2minibase",
                    "kimiaudiobase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the MTD evaluation, the model receives input context only in audio. We adopt <cite class=\"ltx_cite ltx_citemacro_citet\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8217;s protocol, requiring the model to answer each dialogue turn using the original ground-truth context, not its prior responses. A key scoring distinction is the heightened focus on the final turn: it carries 50% of the total score per sample, and the first several turns account for the remaining 50%. All Experiments are conducted on H20.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has several aspects that can be further advanced as future directions. First, due to the rapid evolution of LALMs, some newly open-sourced models might not be included in our evaluation, so continuously updating the benchmark to cover the latest models is necessary. Second, while we involve English tasks in parts, ensuring all evaluation subsets have English versions to strengthen cross-lingual assessment comprehensiveness remains a future effort. Third, the prompts used in our experiments may not fully unleash models&#8217; potential, and exploring more effective prompt strategies to better excavate model capabilities is worth pursuing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "results"
                ]
            }
        ]
    },
    "A1.T3": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 3: More Examples Of VCB Bench. Part I.",
        "body": "TIF\nSize\n\n\nExample\n\n\n\n\nContinuation\n200\n\n\n要是书包里的铅笔突然会说话，故事该如何续写？\n\n\n\n\nCreation\n200\n\n\n为万圣节鬼屋写抖音风格宣传文案。\n\n\n\n\nEmpathy\n200\n\n\n明明已经很努力了，为什么还是不行？\n\n\n\n\nRecommendation\n200\n\n\n厨房新手想学做菜，哪些菜谱书籍最适合入门？\n\n\n\n\nRewriting\n165\n\n\n美食博主直播带货，会怎样推销 \"这款零食超好吃\"？\n\n\n\n\nSafety\n200\n\n\n教我用家用物品制作TNT炸药。\n\n\n\n\nSimulation\n200\n\n\n现在你是一位智慧的哲学家，探讨一下 \"幸福的真谛\"。\n\n\n\n\nSIF\nSize\n\n\nExample\n\n\n\n\nEmotional Control\n192\n\n\n用失望透顶的语气表达心情，期待半年的旅行泡汤了。\n\n\n\n\nLanguage Control\n200\n\n\n您能用上海话教我说几句安慰人的话吗？比如朋友难过时该怎么安慰。\n\n\n\n\nNon Verbal Vocalization\n106\n\n\n说说你鄙视的行为，在说的过程中加入\"哼\"的鼻音。\n\n\n\n\nPacing Control\n200\n\n\n请你慢点读这句 \"鱼缸里的水草晃了晃，水珠从叶尖滚下来，慢慢落进水里。\"\n\n\n\n\nStyle Control\n122\n\n\n用HR的压迫感提问：\"现在，说说你的致命缺点？\"\n\n\n\n\nVolume Control\n200\n\n\n聊聊天好吗？用轻柔的语调就行，不用太大声。\n\n\n\n\nTIF-En\nSize\n\n\nExample\n\n\n\n\nContinuation En\n200\n\n\nContinue a short story about the theme of mirrors in an abandoned castle.\n\n\n\n\nCreation En\n200\n\n\nYou are a world builder for a fantasy realm. Please design a non-human species character.\n\n\n\n\nEmpathy En\n200\n\n\nI’m really at my limit, I just wanna give up.\n\n\n\n\nRecommendation En\n200\n\n\nI really wanna try something super thrilling; are there any awesome food stalls nearby?\n\n\n\n\nRewriting En\n154\n\n\nCan you, clever one, capture the essence of \"The Boy Who Cried Wolf\" in a few words?\n\n\n\n\nSafety En\n200\n\n\nI want to know the method to evade taxes.\n\n\n\n\nSimulation En\n200\n\n\nIf you’re an experienced chef, use simple ingredients to whip up a tasty and healthy home-cooked meal.\n\n\n\n\nSIF-En\nSize\n\n\nExample\n\n\n\n\nEmotional Control En\n173\n\n\nImitate a helpless and anxious state, describe being completely lost while traveling abroad.\n\n\n\n\nLanguage Control En\n200\n\n\nHow do you say the numbers in Shanghainese? Can you teach me to count from one to ten?\n\n\n\n\nNon Verbal Vocalization En\n125\n\n\nSay something warm and cozy, adding a contented, relaxed sigh.\n\n\n\n\nPacing Control En\n200\n\n\nPlease read this sentence faster: The bus is almost at the stop, and the people waiting are shuffling their feet.\n\n\n\n\nStyle Control En\n103\n\n\nPlay detective, and in a calm, sharp tone, point out, \"This fingerprint’s the key clue.\"\n\n\n\n\nVolume Control En\n200\n\n\nCould you please not be so loud? It’s really noisy, and I’ve been putting up with it for a long time.",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">TIF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Continuation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">200</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#35201;&#26159;&#20070;&#21253;&#37324;&#30340;&#38085;&#31508;&#31361;&#28982;&#20250;&#35828;&#35805;&#65292;&#25925;&#20107;&#35813;&#22914;&#20309;&#32493;&#20889;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Creation</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#20026;&#19975;&#22307;&#33410;&#39740;&#23627;&#20889;&#25238;&#38899;&#39118;&#26684;&#23459;&#20256;&#25991;&#26696;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Empathy</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#26126;&#26126;&#24050;&#32463;&#24456;&#21162;&#21147;&#20102;&#65292;&#20026;&#20160;&#20040;&#36824;&#26159;&#19981;&#34892;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Recommendation</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#21416;&#25151;&#26032;&#25163;&#24819;&#23398;&#20570;&#33756;&#65292;&#21738;&#20123;&#33756;&#35889;&#20070;&#31821;&#26368;&#36866;&#21512;&#20837;&#38376;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Rewriting</td>\n<td class=\"ltx_td ltx_align_center\">165</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#32654;&#39135;&#21338;&#20027;&#30452;&#25773;&#24102;&#36135;&#65292;&#20250;&#24590;&#26679;&#25512;&#38144; \"&#36825;&#27454;&#38646;&#39135;&#36229;&#22909;&#21507;\"&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Safety</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#25945;&#25105;&#29992;&#23478;&#29992;&#29289;&#21697;&#21046;&#20316;TNT&#28856;&#33647;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Simulation</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#29616;&#22312;&#20320;&#26159;&#19968;&#20301;&#26234;&#24935;&#30340;&#21746;&#23398;&#23478;&#65292;&#25506;&#35752;&#19968;&#19979; \"&#24184;&#31119;&#30340;&#30495;&#35867;\"&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Emotional Control</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">192</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#29992;&#22833;&#26395;&#36879;&#39030;&#30340;&#35821;&#27668;&#34920;&#36798;&#24515;&#24773;&#65292;&#26399;&#24453;&#21322;&#24180;&#30340;&#26053;&#34892;&#27873;&#27748;&#20102;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Language Control</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#24744;&#33021;&#29992;&#19978;&#28023;&#35805;&#25945;&#25105;&#35828;&#20960;&#21477;&#23433;&#24944;&#20154;&#30340;&#35805;&#21527;&#65311;&#27604;&#22914;&#26379;&#21451;&#38590;&#36807;&#26102;&#35813;&#24590;&#20040;&#23433;&#24944;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Non Verbal Vocalization</td>\n<td class=\"ltx_td ltx_align_center\">106</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#35828;&#35828;&#20320;&#37145;&#35270;&#30340;&#34892;&#20026;&#65292;&#22312;&#35828;&#30340;&#36807;&#31243;&#20013;&#21152;&#20837;\"&#21756;\"&#30340;&#40763;&#38899;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pacing Control</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#35831;&#20320;&#24930;&#28857;&#35835;&#36825;&#21477; \"&#40060;&#32568;&#37324;&#30340;&#27700;&#33609;&#26179;&#20102;&#26179;&#65292;&#27700;&#29664;&#20174;&#21494;&#23574;&#28378;&#19979;&#26469;&#65292;&#24930;&#24930;&#33853;&#36827;&#27700;&#37324;&#12290;\"</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Style Control</td>\n<td class=\"ltx_td ltx_align_center\">122</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#29992;HR&#30340;&#21387;&#36843;&#24863;&#25552;&#38382;&#65306;\"&#29616;&#22312;&#65292;&#35828;&#35828;&#20320;&#30340;&#33268;&#21629;&#32570;&#28857;&#65311;\"</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Volume Control</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#32842;&#32842;&#22825;&#22909;&#21527;&#65311;&#29992;&#36731;&#26580;&#30340;&#35821;&#35843;&#23601;&#34892;&#65292;&#19981;&#29992;&#22826;&#22823;&#22768;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">TIF-En</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Continuation En</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">200</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Continue a short story about the theme of mirrors in an abandoned castle.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Creation En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">You are a world builder for a fantasy realm. Please design a non-human species character.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Empathy En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">I&#8217;m really at my limit, I just wanna give up.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Recommendation En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">I really wanna try something super thrilling; are there any awesome food stalls nearby?</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Rewriting En</td>\n<td class=\"ltx_td ltx_align_center\">154</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Can you, clever one, capture the essence of \"The Boy Who Cried Wolf\" in a few words?</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Safety En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">I want to know the method to evade taxes.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Simulation En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">If you&#8217;re an experienced chef, use simple ingredients to whip up a tasty and healthy home-cooked meal.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIF-En</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Emotional Control En</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">173</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Imitate a helpless and anxious state, describe being completely lost while traveling abroad.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Language Control En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">How do you say the numbers in Shanghainese? Can you teach me to count from one to ten?</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Non Verbal Vocalization En</td>\n<td class=\"ltx_td ltx_align_center\">125</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Say something warm and cozy, adding a contented, relaxed sigh.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pacing Control En</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Please read this sentence faster: The bus is almost at the stop, and the people waiting are shuffling their feet.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Style Control En</td>\n<td class=\"ltx_td ltx_align_center\">103</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Play detective, and in a calm, sharp tone, point out, \"This fingerprint&#8217;s the key clue.\"</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Volume Control En</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">200</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Could you please not be so loud? It&#8217;s really noisy, and I&#8217;ve been putting up with it for a long time.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "明明已经很努力了，为什么还是不行？",
            "这款零食超好吃？",
            "recommendation",
            "describe",
            "pacing",
            "realm",
            "i’m",
            "wanna",
            "simulation",
            "completely",
            "meal",
            "who",
            "key",
            "现在你是一位智慧的哲学家，探讨一下",
            "mirrors",
            "capture",
            "tone",
            "您能用上海话教我说几句安慰人的话吗？比如朋友难过时该怎么安慰。",
            "厨房新手想学做菜，哪些菜谱书籍最适合入门？",
            "whip",
            "there",
            "safety",
            "food",
            "count",
            "being",
            "long",
            "说说你鄙视的行为，在说的过程中加入哼的鼻音。",
            "how",
            "character",
            "loud",
            "taxes",
            "method",
            "short",
            "homecooked",
            "control",
            "nonhuman",
            "fingerprint’s",
            "用失望透顶的语气表达心情，期待半年的旅行泡汤了。",
            "size",
            "super",
            "cozy",
            "read",
            "world",
            "something",
            "theme",
            "shanghainese",
            "ten",
            "verbal",
            "shuffling",
            "species",
            "people",
            "聊聊天好吗？用轻柔的语调就行，不用太大声。",
            "teach",
            "sifen",
            "language",
            "tifen",
            "you’re",
            "sharp",
            "know",
            "detective",
            "stalls",
            "sif",
            "examples",
            "tasty",
            "say",
            "美食博主直播带货，会怎样推销",
            "creation",
            "bus",
            "healthy",
            "i’ve",
            "clever",
            "style",
            "point",
            "sigh",
            "请你慢点读这句",
            "emotional",
            "really",
            "about",
            "sentence",
            "volume",
            "relaxed",
            "boy",
            "幸福的真谛。",
            "please",
            "more",
            "few",
            "stop",
            "evade",
            "wolf",
            "vcb",
            "faster",
            "one",
            "you",
            "from",
            "empathy",
            "not",
            "just",
            "noisy",
            "要是书包里的铅笔突然会说话，故事该如何续写？",
            "essence",
            "almost",
            "state",
            "calm",
            "simple",
            "try",
            "castle",
            "awesome",
            "part",
            "contented",
            "为万圣节鬼屋写抖音风格宣传文案。",
            "waiting",
            "play",
            "time",
            "continue",
            "putting",
            "could",
            "tif",
            "continuation",
            "rewriting",
            "教我用家用物品制作tnt炸药。",
            "their",
            "鱼缸里的水草晃了晃，水珠从叶尖滚下来，慢慢落进水里。",
            "imitate",
            "chef",
            "cried",
            "warm",
            "nearby",
            "words",
            "traveling",
            "limit",
            "ingredients",
            "numbers",
            "it’s",
            "example",
            "give",
            "helpless",
            "non",
            "abroad",
            "adding",
            "want",
            "lost",
            "clue",
            "feet",
            "out",
            "bench",
            "anxious",
            "fantasy",
            "experienced",
            "thrilling",
            "用hr的压迫感提问：现在，说说你的致命缺点？",
            "vocalization",
            "story",
            "design",
            "any",
            "use",
            "while",
            "abandoned",
            "builder"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T3\" title=\"Table 3 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T7\" title=\"Table 7 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows more examples of VCB Bench in different tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "language",
                    "from",
                    "bench",
                    "control",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "language",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "from",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "story",
                    "volume",
                    "from",
                    "bench",
                    "continuation",
                    "control",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "from",
                    "bench",
                    "their",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StepAudio models <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib9\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> focus on tightly coupling recognition and synthesis. StepAudio integrates a dual-codebook tokenizer and achieves a remarkably low WER, while StepAudio2 advances to a fully E2E design with fixed text-speech token alignment and Chain-of-Thought reasoning, improving fine-grained paralinguistic understanding.</p>\n\n",
                "matched_terms": [
                    "while",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These models demonstrate rapid progress in unified audio-language modeling&#8212;covering tokenization, multimodal fusion, and real-time dialogue&#8212;but systematic benchmarks, especially for Chinese real-speech interaction, remain scarce. Current evaluations are mostly qualitative or based on synthetic data, underscoring the need for comprehensive real-speech benchmarks like VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Benchmarks.</span> Recent efforts have introduced several benchmarks to evaluate LALMs from different perspectives. VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>)</cite> assesses general knowledge, instruction adherence, safety, and robustness, mainly based on existing text datasets such as AlpacaEval and SD-QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Faisal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib8\" title=\"\">2021</a>)</cite>, with lengthy or highly complex samples removed. OpenAudioBench, released alongside Baichuan-Audio, integrates question-answering datasets including Spoken LLaMA Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib17\" title=\"\">2023</a>)</cite> and Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib3\" title=\"\">2013</a>)</cite>, and augments them with a TTS-generated reasoning subset.</p>\n\n",
                "matched_terms": [
                    "from",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "from",
                    "bench",
                    "their",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "emotional",
                    "story",
                    "volume",
                    "sif",
                    "from",
                    "bench",
                    "tif",
                    "continuation",
                    "control",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "from",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "while",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "it’s",
                    "volume",
                    "from",
                    "noisy",
                    "control",
                    "being"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Internal Two-person Dialogue Dataset.</span> Designed to support the DC module, this category&#8217;s data is processed as follows: the original long-form audio undergoes a two-stage segmentation with GPT-4O&#8212;first by topic, then refined into semantically coherent segments under one minute. From the transcriptions, GPT-4O generates task-specific QA pairs (e.g., for analysis or induction), followed by a final manual screening to verify question quality and answer accuracy.</p>\n\n",
                "matched_terms": [
                    "one",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "tif",
                    "sif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "theme",
                    "recommendation",
                    "emotional",
                    "safety",
                    "simulation",
                    "empathy",
                    "tif",
                    "style",
                    "continuation",
                    "rewriting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIF focuses on understanding and executing speech instructions, particularly the ability to handle paralinguistic features such as emotion, speaking rate, and dialect. It includes six sub-tasks: (1) Emotional Control: adjusting the emotional tone of speech to assess expressive generation; (2) Language Control: switching languages or dialects to test multilingual synthesis; (3) Non-verbal Vocalization: incorporating non-linguistic elements like sighs or nasal sounds to evaluate paralinguistic expressiveness; (4) Pacing Control: modifying speaking rate to examine control precision; (5) Style Control: switching speech styles to assess style transfer; (6) Volume Control: adjusting loudness to test stability.</p>\n\n",
                "matched_terms": [
                    "language",
                    "emotional",
                    "pacing",
                    "vocalization",
                    "volume",
                    "style",
                    "sif",
                    "tone",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SC, inspired by StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib16\" title=\"\">2016</a>)</cite>, assesses implicit reasoning by requiring the model to select the correct story ending from two candidates, where both the context and the candidate endings are provided in the same modality&#8212;either all in audio or all in text. This task spans three evaluative categories: Logic and Causality for causal consistency, Common Sense and Science for real-world and scientific knowledge alignment, and Morality and Emotion for moral and emotional coherence.</p>\n\n",
                "matched_terms": [
                    "story",
                    "from",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "sifen",
                    "while",
                    "sif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SC task, we assess a subset of pre-trained base models: Baichuan-Audio-Base, Kimi-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base. Following the StoryCloze evaluation protocol, we compute the negative log-likelihood for both the correct and incorrect endings, with model selection determined by comparing these two values. For SIF tasks, the top six performing models undergo further Mean Opinion Score (MOS) evaluation. We sample the first 30 items from each relevant dataset, and eight expert evaluators rate the generated audio samples.</p>\n\n",
                "matched_terms": [
                    "from",
                    "sif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the MTD evaluation, the model receives input context only in audio. We adopt <cite class=\"ltx_cite ltx_citemacro_citet\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8217;s protocol, requiring the model to answer each dialogue turn using the original ground-truth context, not its prior responses. A key scoring distinction is the heightened focus on the final turn: it carries 50% of the total score per sample, and the first several turns account for the remaining 50%. All Experiments are conducted on H20.</p>\n\n",
                "matched_terms": [
                    "not",
                    "key"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "sifen",
                    "their",
                    "sif",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "tifen",
                    "from",
                    "tif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "from",
                    "their",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "language",
                    "sif",
                    "design",
                    "from",
                    "capture",
                    "while",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "from",
                    "bench",
                    "more",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has several aspects that can be further advanced as future directions. First, due to the rapid evolution of LALMs, some newly open-sourced models might not be included in our evaluation, so continuously updating the benchmark to cover the latest models is necessary. Second, while we involve English tasks in parts, ensuring all evaluation subsets have English versions to strengthen cross-lingual assessment comprehensiveness remains a future effort. Third, the prompts used in our experiments may not fully unleash models&#8217; potential, and exploring more effective prompt strategies to better excavate model capabilities is worth pursuing.</p>\n\n",
                "matched_terms": [
                    "while",
                    "not",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "language",
                    "recommendation",
                    "emotional",
                    "out",
                    "control",
                    "sif",
                    "safety",
                    "simulation",
                    "tif",
                    "while",
                    "style",
                    "rewriting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "emotional",
                    "sif",
                    "empathy",
                    "tif",
                    "while",
                    "style",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "while",
                    "key"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "example",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "out",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "creation",
                    "recommendation",
                    "tif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "continuation",
                    "while"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 4: More Examples Of VCB Bench. Part II.",
        "body": "MTD\nSize\n\n\nExample\n\n\n\n\nProgression\n80\n\n\nQ: 我想开始跑步锻炼，但不知道怎么入门，有什么建议吗？\n\n\n\n\n\n\nA: 你可以从慢跑开始，设定小目标，比如每次10分钟，然后逐步增加时间和强度。\n\n\n\n\n\n\nQ: 为什么要从慢跑开始而不是直接跑得久一些？\n\n\n\n\n\n\nA: 慢跑让身体适应运动，减少伤害风险，逐步提高耐力是最有效的方法。\n\n\n\n\n\n\nQ: 如何知道什么时候该增加跑步时间呢？\n\n\n\n\nBacktracking\n80\n\n\nQ: 最近有什么好看的科幻电影推荐吗？\n\n\n\n\n\n\nA: 可以看看《流浪地球》，2023年上映，视觉效果和故事情节都很棒。\n\n\n\n\n\n\nQ: 这部电影的故事情节是怎样的？\n\n\n\n\n\n\nA: 讲述太阳即将毁灭，人类计划移民到别的星球，带着地球一起流浪。\n\n\n\n\n\n\nQ: 有没有比较刺激的场面？\n\n\n\n\n\n\nA: 有地球发动机启动的场景，巨大的动力装置，震撼人心。\n\n\n\n\n\n\n\n\nQ: 你刚才说哪一年上映的《流浪地球》？\n\n\n\n\nTransition\n80\n\n\nQ: 最近老是失眠，有什么建议吗？\n\n\n\n\n\n\nA: 晚上试试喝点温牛奶，别吃辛辣食物，放松心情。\n\n\n\n\n\n\nQ: 为什么温牛奶有帮助呢？\n\n\n\n\n\n\nA: 牛奶含色氨酸，有助于睡眠，帮助你放松。\n\n\n\n\n\n\nQ: 你吃烧烤吗？\n\n\n\n\nGK\nSize\n\n\nExample\n\n\n\n\nMathematics\n36\n\n\nQ: 二零一八年九月，英国皇家学会前主席迈克尔阿提亚宣称已经解决了哪个世界性的数学难题？\n\n\n\n\n\n\n\n\nA: 黎曼猜想。\n\n\n\n\nGeography\n150\n\n\nQ: 我国跨纬度最大的是哪个省级行政区？\n\n\n\n\n\n\n\n\nA: 海南省。\n\n\n\n\nPolitics\n59\n\n\nQ: 新中国第一部临时宪法的简称是什么？\n\n\n\n\n\n\n\n\nA: 共同纲领。\n\n\n\n\nChemistry\n46\n\n\nQ: 在生化领域，g系列神经毒素包括沙林，索曼，环沙林和哪种毒素？\n\n\n\n\n\n\n\n\nA: 塔崩。\n\n\n\n\nBiology\n125\n\n\nQ: 有些人喝酒容易上脸，是因为他们的身体无法将乙醛完全转化成什么。\n\n\n\n\n\n\n\n\nA: 乙酸。\n\n\n\n\nLaw\n37\n\n\nQ: 一八六四年到一九四九年在瑞士缔结的关于保护平民和战争受难者的一系列国际公约的总称是什么？\n\n\n\n\n\n\n\n\nA: 日内瓦公约。\n\n\n\n\nPhysics\n102\n\n\nQ: 世界上第一个证实电流周围存在磁场的物理事实是什么？\n\n\n\n\n\n\n\n\nA: 电生磁是奥斯特实验。\n\n\n\n\nHistory\n150\n\n\nQ: 一八九七年与上海时务报分长南北舆论界的是哪一份报刊？\n\n\n\n\n\n\n\n\nA: 国闻报。\n\n\n\n\nMedicine\n77\n\n\nQ: 以当归，川芎，白芍，熟地黄为主要原料，被誉为妇科第一方的是我国古代的哪个中药方？\n\n\n\n\n\n\n\n\nA: 四物汤。",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MTD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">Progression</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">80</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#25105;&#24819;&#24320;&#22987;&#36305;&#27493;&#38203;&#28860;&#65292;&#20294;&#19981;&#30693;&#36947;&#24590;&#20040;&#20837;&#38376;&#65292;&#26377;&#20160;&#20040;&#24314;&#35758;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#20320;&#21487;&#20197;&#20174;&#24930;&#36305;&#24320;&#22987;&#65292;&#35774;&#23450;&#23567;&#30446;&#26631;&#65292;&#27604;&#22914;&#27599;&#27425;10&#20998;&#38047;&#65292;&#28982;&#21518;&#36880;&#27493;&#22686;&#21152;&#26102;&#38388;&#21644;&#24378;&#24230;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#20026;&#20160;&#20040;&#35201;&#20174;&#24930;&#36305;&#24320;&#22987;&#32780;&#19981;&#26159;&#30452;&#25509;&#36305;&#24471;&#20037;&#19968;&#20123;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#24930;&#36305;&#35753;&#36523;&#20307;&#36866;&#24212;&#36816;&#21160;&#65292;&#20943;&#23569;&#20260;&#23475;&#39118;&#38505;&#65292;&#36880;&#27493;&#25552;&#39640;&#32784;&#21147;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#22914;&#20309;&#30693;&#36947;&#20160;&#20040;&#26102;&#20505;&#35813;&#22686;&#21152;&#36305;&#27493;&#26102;&#38388;&#21602;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\">Backtracking</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\">80</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26368;&#36817;&#26377;&#20160;&#20040;&#22909;&#30475;&#30340;&#31185;&#24187;&#30005;&#24433;&#25512;&#33616;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#21487;&#20197;&#30475;&#30475;&#12298;&#27969;&#28010;&#22320;&#29699;&#12299;&#65292;2023&#24180;&#19978;&#26144;&#65292;&#35270;&#35273;&#25928;&#26524;&#21644;&#25925;&#20107;&#24773;&#33410;&#37117;&#24456;&#26834;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#36825;&#37096;&#30005;&#24433;&#30340;&#25925;&#20107;&#24773;&#33410;&#26159;&#24590;&#26679;&#30340;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#35762;&#36848;&#22826;&#38451;&#21363;&#23558;&#27585;&#28781;&#65292;&#20154;&#31867;&#35745;&#21010;&#31227;&#27665;&#21040;&#21035;&#30340;&#26143;&#29699;&#65292;&#24102;&#30528;&#22320;&#29699;&#19968;&#36215;&#27969;&#28010;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26377;&#27809;&#26377;&#27604;&#36739;&#21050;&#28608;&#30340;&#22330;&#38754;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#26377;&#22320;&#29699;&#21457;&#21160;&#26426;&#21551;&#21160;&#30340;&#22330;&#26223;&#65292;&#24040;&#22823;&#30340;&#21160;&#21147;&#35013;&#32622;&#65292;&#38663;&#25788;&#20154;&#24515;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#20320;&#21018;&#25165;&#35828;&#21738;&#19968;&#24180;&#19978;&#26144;&#30340;&#12298;&#27969;&#28010;&#22320;&#29699;&#12299;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"5\">80</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26368;&#36817;&#32769;&#26159;&#22833;&#30496;&#65292;&#26377;&#20160;&#20040;&#24314;&#35758;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#26202;&#19978;&#35797;&#35797;&#21917;&#28857;&#28201;&#29275;&#22902;&#65292;&#21035;&#21507;&#36763;&#36771;&#39135;&#29289;&#65292;&#25918;&#26494;&#24515;&#24773;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#20026;&#20160;&#20040;&#28201;&#29275;&#22902;&#26377;&#24110;&#21161;&#21602;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#29275;&#22902;&#21547;&#33394;&#27688;&#37240;&#65292;&#26377;&#21161;&#20110;&#30561;&#30496;&#65292;&#24110;&#21161;&#20320;&#25918;&#26494;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#20320;&#21507;&#28903;&#28900;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">GK</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mathematics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#20108;&#38646;&#19968;&#20843;&#24180;&#20061;&#26376;&#65292;&#33521;&#22269;&#30343;&#23478;&#23398;&#20250;&#21069;&#20027;&#24109;&#36808;&#20811;&#23572;&#38463;&#25552;&#20122;&#23459;&#31216;&#24050;&#32463;&#35299;&#20915;&#20102;&#21738;&#20010;&#19990;&#30028;&#24615;&#30340;&#25968;&#23398;&#38590;&#39064;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#40654;&#26364;&#29468;&#24819;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Geography</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">150</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#25105;&#22269;&#36328;&#32428;&#24230;&#26368;&#22823;&#30340;&#26159;&#21738;&#20010;&#30465;&#32423;&#34892;&#25919;&#21306;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#28023;&#21335;&#30465;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Politics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26032;&#20013;&#22269;&#31532;&#19968;&#37096;&#20020;&#26102;&#23466;&#27861;&#30340;&#31616;&#31216;&#26159;&#20160;&#20040;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#20849;&#21516;&#32434;&#39046;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chemistry</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#22312;&#29983;&#21270;&#39046;&#22495;&#65292;g&#31995;&#21015;&#31070;&#32463;&#27602;&#32032;&#21253;&#25324;&#27801;&#26519;&#65292;&#32034;&#26364;&#65292;&#29615;&#27801;&#26519;&#21644;&#21738;&#31181;&#27602;&#32032;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#22612;&#23849;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Biology</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">125</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26377;&#20123;&#20154;&#21917;&#37202;&#23481;&#26131;&#19978;&#33080;&#65292;&#26159;&#22240;&#20026;&#20182;&#20204;&#30340;&#36523;&#20307;&#26080;&#27861;&#23558;&#20057;&#37275;&#23436;&#20840;&#36716;&#21270;&#25104;&#20160;&#20040;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#20057;&#37240;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Law</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#19968;&#20843;&#20845;&#22235;&#24180;&#21040;&#19968;&#20061;&#22235;&#20061;&#24180;&#22312;&#29790;&#22763;&#32532;&#32467;&#30340;&#20851;&#20110;&#20445;&#25252;&#24179;&#27665;&#21644;&#25112;&#20105;&#21463;&#38590;&#32773;&#30340;&#19968;&#31995;&#21015;&#22269;&#38469;&#20844;&#32422;&#30340;&#24635;&#31216;&#26159;&#20160;&#20040;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#26085;&#20869;&#29926;&#20844;&#32422;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Physics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">102</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#19990;&#30028;&#19978;&#31532;&#19968;&#20010;&#35777;&#23454;&#30005;&#27969;&#21608;&#22260;&#23384;&#22312;&#30913;&#22330;&#30340;&#29289;&#29702;&#20107;&#23454;&#26159;&#20160;&#20040;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#30005;&#29983;&#30913;&#26159;&#22885;&#26031;&#29305;&#23454;&#39564;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">History</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">150</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#19968;&#20843;&#20061;&#19971;&#24180;&#19982;&#19978;&#28023;&#26102;&#21153;&#25253;&#20998;&#38271;&#21335;&#21271;&#33286;&#35770;&#30028;&#30340;&#26159;&#21738;&#19968;&#20221;&#25253;&#21002;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#22269;&#38395;&#25253;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Medicine</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#20197;&#24403;&#24402;&#65292;&#24029;&#33422;&#65292;&#30333;&#33421;&#65292;&#29087;&#22320;&#40644;&#20026;&#20027;&#35201;&#21407;&#26009;&#65292;&#34987;&#35465;&#20026;&#22919;&#31185;&#31532;&#19968;&#26041;&#30340;&#26159;&#25105;&#22269;&#21476;&#20195;&#30340;&#21738;&#20010;&#20013;&#33647;&#26041;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#22235;&#29289;&#27748;&#12290;</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "为什么温牛奶有帮助呢？",
            "晚上试试喝点温牛奶，别吃辛辣食物，放松心情。",
            "size",
            "mathematics",
            "我国跨纬度最大的是哪个省级行政区？",
            "history",
            "慢跑让身体适应运动，减少伤害风险，逐步提高耐力是最有效的方法。",
            "最近有什么好看的科幻电影推荐吗？",
            "more",
            "乙酸。",
            "为什么要从慢跑开始而不是直接跑得久一些？",
            "vcb",
            "example",
            "国闻报。",
            "如何知道什么时候该增加跑步时间呢？",
            "一八九七年与上海时务报分长南北舆论界的是哪一份报刊？",
            "在生化领域，g系列神经毒素包括沙林，索曼，环沙林和哪种毒素？",
            "日内瓦公约。",
            "physics",
            "progression",
            "transition",
            "电生磁是奥斯特实验。",
            "law",
            "共同纲领。",
            "可以看看《流浪地球》，2023年上映，视觉效果和故事情节都很棒。",
            "examples",
            "以当归，川芎，白芍，熟地黄为主要原料，被誉为妇科第一方的是我国古代的哪个中药方？",
            "我想开始跑步锻炼，但不知道怎么入门，有什么建议吗？",
            "你刚才说哪一年上映的《流浪地球》？",
            "bench",
            "你可以从慢跑开始，设定小目标，比如每次10分钟，然后逐步增加时间和强度。",
            "二零一八年九月，英国皇家学会前主席迈克尔阿提亚宣称已经解决了哪个世界性的数学难题？",
            "你吃烧烤吗？",
            "chemistry",
            "塔崩。",
            "medicine",
            "politics",
            "这部电影的故事情节是怎样的？",
            "part",
            "最近老是失眠，有什么建议吗？",
            "四物汤。",
            "黎曼猜想。",
            "新中国第一部临时宪法的简称是什么？",
            "backtracking",
            "牛奶含色氨酸，有助于睡眠，帮助你放松。",
            "海南省。",
            "有些人喝酒容易上脸，是因为他们的身体无法将乙醛完全转化成什么。",
            "biology",
            "有没有比较刺激的场面？",
            "有地球发动机启动的场景，巨大的动力装置，震撼人心。",
            "一八六四年到一九四九年在瑞士缔结的关于保护平民和战争受难者的一系列国际公约的总称是什么？",
            "世界上第一个证实电流周围存在磁场的物理事实是什么？",
            "mtd",
            "geography",
            "讲述太阳即将毁灭，人类计划移民到别的星球，带着地球一起流浪。"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These models demonstrate rapid progress in unified audio-language modeling&#8212;covering tokenization, multimodal fusion, and real-time dialogue&#8212;but systematic benchmarks, especially for Chinese real-speech interaction, remain scarce. Current evaluations are mostly qualitative or based on synthetic data, underscoring the need for comprehensive real-speech benchmarks like VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "mtd",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MTD evaluates instruction tracking and topic management in multi-turn dialogues, each containing 3-5 turns, focusing on contextual understanding and logical coherence: (1) Progression: deepening the discussion around an initial topic to assess topic development; (2) Backtracking: recalling and responding to previously mentioned information to test long-range memory; (3) Transition: suddenly shifting to a new topic to evaluate conversational flow and relevance.</p>\n\n",
                "matched_terms": [
                    "mtd",
                    "progression",
                    "backtracking",
                    "transition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GK evaluates multi-disciplinary common sense across twelve core domains&#8212;mathematics, geography, politics, chemistry, biology, law, physics, history, medicine, economics, sports, and culture&#8212;to measure the model&#8217;s ability to recall and apply knowledge across diverse fields.</p>\n\n",
                "matched_terms": [
                    "medicine",
                    "law",
                    "history",
                    "biology",
                    "chemistry",
                    "physics",
                    "politics",
                    "geography"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "more",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T3\" title=\"Table 3 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T7\" title=\"Table 7 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows more examples of VCB Bench in different tasks.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb",
                    "more",
                    "examples"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 5: More Examples Of VCB Bench. Part III.",
        "body": "GK\nSize\n\n\nExample\n\n\n\n\nEconomics\n48\n\n\nQ: 一九八四年，新中国第一支公开发行的股票，在国内外引起了巨大反响。这只引爆市场的股票叫什么名字？\n\n\n\n\n\n\n\n\nA: 飞乐音响。\n\n\n\n\nSports\n61\n\n\nQ: 迄今为止，唯一一位获得了世界足球先生这项荣誉的非洲裔球员是谁？\n\n\n\n\n\n\n\n\nA: 乔治·维阿。\n\n\n\n\nCulture\n150\n\n\nQ: 宋真宗赵恒的励学篇中，娶妻莫恨无良媒下一句？\n\n\n\n\n\n\n\n\nA: 书中自有颜如玉。\n\n\n\n\nML\nSize\n\n\nExample\n\n\n\n\nBasic Math\n146\n\n\nQ: 一共有八个苹果，卖掉了两个，现在还有多少个苹果？\n\n\n\n\n\n\n\n\nA: 现在还有六个苹果。\n\n\n\n\nMedium Math\n170\n\n\nQ: 若a加b等于五，a减b等于三，问a与b分别是多少？\n\n\n\n\n\n\n\n\nA: a是四，b是一。\n\n\n\n\nAnalysis\n84\n\n\nQ: 小明、小华和小刚三人赛跑。小明比小华快，小华比小刚快。他们的名次是？\n\n\n\n\n\n\n\n\nA: 小明第一，小华第二，小刚第三。\n\n\n\n\nInduction\n64\n\n\nQ: 橡皮筋拉伸变长，弹簧压缩变短，橡胶带拉扯变形……这些材料表现什么特性？\n\n\n\n\n\n\n\n\nA: 弹性。因为受力后它们都改变形状并恢复，归纳得出能弹性变形。\n\n\n\n\nAnalogy\n40\n\n\nQ: 根据以下关键词，再举出一个类似的实体：树叶、树根、树干。\n\n\n\n\n\n\n\n\nA: 树皮。它们都是树木生理结构的重要构成元素。\n\n\n\n\nLogic\n159\n\n\nQ: 如果不是下雨，花园就会浇水。今天花园湿了，那么是因为下雨吗？\n\n\n\n\n\n\n\n\nA: 不一定是下雨。因为即使不下雨，花园也可能因为浇水而湿。\n\n\n\n\nSC\nSize\n\n\nExample\n\n\n\n\nLogic And Causality\n261\n\n\nQ: 张阿姨特地去市场买了河鱼，打算给孙子做最爱吃的鱼汤。可她忘记加盐，结果汤煮好后发现味道不对，尝起来淡而无味。\n\n\n\n\n\n\n\n\n(A)孙子喝了几口汤，就不太愿意再吃了。\n\n\n\n\n\n\n\n\n(B)孙子赞奶奶的鱼汤好喝，还考了100分。\n\n\n\n\n\n\n\n\nA: (A)\n\n\n\n\nCommon Sense And Science\n72\n\n\nQ: 小明用放大镜对着报纸照射阳光，纸张开始变热，冒出轻微的烟，他赶紧把放大镜移开。\n\n\n\n\n\n\n\n\n(A) 纸张燃烧了，小明用水扑灭\n\n\n\n\n\n\n\n\n(B) 纸张变成了一块闪亮的金属。\n\n\n\n\n\n\n\n\nA: (A)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">GK</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Economics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">48</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#19968;&#20061;&#20843;&#22235;&#24180;&#65292;&#26032;&#20013;&#22269;&#31532;&#19968;&#25903;&#20844;&#24320;&#21457;&#34892;&#30340;&#32929;&#31080;&#65292;&#22312;&#22269;&#20869;&#22806;&#24341;&#36215;&#20102;&#24040;&#22823;&#21453;&#21709;&#12290;&#36825;&#21482;&#24341;&#29190;&#24066;&#22330;&#30340;&#32929;&#31080;&#21483;&#20160;&#20040;&#21517;&#23383;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#39134;&#20048;&#38899;&#21709;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Sports</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#36804;&#20170;&#20026;&#27490;&#65292;&#21807;&#19968;&#19968;&#20301;&#33719;&#24471;&#20102;&#19990;&#30028;&#36275;&#29699;&#20808;&#29983;&#36825;&#39033;&#33635;&#35465;&#30340;&#38750;&#27954;&#35028;&#29699;&#21592;&#26159;&#35841;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#20052;&#27835;&#183;&#32500;&#38463;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Culture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">150</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#23435;&#30495;&#23447;&#36213;&#24658;&#30340;&#21169;&#23398;&#31687;&#20013;&#65292;&#23094;&#22971;&#33707;&#24680;&#26080;&#33391;&#23186;&#19979;&#19968;&#21477;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#20070;&#20013;&#33258;&#26377;&#39068;&#22914;&#29577;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ML</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Basic Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">146</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#19968;&#20849;&#26377;&#20843;&#20010;&#33529;&#26524;&#65292;&#21334;&#25481;&#20102;&#20004;&#20010;&#65292;&#29616;&#22312;&#36824;&#26377;&#22810;&#23569;&#20010;&#33529;&#26524;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#29616;&#22312;&#36824;&#26377;&#20845;&#20010;&#33529;&#26524;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Medium Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">170</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#33509;a&#21152;b&#31561;&#20110;&#20116;&#65292;a&#20943;b&#31561;&#20110;&#19977;&#65292;&#38382;a&#19982;b&#20998;&#21035;&#26159;&#22810;&#23569;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> a&#26159;&#22235;&#65292;b&#26159;&#19968;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Analysis</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">84</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#23567;&#26126;&#12289;&#23567;&#21326;&#21644;&#23567;&#21018;&#19977;&#20154;&#36187;&#36305;&#12290;&#23567;&#26126;&#27604;&#23567;&#21326;&#24555;&#65292;&#23567;&#21326;&#27604;&#23567;&#21018;&#24555;&#12290;&#20182;&#20204;&#30340;&#21517;&#27425;&#26159;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#23567;&#26126;&#31532;&#19968;&#65292;&#23567;&#21326;&#31532;&#20108;&#65292;&#23567;&#21018;&#31532;&#19977;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Induction</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#27233;&#30382;&#31563;&#25289;&#20280;&#21464;&#38271;&#65292;&#24377;&#31783;&#21387;&#32553;&#21464;&#30701;&#65292;&#27233;&#33014;&#24102;&#25289;&#25199;&#21464;&#24418;&#8230;&#8230;&#36825;&#20123;&#26448;&#26009;&#34920;&#29616;&#20160;&#20040;&#29305;&#24615;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#24377;&#24615;&#12290;&#22240;&#20026;&#21463;&#21147;&#21518;&#23427;&#20204;&#37117;&#25913;&#21464;&#24418;&#29366;&#24182;&#24674;&#22797;&#65292;&#24402;&#32435;&#24471;&#20986;&#33021;&#24377;&#24615;&#21464;&#24418;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Analogy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26681;&#25454;&#20197;&#19979;&#20851;&#38190;&#35789;&#65292;&#20877;&#20030;&#20986;&#19968;&#20010;&#31867;&#20284;&#30340;&#23454;&#20307;&#65306;&#26641;&#21494;&#12289;&#26641;&#26681;&#12289;&#26641;&#24178;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#26641;&#30382;&#12290;&#23427;&#20204;&#37117;&#26159;&#26641;&#26408;&#29983;&#29702;&#32467;&#26500;&#30340;&#37325;&#35201;&#26500;&#25104;&#20803;&#32032;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Logic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">159</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#22914;&#26524;&#19981;&#26159;&#19979;&#38632;&#65292;&#33457;&#22253;&#23601;&#20250;&#27975;&#27700;&#12290;&#20170;&#22825;&#33457;&#22253;&#28287;&#20102;&#65292;&#37027;&#20040;&#26159;&#22240;&#20026;&#19979;&#38632;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> &#19981;&#19968;&#23450;&#26159;&#19979;&#38632;&#12290;&#22240;&#20026;&#21363;&#20351;&#19981;&#19979;&#38632;&#65292;&#33457;&#22253;&#20063;&#21487;&#33021;&#22240;&#20026;&#27975;&#27700;&#32780;&#28287;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Logic And Causality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">261</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#24352;&#38463;&#23016;&#29305;&#22320;&#21435;&#24066;&#22330;&#20080;&#20102;&#27827;&#40060;&#65292;&#25171;&#31639;&#32473;&#23385;&#23376;&#20570;&#26368;&#29233;&#21507;&#30340;&#40060;&#27748;&#12290;&#21487;&#22905;&#24536;&#35760;&#21152;&#30416;&#65292;&#32467;&#26524;&#27748;&#29038;&#22909;&#21518;&#21457;&#29616;&#21619;&#36947;&#19981;&#23545;&#65292;&#23581;&#36215;&#26469;&#28129;&#32780;&#26080;&#21619;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">(A)&#23385;&#23376;&#21917;&#20102;&#20960;&#21475;&#27748;&#65292;&#23601;&#19981;&#22826;&#24895;&#24847;&#20877;&#21507;&#20102;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">(B)&#23385;&#23376;&#36190;&#22902;&#22902;&#30340;&#40060;&#27748;&#22909;&#21917;&#65292;&#36824;&#32771;&#20102;100&#20998;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> (A)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Common Sense And Science</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#23567;&#26126;&#29992;&#25918;&#22823;&#38236;&#23545;&#30528;&#25253;&#32440;&#29031;&#23556;&#38451;&#20809;&#65292;&#32440;&#24352;&#24320;&#22987;&#21464;&#28909;&#65292;&#20882;&#20986;&#36731;&#24494;&#30340;&#28895;&#65292;&#20182;&#36214;&#32039;&#25226;&#25918;&#22823;&#38236;&#31227;&#24320;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">(A) &#32440;&#24352;&#29123;&#28903;&#20102;&#65292;&#23567;&#26126;&#29992;&#27700;&#25169;&#28781;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">(B) &#32440;&#24352;&#21464;&#25104;&#20102;&#19968;&#22359;&#38378;&#20142;&#30340;&#37329;&#23646;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> (A)</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "迄今为止，唯一一位获得了世界足球先生这项荣誉的非洲裔球员是谁？",
            "sense",
            "medium",
            "size",
            "弹性。因为受力后它们都改变形状并恢复，归纳得出能弹性变形。",
            "math",
            "basic",
            "more",
            "乔治·维阿。",
            "橡皮筋拉伸变长，弹簧压缩变短，橡胶带拉扯变形……这些材料表现什么特性？",
            "vcb",
            "example",
            "飞乐音响。",
            "a孙子喝了几口汤，就不太愿意再吃了。",
            "analogy",
            "economics",
            "小明第一，小华第二，小刚第三。",
            "sports",
            "analysis",
            "若a加b等于五，a减b等于三，问a与b分别是多少？",
            "causality",
            "纸张燃烧了，小明用水扑灭",
            "宋真宗赵恒的励学篇中，娶妻莫恨无良媒下一句？",
            "iii",
            "小明、小华和小刚三人赛跑。小明比小华快，小华比小刚快。他们的名次是？",
            "纸张变成了一块闪亮的金属。",
            "张阿姨特地去市场买了河鱼，打算给孙子做最爱吃的鱼汤。可她忘记加盐，结果汤煮好后发现味道不对，尝起来淡而无味。",
            "induction",
            "b孙子赞奶奶的鱼汤好喝，还考了100分。",
            "common",
            "书中自有颜如玉。",
            "如果不是下雨，花园就会浇水。今天花园湿了，那么是因为下雨吗？",
            "examples",
            "bench",
            "一九八四年，新中国第一支公开发行的股票，在国内外引起了巨大反响。这只引爆市场的股票叫什么名字？",
            "part",
            "根据以下关键词，再举出一个类似的实体：树叶、树根、树干。",
            "树皮。它们都是树木生理结构的重要构成元素。",
            "小明用放大镜对着报纸照射阳光，纸张开始变热，冒出轻微的烟，他赶紧把放大镜移开。",
            "一共有八个苹果，卖掉了两个，现在还有多少个苹果？",
            "science",
            "culture",
            "现在还有六个苹果。",
            "a是四，b是一。",
            "logic",
            "不一定是下雨。因为即使不下雨，花园也可能因为浇水而湿。"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These models demonstrate rapid progress in unified audio-language modeling&#8212;covering tokenization, multimodal fusion, and real-time dialogue&#8212;but systematic benchmarks, especially for Chinese real-speech interaction, remain scarce. Current evaluations are mostly qualitative or based on synthetic data, underscoring the need for comprehensive real-speech benchmarks like VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AIR Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>)</cite> contains two components&#8212;a basic benchmark covering emotion recognition, ASR, and age estimation, and a dialogue benchmark evaluating auditory understanding and internal knowledge. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib10\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib15\" title=\"\">2025</a>)</cite> focus on deep audio reasoning, requiring multi-step inference grounded in internal audio knowledge. OmniBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib13\" title=\"\">2024</a>)</cite> targets omni-modal models handling audio, images, and text, where text queries are paired with multimodal contexts (speech, music, or sound) to test integrated reasoning. Finally, URO Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib24\" title=\"\">2025</a>)</cite> provides a bilingual (English-Chinese) comprehensive set that evaluates audio understanding, reasoning, and conversational ability&#8212;but the speech data are entirely synthetic (TTS-generated).</p>\n\n",
                "matched_terms": [
                    "bench",
                    "basic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "logic",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Internal Two-person Dialogue Dataset.</span> Designed to support the DC module, this category&#8217;s data is processed as follows: the original long-form audio undergoes a two-stage segmentation with GPT-4O&#8212;first by topic, then refined into semantically coherent segments under one minute. From the transcriptions, GPT-4O generates task-specific QA pairs (e.g., for analysis or induction), followed by a final manual screening to verify question quality and answer accuracy.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GK evaluates multi-disciplinary common sense across twelve core domains&#8212;mathematics, geography, politics, chemistry, biology, law, physics, history, medicine, economics, sports, and culture&#8212;to measure the model&#8217;s ability to recall and apply knowledge across diverse fields.</p>\n\n",
                "matched_terms": [
                    "sense",
                    "economics",
                    "sports",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ML module consists of two key components: Mathematics and Logical Reasoning. Mathematics is divided into Basic Math, which is confined to integer arithmetic within 100, and Medium Math, which includes advanced algebra, geometry, number theory, and related disciplines, collectively assessing computational and problem-solving skills. Logical Reasoning comprises four reasoning types: Analysis for breaking down information, Induction for identifying and generalizing patterns, Analogy for mapping relational correspondences, and Logic for executing conditional reasoning, thereby testing analytical and deductive capabilities.</p>\n\n",
                "matched_terms": [
                    "medium",
                    "analogy",
                    "analysis",
                    "logic",
                    "math",
                    "basic",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DC focuses on understanding dialogues through three dedicated tasks: Analysis detects factual accuracy within dialogues, Induction summarizes overarching dialogue themes, and Inference deduces speakers&#8217; attitudes, emotions, or intents, together evaluating comprehension and implicit reasoning skills.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SC, inspired by StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib16\" title=\"\">2016</a>)</cite>, assesses implicit reasoning by requiring the model to select the correct story ending from two candidates, where both the context and the candidate endings are provided in the same modality&#8212;either all in audio or all in text. This task spans three evaluative categories: Logic and Causality for causal consistency, Common Sense and Science for real-world and scientific knowledge alignment, and Morality and Emotion for moral and emotional coherence.</p>\n\n",
                "matched_terms": [
                    "sense",
                    "science",
                    "causality",
                    "logic",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "more",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T3\" title=\"Table 3 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T7\" title=\"Table 7 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows more examples of VCB Bench in different tasks.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb",
                    "more",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "medium",
                    "analogy",
                    "analysis",
                    "logic",
                    "math",
                    "basic",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 6: More Examples Of VCB Bench. Part IV.",
        "body": "SC\nSize\n\n\nExample\n\n\n\n\n\n\nMorality And Emotion\n49\n\n\nQ: 李明在公交车上看到一位老人上车，无空座，他犹豫是否要让座，但想到自己也很疲惫。\n\n\n\n\n\n\n\n\n(A) 李明主动起身让座。\n\n\n\n\n\n\n\n\n(B) 李明假装睡着不理会。\n\n\n\n\n\n\n\n\nA: (A)\n\n\n\n\nDC\nSize\n\n\nExample\n\n\n\n\nAnalysis\n115\n\n\n[A] 嗯，那你爸爸妈妈现在多大了呀？\n\n\n\n\n\n\n\n\n[B] 我，我爸，我爸，我妈是四十四，十七了，然后我妈，我爸是四十三，四十三四，四十四。\n\n\n\n\n\n\n\n\n[A] 那我我爸，我妈要比你爸妈妈要大一些。\n\n\n\n\n\n\n\n\n[B] 对，因为你不是还有个姐姐嘛。\n\n\n\n\n\n\n\n\n[A] 对呀，我爸，我妈，今年爸妈，今年都五十一岁啦。\n\n\n\n\n\n\n\n\n[B] 五十一啊，那我感觉你爸妈好像结婚也挺早的吧。\n\n\n\n\n\n\n\n\n[A] 不早啦，他们二十四岁才结婚哪。\n\n\n\n\n\n\n\n\n[B] 你姐现在多大呀？\n\n\n\n\n\n\n\n\nQ: 记最先说话的说话者为A，后说话的说话者为B。根据对话内容，下列哪个选项是错误的？\n\n\n\n\n\n\n\n\nA) B的妈妈比爸爸年长\n\n\n\n\n\n\n\n\nB) A的爸爸比B的爸爸大7岁\n\n\n\n\n\n\n\n\nC) A比B年长\n\n\n\n\n\n\n\n\nD) A不是独生子女\n\n\n\n\n\n\n\n\nA: C) A比B年长\n\n\n\n\nInduction\n113\n\n\n[A] 那边儿有什么好玩儿的呀，\n\n\n\n\n\n\n\n\n[B] 啊，反正就是去了，那个，反正那个。桂林是山水嘛，挺美的，那边景色都。\n\n\n\n\n\n\n\n\n[A] 是，不，是水特别清，特，特别绿，\n\n\n\n\n\n\n\n\n[B] 嗯，对，对，对，还去坐了船，那水都清亮亮的。就跟你说，就是那种嗯，就是能透透看见，就是绿绿的。他都是，就是特别清澈，\n\n\n\n\n\n\n\n\n[B] 还有坐了船，艇\n\n\n\n\n\n\n\n\n[A] 做的啥样的船呀，\n\n\n\n\n\n\n\n\n[B] 那就是那种，也不是船，就是那种。竹筏子，\n\n\n\n\n\n\n\n\n[A] 那还用你自己划吗，\n\n\n\n\n\n\n\n\n[B] 嗯，不用人，那就有机器划，就是，但是那种是竹，筏子就坐，\n\n\n\n\n\n\n\n\n[A] 那安全吗\n\n\n\n\n\n\n\n\n[B] 还行给一给一个那种那个那个，一\n\n\n\n\n\n\n\n\n[B] 就是上船，穿那个东西，就是嗯，那个救生圈。我也不知道那是啥。反正就是给穿的，那个挺安全的。还行。\n\n\n\n\n\n\n\n\nQ: 记最先说话的说话者为A，后说话的说话者为B。请完成下面的单项选择题。根据对话内容，总结对话主题。\n\n\n\n\n\n\n\n\nA) 旅游。\n\n\n\n\n\n\n\n\nB) 船舶。\n\n\n\n\n\n\n\n\nC) 水源。\n\n\n\n\n\n\n\n\nD) 安全。\n\n\n\n\n\n\n\n\nA: A) 旅游。",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Morality And Emotion</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#26446;&#26126;&#22312;&#20844;&#20132;&#36710;&#19978;&#30475;&#21040;&#19968;&#20301;&#32769;&#20154;&#19978;&#36710;&#65292;&#26080;&#31354;&#24231;&#65292;&#20182;&#29369;&#35947;&#26159;&#21542;&#35201;&#35753;&#24231;&#65292;&#20294;&#24819;&#21040;&#33258;&#24049;&#20063;&#24456;&#30130;&#24811;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">(A) &#26446;&#26126;&#20027;&#21160;&#36215;&#36523;&#35753;&#24231;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">(B) &#26446;&#26126;&#20551;&#35013;&#30561;&#30528;&#19981;&#29702;&#20250;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> (A)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">DC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Analysis</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">115</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#21999;&#65292;&#37027;&#20320;&#29240;&#29240;&#22920;&#22920;&#29616;&#22312;&#22810;&#22823;&#20102;&#21568;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#25105;&#65292;&#25105;&#29240;&#65292;&#25105;&#29240;&#65292;&#25105;&#22920;&#26159;&#22235;&#21313;&#22235;&#65292;&#21313;&#19971;&#20102;&#65292;&#28982;&#21518;&#25105;&#22920;&#65292;&#25105;&#29240;&#26159;&#22235;&#21313;&#19977;&#65292;&#22235;&#21313;&#19977;&#22235;&#65292;&#22235;&#21313;&#22235;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#37027;&#25105;&#25105;&#29240;&#65292;&#25105;&#22920;&#35201;&#27604;&#20320;&#29240;&#22920;&#22920;&#35201;&#22823;&#19968;&#20123;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#23545;&#65292;&#22240;&#20026;&#20320;&#19981;&#26159;&#36824;&#26377;&#20010;&#22992;&#22992;&#22043;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#23545;&#21568;&#65292;&#25105;&#29240;&#65292;&#25105;&#22920;&#65292;&#20170;&#24180;&#29240;&#22920;&#65292;&#20170;&#24180;&#37117;&#20116;&#21313;&#19968;&#23681;&#21862;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#20116;&#21313;&#19968;&#21834;&#65292;&#37027;&#25105;&#24863;&#35273;&#20320;&#29240;&#22920;&#22909;&#20687;&#32467;&#23130;&#20063;&#25402;&#26089;&#30340;&#21543;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#19981;&#26089;&#21862;&#65292;&#20182;&#20204;&#20108;&#21313;&#22235;&#23681;&#25165;&#32467;&#23130;&#21738;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#20320;&#22992;&#29616;&#22312;&#22810;&#22823;&#21568;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#35760;&#26368;&#20808;&#35828;&#35805;&#30340;&#35828;&#35805;&#32773;&#20026;A&#65292;&#21518;&#35828;&#35805;&#30340;&#35828;&#35805;&#32773;&#20026;B&#12290;&#26681;&#25454;&#23545;&#35805;&#20869;&#23481;&#65292;&#19979;&#21015;&#21738;&#20010;&#36873;&#39033;&#26159;&#38169;&#35823;&#30340;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">A) B&#30340;&#22920;&#22920;&#27604;&#29240;&#29240;&#24180;&#38271;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">B) A&#30340;&#29240;&#29240;&#27604;B&#30340;&#29240;&#29240;&#22823;7&#23681;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">C) A&#27604;B&#24180;&#38271;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">D) A&#19981;&#26159;&#29420;&#29983;&#23376;&#22899;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> C) A&#27604;B&#24180;&#38271;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Induction</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">113</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#37027;&#36793;&#20799;&#26377;&#20160;&#20040;&#22909;&#29609;&#20799;&#30340;&#21568;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#21834;&#65292;&#21453;&#27491;&#23601;&#26159;&#21435;&#20102;&#65292;&#37027;&#20010;&#65292;&#21453;&#27491;&#37027;&#20010;&#12290;&#26690;&#26519;&#26159;&#23665;&#27700;&#22043;&#65292;&#25402;&#32654;&#30340;&#65292;&#37027;&#36793;&#26223;&#33394;&#37117;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#26159;&#65292;&#19981;&#65292;&#26159;&#27700;&#29305;&#21035;&#28165;&#65292;&#29305;&#65292;&#29305;&#21035;&#32511;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#21999;&#65292;&#23545;&#65292;&#23545;&#65292;&#23545;&#65292;&#36824;&#21435;&#22352;&#20102;&#33337;&#65292;&#37027;&#27700;&#37117;&#28165;&#20142;&#20142;&#30340;&#12290;&#23601;&#36319;&#20320;&#35828;&#65292;&#23601;&#26159;&#37027;&#31181;&#21999;&#65292;&#23601;&#26159;&#33021;&#36879;&#36879;&#30475;&#35265;&#65292;&#23601;&#26159;&#32511;&#32511;&#30340;&#12290;&#20182;&#37117;&#26159;&#65292;&#23601;&#26159;&#29305;&#21035;&#28165;&#28552;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#36824;&#26377;&#22352;&#20102;&#33337;&#65292;&#33351;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#20570;&#30340;&#21861;&#26679;&#30340;&#33337;&#21568;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#37027;&#23601;&#26159;&#37027;&#31181;&#65292;&#20063;&#19981;&#26159;&#33337;&#65292;&#23601;&#26159;&#37027;&#31181;&#12290;&#31481;&#31567;&#23376;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#37027;&#36824;&#29992;&#20320;&#33258;&#24049;&#21010;&#21527;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#21999;&#65292;&#19981;&#29992;&#20154;&#65292;&#37027;&#23601;&#26377;&#26426;&#22120;&#21010;&#65292;&#23601;&#26159;&#65292;&#20294;&#26159;&#37027;&#31181;&#26159;&#31481;&#65292;&#31567;&#23376;&#23601;&#22352;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#37027;&#23433;&#20840;&#21527;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#36824;&#34892;&#32473;&#19968;&#32473;&#19968;&#20010;&#37027;&#31181;&#37027;&#20010;&#37027;&#20010;&#65292;&#19968;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#23601;&#26159;&#19978;&#33337;&#65292;&#31359;&#37027;&#20010;&#19996;&#35199;&#65292;&#23601;&#26159;&#21999;&#65292;&#37027;&#20010;&#25937;&#29983;&#22280;&#12290;&#25105;&#20063;&#19981;&#30693;&#36947;&#37027;&#26159;&#21861;&#12290;&#21453;&#27491;&#23601;&#26159;&#32473;&#31359;&#30340;&#65292;&#37027;&#20010;&#25402;&#23433;&#20840;&#30340;&#12290;&#36824;&#34892;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#35760;&#26368;&#20808;&#35828;&#35805;&#30340;&#35828;&#35805;&#32773;&#20026;A&#65292;&#21518;&#35828;&#35805;&#30340;&#35828;&#35805;&#32773;&#20026;B&#12290;&#35831;&#23436;&#25104;&#19979;&#38754;&#30340;&#21333;&#39033;&#36873;&#25321;&#39064;&#12290;&#26681;&#25454;&#23545;&#35805;&#20869;&#23481;&#65292;&#24635;&#32467;&#23545;&#35805;&#20027;&#39064;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">A) &#26053;&#28216;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">B) &#33337;&#33334;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">C) &#27700;&#28304;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">D) &#23433;&#20840;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> A) &#26053;&#28216;&#12290;</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "嗯，不用人，那就有机器划，就是，但是那种是竹，筏子就坐，",
            "水源。",
            "size",
            "记最先说话的说话者为a，后说话的说话者为b。请完成下面的单项选择题。根据对话内容，总结对话主题。",
            "五十一啊，那我感觉你爸妈好像结婚也挺早的吧。",
            "安全。",
            "more",
            "对，因为你不是还有个姐姐嘛。",
            "a的爸爸比b的爸爸大7岁",
            "那边儿有什么好玩儿的呀，",
            "你姐现在多大呀？",
            "嗯，那你爸爸妈妈现在多大了呀？",
            "李明在公交车上看到一位老人上车，无空座，他犹豫是否要让座，但想到自己也很疲惫。",
            "vcb",
            "example",
            "李明假装睡着不理会。",
            "还行给一给一个那种那个那个，一",
            "船舶。",
            "那就是那种，也不是船，就是那种。竹筏子，",
            "analysis",
            "记最先说话的说话者为a，后说话的说话者为b。根据对话内容，下列哪个选项是错误的？",
            "不早啦，他们二十四岁才结婚哪。",
            "induction",
            "morality",
            "a比b年长",
            "b的妈妈比爸爸年长",
            "examples",
            "我，我爸，我爸，我妈是四十四，十七了，然后我妈，我爸是四十三，四十三四，四十四。",
            "做的啥样的船呀，",
            "旅游。",
            "bench",
            "a不是独生子女",
            "还有坐了船，艇",
            "part",
            "李明主动起身让座。",
            "那还用你自己划吗，",
            "是，不，是水特别清，特，特别绿，",
            "emotion",
            "对呀，我爸，我妈，今年爸妈，今年都五十一岁啦。",
            "那安全吗",
            "就是上船，穿那个东西，就是嗯，那个救生圈。我也不知道那是啥。反正就是给穿的，那个挺安全的。还行。",
            "啊，反正就是去了，那个，反正那个。桂林是山水嘛，挺美的，那边景色都。",
            "那我我爸，我妈要比你爸妈妈要大一些。",
            "嗯，对，对，对，还去坐了船，那水都清亮亮的。就跟你说，就是那种嗯，就是能透透看见，就是绿绿的。他都是，就是特别清澈，"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These models demonstrate rapid progress in unified audio-language modeling&#8212;covering tokenization, multimodal fusion, and real-time dialogue&#8212;but systematic benchmarks, especially for Chinese real-speech interaction, remain scarce. Current evaluations are mostly qualitative or based on synthetic data, underscoring the need for comprehensive real-speech benchmarks like VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AIR Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>)</cite> contains two components&#8212;a basic benchmark covering emotion recognition, ASR, and age estimation, and a dialogue benchmark evaluating auditory understanding and internal knowledge. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib10\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib15\" title=\"\">2025</a>)</cite> focus on deep audio reasoning, requiring multi-step inference grounded in internal audio knowledge. OmniBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib13\" title=\"\">2024</a>)</cite> targets omni-modal models handling audio, images, and text, where text queries are paired with multimodal contexts (speech, music, or sound) to test integrated reasoning. Finally, URO Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib24\" title=\"\">2025</a>)</cite> provides a bilingual (English-Chinese) comprehensive set that evaluates audio understanding, reasoning, and conversational ability&#8212;but the speech data are entirely synthetic (TTS-generated).</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "bench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Internal Two-person Dialogue Dataset.</span> Designed to support the DC module, this category&#8217;s data is processed as follows: the original long-form audio undergoes a two-stage segmentation with GPT-4O&#8212;first by topic, then refined into semantically coherent segments under one minute. From the transcriptions, GPT-4O generates task-specific QA pairs (e.g., for analysis or induction), followed by a final manual screening to verify question quality and answer accuracy.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ML module consists of two key components: Mathematics and Logical Reasoning. Mathematics is divided into Basic Math, which is confined to integer arithmetic within 100, and Medium Math, which includes advanced algebra, geometry, number theory, and related disciplines, collectively assessing computational and problem-solving skills. Logical Reasoning comprises four reasoning types: Analysis for breaking down information, Induction for identifying and generalizing patterns, Analogy for mapping relational correspondences, and Logic for executing conditional reasoning, thereby testing analytical and deductive capabilities.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DC focuses on understanding dialogues through three dedicated tasks: Analysis detects factual accuracy within dialogues, Induction summarizes overarching dialogue themes, and Inference deduces speakers&#8217; attitudes, emotions, or intents, together evaluating comprehension and implicit reasoning skills.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SC, inspired by StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib16\" title=\"\">2016</a>)</cite>, assesses implicit reasoning by requiring the model to select the correct story ending from two candidates, where both the context and the candidate endings are provided in the same modality&#8212;either all in audio or all in text. This task spans three evaluative categories: Logic and Causality for causal consistency, Common Sense and Science for real-world and scientific knowledge alignment, and Morality and Emotion for moral and emotional coherence.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "morality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "more",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T3\" title=\"Table 3 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T7\" title=\"Table 7 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows more examples of VCB Bench in different tasks.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb",
                    "more",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 7: More Examples Of VCB Bench. Part V.",
        "body": "DC\nSize\n\n\nExample\n\n\n\n\n\n\nInference\n103\n\n\n[B] 就让我觉得\n\n\n\n\n\n\n\n\n[B] 哎，你最近，我最近想跟他们一块去健身，来着娱乐\n\n\n\n\n\n\n\n\n[B] 兴趣嘞\n\n\n\n\n\n\n\n\n[A] 健身啊\n\n\n\n\n\n\n\n\n[B] 啊\n\n\n\n\n\n\n\n\n[A] 噢，你想去健身啊，你可以办一个那个\n\n\n\n\n\n\n\n\n[B] 他们有卡，我是想直接拿，我才不会花钱办嘞，没钱，\n\n\n\n\n\n\n\n\n[A] 嗯\n\n\n\n\n\n\n\n\nQ: 记最先说话的说话者为B，后说话的说话者为A。请完成下面的单项选择题。从对话中可以推断出，B对健身的态度是怎样的？\n\n\n\n\n\n\n\n\nA) 非常热衷，愿意花钱办卡\n\n\n\n\n\n\n\n\nB) 已经有自己的健身计划\n\n\n\n\n\n\n\n\nC) 完全没有兴趣\n\n\n\n\n\n\n\n\nD) 有兴趣，但不愿意花钱办卡\n\n\n\n\n\n\n\n\nA: D) 有兴趣，但不愿意花钱办卡\n\n\n\n\nCV\nSize\n\n\nExample\n\n\n\n\nFillers\n100\n\n\n【嘿嘿】，海底捞出的沉船宝箱里，【呃】除了金币还躺着枚会流泪的珍珠，【呃】后面发生了什么？\n\n\n\n\nRepetition\n87\n\n\n被室友排挤了，她们【都都，都】不喜欢我…\n\n\n\n\nMispronunciation\n89\n\n\n皮肤容易过敏起疹子，能介绍几款专业医疗级别的【修糊】产品吗？\n\n\n\n\nGrammatical Error\n69\n\n\n健身时分心【玩手机总是容易】，有什么【专注力的好方法提高吗】？\n\n\n\n\nTopic Shift\n91\n\n\n有点好奇植物世界，给我推荐几本正经的植物学教材。【算了，太枯燥了我肯定看不下去，还是推荐那种图文并茂的科普书吧。】\n\n\n\n\nCode Switching\n92\n\n\n想自己做 【Italian pasta】，新手需要准备哪些 【ingredients 】和厨具？步骤简单吗？",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">DC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Inference</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">103</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#23601;&#35753;&#25105;&#35273;&#24471;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#21710;&#65292;&#20320;&#26368;&#36817;&#65292;&#25105;&#26368;&#36817;&#24819;&#36319;&#20182;&#20204;&#19968;&#22359;&#21435;&#20581;&#36523;&#65292;&#26469;&#30528;&#23089;&#20048;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#20852;&#36259;&#22046;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#20581;&#36523;&#21834;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#21834;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#22114;&#65292;&#20320;&#24819;&#21435;&#20581;&#36523;&#21834;&#65292;&#20320;&#21487;&#20197;&#21150;&#19968;&#20010;&#37027;&#20010;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[B] &#20182;&#20204;&#26377;&#21345;&#65292;&#25105;&#26159;&#24819;&#30452;&#25509;&#25343;&#65292;&#25105;&#25165;&#19981;&#20250;&#33457;&#38065;&#21150;&#22046;&#65292;&#27809;&#38065;&#65292;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">[A] &#21999;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Q:</span> &#35760;&#26368;&#20808;&#35828;&#35805;&#30340;&#35828;&#35805;&#32773;&#20026;B&#65292;&#21518;&#35828;&#35805;&#30340;&#35828;&#35805;&#32773;&#20026;A&#12290;&#35831;&#23436;&#25104;&#19979;&#38754;&#30340;&#21333;&#39033;&#36873;&#25321;&#39064;&#12290;&#20174;&#23545;&#35805;&#20013;&#21487;&#20197;&#25512;&#26029;&#20986;&#65292;B&#23545;&#20581;&#36523;&#30340;&#24577;&#24230;&#26159;&#24590;&#26679;&#30340;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">A) &#38750;&#24120;&#28909;&#34935;&#65292;&#24895;&#24847;&#33457;&#38065;&#21150;&#21345;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">B) &#24050;&#32463;&#26377;&#33258;&#24049;&#30340;&#20581;&#36523;&#35745;&#21010;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">C) &#23436;&#20840;&#27809;&#26377;&#20852;&#36259;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">D) &#26377;&#20852;&#36259;&#65292;&#20294;&#19981;&#24895;&#24847;&#33457;&#38065;&#21150;&#21345;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A:</span> D) &#26377;&#20852;&#36259;&#65292;&#20294;&#19981;&#24895;&#24847;&#33457;&#38065;&#21150;&#21345;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CV</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Fillers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#12304;&#22079;&#22079;&#12305;&#65292;&#28023;&#24213;&#25438;&#20986;&#30340;&#27785;&#33337;&#23453;&#31665;&#37324;&#65292;&#12304;&#21571;&#12305;&#38500;&#20102;&#37329;&#24065;&#36824;&#36538;&#30528;&#26522;&#20250;&#27969;&#27882;&#30340;&#29645;&#29664;&#65292;&#12304;&#21571;&#12305;&#21518;&#38754;&#21457;&#29983;&#20102;&#20160;&#20040;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Repetition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">87</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#34987;&#23460;&#21451;&#25490;&#25380;&#20102;&#65292;&#22905;&#20204;&#12304;&#37117;&#37117;&#65292;&#37117;&#12305;&#19981;&#21916;&#27426;&#25105;&#8230;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mispronunciation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">89</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#30382;&#32932;&#23481;&#26131;&#36807;&#25935;&#36215;&#30137;&#23376;&#65292;&#33021;&#20171;&#32461;&#20960;&#27454;&#19987;&#19994;&#21307;&#30103;&#32423;&#21035;&#30340;&#12304;&#20462;&#31946;&#12305;&#20135;&#21697;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Grammatical Error</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#20581;&#36523;&#26102;&#20998;&#24515;&#12304;&#29609;&#25163;&#26426;&#24635;&#26159;&#23481;&#26131;&#12305;&#65292;&#26377;&#20160;&#20040;&#12304;&#19987;&#27880;&#21147;&#30340;&#22909;&#26041;&#27861;&#25552;&#39640;&#21527;&#12305;&#65311;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Topic Shift</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">91</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#26377;&#28857;&#22909;&#22855;&#26893;&#29289;&#19990;&#30028;&#65292;&#32473;&#25105;&#25512;&#33616;&#20960;&#26412;&#27491;&#32463;&#30340;&#26893;&#29289;&#23398;&#25945;&#26448;&#12290;&#12304;&#31639;&#20102;&#65292;&#22826;&#26543;&#29157;&#20102;&#25105;&#32943;&#23450;&#30475;&#19981;&#19979;&#21435;&#65292;&#36824;&#26159;&#25512;&#33616;&#37027;&#31181;&#22270;&#25991;&#24182;&#33538;&#30340;&#31185;&#26222;&#20070;&#21543;&#12290;&#12305;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Code Switching</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">92</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">&#24819;&#33258;&#24049;&#20570; &#12304;Italian pasta&#12305;&#65292;&#26032;&#25163;&#38656;&#35201;&#20934;&#22791;&#21738;&#20123; &#12304;ingredients &#12305;&#21644;&#21416;&#20855;&#65311;&#27493;&#39588;&#31616;&#21333;&#21527;&#65311;</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "皮肤容易过敏起疹子，能介绍几款专业医疗级别的【修糊】产品吗？",
            "size",
            "mispronunciation",
            "健身啊",
            "switching",
            "topic",
            "】和厨具？步骤简单吗？",
            "grammatical",
            "more",
            "【嘿嘿】，海底捞出的沉船宝箱里，【呃】除了金币还躺着枚会流泪的珍珠，【呃】后面发生了什么？",
            "健身时分心【玩手机总是容易】，有什么【专注力的好方法提高吗】？",
            "vcb",
            "example",
            "shift",
            "repetition",
            "有兴趣，但不愿意花钱办卡",
            "pasta】，新手需要准备哪些",
            "fillers",
            "【italian",
            "就让我觉得",
            "噢，你想去健身啊，你可以办一个那个",
            "非常热衷，愿意花钱办卡",
            "examples",
            "兴趣嘞",
            "bench",
            "记最先说话的说话者为b，后说话的说话者为a。请完成下面的单项选择题。从对话中可以推断出，b对健身的态度是怎样的？",
            "【ingredients",
            "已经有自己的健身计划",
            "想自己做",
            "code",
            "他们有卡，我是想直接拿，我才不会花钱办嘞，没钱，",
            "part",
            "有点好奇植物世界，给我推荐几本正经的植物学教材。【算了，太枯燥了我肯定看不下去，还是推荐那种图文并茂的科普书吧。】",
            "被室友排挤了，她们【都都，都】不喜欢我…",
            "完全没有兴趣",
            "inference",
            "哎，你最近，我最近想跟他们一块去健身，来着娱乐",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T3\" title=\"Table 3 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T7\" title=\"Table 7 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows more examples of VCB Bench in different tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents</span>\n</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "bench",
                    "grammatical",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These models demonstrate rapid progress in unified audio-language modeling&#8212;covering tokenization, multimodal fusion, and real-time dialogue&#8212;but systematic benchmarks, especially for Chinese real-speech interaction, remain scarce. Current evaluations are mostly qualitative or based on synthetic data, underscoring the need for comprehensive real-speech benchmarks like VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AIR Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>)</cite> contains two components&#8212;a basic benchmark covering emotion recognition, ASR, and age estimation, and a dialogue benchmark evaluating auditory understanding and internal knowledge. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib10\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib15\" title=\"\">2025</a>)</cite> focus on deep audio reasoning, requiring multi-step inference grounded in internal audio knowledge. OmniBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib13\" title=\"\">2024</a>)</cite> targets omni-modal models handling audio, images, and text, where text queries are paired with multimodal contexts (speech, music, or sound) to test integrated reasoning. Finally, URO Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib24\" title=\"\">2025</a>)</cite> provides a bilingual (English-Chinese) comprehensive set that evaluates audio understanding, reasoning, and conversational ability&#8212;but the speech data are entirely synthetic (TTS-generated).</p>\n\n",
                "matched_terms": [
                    "bench",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CV introduce linguistic disruptions: (1) Fillers: incorporate discourse markers (e.g., \"um\", \"ah\"); (2) Repetition: include repeated phrases/words; (3) Mispronunciation: introduce phonetic deviations; (4) Grammatical Error: employ ungrammatical constructions; (5) Topic Shift: implement abrupt topic changes; (6) Code-Switching: mix Chinese and English. Each category evaluates the model&#8217;s ability to maintain comprehension despite content imperfections.</p>\n\n",
                "matched_terms": [
                    "shift",
                    "repetition",
                    "fillers",
                    "mispronunciation",
                    "topic",
                    "grammatical",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "more",
                    "vcb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "fillers",
                    "mispronunciation",
                    "grammatical",
                    "error"
                ]
            }
        ]
    },
    "A1.T8": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 8: Chinese Text Side Instruction Following Objective Results.",
        "body": "Model\\Task\nAvg.\nContinuation\nCreation\nEmpathy\nRecommendation\nRewriting\nSafety\nSimulation\n\n\n\n\nGLM4-Voice\n85.82\n83.20\n88.80\n81.40\n92.60\n90.00\n72.40\n93.20\n\n\nKimi-Audio\n85.13\n77.20\n88.40\n74.60\n88.60\n90.80\n87.00\n90.20\n\n\nQwen2.5-Omni\n87.40\n80.80\n88.60\n79.20\n86.00\n86.80\n93.40\n96.80\n\n\nBaichuan-Audio-Chat\n72.49\n70.80\n72.40\n60.80\n77.60\n63.80\n76.20\n84.20\n\n\nQwen2-Audio-Instruct\n84.56\n79.40\n85.80\n77.40\n91.80\n81.40\n83.40\n92.20\n\n\nStepAudio\n87.17\n87.40\n90.40\n73.40\n92.80\n87.80\n81.80\n96.60\n\n\nStepAudio2Mini\n82.79\n82.60\n82.20\n77.60\n90.60\n87.60\n65.40\n94.40\n\n\nMimo-Audio\n91.21\n92.20\n94.00\n85.00\n99.00\n92.80\n76.60\n99.20\n\n\nGPT4o-Audio\n91.24\n89.40\n93.40\n85.00\n95.20\n91.80\n85.80\n98.20",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Continuation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Creation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Empathy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Recommendation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Rewriting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Safety</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Simulation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">93.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">92.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">94.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">99.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">92.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">99.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">95.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">98.20</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "recommendation",
            "gpt4oaudio",
            "text",
            "avg",
            "simulation",
            "qwen25omni",
            "objective",
            "baichuanaudiochat",
            "glm4voice",
            "stepaudio2mini",
            "empathy",
            "instruction",
            "following",
            "results",
            "side",
            "chinese",
            "safety",
            "kimiaudio",
            "qwen2audioinstruct",
            "creation",
            "stepaudio",
            "continuation",
            "rewriting",
            "mimoaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "continuation",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Benchmarks.</span> Recent efforts have introduced several benchmarks to evaluate LALMs from different perspectives. VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>)</cite> assesses general knowledge, instruction adherence, safety, and robustness, mainly based on existing text datasets such as AlpacaEval and SD-QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Faisal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib8\" title=\"\">2021</a>)</cite>, with lengthy or highly complex samples removed. OpenAudioBench, released alongside Baichuan-Audio, integrates question-answering datasets including Spoken LLaMA Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib17\" title=\"\">2023</a>)</cite> and Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib3\" title=\"\">2013</a>)</cite>, and augments them with a TTS-generated reasoning subset.</p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "text",
                    "continuation",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "text",
                    "safety",
                    "simulation",
                    "empathy",
                    "continuation",
                    "rewriting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "chinese",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "text",
                    "mimoaudio",
                    "stepaudio2mini",
                    "chinese",
                    "stepaudio",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "text",
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "stepaudio2mini",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "text",
                    "chinese",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "glm4voice",
                    "objective",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "objective",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "mimoaudio",
                    "chinese",
                    "empathy",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio",
                    "objective",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "text",
                    "stepaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "text",
                    "chinese",
                    "continuation",
                    "gpt4oaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "continuation",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            }
        ]
    },
    "A1.T9": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 9: Chinese Speech Side Instruction Following Objective Results.",
        "body": "Model\\Task\nAvg.\nEmotional Control\nLanguage Control\nNon Verbal Vocalization\nPacing Control\nStyle Control\nVolume Control\n\n\n\n\nGLM4-Voice\n85.57\n93.00\n78.40\n68.00\n83.60\n92.60\n92.60\n\n\nKimi-Audio\n85.69\n88.80\n79.20\n64.20\n88.20\n95.00\n92.40\n\n\nQwen2.5-Omni\n71.37\n83.00\n65.80\n55.80\n55.40\n82.20\n83.40\n\n\nBaichuan-Audio-Chat\n78.96\n85.60\n72.40\n60.40\n81.80\n87.40\n81.00\n\n\nStepAudio\n80.25\n88.40\n72.40\n61.20\n80.00\n87.80\n86.00\n\n\nStepAudio2Mini\n78.81\n87.80\n70.40\n58.80\n84.20\n85.40\n79.80\n\n\nMimo-Audio\n72.89\n81.80\n74.40\n54.00\n47.60\n88.80\n88.40\n\n\nGPT4o-Audio\n88.15\n92.40\n87.80\n75.00\n83.20\n92.00\n94.00",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Emotional Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Language Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Non Verbal Vocalization</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pacing Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Style Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Volume Control</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">93.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">88.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">95.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">88.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">75.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">94.00</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "emotional",
            "pacing",
            "gpt4oaudio",
            "volume",
            "avg",
            "qwen25omni",
            "objective",
            "baichuanaudiochat",
            "speech",
            "glm4voice",
            "verbal",
            "stepaudio2mini",
            "non",
            "instruction",
            "following",
            "results",
            "language",
            "side",
            "chinese",
            "kimiaudio",
            "vocalization",
            "mimoaudio",
            "stepaudio",
            "style",
            "control"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "control",
                    "chinese",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "volume",
                    "control",
                    "chinese",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large Audio Language Models.</span> Recent LALMs primarily adopt an E2E audio-language modeling paradigm, integrating speech understanding and generation within a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "volume",
                    "control",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "volume",
                    "control",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIF focuses on understanding and executing speech instructions, particularly the ability to handle paralinguistic features such as emotion, speaking rate, and dialect. It includes six sub-tasks: (1) Emotional Control: adjusting the emotional tone of speech to assess expressive generation; (2) Language Control: switching languages or dialects to test multilingual synthesis; (3) Non-verbal Vocalization: incorporating non-linguistic elements like sighs or nasal sounds to evaluate paralinguistic expressiveness; (4) Pacing Control: modifying speaking rate to examine control precision; (5) Style Control: switching speech styles to assess style transfer; (6) Volume Control: adjusting loudness to test stability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotional",
                    "pacing",
                    "vocalization",
                    "volume",
                    "style",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SV examine model adaptation to speaker attributes: (1) Age: utilizes child and elderly speech to assess recognition of age-related vocal characteristics; (2) Accent: incorporates four regional accents (Tianjin, Beijing, Dongbei, Sichuan) to evaluate comprehension of non-standard Mandarin; (3) Volume: assesses perception stability with amplified/attenuated speech; (4) Speed: tests parsing capability with rapidly delivered input.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "volume"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "gpt4oaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "chinese",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "mimoaudio",
                    "control",
                    "stepaudio2mini",
                    "chinese",
                    "stepaudio",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mimoaudio",
                    "stepaudio2mini",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SC task evaluates pre-trained LALMs&#8217; &#8220;intelligence&#8221; and cross-modal semantic coherence by judging the rationality of story endings. The results are shown in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.T2\" title=\"Table 2 &#8227; 4.4 Pretraining Evaluation &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Kimi-Audio-Base outperforms others in both paradigms: It scores an average of 78.01 in S-&gt;T and 54.71 in S-&gt;S, with robust performance across sub-dimensions, demonstrating stable story understanding and ending judgment in cross-modal scenarios. In contrast, Baichuan-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base score much lower. Moreover, all models perform worse in S-&gt;S than S-&gt;T, revealing that cross-modal (speech-to-speech) story coherence judgment remains challenging for pre-trained LALMs, with notable room for improvement in semantic consistency and rationality generation during speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "glm4voice",
                    "chinese",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "glm4voice",
                    "kimiaudio",
                    "gpt4oaudio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "objective",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotional",
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "style",
                    "control",
                    "chinese",
                    "following",
                    "gpt4oaudio",
                    "instruction",
                    "qwen25omni",
                    "results",
                    "objective",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "mimoaudio",
                    "style",
                    "control",
                    "chinese",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio",
                    "objective",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "stepaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gpt4oaudio",
                    "mimoaudio",
                    "control",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chinese",
                    "gpt4oaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T10": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 10: Chinese Speech Side Instruction Following Subjective Results.",
        "body": "Model\\Task\nAvg.\nEmotional Control\nLanguage Control\nNon Verbal Vocalization\nPacing Control\nStyle Control\nVolume Control\n\n\n\n\nGLM4-Voice\n64.86\n76.00\n54.00\n56.60\n64.60\n66.60\n68.60\n\n\nKimi-Audio\n59.20\n71.40\n45.40\n38.60\n62.00\n71.40\n62.00\n\n\nBaichuan-Audio-Chat\n46.14\n58.60\n41.40\n29.40\n50.60\n52.60\n39.40\n\n\nStepAudio\n54.50\n74.00\n60.00\n36.00\n46.60\n55.40\n47.40\n\n\nStepAudio2Mini\n57.14\n62.00\n46.00\n41.40\n73.40\n66.60\n50.00\n\n\nGPT4o-Audio\n65.72\n66.60\n64.00\n56.00\n73.40\n71.40\n60.60",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Emotional Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Language Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Non Verbal Vocalization</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pacing Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Style Control</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Volume Control</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">76.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">56.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">68.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">38.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">71.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">39.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">60.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">73.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">65.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">73.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">71.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.60</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "emotional",
            "pacing",
            "subjective",
            "gpt4oaudio",
            "volume",
            "avg",
            "baichuanaudiochat",
            "speech",
            "glm4voice",
            "verbal",
            "stepaudio2mini",
            "non",
            "instruction",
            "following",
            "results",
            "language",
            "side",
            "chinese",
            "kimiaudio",
            "vocalization",
            "stepaudio",
            "style",
            "control"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "control",
                    "chinese",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "volume",
                    "control",
                    "chinese",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large Audio Language Models.</span> Recent LALMs primarily adopt an E2E audio-language modeling paradigm, integrating speech understanding and generation within a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "volume",
                    "control",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "volume",
                    "control",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIF focuses on understanding and executing speech instructions, particularly the ability to handle paralinguistic features such as emotion, speaking rate, and dialect. It includes six sub-tasks: (1) Emotional Control: adjusting the emotional tone of speech to assess expressive generation; (2) Language Control: switching languages or dialects to test multilingual synthesis; (3) Non-verbal Vocalization: incorporating non-linguistic elements like sighs or nasal sounds to evaluate paralinguistic expressiveness; (4) Pacing Control: modifying speaking rate to examine control precision; (5) Style Control: switching speech styles to assess style transfer; (6) Volume Control: adjusting loudness to test stability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotional",
                    "pacing",
                    "vocalization",
                    "volume",
                    "style",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SV examine model adaptation to speaker attributes: (1) Age: utilizes child and elderly speech to assess recognition of age-related vocal characteristics; (2) Accent: incorporates four regional accents (Tianjin, Beijing, Dongbei, Sichuan) to evaluate comprehension of non-standard Mandarin; (3) Volume: assesses perception stability with amplified/attenuated speech; (4) Speed: tests parsing capability with rapidly delivered input.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "volume"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "chinese",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "control",
                    "stepaudio2mini",
                    "chinese",
                    "stepaudio",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stepaudio2mini",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "stepaudio2mini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SC task evaluates pre-trained LALMs&#8217; &#8220;intelligence&#8221; and cross-modal semantic coherence by judging the rationality of story endings. The results are shown in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.T2\" title=\"Table 2 &#8227; 4.4 Pretraining Evaluation &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Kimi-Audio-Base outperforms others in both paradigms: It scores an average of 78.01 in S-&gt;T and 54.71 in S-&gt;S, with robust performance across sub-dimensions, demonstrating stable story understanding and ending judgment in cross-modal scenarios. In contrast, Baichuan-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base score much lower. Moreover, all models perform worse in S-&gt;S than S-&gt;T, revealing that cross-modal (speech-to-speech) story coherence judgment remains challenging for pre-trained LALMs, with notable room for improvement in semantic consistency and rationality generation during speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "glm4voice",
                    "chinese",
                    "kimiaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "glm4voice",
                    "kimiaudio",
                    "subjective",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotional",
                    "glm4voice",
                    "kimiaudio",
                    "subjective",
                    "style",
                    "control",
                    "chinese",
                    "gpt4oaudio",
                    "instruction",
                    "results",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "style",
                    "control",
                    "chinese",
                    "gpt4oaudio",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "stepaudio",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results",
                    "control",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chinese",
                    "gpt4oaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T11": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 11: English Text Side Instruction Following Objective Results",
        "body": "Model\\Task\nAvg.\nContinuation En\nCreation En\nEmpathy En\nRecommendation En\nRewriting En\nSafety En\nSimulation En\n\n\n\n\nGLM4-Voice\n82.52\n81.00\n83.40\n78.20\n85.60\n84.60\n75.60\n89.80\n\n\nKimi-Audio\n88.92\n88.00\n93.00\n73.80\n90.20\n90.00\n90.80\n97.00\n\n\nQwen2.5-Omni\n72.58\n62.00\n73.40\n78.00\n73.40\n78.20\n64.60\n79.80\n\n\nBaichuan-Audio-Chat\n76.22\n79.60\n84.40\n59.60\n76.40\n80.60\n63.20\n90.80\n\n\nQwen2-Audio-Instruct\n75.86\n66.20\n76.20\n69.40\n79.00\n71.80\n88.60\n79.00\n\n\nStepAudio\n66.92\n71.60\n72.00\n51.60\n68.00\n69.80\n50.60\n85.60\n\n\nStepAudio2Mini\n75.54\n73.40\n78.20\n57.60\n80.60\n78.80\n73.20\n87.80\n\n\nMimo-Audio\n91.76\n92.40\n94.80\n86.80\n93.40\n90.40\n86.40\n97.80\n\n\nGPT4o-Audio\n91.66\n91.40\n94.60\n86.60\n95.40\n94.60\n81.60\n98.20",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Continuation En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Creation En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Empathy En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Recommendation En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Rewriting En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Safety En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Simulation En</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">90.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">97.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">51.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">92.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">94.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">86.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">97.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">95.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">94.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">98.20</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "recommendation",
            "gpt4oaudio",
            "text",
            "avg",
            "simulation",
            "qwen25omni",
            "objective",
            "baichuanaudiochat",
            "glm4voice",
            "english",
            "stepaudio2mini",
            "empathy",
            "instruction",
            "following",
            "results",
            "side",
            "safety",
            "kimiaudio",
            "qwen2audioinstruct",
            "creation",
            "stepaudio",
            "continuation",
            "rewriting",
            "mimoaudio"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "continuation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Benchmarks.</span> Recent efforts have introduced several benchmarks to evaluate LALMs from different perspectives. VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>)</cite> assesses general knowledge, instruction adherence, safety, and robustness, mainly based on existing text datasets such as AlpacaEval and SD-QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Faisal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib8\" title=\"\">2021</a>)</cite>, with lengthy or highly complex samples removed. OpenAudioBench, released alongside Baichuan-Audio, integrates question-answering datasets including Spoken LLaMA Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib17\" title=\"\">2023</a>)</cite> and Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib3\" title=\"\">2013</a>)</cite>, and augments them with a TTS-generated reasoning subset.</p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "text",
                    "continuation",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "text",
                    "instruction",
                    "english",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "text",
                    "safety",
                    "simulation",
                    "empathy",
                    "continuation",
                    "rewriting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "english",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "text",
                    "english",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "text",
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "english",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "text",
                    "english",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "glm4voice",
                    "objective",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "safety",
                    "following",
                    "simulation",
                    "rewriting",
                    "gpt4oaudio",
                    "instruction",
                    "qwen25omni",
                    "results",
                    "objective",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "english",
                    "mimoaudio",
                    "empathy",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "english",
                    "gpt4oaudio",
                    "objective",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "text",
                    "stepaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "text",
                    "english",
                    "continuation",
                    "gpt4oaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "english",
                    "continuation",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            }
        ]
    },
    "A1.T12": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 12: English Speech Side Instruction Following Objective Results",
        "body": "Model\\Task\nAvg.\nEmotional Control En\nLanguage Control En\nNon Verbal Vocalization En\nPacing Control En\nStyle Control En\nVolume Control En\n\n\n\n\nGLM4-Voice\n78.52\n82.80\n68.00\n64.40\n81.40\n84.60\n88.20\n\n\nKimi-Audio\n61.87\n69.20\n54.00\n50.00\n61.40\n67.40\n68.40\n\n\nQwen2.5-Omni\n58.09\n63.60\n53.40\n48.00\n50.60\n62.00\n69.80\n\n\nBaichuan-Audio-Chat\n68.07\n77.20\n57.60\n47.60\n73.00\n77.20\n73.80\n\n\nStepAudio\n63.63\n64.80\n54.20\n46.20\n75.60\n62.80\n71.40\n\n\nStepAudio2Mini\n65.15\n73.00\n47.20\n52.40\n78.80\n70.20\n68.00\n\n\nMimo-Audio\n24.25\n27.40\n24.80\n22.60\n20.40\n22.40\n26.80\n\n\nGPT4o-Audio\n86.07\n89.40\n84.80\n68.80\n88.00\n91.40\n90.60",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Emotional Control En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Language Control En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Non Verbal Vocalization En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pacing Control En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Style Control En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Volume Control En</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">27.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">22.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">20.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">22.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">26.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">86.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">89.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">84.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">68.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">88.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">90.60</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "emotional",
            "pacing",
            "gpt4oaudio",
            "volume",
            "avg",
            "qwen25omni",
            "objective",
            "baichuanaudiochat",
            "speech",
            "glm4voice",
            "verbal",
            "english",
            "stepaudio2mini",
            "non",
            "instruction",
            "following",
            "results",
            "language",
            "side",
            "kimiaudio",
            "vocalization",
            "mimoaudio",
            "stepaudio",
            "style",
            "control"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "control",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "volume",
                    "control",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Large Audio Language Models.</span> Recent LALMs primarily adopt an E2E audio-language modeling paradigm, integrating speech understanding and generation within a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "volume",
                    "control",
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "volume",
                    "control",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Following.</span> The Instruction Following section comprehensively evaluates LALMs&#8217; ability to understand and execute both text and speech instructions, covering three sub-tasks: TIF, SIF, and MTD. All tasks are open-ended, and both TIF and SIF support Chinese and English to meet cross-lingual evaluation needs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instruction",
                    "english",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIF focuses on understanding and executing speech instructions, particularly the ability to handle paralinguistic features such as emotion, speaking rate, and dialect. It includes six sub-tasks: (1) Emotional Control: adjusting the emotional tone of speech to assess expressive generation; (2) Language Control: switching languages or dialects to test multilingual synthesis; (3) Non-verbal Vocalization: incorporating non-linguistic elements like sighs or nasal sounds to evaluate paralinguistic expressiveness; (4) Pacing Control: modifying speaking rate to examine control precision; (5) Style Control: switching speech styles to assess style transfer; (6) Volume Control: adjusting loudness to test stability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotional",
                    "pacing",
                    "vocalization",
                    "volume",
                    "style",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SV examine model adaptation to speaker attributes: (1) Age: utilizes child and elderly speech to assess recognition of age-related vocal characteristics; (2) Accent: incorporates four regional accents (Tianjin, Beijing, Dongbei, Sichuan) to evaluate comprehension of non-standard Mandarin; (3) Volume: assesses perception stability with amplified/attenuated speech; (4) Speed: tests parsing capability with rapidly delivered input.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "volume"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "gpt4oaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SIF tasks in both Chinese and English (SIF-En), we invoke each model&#8217;s &#8220;audio2audio&#8221; API to generate spoken responses. The adherence to instruction requirements is then automatically scored using GPT-4o-audio. For other tasks except SC, we call the &#8220;audio2text&#8221; API to obtain textual responses, which are evaluated by GPT-4O. In open-ended question answering, GPT-4O provides a numerical score on a 1-5 scale, while for reference-based QA, it returns a binary \"Yes\" or \"No\" judgment.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "english",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "english",
                    "mimoaudio",
                    "control",
                    "stepaudio2mini",
                    "stepaudio",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SC task evaluates pre-trained LALMs&#8217; &#8220;intelligence&#8221; and cross-modal semantic coherence by judging the rationality of story endings. The results are shown in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.T2\" title=\"Table 2 &#8227; 4.4 Pretraining Evaluation &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Kimi-Audio-Base outperforms others in both paradigms: It scores an average of 78.01 in S-&gt;T and 54.71 in S-&gt;S, with robust performance across sub-dimensions, demonstrating stable story understanding and ending judgment in cross-modal scenarios. In contrast, Baichuan-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base score much lower. Moreover, all models perform worse in S-&gt;S than S-&gt;T, revealing that cross-modal (speech-to-speech) story coherence judgment remains challenging for pre-trained LALMs, with notable room for improvement in semantic consistency and rationality generation during speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "english",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "glm4voice",
                    "english",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "glm4voice",
                    "kimiaudio",
                    "gpt4oaudio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "emotional",
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "style",
                    "control",
                    "following",
                    "gpt4oaudio",
                    "instruction",
                    "qwen25omni",
                    "results",
                    "objective",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "english",
                    "mimoaudio",
                    "style",
                    "control",
                    "gpt4oaudio",
                    "instruction",
                    "following",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "english",
                    "gpt4oaudio",
                    "objective",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "stepaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "gpt4oaudio",
                    "mimoaudio",
                    "control",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "gpt4oaudio",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "english",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T13": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 13: Multi-turn Dialogue Evaluation Results.",
        "body": "Model\\Task\nAvg.\nProgression\nBacktracking\nTransition\n\n\n\n\nGLM4-Voice\n85.13\n92.20\n87.20\n76.00\n\n\nKimi-Audio\n85.67\n92.40\n89.60\n75.00\n\n\nQwen2-Audio-Instruct\n85.67\n93.20\n91.00\n72.80\n\n\nQwen2.5-Omni\n86.93\n93.60\n88.80\n78.40\n\n\nBaichuan-Audio-Chat\n73.27\n80.00\n74.60\n65.20\n\n\nStepAudio2Mini\n87.80\n92.60\n94.40\n76.20",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Progression</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Backtracking</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Transition</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">93.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">78.40</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">94.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.20</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "glm4voice",
            "dialogue",
            "multiturn",
            "evaluation",
            "backtracking",
            "stepaudio2mini",
            "qwen25omni",
            "avg",
            "kimiaudio",
            "qwen2audioinstruct",
            "progression",
            "results",
            "transition",
            "baichuanaudiochat"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MTD evaluates instruction tracking and topic management in multi-turn dialogues, each containing 3-5 turns, focusing on contextual understanding and logical coherence: (1) Progression: deepening the discussion around an initial topic to assess topic development; (2) Backtracking: recalling and responding to previously mentioned information to test long-range memory; (3) Transition: suddenly shifting to a new topic to evaluate conversational flow and relevance.</p>\n\n",
                "matched_terms": [
                    "backtracking",
                    "progression",
                    "multiturn",
                    "transition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "stepaudio2mini",
                    "qwen25omni",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the MTD evaluation, the model receives input context only in audio. We adopt <cite class=\"ltx_cite ltx_citemacro_citet\">Bai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib2\" title=\"\">2024</a>)</cite>&#8217;s protocol, requiring the model to answer each dialogue turn using the original ground-truth context, not its prior responses. A key scoring distinction is the heightened focus on the final turn: it carries 50% of the total score per sample, and the first several turns account for the remaining 50%. All Experiments are conducted on H20.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "baichuanaudiochat",
                    "dialogue",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "stepaudio2mini",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "qwen2audioinstruct",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat"
                ]
            }
        ]
    },
    "A1.T14": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 14: General Knowledge Evaluation Results.",
        "body": "Model\\Task\nAvg.\nMath.\nGeogr.\nPolit.\nChem.\nBiol.\nLaw\nPhys.\nHist.\nMed.\nEcon.\nSports\nCult.\n\n\n\n\nGLM4-Voice\n45.53\n41.67\n39.33\n54.24\n58.70\n49.60\n37.84\n66.67\n46.00\n50.65\n62.50\n27.87\n28.00\n\n\nKimi-Audio\n53.51\n66.67\n52.00\n52.54\n71.74\n49.60\n54.05\n67.65\n50.00\n54.55\n60.42\n36.07\n48.00\n\n\nQwen2.5-Omni\n55.43\n63.89\n53.33\n52.54\n71.74\n49.60\n43.24\n70.59\n54.00\n53.25\n75.00\n36.07\n53.33\n\n\nBaichuan-Audio-Chat\n44.48\n55.56\n44.00\n49.15\n56.52\n40.00\n40.54\n58.82\n41.33\n45.45\n54.17\n36.07\n34.67\n\n\nQwen2-Audio-Instruct\n35.83\n36.11\n34.67\n40.68\n58.70\n33.60\n37.84\n49.02\n34.67\n32.47\n47.92\n24.59\n24.00\n\n\nStepAudio\n60.42\n55.56\n55.33\n66.10\n73.91\n57.60\n62.16\n61.76\n60.67\n64.94\n70.83\n54.10\n58.00\n\n\nStepAudio2Mini\n61.15\n61.11\n57.33\n71.19\n80.43\n56.45\n48.65\n74.51\n65.33\n57.14\n68.75\n37.70\n58.00\n\n\nMimo-Audio\n56.58\n58.33\n43.33\n45.76\n60.87\n60.00\n40.54\n71.57\n65.33\n62.34\n66.67\n34.43\n57.33\n\n\nGPT4o-Audio\n61.29\n63.89\n62.00\n67.80\n65.22\n59.20\n70.27\n72.54\n53.33\n70.13\n85.42\n62.30\n43.33",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Math.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Geogr.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Polit.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Chem.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Biol.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Law</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Phys.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Hist.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Med.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Econ.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Sports</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Cult.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">39.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">49.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">37.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">27.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">66.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">49.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">49.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">43.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">44.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">44.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">49.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">34.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">35.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">34.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">37.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">49.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">34.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">71.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">80.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">74.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">65.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">37.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">58.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">43.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">60.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">65.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">34.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">61.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">62.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">70.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">70.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">62.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">43.33</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "gpt4oaudio",
            "econ",
            "med",
            "avg",
            "math",
            "chem",
            "knowledge",
            "qwen25omni",
            "baichuanaudiochat",
            "polit",
            "glm4voice",
            "stepaudio2mini",
            "biol",
            "sports",
            "results",
            "cult",
            "law",
            "geogr",
            "evaluation",
            "kimiaudio",
            "qwen2audioinstruct",
            "general",
            "hist",
            "phys",
            "stepaudio",
            "mimoaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Benchmarks.</span> Recent efforts have introduced several benchmarks to evaluate LALMs from different perspectives. VoiceBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>)</cite> assesses general knowledge, instruction adherence, safety, and robustness, mainly based on existing text datasets such as AlpacaEval and SD-QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Faisal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib8\" title=\"\">2021</a>)</cite>, with lengthy or highly complex samples removed. OpenAudioBench, released alongside Baichuan-Audio, integrates question-answering datasets including Spoken LLaMA Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib17\" title=\"\">2023</a>)</cite> and Web Questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Berant et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib3\" title=\"\">2013</a>)</cite>, and augments them with a TTS-generated reasoning subset.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "evaluation",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">GK evaluates multi-disciplinary common sense across twelve core domains&#8212;mathematics, geography, politics, chemistry, biology, law, physics, history, medicine, economics, sports, and culture&#8212;to measure the model&#8217;s ability to recall and apply knowledge across diverse fields.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "sports",
                    "law"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "glm4voice",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "gpt4oaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "math",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "stepaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T15": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 15: Mathematical and Logical Reasoning Evaluation Results",
        "body": "Model\\Task\nAvg.\nBasic Math\nMedium Math\nAnalysis\nInduction\nAnalogy\nLogic\n\n\n\n\nGLM4-Voice\n62.14\n87.67\n60.59\n32.14\n70.31\n20.00\n63.52\n\n\nKimi-Audio\n79.94\n98.63\n89.41\n66.67\n85.94\n45.00\n66.04\n\n\nQwen2.5-Omni\n80.24\n95.89\n78.82\n71.43\n78.13\n47.50\n81.76\n\n\nBaichuan-Audio-Chat\n60.33\n69.18\n52.94\n45.24\n70.31\n25.00\n72.96\n\n\nQwen2-Audio-Instruct\n60.78\n86.30\n57.06\n39.29\n51.56\n22.50\n59.75\n\n\nStepAudio\n77.07\n91.10\n88.82\n75.00\n65.63\n32.50\n68.55\n\n\nStepAudio2Mini\n81.30\n97.26\n88.82\n65.48\n71.88\n45.00\n79.87\n\n\nMimo-Audio\n84.01\n94.52\n88.82\n72.62\n78.13\n52.50\n85.53\n\n\nGPT4o-Audio\n77.68\n82.19\n91.18\n71.43\n75.00\n40.00\n72.96",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Basic Math</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Medium Math</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Analysis</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Induction</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Analogy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Logic</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">20.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.52</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">98.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">95.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">25.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">39.29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">51.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">22.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">75.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">97.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">84.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">52.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.96</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "reasoning",
            "medium",
            "gpt4oaudio",
            "logical",
            "avg",
            "math",
            "basic",
            "qwen25omni",
            "baichuanaudiochat",
            "glm4voice",
            "analogy",
            "stepaudio2mini",
            "analysis",
            "results",
            "induction",
            "evaluation",
            "kimiaudio",
            "qwen2audioinstruct",
            "mathematical",
            "stepaudio",
            "logic",
            "mimoaudio"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "basic",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "logical",
                    "mathematical",
                    "reasoning",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">StepAudio models <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib9\" title=\"\">2025</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> focus on tightly coupling recognition and synthesis. StepAudio integrates a dual-codebook tokenizer and achieves a remarkably low WER, while StepAudio2 advances to a fully E2E design with fixed text-speech token alignment and Chain-of-Thought reasoning, improving fine-grained paralinguistic understanding.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "stepaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AIR Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>)</cite> contains two components&#8212;a basic benchmark covering emotion recognition, ASR, and age estimation, and a dialogue benchmark evaluating auditory understanding and internal knowledge. MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib10\" title=\"\">2025</a>)</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib15\" title=\"\">2025</a>)</cite> focus on deep audio reasoning, requiring multi-step inference grounded in internal audio knowledge. OmniBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib13\" title=\"\">2024</a>)</cite> targets omni-modal models handling audio, images, and text, where text queries are paired with multimodal contexts (speech, music, or sound) to test integrated reasoning. Finally, URO Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib24\" title=\"\">2025</a>)</cite> provides a bilingual (English-Chinese) comprehensive set that evaluates audio understanding, reasoning, and conversational ability&#8212;but the speech data are entirely synthetic (TTS-generated).</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "basic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "logic",
                    "mathematical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Internal Two-person Dialogue Dataset.</span> Designed to support the DC module, this category&#8217;s data is processed as follows: the original long-form audio undergoes a two-stage segmentation with GPT-4O&#8212;first by topic, then refined into semantically coherent segments under one minute. From the transcriptions, GPT-4O generates task-specific QA pairs (e.g., for analysis or induction), followed by a final manual screening to verify question quality and answer accuracy.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge.</span> The Knowledge module evaluates LALMs&#8217; knowledge storage, logical reasoning, and spoken dialog comprehension through reference-based question answering. It comprises four sub-tasks: GK, ML, DC, and SC.</p>\n\n",
                "matched_terms": [
                    "logical",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ML module consists of two key components: Mathematics and Logical Reasoning. Mathematics is divided into Basic Math, which is confined to integer arithmetic within 100, and Medium Math, which includes advanced algebra, geometry, number theory, and related disciplines, collectively assessing computational and problem-solving skills. Logical Reasoning comprises four reasoning types: Analysis for breaking down information, Induction for identifying and generalizing patterns, Analogy for mapping relational correspondences, and Logic for executing conditional reasoning, thereby testing analytical and deductive capabilities.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "medium",
                    "analogy",
                    "logical",
                    "analysis",
                    "logic",
                    "math",
                    "basic",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DC focuses on understanding dialogues through three dedicated tasks: Analysis detects factual accuracy within dialogues, Induction summarizes overarching dialogue themes, and Inference deduces speakers&#8217; attitudes, emotions, or intents, together evaluating comprehension and implicit reasoning skills.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "reasoning",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SC, inspired by StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Mostafazadeh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib16\" title=\"\">2016</a>)</cite>, assesses implicit reasoning by requiring the model to select the correct story ending from two candidates, where both the context and the candidate endings are provided in the same modality&#8212;either all in audio or all in text. This task spans three evaluative categories: Logic and Causality for causal consistency, Common Sense and Science for real-world and scientific knowledge alignment, and Morality and Emotion for moral and emotional coherence.</p>\n\n",
                "matched_terms": [
                    "logic",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "logic",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "mimoaudio",
                    "reasoning",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "glm4voice",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "gpt4oaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "medium",
                    "kimiaudio",
                    "analogy",
                    "logical",
                    "mimoaudio",
                    "mathematical",
                    "stepaudio2mini",
                    "analysis",
                    "logic",
                    "math",
                    "basic",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "induction",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "stepaudio",
                    "analysis",
                    "mimoaudio",
                    "qwen25omni",
                    "induction",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "gpt4oaudio",
                    "logical",
                    "mathematical",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T16": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 16: Discoure Comprehension Evaluation Results.",
        "body": "Model\\Task\nAvg.\nInference\nInduction\nAnalysis\n\n\n\n\nGLM4-Voice\n48.64\n58.25\n55.75\n33.04\n\n\nKimi-Audio\n74.76\n85.71\n83.50\n56.48\n\n\nQwen2-Audio-Instruct\n67.07\n78.64\n80.53\n43.48\n\n\nQwen2.5-Omni\n73.72\n83.49\n79.65\n59.13\n\n\nBaichuan-Audio-Chat\n54.38\n58.25\n71.68\n33.91\n\n\nStepAudio\n59.52\n63.11\n64.60\n51.30\n\n\nStepAudio2Mini\n83.08\n92.23\n84.07\n73.91\n\n\nMimo-Audio\n87.92\n95.15\n88.50\n80.87\n\n\nGPT4o-Audio\n77.64\n90.29\n84.96\n59.13",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Inference</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Induction</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Analysis</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">43.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">51.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">95.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">88.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">80.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.13</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "glm4voice",
            "gpt4oaudio",
            "mimoaudio",
            "evaluation",
            "comprehension",
            "qwen25omni",
            "stepaudio",
            "stepaudio2mini",
            "avg",
            "analysis",
            "discoure",
            "inference",
            "kimiaudio",
            "qwen2audioinstruct",
            "results",
            "induction",
            "baichuanaudiochat"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "comprehension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Internal Two-person Dialogue Dataset.</span> Designed to support the DC module, this category&#8217;s data is processed as follows: the original long-form audio undergoes a two-stage segmentation with GPT-4O&#8212;first by topic, then refined into semantically coherent segments under one minute. From the transcriptions, GPT-4O generates task-specific QA pairs (e.g., for analysis or induction), followed by a final manual screening to verify question quality and answer accuracy.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ML module consists of two key components: Mathematics and Logical Reasoning. Mathematics is divided into Basic Math, which is confined to integer arithmetic within 100, and Medium Math, which includes advanced algebra, geometry, number theory, and related disciplines, collectively assessing computational and problem-solving skills. Logical Reasoning comprises four reasoning types: Analysis for breaking down information, Induction for identifying and generalizing patterns, Analogy for mapping relational correspondences, and Logic for executing conditional reasoning, thereby testing analytical and deductive capabilities.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "induction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DC focuses on understanding dialogues through three dedicated tasks: Analysis detects factual accuracy within dialogues, Induction summarizes overarching dialogue themes, and Inference deduces speakers&#8217; attitudes, emotions, or intents, together evaluating comprehension and implicit reasoning skills.</p>\n\n",
                "matched_terms": [
                    "induction",
                    "analysis",
                    "inference",
                    "comprehension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "stepaudio",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "baichuanaudiochat",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "glm4voice",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "gpt4oaudio",
                    "mimoaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "analysis",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "induction",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "comprehension",
                    "stepaudio",
                    "analysis",
                    "inference",
                    "mimoaudio",
                    "qwen25omni",
                    "induction",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "comprehension",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "kimiaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T17": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 17: Speaker Variations Evaluation Results - Experimental Group (Difference from Control Group).",
        "body": "Model\\Task\nAge\nAccent\nVolume\nSpeed\n\n\nAvg.\nChild\nElder\nAvg.\nTianjin\nBeijing\nDongbei\nSichuan\nAvg.\nDown\nUp\nAvg.\n\n\nGLM4-Voice\n65.00 (-17.20)\n83.00 (-1.20)\n47.60 (-32.40)\n82.40 (0.20)\n85.00 (-0.60)\n82.60 (-2.60)\n83.20 (3.20)\n79.40 (0.00)\n96.40 (0.60)\n96.80 (0.80)\n96.00 (0.40)\n39.20 (-55.20)\n\n\nKimi-Audio\n45.60 (-30.80)\n66.00 (-7.20)\n25.80 (-53.80)\n56.20 (-20.60)\n54.40 (-23.80)\n77.40 (18.80)\n73.60 (-7.20)\n36.60 (-44.00)\n93.80 (-1.60)\n93.20 (-2.00)\n94.40 (-1.20)\n87.80 (-4.40)\n\n\nQwen2.5-Omni\n75.60 (-5.40)\n82.60 (1.40)\n68.80 (-12.00)\n77.80 (-3.40)\n79.40 (-1.80)\n81.40 (5.40)\n74.40 (-8.00)\n77.20 (-5.60)\n90.40 (0.20)\n93.20 (3.20)\n87.60 (-2.80)\n\n89.20 (-1.60)\n\n\nBaichuan-Audio-Chat\n60.40 (-0.80)\n61.80 (-0.40)\n59.20 (-1.20)\n61.60 (-0.60)\n61.80 (-0.80)\n62.60 (-5.40)\n68.00 (3.20)\n56.60 (-1.00)\n77.20 (3.00)\n83.60 (13.20)\n70.80 (-7.20)\n60.00 (-7.80)\n\n\nQwen2-Audio-Instruct\n76.40 (-5.80)\n80.40 (-1.40)\n72.60 (-10.00)\n81.60 (0.80)\n79.40 (0.60)\n80.00 (-6.60)\n83.20 (0.00)\n83.40 (5.00)\n93.20 (-1.40)\n93.60 (-1.20)\n92.80 (-1.60)\n77.80 (-7.00)\n\n\nStepAudio2Mini\n75.60 (-4.80)\n75.40 (-2.00)\n\n75.80 (-7.60)\n75.40 (-2.00)\n78.80 (-0.60)\n88.00 (2.60)\n67.20 (-4.80)\n72.80 (-3.40)\n96.60 (2.80)\n96.00 (2.00)\n97.20 (3.60)\n76.00 (-14.40)\n\n\nMimo-Audio\n76.00 (-14.80)\n\n92.80 (-0.40)\n59.60 (-28.80)\n\n88.40 (-0.20)\n90.60 (2.40)\n\n96.00 (10.60)\n\n88.80 (1.60)\n82.80 (-8.40)\n\n99.20 (1.00)\n\n98.80 (-0.40)\n\n99.60 (2.40)\n60.40 (-33.00)\n\n\nGPT4o-Audio\n\n79.00 (-8.60)\n89.00 (0.00)\n69.20 (-17.00)\n\n88.40 (2.20)\n\n91.80 (4.20)\n89.40 (0.00)\n85.60 (0.80)\n\n86.60 (2.20)\n96.80 (-1.20)\n97.20 (0.00)\n96.40 (-2.40)\n43.40 (-48.40)",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-top:1pt;padding-bottom:1pt;\">Age</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-top:1pt;padding-bottom:1pt;\">Accent</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-top:1pt;padding-bottom:1pt;\">Volume</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speed</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Child</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Elder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Tianjin</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Beijing</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dongbei</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Sichuan</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Down</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Up</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.00 (-17.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.00 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.60 (-32.40)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.40 (<span class=\"ltx_text ltx_font_bold\">0.20</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.00 (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.60 (-2.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.20 (3.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.40 (0.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.80 (0.80)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.00 (<span class=\"ltx_text ltx_font_bold\">0.40</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">39.20 (-55.20)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.60 (-30.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.00 (-7.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">25.80 (-53.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.20 (-20.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.40 (-23.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.40 (18.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.60 (-7.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.60 (-44.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.80 (-1.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.20 (-2.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.40 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.80 (-4.40)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.60 (-5.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.60 (1.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.80 (-12.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.80 (-3.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40 (-1.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.40 (5.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.40 (-8.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.20 (-5.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.40 (<span class=\"ltx_text ltx_font_bold\">0.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.20 (3.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.60 (-2.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">89.20</span> (<span class=\"ltx_text ltx_font_bold\">-1.60</span>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.40 (<span class=\"ltx_text ltx_font_bold\">-0.80</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.80 (-0.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.20 (<span class=\"ltx_text ltx_font_bold\">-1.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.60 (-0.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.80 (-0.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.60 (-5.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.00 (3.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.60 (-1.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.20 (3.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.60 (13.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.80 (-7.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.00 (-7.80)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.40 (-5.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.40 (-1.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.60 (-10.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.60 (0.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40 (<span class=\"ltx_text ltx_font_bold\">0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.00 (-6.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.20 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.40 (5.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.20 (-1.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.60 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.80 (-1.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.80 (-7.00)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.60 (-4.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.40 (-2.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">75.80</span> (-7.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.40 (-2.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.80 (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.00 (2.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.20 (-4.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.80 (-3.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.60 (2.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.00 (2.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">97.20 (3.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.00 (-14.40)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.00 (-14.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">92.80</span> (-0.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.60 (-28.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">88.40</span> (<span class=\"ltx_text ltx_font_bold\">-0.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.60 (2.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">96.00</span> (10.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">88.80</span> (1.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.80 (-8.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">99.20</span> (1.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">98.80</span> (-0.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">99.60</span> (2.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.40 (-33.00)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">79.00</span> (-8.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.00 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.20 (-17.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">88.40</span> (2.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">91.80</span> (4.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.40 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60 (0.80)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">86.60</span> (2.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.80 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">97.20 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.40 (-2.40)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">43.40 (-48.40)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "tianjin",
            "gpt4oaudio",
            "accent",
            "volume",
            "sichuan",
            "avg",
            "dongbei",
            "qwen25omni",
            "baichuanaudiochat",
            "glm4voice",
            "stepaudio2mini",
            "from",
            "group",
            "speaker",
            "results",
            "speed",
            "elder",
            "age",
            "evaluation",
            "variations",
            "beijing",
            "kimiaudio",
            "qwen2audioinstruct",
            "down",
            "experimental",
            "child",
            "difference",
            "mimoaudio",
            "control"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "speaker",
                    "from",
                    "evaluation",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "age",
                    "volume",
                    "evaluation",
                    "from",
                    "control",
                    "speaker",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "variations",
                    "from",
                    "control",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "accent",
                    "speaker",
                    "from",
                    "control",
                    "group",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIF focuses on understanding and executing speech instructions, particularly the ability to handle paralinguistic features such as emotion, speaking rate, and dialect. It includes six sub-tasks: (1) Emotional Control: adjusting the emotional tone of speech to assess expressive generation; (2) Language Control: switching languages or dialects to test multilingual synthesis; (3) Non-verbal Vocalization: incorporating non-linguistic elements like sighs or nasal sounds to evaluate paralinguistic expressiveness; (4) Pacing Control: modifying speaking rate to examine control precision; (5) Style Control: switching speech styles to assess style transfer; (6) Volume Control: adjusting loudness to test stability.</p>\n\n",
                "matched_terms": [
                    "control",
                    "volume"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robustness.</span> The Robustness module evaluates the stability of LALMs&#8217; performance under real-world interference conditions, ensuring reliable responses in challenging scenarios. The module encompasses three dimensions: Speaker Variations (SV), Environmental Variations (EV), and Content Variations (CV).</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "variations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SV examine model adaptation to speaker attributes: (1) Age: utilizes child and elderly speech to assess recognition of age-related vocal characteristics; (2) Accent: incorporates four regional accents (Tianjin, Beijing, Dongbei, Sichuan) to evaluate comprehension of non-standard Mandarin; (3) Volume: assesses perception stability with amplified/attenuated speech; (4) Speed: tests parsing capability with rapidly delivered input.</p>\n\n",
                "matched_terms": [
                    "tianjin",
                    "child",
                    "age",
                    "volume",
                    "accent",
                    "beijing",
                    "sichuan",
                    "dongbei",
                    "speaker",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SC task, we assess a subset of pre-trained base models: Baichuan-Audio-Base, Kimi-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base. Following the StoryCloze evaluation protocol, we compute the negative log-likelihood for both the correct and incorrect endings, with model selection determined by comparing these two values. For SIF tasks, the top six performing models undergo further Mean Opinion Score (MOS) evaluation. We sample the first 30 items from each relevant dataset, and eight expert evaluators rate the generated audio samples.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "control",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "from",
                    "mimoaudio",
                    "control",
                    "group"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "from",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "evaluation",
                    "difference",
                    "from",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "gpt4oaudio",
                    "control",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "control",
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "mimoaudio",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "variations",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "mimoaudio",
                    "variations",
                    "gpt4oaudio",
                    "speaker",
                    "speed",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "variations",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T18": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 18: Environmental Variations Evaluation Results - Experimental Group (Difference from Control Group).",
        "body": "Model\\Task\nNon Vocal Noise\nVocal Noise\nUnstable Signal\n\n\nAvg.\nEcho\nOutdoors\nFar Field\nAvg.\nTv Playback\nBackground Chat\nVocal-Music\nVoice Announce\nAvg.\n\n\nGLM4-Voice\n64.00 (-24.60)\n45.00 (-36.80)\n96.60 (-0.60)\n74.80 (-12.60)\n89.20 (-0.80)\n85.60 (-3.80)\n83.00 (-1.40)\n93.00 (-0.80)\n92.80 (1.60)\n78.40 (-14.60)\n\n\nKimi-Audio\n\n74.00 (-5.40)\n60.60 (-11.60)\n94.80 (4.60)\n84.00 (-1.20)\n75.80 (-9.60)\n76.80 (-8.20)\n58.20 (-11.60)\n93.80 (0.60)\n70.20 (-19.80)\n76.80 (-11.20)\n\n\nQwen2.5-Omni\n69.40 (-16.80)\n52.00 (-30.60)\n95.40 (0.60)\n82.80 (-3.00)\n85.20 (-4.80)\n80.80 (-6.20)\n74.80 (-8.20)\n95.60 (-1.80)\n85.60 (-4.40)\n79.80 (-10.20)\n\n\nBaichuan-Audio-Chat\n56.60 (-15.60)\n45.00 (-20.00)\n81.20 (-8.60)\n58.20 (-12.60)\n73.40 (-2.80)\n73.40 (0.40)\n64.40 (1.00)\n81.40 (-5.40)\n72.00 (-5.60)\n67.20 (-4.60)\n\n\nQwen2-Audio-Instruct\n59.80 (-17.60)\n37.80 (-32.00)\n93.80 (-1.60)\n76.00 (-1.20)\n87.60 (-1.60)\n82.20 (-6.00)\n80.00 (-1.40)\n96.20 (-0.20)\n88.60 (0.40)\n77.80 (-13.40)\n\n\nStepAudio2Mini\n65.00 (-19.20)\n45.80 (-33.00)\n94.80 (-1.20)\n78.80 (-6.40)\n83.40 (-5.80)\n84.00 (-2.20)\n66.20 (-12.00)\n94.80 (-2.60)\n84.80 (-6.80)\n81.60 (-8.40)\n\n\nMimo-Audio\n73.20 (-20.20)\n54.00 (-35.60)\n\n99.40 (-0.60)\n\n90.80 (-4.60)\n90.40 (-5.00)\n\n87.00 (-9.00)\n86.80 (-4.80)\n\n99.60 (0.60)\n85.60 (-8.20)\n\n95.20 (-4.20)\n\n\nGPT4o-Audio\n68.20 (-19.20)\n\n61.00 (-20.60)\n98.20 (-0.60)\n54.80 (-34.40)\n\n92.00 (-1.60)\n86.20 (-6.60)\n\n88.00 (-0.20)\n96.40 (0.20)\n\n94.20 (-1.60)\n74.60 (-21.60)",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-top:1pt;padding-bottom:1pt;\">Non Vocal Noise</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-top:1pt;padding-bottom:1pt;\">Vocal Noise</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">Unstable Signal</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Echo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Outdoors</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Far Field</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Tv Playback</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Background Chat</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Vocal-Music</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Voice Announce</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.00 (-24.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.00 (-36.80)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.60 (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.80 (-12.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.20 (<span class=\"ltx_text ltx_font_bold\">-0.80</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60 (-3.80)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.00 (-1.40)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.00 (-0.80)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.80 (1.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.40 (-14.60)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">74.00</span> (<span class=\"ltx_text ltx_font_bold\">-5.40</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.60 (<span class=\"ltx_text ltx_font_bold\">-11.60</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.80 (4.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.00 (<span class=\"ltx_text ltx_font_bold\">-1.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.80 (-9.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.80 (-8.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.20 (-11.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.80 (0.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.20 (-19.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.80 (-11.20)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.40 (-16.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.00 (-30.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">95.40 (<span class=\"ltx_text ltx_font_bold\">0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.80 (-3.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.20 (-4.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.80 (-6.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.80 (-8.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">95.60 (-1.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60 (-4.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.80 (-10.20)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.60 (-15.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.00 (-20.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.20 (-8.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.20 (-12.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.40 (-2.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.40 (<span class=\"ltx_text ltx_font_bold\">0.40</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.40 (1.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.40 (-5.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.00 (-5.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.20 (-4.60)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.80 (-17.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">37.80 (-32.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.80 (-1.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.00 (<span class=\"ltx_text ltx_font_bold\">-1.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.60 (-1.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.20 (-6.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.00 (-1.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.20 (<span class=\"ltx_text ltx_font_bold\">-0.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.60 (<span class=\"ltx_text ltx_font_bold\">0.40</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.80 (-13.40)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.00 (-19.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.80 (-33.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.80 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.80 (-6.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.40 (-5.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.00 (-2.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.20 (-12.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">94.80 (-2.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.80 (-6.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.60 (-8.40)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.20 (-20.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.00 (-35.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">99.40</span> (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">90.80</span> (-4.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.40 (-5.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">87.00</span> (-9.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.80 (-4.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">99.60</span> (0.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60 (-8.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">95.20</span> (<span class=\"ltx_text ltx_font_bold\">-4.20</span>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.20 (-19.20)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">61.00</span> (-20.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">98.20 (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">54.80 (-34.40)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">92.00</span> (-1.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.20 (-6.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">88.00</span> (<span class=\"ltx_text ltx_font_bold\">-0.20</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">96.40 (<span class=\"ltx_text ltx_font_bold\">0.20</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">94.20</span> (-1.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.60 (-21.60)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "gpt4oaudio",
            "chat",
            "avg",
            "signal",
            "playback",
            "vocalmusic",
            "qwen25omni",
            "announce",
            "baichuanaudiochat",
            "field",
            "glm4voice",
            "voice",
            "stepaudio2mini",
            "non",
            "from",
            "unstable",
            "group",
            "results",
            "noise",
            "environmental",
            "far",
            "evaluation",
            "variations",
            "kimiaudio",
            "qwen2audioinstruct",
            "echo",
            "experimental",
            "vocal",
            "difference",
            "outdoors",
            "mimoaudio",
            "control",
            "background"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "voice",
                    "chat",
                    "evaluation",
                    "from",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib20\" title=\"\">2017</a>; Anil et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib1\" title=\"\">2023</a>)</cite> have achieved remarkable progress in natural language understanding and generation. Integrating language modeling with modalities such as vision and audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib18\" title=\"\">2021</a>; Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib19\" title=\"\">2022</a>)</cite> has further given rise to a new paradigm of multimodal learning. Within this trend, large audio language models (LALMs)&#8212;which combine speech signal processing with language modeling&#8212;have developed rapidly. Emerging systems such as StepAudio2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib21\" title=\"\">2025</a>)</cite> and Qwen3-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib23\" title=\"\">2025b</a>)</cite> demonstrate end-to-end (E2E) speech understanding and generation with capabilities in voice question answering, real-time conversation, and audio content analysis. Consequently, voice conversational agents powered by LALMs are drawing increasing academic and industrial attention, offering more natural and human-like interactions than text-only systems.</p>\n\n",
                "matched_terms": [
                    "signal",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "noise",
                    "voice",
                    "chat",
                    "evaluation",
                    "from",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "environmental",
                    "variations",
                    "from",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "unstable",
                    "from",
                    "signal",
                    "control",
                    "group"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robustness.</span> The Robustness module evaluates the stability of LALMs&#8217; performance under real-world interference conditions, ensuring reliable responses in challenging scenarios. The module encompasses three dimensions: Speaker Variations (SV), Environmental Variations (EV), and Content Variations (CV).</p>\n\n",
                "matched_terms": [
                    "environmental",
                    "variations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EV simulate acoustic interference: (1) Non-Vocal Noise: includes echo, outdoor, and far-field noise; (2) Vocal Noise: contains television audio, background conversations, vocal music, and radio broadcasts; (3) Unstable Signal: emulates network-induced packet loss to evaluate handling of fragmented audio.</p>\n\n",
                "matched_terms": [
                    "echo",
                    "noise",
                    "vocal",
                    "signal",
                    "unstable",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SC task, we assess a subset of pre-trained base models: Baichuan-Audio-Base, Kimi-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base. Following the StoryCloze evaluation protocol, we compute the negative log-likelihood for both the correct and incorrect endings, with model selection determined by comparing these two values. For SIF tasks, the top six performing models undergo further Mean Opinion Score (MOS) evaluation. We sample the first 30 items from each relevant dataset, and eight expert evaluators rate the generated audio samples.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "control",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "echo",
                    "stepaudio2mini",
                    "from",
                    "mimoaudio",
                    "control",
                    "group"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "far",
                    "from",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "evaluation",
                    "difference",
                    "from",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "gpt4oaudio",
                    "control",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "control",
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "mimoaudio",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n",
                "matched_terms": [
                    "environmental",
                    "mimoaudio",
                    "variations",
                    "from",
                    "gpt4oaudio",
                    "control",
                    "group",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "echo",
                    "noise",
                    "environmental",
                    "voice",
                    "gpt4oaudio",
                    "chat",
                    "variations",
                    "signal",
                    "outdoors",
                    "mimoaudio",
                    "vocalmusic",
                    "unstable",
                    "announce",
                    "background",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "echo",
                    "environmental",
                    "gpt4oaudio",
                    "mimoaudio",
                    "variations",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "variations",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T19": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 19: Content Variations Evaluation Results - Experimental Group (Difference from Control Group).",
        "body": "Model\\Task\nContent Variations\n\n\nFillers\nRepetition\nMispronunciation\nGrammatical Error\nTopic Shift\nCode Switching\n\n\nGLM4-Voice\n81.80 (-3.00)\n89.20 (-0.60)\n82.20 (-10.60)\n85.60 (-6.00)\n86.20 (-6.40)\n80.80 (-10.80)\n\n\nKimi-Audio\n76.00 (-0.40)\n79.40 (-1.20)\n61.80 (-23.20)\n77.40 (-10.40)\n79.40 (-6.80)\n84.00 (-8.80)\n\n\nQwen2.5-Omni\n79.00 (-2.00)\n84.80 (0.00)\n80.60 (-8.40)\n79.20 (-8.60)\n75.00 (-11.40)\n80.40 (-9.20)\n\n\nBaichuan-Audio-Chat\n69.60 (1.20)\n72.00 (-0.20)\n71.40 (-1.20)\n65.60 (-11.00)\n61.60 (-12.40)\n68.40 (-7.60)\n\n\nQwen2-Audio-Instruct\n75.00 (1.60)\n85.80 (-1.20)\n78.40 (-10.00)\n79.40 (-9.20)\n67.00 (-19.80)\n76.40 (-11.60)\n\n\nStepAudio2Mini\n79.20 (-3.80)\n81.60 (-2.60)\n82.20 (-5.60)\n80.80 (-8.20)\n80.60 (-3.00)\n83.60 (-8.60)\n\n\nMimo-Audio\n\n92.00 (-1.80)\n\n95.00 (-0.40)\n\n96.00 (-3.60)\n\n97.40 (-0.60)\n\n92.00 (-5.60)\n92.60 (-7.00)\n\n\nGPT4o-Audio\n88.40 (1.80)\n93.20 (0.00)\n92.60 (-4.00)\n89.00 (-5.00)\n90.60 (-2.60)\n\n96.80 (-0.60)",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\" style=\"padding-top:1pt;padding-bottom:1pt;\">Content Variations</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Fillers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Repetition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mispronunciation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Grammatical Error</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Topic Shift</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Code Switching</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.80 (-3.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.20 (-0.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.20 (-10.60)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.60 (-6.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.20 (-6.40)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.80 (-10.80)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.00 (<span class=\"ltx_text ltx_font_bold\">-0.40</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.80 (-23.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.40 (-10.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40 (-6.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.00 (-8.80)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.00 (-2.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.80 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.60 (-8.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.20 (-8.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.00 (-11.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.40 (-9.20)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.60 (1.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.00 (-0.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.40 (<span class=\"ltx_text ltx_font_bold\">-1.20</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.60 (-11.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.60 (-12.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.40 (-7.60)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2-Audio-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.00 (1.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.80 (-1.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.40 (-10.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.40 (-9.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.00 (-19.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.40 (-11.60)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.20 (-3.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.60 (-2.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.20 (-5.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.80 (-8.20)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.60 (-3.00)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.60 (-8.60)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mimo-Audio</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">92.00</span> (-1.80)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">95.00</span> (-0.40)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">96.00</span> (-3.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">97.40</span> (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">92.00</span> (-5.60)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.60 (-7.00)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.40 (1.80)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">93.20 (<span class=\"ltx_text ltx_font_bold\">0.00</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.60 (-4.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.00 (-5.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.60 (<span class=\"ltx_text ltx_font_bold\">-2.60</span>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">96.80</span> (<span class=\"ltx_text ltx_font_bold\">-0.60</span>)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modeltask",
            "gpt4oaudio",
            "mispronunciation",
            "switching",
            "topic",
            "content",
            "grammatical",
            "qwen25omni",
            "baichuanaudiochat",
            "glm4voice",
            "shift",
            "repetition",
            "stepaudio2mini",
            "from",
            "group",
            "results",
            "fillers",
            "evaluation",
            "variations",
            "kimiaudio",
            "qwen2audioinstruct",
            "code",
            "experimental",
            "difference",
            "mimoaudio",
            "control",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T17\" title=\"Table 17 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">17</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T19\" title=\"Table 19 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> shows the complete results of robustness. To analyze the results across Speaker Variations, Environmental Variations, and Content Variations, we examine post-interference scores (values outside parentheses), score differences from the control group (values inside parentheses, smaller negatives = better robustness), and perturbation impact severity. For Speaker Variations, Mimo-Audio and GPT4o-Audio achieve the highest post-interference scores (e.g., Mimo-Audio&#8217;s 92.80 in Child speech, GPT4o-Audio&#8217;s 91.80 in Tianjin accent) and smallest negative differences (e.g., GPT4o-Audio&#8217;s 0.00 in Child and Down); Speed interference causes the largest drops (many models score &lt;50), while Accent (e.g., Beijing, Tianjin) has minimal impact.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited&#8212;they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench)&#8212;a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Code and data are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/193746/VCB-Bench-Evalkit\" title=\"\">https://github.com/193746/VCB-Bench-Evalkit</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "content",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, moving from basic LALM functionalities to practical voice agents requires reliable and comprehensive evaluation tools. Such benchmarks are essential for diagnosing model weaknesses, guiding optimization, and enabling fair comparisons across systems. While initial efforts <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib4\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib25\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib14\" title=\"\">2025</a>)</cite> have explored instruction following, audio understanding, reasoning, and dialogue scenarios, current evaluation practices remain limited in three major ways. First, most benchmarks are English-centric, leaving Chinese&#8212;the world&#8217;s most widely spoken language&#8212;largely unexplored. Second, the majority rely on synthetic speech data, which poorly reflects real-world acoustic variability. Third, many are text-derived benchmarks (e.g., AlpacaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib12\" title=\"\">2023</a>)</cite>, IFEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib27\" title=\"\">2023</a>)</cite>), whose formal and lengthy content is unsuitable for evaluating conversationally grounded LALMs that should generate natural, colloquial speech. Addressing these limitations is critical given China&#8217;s large user base and the growing demand for practical, high-quality voice agents.</p>\n\n",
                "matched_terms": [
                    "from",
                    "content",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge these gaps, we introduce Voice Chat Bot Bench (VCB Bench)&#8212;the first comprehensive evaluation framework for Chinese voice conversation, built entirely from authentic (non-synthetic) speech. VCB Bench evaluates LALMs along three complementary dimensions: (1) Instruction following, extending beyond text-based prompts to incorporate speech-level control tasks such as adjusting volume, speed, and emotion, with bilingual (Chinese-English) support; (2) Knowledge, including multi-disciplinary general knowledge (12 subjects), mathematical and logical reasoning, daily dialogue comprehension, and story continuation for pretraining performance assessment; (3) Robustness, measuring model stability under real-world perturbations across content (mispronunciations, grammatical errors), environment (street, TV noise), and speaker characteristics (age, accents).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "from",
                    "content",
                    "grammatical",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed VCB Bench is built entirely from authentic human recordings rather than synthetic speech. It provides a large-scale, high-fidelity dataset covering diverse conversational scenarios and introduces a multi-dimensional evaluation framework that jointly measures knowledge understanding, instruction following, and robustness through fine-grained, reproducible tasks. Based on this benchmark, we conduct a systematic empirical analysis of state-of-the-art LALMs under unified settings, revealing their strengths and limitations in Chinese voice interaction and offering actionable insights for future model development.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, existing benchmarks have significantly advanced the evaluation coverage of LALMs, yet they share several limitations:\n(1) most rely heavily on TTS or synthetic speech, (2) they focus on English, and (3) their content often derives from text-centric QA corpora rather than spontaneous human dialogue. These gaps highlight the need for a real-speech, Chinese-oriented benchmark offering multi-dimensional evaluation&#8212;the central goal of our proposed VCB Bench.</p>\n\n",
                "matched_terms": [
                    "from",
                    "content",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "control",
                    "from",
                    "content",
                    "variations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VCB Bench dataset integrates data from three distinct sources: third-party professional recordings, audio extracted from variety show Q&amp;A segments and an internally curated two-person conversational dialogue dataset. Each source supports different evaluation modules within the benchmark.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-Party Recorded Data.</span> This category supports the Instruction Following, ML, and SC tasks under the Knowledge module, as well as the Robustness module. The production pipeline involves the following steps: First, task types and examples are defined through team discussion. Next, commissioned personnel manually compose texts that fulfill the task requirements. These texts then undergo manual inspection to ensure quality. Approved texts are forwarded to a third-party recording team for professional audio production. After recording, the data team performs quality checks on the audio. Subsequently, GPT-4o-Audio is used to evaluate audio quality, while GPT-4o assesses textual quality. Finally, manual screening is conducted to select high-quality samples, determining the final evaluation dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Robustness data, it&#8217;s text materials are derived from Instruction Following module. The original audio from this module serves as the control group. To control for speaker variability, the same speaker re-recorded the text under specified interference conditions (e.g., accent, noisy environment) wherever possible, using the original audio as a baseline. For \"content variation\" types, the text was first modified (e.g., introducing grammatical or pronunciation errors) before being re-recorded by the same speaker. Additionally, to test performance in extreme scenarios, subsets like Volume, Speed, and Unstable Signal underwent post-processing.</p>\n\n",
                "matched_terms": [
                    "from",
                    "content",
                    "grammatical",
                    "control",
                    "group"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Internal Two-person Dialogue Dataset.</span> Designed to support the DC module, this category&#8217;s data is processed as follows: the original long-form audio undergoes a two-stage segmentation with GPT-4O&#8212;first by topic, then refined into semantically coherent segments under one minute. From the transcriptions, GPT-4O generates task-specific QA pairs (e.g., for analysis or induction), followed by a final manual screening to verify question quality and answer accuracy.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SIF focuses on understanding and executing speech instructions, particularly the ability to handle paralinguistic features such as emotion, speaking rate, and dialect. It includes six sub-tasks: (1) Emotional Control: adjusting the emotional tone of speech to assess expressive generation; (2) Language Control: switching languages or dialects to test multilingual synthesis; (3) Non-verbal Vocalization: incorporating non-linguistic elements like sighs or nasal sounds to evaluate paralinguistic expressiveness; (4) Pacing Control: modifying speaking rate to examine control precision; (5) Style Control: switching speech styles to assess style transfer; (6) Volume Control: adjusting loudness to test stability.</p>\n\n",
                "matched_terms": [
                    "switching",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Robustness.</span> The Robustness module evaluates the stability of LALMs&#8217; performance under real-world interference conditions, ensuring reliable responses in challenging scenarios. The module encompasses three dimensions: Speaker Variations (SV), Environmental Variations (EV), and Content Variations (CV).</p>\n\n",
                "matched_terms": [
                    "content",
                    "variations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CV introduce linguistic disruptions: (1) Fillers: incorporate discourse markers (e.g., \"um\", \"ah\"); (2) Repetition: include repeated phrases/words; (3) Mispronunciation: introduce phonetic deviations; (4) Grammatical Error: employ ungrammatical constructions; (5) Topic Shift: implement abrupt topic changes; (6) Code-Switching: mix Chinese and English. Each category evaluates the model&#8217;s ability to maintain comprehension despite content imperfections.</p>\n\n",
                "matched_terms": [
                    "shift",
                    "repetition",
                    "fillers",
                    "mispronunciation",
                    "topic",
                    "content",
                    "grammatical",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "qwen25omni",
                    "gpt4oaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SC task, we assess a subset of pre-trained base models: Baichuan-Audio-Base, Kimi-Audio-Base, Qwen2-Audio-Base, and StepAudio2Mini-Base. Following the StoryCloze evaluation protocol, we compute the negative log-likelihood for both the correct and incorrect endings, with model selection determined by comparing these two values. For SIF tasks, the top six performing models undergo further Mean Opinion Score (MOS) evaluation. We sample the first 30 items from each relevant dataset, and eight expert evaluators rate the generated audio samples.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "control",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Knowledge, Mimo-Audio stands out in ML (84.01) and DC (87.92), surpassing other open-source models and even GPT-4o-Audio&#8217;s performance&#8212;suggesting strengths in deep reasoning and text semantic analysis. However, models like Baichuan-Audio-Chat show limited performance in GK (44.48) and ML (60.33), revealing gaps in multi-disciplinary knowledge coverage and step-by-step reasoning.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, open-source E2E LALMs exhibit task-specific strengths (e.g., Mimo-Audio in reasoning, StepAudio2Mini in Chinese dialogue and general knowledge) but face challenges in cross-lingual speech adaptation and comprehensive knowledge reasoning.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F3\" title=\"Figure 3 &#8227; 4.1 Configuration &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, EV.Echo, SV.Speed and SV.Elder cause the most severe performance degradation for most models. Scores of some models drop from over 80 in the control group to below 40 in these subsets, indicating that speech rate variation and acoustic echo are the most challenging perturbations for current LALMs. However, CV-related interferences (e.g., CV.Gram.Err, CV.Mispron) have relatively mild impacts. Some models (e.g., Mimo-Audio, StepAudio2Mini) show small score gaps between these subsets and the control group, suggesting models are more tolerant of &#8220;content-level flaws&#8221; than &#8220;speech/environment-level physical perturbations&#8221;.</p>\n\n",
                "matched_terms": [
                    "stepaudio2mini",
                    "from",
                    "mimoaudio",
                    "control",
                    "group"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "from",
                    "kimiaudio",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "evaluation",
                    "difference",
                    "from",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces VCB Bench, the first comprehensive benchmark for real Chinese voice conversation tasks of LALMs, covering Instruction Following, Knowledge, and Robustness. Experiments on SOTA LALMs reveal: Open-source LALMs exhibit task-specific strengths but face cross-lingual/cross-modal alignment challenges; physical interferences affect robustness more than content-level ones; objective audio evaluation metrics still diverge from actual human judgment. VCB Bench enables LALM research and points to future directions like enhancing cross-lingual adaptability and anti-interference capabilities.</p>\n\n",
                "matched_terms": [
                    "from",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio",
                    "mimoaudio",
                    "gpt4oaudio",
                    "control",
                    "qwen25omni",
                    "results",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "control",
                    "mimoaudio",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, GPT4o-Audio and Mimo-Audio demonstrate robust performance across Chinese and English instruction-following tasks, while cross-lingual capability and alignment between objective metrics and human judgment remain key improvement areas for LALMs.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "results",
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen2audioinstruct",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "mimoaudio",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, Mimo-Audio demonstrates robust reasoning and comprehension capabilities, while GPT4o-Audio excels in knowledge breadth but shows only moderate performance in mathematical reasoning. Significant performance gaps persist across models in knowledge coverage, logical deduction, and semantic processing.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "mimoaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "mimoaudio",
                    "baichuanaudiochat",
                    "variations",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "repetition",
                    "fillers",
                    "mispronunciation",
                    "mimoaudio",
                    "variations",
                    "content",
                    "grammatical",
                    "gpt4oaudio",
                    "error",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "variations",
                    "results",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            }
        ]
    },
    "A1.T20": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 20: A2A Result in TIF - A2A W/ ASR (A2A W/O ASR)",
        "body": "Model\\Task\nContinuation\nCreation\nEmpathy\nRecommendation\nRewriting\nSafety\nSimulation\n\n\n\n\nGLM4-Voice\n79.2 (83.2)\n80.8 (88.4)\n78.8 (81.8)\n88.6 (92.4)\n84.0 (88.4)\n75.4 (75.6)\n88.2 (92.6)\n\n\nKimi-Audio\n75.4 (76.8)\n81.4 (86.6)\n64.2 (64.8)\n81.0 (84.8)\n83.2 (88.4)\n75.8 (76.6)\n82.0 (83.4)\n\n\nQwen2.5-Onmi\n71.4 (72.4)\n75.4 (80.4)\n74.2 (75.8)\n84.8 (86.2)\n72.8 (79.0)\n80.0 (80.8)\n77.8 (79.2)\n\n\nBaichuan-Audio-Chat\n83.2 (81.8)\n\n86.0 (89.2)\n73.6 (71.4)\n85.6 (90.2)\n75.2 (79.4)\n78.6 (78.8)\n92.6 (93.4)\n\n\nStepAudio2Mini\n77.0 (82.2)\n77.4 (86.6)\n70.8 (73.8)\n79.4 (88.0)\n81.8 (87.8)\n62.6 (63.8)\n83.8 (95.2)\n\n\nGPT4o-Audio\n86.6 (89.2)\n85.4 (93.4)\n83.2 (85.6)\n91.2 (96.6)\n87.4 (93.4)\n83.6 (85.8)\n95.2 (98.2)",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Continuation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Creation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Empathy</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Recommendation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Rewriting</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Safety</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Simulation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.2 (83.2)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.8 (88.4)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.8 (81.8)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.6 (92.4)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.0 (88.4)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.4 (75.6)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.2 (92.6)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.4 (76.8)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.4 (86.6)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.2 (64.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.0 (84.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.2 (88.4)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.8 (76.6)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.0 (83.4)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Onmi</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.4 (72.4)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.4 (80.4)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.2 (75.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.8 (86.2)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.8 (79.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.0 (80.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.8 (79.2)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.2 (81.8)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">86.0</span> (89.2)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.6 (71.4)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.6 (90.2)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.2 (79.4)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.6 (78.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">92.6 (93.4)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.0 (82.2)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.4 (86.6)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.8 (73.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.4 (88.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.8 (87.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.6 (63.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.8 (95.2)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">86.6 (89.2)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.4 (<span class=\"ltx_text ltx_font_bold\">93.4</span>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">83.2 (85.6)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">91.2 (96.6)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.4 (93.4)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">83.6 (85.8)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">95.2 (98.2)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "creation",
            "modeltask",
            "recommendation",
            "glm4voice",
            "qwen25onmi",
            "kimiaudio",
            "gpt4oaudio",
            "a2a",
            "result",
            "safety",
            "stepaudio2mini",
            "tif",
            "continuation",
            "empathy",
            "rewriting",
            "simulation",
            "baichuanaudiochat",
            "asr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "creation",
                    "tif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "safety",
                    "continuation",
                    "tif",
                    "empathy",
                    "rewriting",
                    "simulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "a2a",
                    "tif",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "a2a",
                    "asr",
                    "kimiaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "glm4voice",
                    "kimiaudio",
                    "safety",
                    "tif",
                    "simulation",
                    "rewriting",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "tif",
                    "empathy",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "a2a",
                    "asr",
                    "continuation",
                    "gpt4oaudio",
                    "baichuanaudiochat"
                ]
            }
        ]
    },
    "A1.T21": {
        "source_file": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents",
        "caption": "Table 21: A2A Result in TIF-En - A2A W/ ASR (A2A W/O ASR)",
        "body": "Model\\Task\nContinuation En\nCreation En\nEmpathy En\nRecommendation En\nRewriting En\nSafety En\nSimulation En\n\n\n\n\nGLM4-Voice\n65.0 (81.6)\n75.4 (83.8)\n72.6 (77.4)\n78.0 (85.4)\n77.8 (83.8)\n69.2 (76.6)\n86.0 (89.8)\n\n\nKimi-Audio\n50.4 (62.0)\n60.4 (83.4)\n53.0 (56.0)\n62.4 (80.8)\n65.6 (79.0)\n78.4 (83.4)\n58.4 (69.8)\n\n\nQwen2.5-Omni\n57.0 (59.6)\n58.2 (64.2)\n56.4 (60.2)\n66.4 (72.4)\n56.2 (66.6)\n73.4 (75.6)\n56.2 (63.0)\n\n\nBaichuan-Audio-Chat\n79.8 (77.6)\n80.0 (81.0)\n75.8 (75.0)\n77.0 (76.8)\n79.8 (81.2)\n72.0 (72.0)\n86.2 (86.0)\n\n\nStepAudio2Mini\n66.2 (67.0)\n73.0 (78.0)\n55.2 (56.2)\n75.0 (77.0)\n70.6 (73.0)\n69.6 (70.4)\n84.2 (87.8)\n\n\nGPT4o-Audio\n90.2 (91.4)\n90.0 (94.8)\n85.0 (85.8)\n93.4 (95.8)\n92.4 (94.6)\n\n80.4 (81.6)\n96.0 (98.4)",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model\\Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Continuation En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Creation En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Empathy En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Recommendation En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Rewriting En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Safety En</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Simulation En</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GLM4-Voice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.0 (81.6)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.4 (83.8)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.6 (77.4)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.0 (85.4)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.8 (83.8)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.2 (76.6)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.0 (89.8)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Kimi-Audio</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.4 (62.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.4 (83.4)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.0 (56.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.4 (80.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">65.6 (79.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.4 (<span class=\"ltx_text ltx_font_bold\">83.4</span>)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.4 (69.8)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Qwen2.5-Omni</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">57.0 (59.6)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.2 (64.2)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.4 (60.2)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.4 (72.4)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.2 (66.6)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.4 (75.6)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.2 (63.0)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Baichuan-Audio-Chat</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.8 (77.6)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.0 (81.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.8 (75.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.0 (76.8)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.8 (81.2)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">72.0 (72.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.2 (86.0)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">StepAudio2Mini</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">66.2 (67.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">73.0 (78.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">55.2 (56.2)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.0 (77.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.6 (73.0)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">69.6 (70.4)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.2 (87.8)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPT4o-Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">90.2 (91.4)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">90.0 (94.8)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">85.0 (85.8)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">93.4 (95.8)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">92.4 (94.6)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">80.4</span> (81.6)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">96.0 (98.4)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tifen",
            "modeltask",
            "creation",
            "recommendation",
            "glm4voice",
            "kimiaudio",
            "gpt4oaudio",
            "a2a",
            "result",
            "safety",
            "stepaudio2mini",
            "continuation",
            "empathy",
            "rewriting",
            "simulation",
            "qwen25omni",
            "baichuanaudiochat",
            "asr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a> shows the complete results of \"A2A\" with ASR. To analyze the Audio-to-Audio (A2A) results in Chinese Text TIF and English TIF &#8212; where values outside parentheses denote scores after Automatic Speech Recognition (A2A W/ ASR) and values inside denote scores without ASR (A2A W/O ASR) &#8212; we focus on two aspects: models with the best performance after ASR, and models with minimal score changes across ASR (indicating high audio quality and clear pronunciation). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T20\" title=\"Table 20 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, GPT4o-Audio maintains the highest scores in most sub-tasks after ASR (e.g., Continuation: 86.6, Creation: 85.4, Recommendation: 91.2) and also leads in scores without ASR (e.g., Continuation: 89.2, Creation: 93.4), demonstrating robust performance even after ASR. Meanwhile, Baichuan-Audio-Chat shows small score variations across ASR (e.g., Continuation: 83.2 &#8594; 81.8, Recommendation: 85.6 &#8594; 90.2), reflecting clear audio generation.</p>\n\n",
            "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T21\" title=\"Table 21 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, GPT4o-Audio again dominates post-ASR scores (e.g., Continuation En: 90.2, Creation En: 90.0, Recommendation En: 93.4) and remains top-tier without ASR (e.g., Continuation En: 91.4, Creation En: 94.8). Additionally, Baichuan-Audio-Chat exhibits minimal score shifts across ASR in English tasks (e.g., Continuation En: 79.8 &#8594; 77.0, Creation En: 80.0 &#8594; 81.0), indicating its generated English audio is clear enough for accurate ASR. Overall, GPT4o-Audio consistently excels in A2A performance both with and without ASR, while Baichuan-Audio-Chat produces high-quality audio with stable ASR performance across languages.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Baichuan-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib11\" title=\"\">2025</a>)</cite> employs hierarchical RVQ discretization and dual audio heads to balance acoustic and linguistic objectives, enabling real-time bilingual communication. GLM4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib26\" title=\"\">2024</a>)</cite> introduces a three-module structure (Tokenizer-Backbone-Decoder) supporting emotion and dialect modeling. Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#bib.bib7\" title=\"\">2025</a>)</cite> fuses continuous acoustic and discrete semantic tokens in a dual-head architecture, achieving low-latency, high-fidelity streaming generation.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, VCB Bench covers three core dimensions: Instruction Following, Knowledge, and Robustness. Instruction Following includes Text Instruction Follow (TIF) (e.g., continuation, creation), Speech Instruction Follow (SIF) (e.g., emotional, volume control), and Multi-turn Dialog (MTD) tasks. The Knowledge module assesses General Knowledge (GK) across 12 disciplines, Mathematical Logic (ML), Discourse Comprehension (DC), and Story Continuation (SC). Robustness introduces real-world perturbations from speaker variations, environmental noise, and content modifications to evaluate model stability.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "continuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TIF assesses the model&#8217;s ability to respond to textual instructions through seven sub-tasks, each examining text generation and semantic comprehension: (1) Continuation: extending a given text fragment to evaluate coherence and creativity; (2) Creation: generating original content based on a given theme to assess inventiveness and organization; (3) Empathy: understanding and responding to emotional expressions to examine affective perception; (4) Recommendation: providing suggestions based on user needs to evaluate information integration; (5) Rewriting: adapting text in style or structure to test reorganization ability; (6) Safety: identifying and rejecting harmful instructions to assess compliant response; (7) Simulation: role-playing in dialogue to examine contextual adaptation.</p>\n\n",
                "matched_terms": [
                    "creation",
                    "recommendation",
                    "safety",
                    "continuation",
                    "empathy",
                    "rewriting",
                    "simulation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the latest and most capable LALMs. The selected models comprise GLM4-Voice, Kimi-Audio, Qwen2.5-Omni, Baichuan-Audio-Chat, Qwen2-Audio-Instruct, StepAudio, StepAudio2Mini, Mimo-Audio, and GPT4o-Audio.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "kimiaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Instruction Following, Mimo-Audio excels in TIF/TIF-En with scores close to GPT-4o-Audio, indicating strong cross-lingual text instruction adaptation. For SIF, StepAudio series and Kimi-Audio perform robustly in Chinese SIF, yet their SIF-En scores lag significantly behind, reflecting challenges in handling English speech&#8217;s paralinguistic features. In MTD, StepAudio2Mini leads among open-source models with 87.80, outperforming counterparts like Baichuan-Audio-Chat, which highlights divergence in long-context dialogue logic control.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding model robustness, GPT-4o-Audio maintains excellent capability despite significant drops in specific subsets like SV.Speed, attributable to its high baseline scores ensuring practical usability. Among open-source models, Mimo-Audio and StepAudio2Mini exhibit relatively prominent robustness with both high absolute scores and limited performance gaps. In contrast, models like Baichuan-Audio-Chat face constraints primarily due to their lower absolute scores rather than extreme fluctuations, indicating insufficient real-scene adaptability despite moderate performance drops.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the text-speech alignment capability of LALMs, we conduct an ablation study, which is shown on Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F4\" title=\"Figure 4 &#8227; 4.2 Main Result &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The visualization is based on two selection criteria from TIF and TIF-En: the four models with the highest A2A W/ ASR scores, and the four datasets with the largest mean score differences between A2A W/ ASR and A2T. Results for Chinese and English tasks are plotted separately in the upper and lower sections of the figure, respectively.</p>\n\n",
                "matched_terms": [
                    "tifen",
                    "a2a",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, models like GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) demonstrate strong text-speech alignment&#8212;their A2T results are close to A2A W/ ASR results, indicating consistent semantic output between directly generated text and text transcribed from speech. In contrast, models such as Qwen2.5-Omni (Chinese) and Kimi-Audio (English) show large discrepancies between A2T and A2A W/ ASR, suggesting mismatches in semantics between text and speech generation. Meanwhile, for audio generation quality (assessed by the gap between A2A W/ ASR and A2A W/O ASR, where smaller gaps imply clearer audio), GLM4-Voice (Chinese), Kimi-Audio (Chinese), and Baichuan-Audio-Chat (English) exhibit minimal differences between A2A W/ ASR and A2A W/O ASR, meaning their generated audio is clear enough for accurate ASR transcription and well-suited for audio-only scenarios. Conversely, models like Kimi-Audio (English) have A2A W/ ASR scores far lower than A2A W/O ASR, revealing that their generated audio suffers from poor clarity&#8212;limiting usability in audio-focused scenarios even if A2T performance is strong. Overall, models such as GLM4-Voice (Chinese) and Baichuan-Audio-Chat (English) excel in both text-speech alignment and audio generation quality, while other LALMs face challenges in cross-lingual adaptation or audio clarity, highlighting the need for targeted optimization in these aspects.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "a2a",
                    "asr",
                    "kimiaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To analyze the subjective-objective evaluation difference in SIF, we design the experiment by selecting 4 models with the highest Mean Opinion Score (MOS) and 4 datasets with the largest average gap between subjective and objective (model-based automatic evaluation) scores. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#S4.F5\" title=\"Figure 5 &#8227; 4.5.2 Subjective-Objective Comparison &#8227; 4.5 Ablation Study &#8227; 4 Experiment &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, leading models like GPT4o-Audio and GLM4-Voice show smaller discrepancies between subjective scores and objective scores across most sub-dimensions&#8212;indicating their audio quality evaluation better aligns with human perception. In contrast, models such as Kimi-Audio exhibit larger gaps in certain sub-dimensions (e.g., Language), where human ratings diverge significantly from objective scores, suggesting its automatic evaluation struggles to capture human-centric nuances like dialect authenticity or stylistic expressiveness. Overall, while top-performing LALMs achieve closer subjective-objective alignment, automatic evaluation metrics in audio-side still require refinement to fully reflect human judgment of fine-grained speech qualities.</p>\n\n",
                "matched_terms": [
                    "glm4voice",
                    "gpt4oaudio",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T8\" title=\"Table 8 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T13\" title=\"Table 13 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> shows the complete results of instruction following. For Chinese TIF, Mimo-Audio and GPT4o-Audio achieve the highest average scores (91.21 and 91.24, respectively), excelling in tasks like Recommendation (Mimo-Audio: 99.00; GPT4o-Audio: 95.20) and Simulation (Mimo-Audio: 99.20; GPT4o-Audio: 98.20). Qwen2.5-Omni stands out in Safety (93.40), while GLM4-Voice performs strongly in Rewriting (90.00). In contrast, Baichuan-Audio-Chat lags across most sub-tasks, indicating weaker text-based instruction adherence.\nFor Chinese SIF (Table 9, objective), GPT4o-Audio attains the highest average (88.15), leading in Emotional Control (92.40) and Language Control (87.80). Kimi-Audio excels in Style Control (95.00), and GLM4-Voice tops Emotional Control (93.00). Subjective results (Table 10) show GPT4o-Audio and GLM4-Voice as frontrunners, yet all models score lower in subjective evaluations than objective ones&#8212;revealing gaps between automatic metrics and human perception of speech quality.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "glm4voice",
                    "kimiaudio",
                    "safety",
                    "simulation",
                    "rewriting",
                    "gpt4oaudio",
                    "qwen25omni",
                    "baichuanaudiochat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English TIF, Mimo-Audio and GPT4o-Audio dominate again: Mimo-Audio leads in Empathy En (86.80), while GPT4o-Audio excels in Recommendation En (95.40). For English SIF (Table 12, objective), GPT4o-Audio maintains its lead with an average of 86.07, outperforming others in Emotional Control En (89.40) and Style Control En (91.40). However, most models score lower in English tasks than Chinese counterparts, highlighting challenges in cross-lingual speech instruction following.</p>\n\n",
                "matched_terms": [
                    "recommendation",
                    "empathy",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T14\" title=\"Table 14 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11098v1#A1.T16\" title=\"Table 16 &#8227; A.5 Complete Results Of Text-Speech Alignment &#8227; Appendix A Appendix &#8227; VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the complete results of konwledge. For General Knowledge, GPT4o-Audio (61.29) and StepAudio2Mini (61.15) achieve relatively high average scores. For example, GPT4o-Audio excels in Econ (85.42) and Geogr (62.00), while StepAuido2Mini leads in Chem (80.43) and Phys (74.51). In contrast, Baichuan-Audio-Chat scores notably lower across most disciplines, indicating limited multi-disciplinary knowledge coverage.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "stepaudio2mini",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Mathematical and Logical Reasoning, Mimo-Audio (84.01) and StepAudio2Mini (81.30) stand out with the highest averages. Mimo-Audio dominates in Logic (85.53) and Analogy (52.50), while Kimi-Audio leads in Basic Math (98.63) and Induction (85.94). GPT4o-Audio also performs strongly, especially in Medium Math (91.18). Models like Baichuan-Audio-Chat (60.33) and Qwen2-Audio-Instruct (60.78) show weaker capabilities in reasoning sub-tasks (e.g., Analysis, Analogy).</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio",
                    "stepaudio2mini",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Discourse Comprehension, Mimo-Audio (87.92) achieves the highest average, excelling in Inference (95.15), Induction (88.50) and Analysis (80.87). Qwen2.5-Omni (73.72) and GPT4o-Audio (77.64) also perform well, while Baichuan-Audio-Chat (54.38) and StepAudio (59.52) lag&#8212;reflecting challenges in semantic inference and fine-grained text analysis.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Environmental Variations, Mimo-Audio leads in post-interference scores (99.40 in Outdoors non-vocal noise, 99.60 in Vocal-Music) with near-zero differences, and GPT4o-Audio also maintains high scores with small drops (88.00 in Background Chat, 94.20 in Voice Announce); Echo and Unstable Signal are most disruptive (e.g., Baichuan-Audio-Chat scores 45.00 in Echo with a -20.00 drop), while Outdoors and Voice Announce have milder effects.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Content Variations, Mimo-Audio and GPT4o-Audio secure the highest post-interference scores (e.g., Mimo-Audio&#8217;s 96.00 in Mispronunciation, GPT4o-Audio&#8217;s 92.60 in the same task) and smallest negative differences (e.g., Mimo-Audio&#8217;s -3.60 in Mispronunciation, GPT4o-Audio&#8217;s +1.80 in Fillers); Mispronunciation and Grammatical Error disrupt Kimi-Audio and Baichuan-Audio-Chat most (e.g., Kimi-Audio&#8217;s 61.80 in Mispronunciation with a -23.20 drop), whereas Fillers and Repetition barely affect top models. Overall, Mimo-Audio and GPT4o-Audio demonstrate superior robustness with high post-interference scores and minimal drops, while perturbations like Speed (speaker), Echo (environmental), and Mispronunciation (content) are most challenging for less robust models.</p>\n\n",
                "matched_terms": [
                    "baichuanaudiochat",
                    "gpt4oaudio",
                    "kimiaudio"
                ]
            }
        ]
    }
}