{
    "S3.T1": {
        "source_file": "POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation",
        "caption": "Table 1: BLEU scores on the FLEURS test set for five training languages and two zero-shot evaluation languages",
        "body": "Methods\nFLEURS\nFLEURS-Zero\n\n\nen→\\rightarrowzh\nja→\\rightarrowzh\nes→\\rightarrowzh\nko→\\rightarrowzh\nru→\\rightarrowzh\navg.\nast(es)\nky(en)\n\n\nBaseline Models\n\n\nQwen2.5-Omni [qwenomni]\n\n38.35\n23.76\n27.81\n27.05\n29.69\n29.33\n24.95\n2.86\n\n\nMinMo [minmo]\n\n40.68\n25.14\n32.05\n23.39\n34.34\n31.12\n-\n-\n\n\nWhisperV3+Qwen2.5 (end-to-end)\n40.39\n24.92\n30.34\n27.47\n31.43\n30.91\n17.52\n3.25\n\n\nOur Models\n\n\nOur Model\n40.87\n25.97\n31.10\n28.97\n32.30\n31.84\n25.11\n5.76\n\n\n     (i) w/o bias compensation module\n40.44\n24.83\n30.91\n28.06\n31.31\n31.11\n25.05\n5.62\n\n\n    (ii) w/o align\n40.22\n25.22\n30.89\n25.91\n31.40\n30.73\n24.85\n4.96\n\n\n    (i) align en-x w/o engrd\n40.15\n24.51\n30.68\n27.17\n31.51\n30.80\n18.43\n4.30\n\n\n    (ii) align en-x\n23.69\n25.22\n19.50\n27.09\n31.21\n25.34\n18.76\n4.30",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Methods</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\">FLEURS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">FLEURS-Zero</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ja<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">es<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ko<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ru<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ast(es)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ky(en)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\">Baseline Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwenomni</span>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">38.35</td>\n<td class=\"ltx_td ltx_align_center\">23.76</td>\n<td class=\"ltx_td ltx_align_center\">27.81</td>\n<td class=\"ltx_td ltx_align_center\">27.05</td>\n<td class=\"ltx_td ltx_align_center\">29.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">29.33</td>\n<td class=\"ltx_td ltx_align_center\">24.95</td>\n<td class=\"ltx_td ltx_align_center\">2.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">MinMo <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minmo</span>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">40.68</td>\n<td class=\"ltx_td ltx_align_center\">25.14</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">32.05</span></td>\n<td class=\"ltx_td ltx_align_center\">23.39</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">34.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">31.12</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">WhisperV3+Qwen2.5 (end-to-end)</td>\n<td class=\"ltx_td ltx_align_center\">40.39</td>\n<td class=\"ltx_td ltx_align_center\">24.92</td>\n<td class=\"ltx_td ltx_align_center\">30.34</td>\n<td class=\"ltx_td ltx_align_center\">27.47</td>\n<td class=\"ltx_td ltx_align_center\">31.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">30.91</td>\n<td class=\"ltx_td ltx_align_center\">17.52</td>\n<td class=\"ltx_td ltx_align_center\">3.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\">Our Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Our Model</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">40.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">25.97</span></td>\n<td class=\"ltx_td ltx_align_center\">31.10</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">28.97</span></td>\n<td class=\"ltx_td ltx_align_center\">32.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">31.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">25.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">5.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;   (i) w/o bias compensation module</td>\n<td class=\"ltx_td ltx_align_center\">40.44</td>\n<td class=\"ltx_td ltx_align_center\">24.83</td>\n<td class=\"ltx_td ltx_align_center\">30.91</td>\n<td class=\"ltx_td ltx_align_center\">28.06</td>\n<td class=\"ltx_td ltx_align_center\">31.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">31.11</td>\n<td class=\"ltx_td ltx_align_center\">25.05</td>\n<td class=\"ltx_td ltx_align_center\">5.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;  (ii) w/o align</td>\n<td class=\"ltx_td ltx_align_center\">40.22</td>\n<td class=\"ltx_td ltx_align_center\">25.22</td>\n<td class=\"ltx_td ltx_align_center\">30.89</td>\n<td class=\"ltx_td ltx_align_center\">25.91</td>\n<td class=\"ltx_td ltx_align_center\">31.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">30.73</td>\n<td class=\"ltx_td ltx_align_center\">24.85</td>\n<td class=\"ltx_td ltx_align_center\">4.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">&#8194;&#8202;  (i) align en-x w/o engrd</td>\n<td class=\"ltx_td ltx_align_center\">40.15</td>\n<td class=\"ltx_td ltx_align_center\">24.51</td>\n<td class=\"ltx_td ltx_align_center\">30.68</td>\n<td class=\"ltx_td ltx_align_center\">27.17</td>\n<td class=\"ltx_td ltx_align_center\">31.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">30.80</td>\n<td class=\"ltx_td ltx_align_center\">18.43</td>\n<td class=\"ltx_td ltx_align_center\">4.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">&#8194;&#8202;  (ii) align en-x</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">23.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">19.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">27.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">31.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">25.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">18.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.30</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "en→rightarrowzh",
            "astes",
            "avg",
            "ja→rightarrowzh",
            "enx",
            "qwen25omni",
            "whisperv3qwen25",
            "our",
            "test",
            "kyen",
            "five",
            "zeroshot",
            "baseline",
            "methods",
            "compensation",
            "endtoend",
            "model",
            "evaluation",
            "fleurszero",
            "es→rightarrowzh",
            "ru→rightarrowzh",
            "module",
            "bias",
            "align",
            "minmo",
            "set",
            "bleu",
            "models",
            "qwenomni",
            "scores",
            "two",
            "fleurs",
            "languages",
            "engrd",
            "ko→rightarrowzh"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the BLEU scores on the FLEURS test set for five training languages and two zero-shot languages. We first constructed a baseline model consisting of Whisper-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>]</cite>, a Q-Former module, and Qwen-2.5-7B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen</span>]</cite>. This baseline was then augmented with our cross-lingual alignment framework. We also benchmark our system against the strongest recent SpeechLLMs.\nThe proposed method yields consistent BLEU improvements (average +0.93). The gains are particularly large for zero-shot languages (average +5.05). Although large-scale models like Minmo<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minmo</span>]</cite> (not publicly available) are trained on much larger corpora, our system already surpasses them on several metrics. As the alignment mechanism is model-agnostic, it can be integrated into larger systems to further enhance multilingual performance.</p>\n\n",
            "<p class=\"ltx_p\">To quantify the contribution of each alignment module, we conducted a series of ablation studies.\nAll experiments used an identical model backbone and incrementally activated two alignment mechanisms: (i) Bias Compensation module and (ii) OT alignment. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that the coarse alignment yields noticeable performance gains, while the fine-grained OT alignment further improves overall translation quality.</p>\n\n",
            "<p class=\"ltx_p\">When selecting parallel speech pairs for alignment, we evaluated three distinct strategies:\n(i) English-anchor (frozen English): alignment loss not back-propagated through the English branch.\n(ii) English-anchor: alignment loss back-propagated through the English branch.\n(iii) Random pairwise alignment: at each step, two randomly chosen languages are aligned symmetrically.\nThe results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that Strategy (ii) yields the poorest performance, and Strategy (i) is slightly better. Strategy (iii), used in our final framework, achieves the best results. Both Strategies (i) and (ii) retain an English-centric asymmetry. Strategy (ii)&#8217;s bidirectional updates amplify representation drift and forgetting in low-resource languages. Strategy (i) is more stable but still constrained by a fixed anchor. By contrast, Strategy (iii) symmetrically aligns randomly chosen language pairs at each step, removing anchor dominance and fostering cross-lingual co-learning, which explains its superior performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT).\nHowever, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance.\nIn this work, we propose <span class=\"ltx_text ltx_font_bold\">POTSA</span> (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps.\nFirst, we introduce a Bias Compensation module to coarsely align initial speech representations across languages.\nSecond, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations.\nThen, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers.\nExperiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.</p>\n\n",
                "matched_terms": [
                    "compensation",
                    "bleu",
                    "models",
                    "five",
                    "module",
                    "zeroshot",
                    "bias",
                    "fleurs",
                    "languages",
                    "align",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid advances of large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen</span>]</cite> in natural language processing tasks<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4</span>]</cite> are driving a paradigm shift within the speech community<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">survey</span>]</cite>.\nSpeechLLMs have consequently emerged, marking a transition from the conventional encoder-decoder pipeline to a unified, language-model-centric framework\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salmon</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwenaudio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwenomni</span>]</cite>.\nBy mapping speech representation into a textual embedding space, these models support reasoning and generation directly from speech<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">survey</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet fulfilling this requirement remains challenging because state-of-the-art S2TT models achieve high accuracy on high-resource pairs but lag on low-resource pairs <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stsurvey</span>]</cite>.\nAlthough this performance bias is partially attributable to skewed training data distributions, the deeper underlying cause lies in insufficient cross-lingual comparability of speech representations\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onlyshared</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whispershared</span>]</cite>.\nAs illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. (a), we select the state-of-the-art multilingual encoder Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>]</cite> and visualize its speech representations, which form language-specific clusters:\nsamples from the identical language coalesce rather than being grouped by shared semantic content. This prevents the decoder from effectively reusing learned translation mappings, thus amplifying the performance gap.\nCross-lingual alignment is therefore the key to mitigating this bias. As illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. (b), this alignment encourages the model to induce a cross-lingually shared representation: an abstract &#8216;pivot language&#8217;.</p>\n\n",
                "matched_terms": [
                    "models",
                    "bias",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Nevertheless, current alignment research in the speech domain is confined to speech-to-text mappings.\nBy bridging the modality gap between speech and text, these methods facilitate knowledge transfer from machine translation (MT) decoders<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign1</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign3</span>]</cite>.\nWhile such a strategy highlights the importance of cross-modal modeling, it implicitly carries two limitations:\nFirst, alignment is typically constrained to the speech-to-text direction, where the learning objective is defined by textual supervision<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign3</span>]</cite>. As a result, the speech encoder is forced to follow text-driven constraints, often compressing fine-grained acoustic details not represented in text.\nSecond, this paradigm treats each source language independently, neglecting the underlying structural similarities and transferable representations shared across languages<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">multilingual</span>]</cite>. This isolation restricts cross-lingual knowledge transfer and reduces the capacity for generalization.</p>\n\n",
                "matched_terms": [
                    "methods",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these observations, we propose <span class=\"ltx_text ltx_font_bold\">POTSA</span> (Parallel Optimal Transport for Speech Alignment), a novel alignment framework based on cross-lingual parallel speech pairs and Optimal Transport (OT) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ot</span>]</cite> to explicitly strengthen the consistency of cross-lingual representations for SpeechLLM in S2TT.\nThe framework trains only the Q-Former<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">blip</span>]</cite> to project encoder outputs cross-lingually, while freezing both the encoder and LLM.\nFirst, we introduce a Bias Compensation module to mitigate language-specific shifts at the encoder output, narrowing cross-lingual gaps and preparing for fine-grained alignment within the speech representations.\nSubsequently, we leverage cross-lingual parallel speech pairs with identical semantic content and impose token-level OT constraints within the Q-Former. This constraint achieves a finer-grained alignment of source speech representations.\nFinally, we adopt layer-wise contributions and an online reward-guided scheduling strategy that applies OT constraints to the most effective layers.\nExperiments on the FLEURS dataset show state-of-the-art performance, with +0.93 BLEU across five common languages and +5.05 BLEU on zero-shot languages using only 10 hours of parallel speech per source language. Our code is publicly available at <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Sslnon/POTSA\" title=\"\">https://github.com/Sslnon/POTSA</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "compensation",
                    "bleu",
                    "five",
                    "module",
                    "zeroshot",
                    "bias",
                    "fleurs",
                    "languages",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of the SpeechLLM architecture adopted in this study. Our goal is to improve cross-lingual consistency of source speech representations in S2TT. We integrate an alignment framework into the model, as highlighted by the dashed box.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a Bias Compensation Module to coarsely align source speech representations and enforce cross-lingual consistency.\nWe assume that each encoder output can be additively decomposed into a language-neutral component and a language-specific bias term, i.e., <math alttext=\"H^{(i)}_{x}=\\tilde{H}^{(i)}_{x}+b_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>H</mi><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mrow><msubsup><mover accent=\"true\"><mi>H</mi><mo>~</mo></mover><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>+</mo><msub><mi>b</mi><mi>x</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">H^{(i)}_{x}=\\tilde{H}^{(i)}_{x}+b_{x}</annotation></semantics></math><cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bc1</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bc2</span>]</cite>.\nHere, <math alttext=\"\\mathbf{H}_{x}^{(i)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119815;</mi><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{H}_{x}^{(i)}</annotation></semantics></math> denotes the encoder representation of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th utterance in language <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, whereas <math alttext=\"\\mathbf{b}_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119835;</mi><mi>x</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{b}_{x}</annotation></semantics></math> captures language-dependent bias shared by all utterances of that language.\nIn practice, we estimate <math alttext=\"\\mathbf{b}_{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#119835;</mi><mi>x</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{b}_{x}</annotation></semantics></math> by averaging sentence-level representations after temporal pooling:</p>\n\n",
                "matched_terms": [
                    "compensation",
                    "bias",
                    "align",
                    "module"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After coarse alignment by removing language-specific global bias, we propose a fine-grained alignment strategy based on Optimal Transport (OT) and cross-lingual parallel speech pairs.\nSince parallel speech pairs convey identical semantic content, we use them as alignment anchors only during training. Only a limited quantity of such data is sufficient to learn robust cross-lingual representations.\nFor each pair of parallel utterances <math alttext=\"(x_{s},x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>s</mi></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{s},x_{t})</annotation></semantics></math>, the projection layer produces two token sequences of embeddings, <math alttext=\"h_{s}=\\{u_{i}\\}_{i=1}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>s</mi></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">h_{s}=\\{u_{i}\\}_{i=1}^{n}</annotation></semantics></math> and <math alttext=\"h_{t}=\\{v_{j}\\}_{j=1}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">h_{t}=\\{v_{j}\\}_{j=1}^{m}</annotation></semantics></math>.\nTo reinforce consistency of cross-lingual semantic representations, we measure and minimize the discrepancy using the Sinkhorn distance&#8212;an entropy-regularized <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ot</span>]</cite> approximation of the OT objective. Let <math alttext=\"Z\\in\\mathbb{R}_{+}^{n\\times m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><msubsup><mi>&#8477;</mi><mo>+</mo><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>m</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">Z\\in\\mathbb{R}_{+}^{n\\times m}</annotation></semantics></math> denote the transport plan under soft marginal constraints (typically uniform over valid tokens), and let <math alttext=\"c(u_{i},v_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c(u_{i},v_{j})</annotation></semantics></math> denote the ground cost (e.g., cosine distance). The loss is:</p>\n\n",
                "matched_terms": [
                    "bias",
                    "training",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> controls the smoothness (or sparsity) of the transport plan.\nThe Sinkhorn-based OT loss is applied to a selected set of projection (Q-Former) layers <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, encouraging token-wise semantic alignment between languages. Finally, we jointly optimise this alignment loss with the translation cross-entropy:</p>\n\n",
                "matched_terms": [
                    "set",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> balances the two terms, ensuring the model simultaneously learns accurate speech translation and cross-lingual token-level correspondence.\nThe token-wise OT loss complements coarse bias compensation with fine-grained semantic alignment, improving cross-lingual consistency of source speech representations.</p>\n\n",
                "matched_terms": [
                    "compensation",
                    "bias",
                    "model",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first trained a SpeechLLM on the large-scale CoVoST2 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">covost2</span>]</cite> using 364 hours of training data. The training proceeded in two stages: first, the model was pretrained on automatic speech recognition (ASR; English&#8594;English), followed by training on speech-to-text translation (S2TT; English&#8594;Chinese) tasks.\nSubsequently, we performed multilingual mixed-supervision fine-tuning on FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fleurs</span>]</cite>, which covers over 100 languages with approximately 10 hours of supervised speech each. Five source languages&#8212;English (en), Japanese (ja), Spanish (es), Korean (ko), and Russian (ru)&#8212;were used to construct x&#8594;Chinese S2TT tasks.\nModel performance was evaluated on the FLEURS test set using BLEU-4, and alignment was assessed by Recall@1 and Jensen&#8211;Shannon Divergence (JSD).\nWe built a SpeechLLM within the SLAM-LLM framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/X-LANCE/SLAM-LLM\" title=\"\">https://github.com/X-LANCE/SLAM-LLM</a></span></span></span>, where the speech encoder and language model are frozen, and cross-lingual alignment is achieved via a Q-Former (8 Transformer blocks, 80 query tokens) projection layer <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cost</span>]</cite>. Training follows a two-stage schedule with learning rates of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>, respectively, with a warm-up mechanism to stabilize early training. The OT-based joint loss employs a weighting coefficient of 10. All experiments were conducted on NVIDIA RTX 4090 GPUs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "test",
                    "five",
                    "two",
                    "fleurs",
                    "languages",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine how the choice of alignment layer affects cross-lingual alignment, we evaluated five strategies:\n(i) Select a single layer.\n(ii) Select multiple fixed layers\n(iii) Select a random layer.\n(iv) Apply the online reward-guided layer scheduling strategy across all Q-Former layers.\n(v) Restrict strategy to the lower Q-Former layers.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T2\" title=\"Table 2 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that permitting a moderate degree of layer exploration is beneficial. Notably, Strategy (iv), which explores the full depth of the Q-Former, performs the worst, whereas Strategy (v) yields the best alignment quality.\nAmong the three na&#239;ve baselines, Strategy (iii) surpasses Strategy (i). Notably, Strategies (ii) and (iv), which apply alignment to multiple or deeper Q-Former layers, yield the poorest results, whereas Strategy (v) attains the best alignment quality. We attribute this to the higher-level features being closer to the decoder and primarily updated by the cross-entropy (CE) loss. Adding cross-lingual alignment at the upper or multiple layers creates competing objectives. The CE loss promotes translation accuracy, whereas the alignment term forces high-level representations to converge across languages, leading to degraded performance.</p>\n\n",
                "matched_terms": [
                    "five",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we examine whether the coarse- and fine-alignment modules effectively bring speech representations closer across languages.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (up) presents speech representations before and after applying Bias Compensation.\nThe coarse alignment effectively reduces systematic interlingual offset.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (down) illustrates shifts in speech representations due to alignment, plotted by Recall@1 (R@1) and 1-JSD.\nMost language pairs move from the grey baseline points toward the upper-right quadrant, indicating simultaneous improvements in retrieval accuracy and representation consistency.</p>\n\n",
                "matched_terms": [
                    "compensation",
                    "bias",
                    "baseline",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a cross-lingual speech alignment framework to bridge the gap in multilingual speech-to-text translation.\nExperiments on the FLEURS benchmark show consistent translation gains with only a small number of parallel speech pairs.\nSuch limited data suffice to enhance cross-lingual consistency and unlock low-resource translation capability in existing large models.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "fleurs"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation",
        "caption": "Table 2: Performance comparison of different alignment strategies.",
        "body": "ID\nLayer selection strategy\nAvg. BLEU\n\n\n\n\n(i)\nsingle\n31.36\n\n\n(ii)\nmulti\n30.19\n\n\n(iii)\nrandom\n31.50\n\n\n(iv)\nours (across all layers)\n30.86\n\n\n(v)\nours\n31.84",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ID</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Layer selection strategy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Avg. BLEU</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">(i)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">single</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">(ii)</th>\n<td class=\"ltx_td ltx_align_center\">multi</td>\n<td class=\"ltx_td ltx_align_center\">30.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">(iii)</th>\n<td class=\"ltx_td ltx_align_center\">random</td>\n<td class=\"ltx_td ltx_align_center\">31.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">(iv)</th>\n<td class=\"ltx_td ltx_align_center\">ours (across all layers)</td>\n<td class=\"ltx_td ltx_align_center\">30.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">(v)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">31.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "random",
            "strategies",
            "layers",
            "strategy",
            "bleu",
            "across",
            "all",
            "different",
            "ours",
            "selection",
            "avg",
            "single",
            "alignment",
            "layer",
            "performance",
            "comparison",
            "iii",
            "multi"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To examine how the choice of alignment layer affects cross-lingual alignment, we evaluated five strategies:\n(i) Select a single layer.\n(ii) Select multiple fixed layers\n(iii) Select a random layer.\n(iv) Apply the online reward-guided layer scheduling strategy across all Q-Former layers.\n(v) Restrict strategy to the lower Q-Former layers.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T2\" title=\"Table 2 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that permitting a moderate degree of layer exploration is beneficial. Notably, Strategy (iv), which explores the full depth of the Q-Former, performs the worst, whereas Strategy (v) yields the best alignment quality.\nAmong the three na&#239;ve baselines, Strategy (iii) surpasses Strategy (i). Notably, Strategies (ii) and (iv), which apply alignment to multiple or deeper Q-Former layers, yield the poorest results, whereas Strategy (v) attains the best alignment quality. We attribute this to the higher-level features being closer to the decoder and primarily updated by the cross-entropy (CE) loss. Adding cross-lingual alignment at the upper or multiple layers creates competing objectives. The CE loss promotes translation accuracy, whereas the alignment term forces high-level representations to converge across languages, leading to degraded performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT).\nHowever, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance.\nIn this work, we propose <span class=\"ltx_text ltx_font_bold\">POTSA</span> (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps.\nFirst, we introduce a Bias Compensation module to coarsely align initial speech representations across languages.\nSecond, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations.\nThen, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers.\nExperiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "strategy",
                    "across",
                    "bleu",
                    "alignment",
                    "layer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet fulfilling this requirement remains challenging because state-of-the-art S2TT models achieve high accuracy on high-resource pairs but lag on low-resource pairs <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stsurvey</span>]</cite>.\nAlthough this performance bias is partially attributable to skewed training data distributions, the deeper underlying cause lies in insufficient cross-lingual comparability of speech representations\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onlyshared</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whispershared</span>]</cite>.\nAs illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. (a), we select the state-of-the-art multilingual encoder Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>]</cite> and visualize its speech representations, which form language-specific clusters:\nsamples from the identical language coalesce rather than being grouped by shared semantic content. This prevents the decoder from effectively reusing learned translation mappings, thus amplifying the performance gap.\nCross-lingual alignment is therefore the key to mitigating this bias. As illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. (b), this alignment encourages the model to induce a cross-lingually shared representation: an abstract &#8216;pivot language&#8217;.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Nevertheless, current alignment research in the speech domain is confined to speech-to-text mappings.\nBy bridging the modality gap between speech and text, these methods facilitate knowledge transfer from machine translation (MT) decoders<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign1</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign3</span>]</cite>.\nWhile such a strategy highlights the importance of cross-modal modeling, it implicitly carries two limitations:\nFirst, alignment is typically constrained to the speech-to-text direction, where the learning objective is defined by textual supervision<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">s2talign3</span>]</cite>. As a result, the speech encoder is forced to follow text-driven constraints, often compressing fine-grained acoustic details not represented in text.\nSecond, this paradigm treats each source language independently, neglecting the underlying structural similarities and transferable representations shared across languages<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">multilingual</span>]</cite>. This isolation restricts cross-lingual knowledge transfer and reduces the capacity for generalization.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "strategy",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these observations, we propose <span class=\"ltx_text ltx_font_bold\">POTSA</span> (Parallel Optimal Transport for Speech Alignment), a novel alignment framework based on cross-lingual parallel speech pairs and Optimal Transport (OT) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ot</span>]</cite> to explicitly strengthen the consistency of cross-lingual representations for SpeechLLM in S2TT.\nThe framework trains only the Q-Former<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">blip</span>]</cite> to project encoder outputs cross-lingually, while freezing both the encoder and LLM.\nFirst, we introduce a Bias Compensation module to mitigate language-specific shifts at the encoder output, narrowing cross-lingual gaps and preparing for fine-grained alignment within the speech representations.\nSubsequently, we leverage cross-lingual parallel speech pairs with identical semantic content and impose token-level OT constraints within the Q-Former. This constraint achieves a finer-grained alignment of source speech representations.\nFinally, we adopt layer-wise contributions and an online reward-guided scheduling strategy that applies OT constraints to the most effective layers.\nExperiments on the FLEURS dataset show state-of-the-art performance, with +0.93 BLEU across five common languages and +5.05 BLEU on zero-shot languages using only 10 hours of parallel speech per source language. Our code is publicly available at <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Sslnon/POTSA\" title=\"\">https://github.com/Sslnon/POTSA</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "strategy",
                    "across",
                    "bleu",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After coarse alignment by removing language-specific global bias, we propose a fine-grained alignment strategy based on Optimal Transport (OT) and cross-lingual parallel speech pairs.\nSince parallel speech pairs convey identical semantic content, we use them as alignment anchors only during training. Only a limited quantity of such data is sufficient to learn robust cross-lingual representations.\nFor each pair of parallel utterances <math alttext=\"(x_{s},x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>s</mi></msub><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{s},x_{t})</annotation></semantics></math>, the projection layer produces two token sequences of embeddings, <math alttext=\"h_{s}=\\{u_{i}\\}_{i=1}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>s</mi></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">h_{s}=\\{u_{i}\\}_{i=1}^{n}</annotation></semantics></math> and <math alttext=\"h_{t}=\\{v_{j}\\}_{j=1}^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">h_{t}=\\{v_{j}\\}_{j=1}^{m}</annotation></semantics></math>.\nTo reinforce consistency of cross-lingual semantic representations, we measure and minimize the discrepancy using the Sinkhorn distance&#8212;an entropy-regularized <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ot</span>]</cite> approximation of the OT objective. Let <math alttext=\"Z\\in\\mathbb{R}_{+}^{n\\times m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8712;</mo><msubsup><mi>&#8477;</mi><mo>+</mo><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>m</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">Z\\in\\mathbb{R}_{+}^{n\\times m}</annotation></semantics></math> denote the transport plan under soft marginal constraints (typically uniform over valid tokens), and let <math alttext=\"c(u_{i},v_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">c(u_{i},v_{j})</annotation></semantics></math> denote the ground cost (e.g., cosine distance). The loss is:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "strategy",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> controls the smoothness (or sparsity) of the transport plan.\nThe Sinkhorn-based OT loss is applied to a selected set of projection (Q-Former) layers <math alttext=\"\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi><annotation encoding=\"application/x-tex\">\\mathcal{I}</annotation></semantics></math>, encouraging token-wise semantic alignment between languages. Finally, we jointly optimise this alignment loss with the translation cross-entropy:</p>\n\n",
                "matched_terms": [
                    "layers",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As aligning all layers may introduce redundancy and interfere with representation learning, we adaptively select the most effective layers for cross-lingual alignment in a deep architecture. We introduce an <em class=\"ltx_emph ltx_font_italic\">online, reward-guided</em> layer scheduling strategy based on the Upper Confidence Bound (UCB) principle<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ucb</span>]</cite> with temperature-controlled Softmax sampling.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "strategy",
                    "all",
                    "alignment",
                    "layer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each candidate projection (Q-Former) layer <math alttext=\"\\ell\\in\\mathcal{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8464;</mi></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\mathcal{I}</annotation></semantics></math>, we maintain:\n(i) an exponentially weighted moving average (EMA) of its reward <math alttext=\"Q_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>Q</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">Q_{\\ell}</annotation></semantics></math>,\n(ii) the selection count <math alttext=\"n_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>n</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">n_{\\ell}</annotation></semantics></math>, and\n(iii) the task/auxiliary loss at its previous activation.\nWhen layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> is activated at iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we compute a reward from the loss change, and update the EMA:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "selection",
                    "iii"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\beta&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\beta&gt;0</annotation></semantics></math> controls exploration and <math alttext=\"\\tau&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau&gt;0</annotation></semantics></math> controls sampling sharpness.\nThe strategy ensures that cross-lingual alignment is concentrated on the most contributive layers, thereby enhancing cross-lingual consistency while improving efficiency and stability.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "strategy",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first trained a SpeechLLM on the large-scale CoVoST2 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">covost2</span>]</cite> using 364 hours of training data. The training proceeded in two stages: first, the model was pretrained on automatic speech recognition (ASR; English&#8594;English), followed by training on speech-to-text translation (S2TT; English&#8594;Chinese) tasks.\nSubsequently, we performed multilingual mixed-supervision fine-tuning on FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fleurs</span>]</cite>, which covers over 100 languages with approximately 10 hours of supervised speech each. Five source languages&#8212;English (en), Japanese (ja), Spanish (es), Korean (ko), and Russian (ru)&#8212;were used to construct x&#8594;Chinese S2TT tasks.\nModel performance was evaluated on the FLEURS test set using BLEU-4, and alignment was assessed by Recall@1 and Jensen&#8211;Shannon Divergence (JSD).\nWe built a SpeechLLM within the SLAM-LLM framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/X-LANCE/SLAM-LLM\" title=\"\">https://github.com/X-LANCE/SLAM-LLM</a></span></span></span>, where the speech encoder and language model are frozen, and cross-lingual alignment is achieved via a Q-Former (8 Transformer blocks, 80 query tokens) projection layer <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cost</span>]</cite>. Training follows a two-stage schedule with learning rates of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> and <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math>, respectively, with a warm-up mechanism to stabilize early training. The OT-based joint loss employs a weighting coefficient of 10. All experiments were conducted on NVIDIA RTX 4090 GPUs.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "performance",
                    "all",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the BLEU scores on the FLEURS test set for five training languages and two zero-shot languages. We first constructed a baseline model consisting of Whisper-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>]</cite>, a Q-Former module, and Qwen-2.5-7B <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen</span>]</cite>. This baseline was then augmented with our cross-lingual alignment framework. We also benchmark our system against the strongest recent SpeechLLMs.\nThe proposed method yields consistent BLEU improvements (average +0.93). The gains are particularly large for zero-shot languages (average +5.05). Although large-scale models like Minmo<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minmo</span>]</cite> (not publicly available) are trained on much larger corpora, our system already surpasses them on several metrics. As the alignment mechanism is model-agnostic, it can be integrated into larger systems to further enhance multilingual performance.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "performance",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the contribution of each alignment module, we conducted a series of ablation studies.\nAll experiments used an identical model backbone and incrementally activated two alignment mechanisms: (i) Bias Compensation module and (ii) OT alignment. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that the coarse alignment yields noticeable performance gains, while the fine-grained OT alignment further improves overall translation quality.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When selecting parallel speech pairs for alignment, we evaluated three distinct strategies:\n(i) English-anchor (frozen English): alignment loss not back-propagated through the English branch.\n(ii) English-anchor: alignment loss back-propagated through the English branch.\n(iii) Random pairwise alignment: at each step, two randomly chosen languages are aligned symmetrically.\nThe results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that Strategy (ii) yields the poorest performance, and Strategy (i) is slightly better. Strategy (iii), used in our final framework, achieves the best results. Both Strategies (i) and (ii) retain an English-centric asymmetry. Strategy (ii)&#8217;s bidirectional updates amplify representation drift and forgetting in low-resource languages. Strategy (i) is more stable but still constrained by a fixed anchor. By contrast, Strategy (iii) symmetrically aligns randomly chosen language pairs at each step, removing anchor dominance and fostering cross-lingual co-learning, which explains its superior performance.</p>\n\n",
                "matched_terms": [
                    "random",
                    "strategies",
                    "strategy",
                    "alignment",
                    "iii",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we examine whether the coarse- and fine-alignment modules effectively bring speech representations closer across languages.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (up) presents speech representations before and after applying Bias Compensation.\nThe coarse alignment effectively reduces systematic interlingual offset.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09232v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTAL &#8227; POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (down) illustrates shifts in speech representations due to alignment, plotted by Recall@1 (R@1) and 1-JSD.\nMost language pairs move from the grey baseline points toward the upper-right quadrant, indicating simultaneous improvements in retrieval accuracy and representation consistency.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "across"
                ]
            }
        ]
    }
}