{
    "S2.T1": {
        "source_file": "PromptSep: Generative Audio Separation via Multimodal Prompting",
        "caption": "Table 1: Results under the standard extraction setup, evaluated on AudioCaps + ESC50 (ACESC) [yuan2025flowsep], FreeSound (FSD) [wang2025soloaudio], and Adobe Audition Sound Effects (ASFX). Among three benchmarks, ASFX is the only evaluation set that is out-of-domain for all three models.",
        "body": "Models\n\nSDRi ↑\\uparrow\n\n\nL2 Mel ↓\\downarrow\n\n\nF1 Decision Error ↑\\uparrow\n\n\nCLAPScore ↑\\uparrow\n\nCLAPScoreA↑\\text{CLAPScore}_{A}\\uparrow\n\nFADPANN{}_{\\textit{PANN}} ↓\\downarrow\n\n\n\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\n\n\n\n\n\nFlowSep [yuan2025flowsep]\n\n-4.26\n2.05\n-2.75\n3.06\n13.80\n4.93\n0.88\n0.45\n0.55\n0.24\n0.11\n0.14\n0.74\n0.45\n0.63\n23.70\n50.39\n38.68\n\n\n\nSoloAudio [wang2025soloaudio]\n\n2.42\n14.75\n5.15\n8.35\n2.26\n4.73\n0.74\n0.80\n0.53\n0.26\n0.30\n0.19\n0.62\n0.77\n0.61\n21.17\n5.79\n8.82\n\n\nPromptSep\n1.74\n10.89\n5.65\n5.04\n7.60\n4.23\n0.82\n0.60\n0.60\n0.24\n0.22\n0.18\n0.62\n0.58\n0.66\n12.00\n19.75\n3.19",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Models</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SDRi </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">L2 Mel </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F1 Decision Error </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLAPScore </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><math alttext=\"\\text{CLAPScore}_{A}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><mrow><msub><mtext mathsize=\"0.900em\">CLAPScore</mtext><mi mathsize=\"0.900em\">A</mi></msub><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{CLAPScore}_{A}\\uparrow</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD</span><math alttext=\"{}_{\\textit{PANN}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m6\" intent=\":literal\"><semantics><msub><mi/><mtext class=\"ltx_mathvariant_italic\" mathsize=\"0.900em\">PANN</mtext></msub><annotation encoding=\"application/x-tex\">{}_{\\textit{PANN}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FlowSep&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-4.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-2.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">50.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SoloAudio&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.42</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">14.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.82</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.19</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "promptsep",
            "decision",
            "esc50",
            "↓downarrow",
            "acesc",
            "wang2025soloaudio",
            "adobe",
            "audiocaps",
            "clapscorea↑textclapscoreauparrow",
            "soloaudio",
            "standard",
            "three",
            "evaluated",
            "fsd",
            "outofdomain",
            "all",
            "extraction",
            "results",
            "error",
            "fadpanntextitpann",
            "flowsep",
            "benchmarks",
            "mel",
            "evaluation",
            "sdri",
            "freesound",
            "clapscore",
            "only",
            "↑uparrow",
            "yuan2025flowsep",
            "set",
            "among",
            "asfx",
            "audition",
            "models",
            "under",
            "sound",
            "effects",
            "setup"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the results for the language-queried target sound extraction setup. Each model is provided with a text description of the target sounds and evaluated on its ability to extract the sounds from a mixture. We compare PromptSep against two baselines on four benchmarks. Due to the page limitation, we aggregate results from AudioCaps and ESC50 as ACESC using a weighted sum over their metrics, proportional to the number of samples in each dataset.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "promptsep",
                    "models",
                    "sound",
                    "extraction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio source separation aims to isolate specific sounds from an audio mixture.\nPrior work has approached this task across various domains, where task-specific sound sources needed to be defined, including speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2019conv</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2018supervised</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, music&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2019music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rouard2023hybrid</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and general sound events&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ochiai2020listen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023universal</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Target source extraction (TSE) is a variant where a specific source is specified by the users in the form of a conditioning signal for the model, such as class labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">delcroix2021few</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024mdx</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, reference audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seetharaman2019class</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, visual cues&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023target</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2019co</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and other modalities&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">smaragdis2009user</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bryan2014isse</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With recent breakthroughs in machine learning, language-queried audio source separation (LASS) has gradually emerged, where natural language serves as the conditioning input for target audio source separation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dong2022clipsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022separate</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024separate</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hai2024dpm</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "sound",
                    "yuan2025flowsep",
                    "wang2025soloaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional source separation methods have largely been dominated by mask prediction models that minimize point-wise losses between masked audio and target audio signals&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2019conv</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsieh2025tgif</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, masking-based models often introduce distortions, artifacts, and leakages in separation results, as maintaining mask consistency on original audio becomes particularly challenging when multiple sounds overlap. Recently, generative models like diffusion and flow-matching are alternatives to the masking-based methods for separation tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025review</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2022music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mariani2023multi</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">subakan2018generative</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, achieving higher separation audio quality. And such methods are also adopted in the LASS settings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hai2024dpm</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "results",
                    "wang2025soloaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite promising progress, LASS systems still face two major limitations in real-world scenarios. First, most audio separation models only &#8220;extract&#8221; the target source from audio mixtures, while\ntreating separation solely as an extraction operator is restrictive. Users may also wish to remove specific sounds from an audio mixture (i.e., removal), rather than only isolating specific sounds (i.e., extraction). Supporting both extraction and removal within a single framework would better align with real-world needs. Masking-based approaches,\nhowever, struggle to generalize to such multi-operator use cases, as they often fail to deliver an accurate one-time mask for multiple sound targets for removal purpose&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kongWSCWP20</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023universal</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In contrast, generative approaches offer an alternative. It is worth exploring whether generative models can synthesize high-quality audio outputs that support both extraction and removal operators by explicitly modeling the data distribution of high-fidelity audio samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "models",
                    "only",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the above limitations, we propose PromptSep, a latent diffusion model for open-vocabulary target audio source separation with multimodal cues.\nSpecifically, we first extend the standard extraction-only separation process with </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> operator.\nSecond, we incorporate </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">vocal imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as another conditioning signal beyond text prompts, where users can mimic the target sound to guide the separation target. This makes the system align better with how humans naturally describe sounds. To achieve this, we explore how to augment vocal imitation data by leveraging existing sound effect generation models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and modules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With PromptSep, users can perform either sound extraction or removal by leveraging either a textual description or a vocal imitation as a query. Vocal imitation alleviates the need to locate audio samples when the textual description is ambiguous.\nOur contributions are three-fold:</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "promptsep",
                    "models",
                    "sound",
                    "extraction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We provide a thorough evaluation and demonstrate PromptSep achieves superior performance on sound removal and vocal-imitation-guided extraction, and maintains competitive results on standard language-queried separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "promptsep",
                    "evaluation",
                    "sound",
                    "extraction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Moreover, we extend support for both the number of target sounds and separation operations. Previous LASS approaches typically rely on text prompts that specify a single target sound, and the models are also designed to handle only one target sound at a time. This constraint reduces practicality in real-world scenarios, where users want to extract multiple sounds simultaneously. In addition, users sometimes want to remove specific sounds rather than extract them. Indeed, such removal cases are particularly common, as it is often easier to describe the sounds to remove while preserving the remaining sounds. Previous approaches do not support this sort of interactions, to the best of our knowledge.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "only",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this gap, we first generalize the target of a single sound source to any subset of sources in the mixture.\nThen we introduce two text-based operators, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">extraction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and use GPT-5 to generate 100 textual templates that paraphrase extraction or removal operations (50 for each).\nThe specification of these templates can be viewed at our accompaniment webpage</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://yutongwen.github.io/PromptSep/\" title=\"\">https://yutongwen.github.io/PromptSep/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vocal Imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; As mentioned in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, certain sounds (e.g., &#8220;distortion&#8221; and &#8220;buzzing&#8221;) can be too abstract to describe accurately using text alone, or too ambiguous for the model to only identify the single candidate. To enable more flexible and intuitive sound specification, we introduce an additional guidance modality for separation: vocal imitation. PromptSep can be conditioned on reference audio samples in which a user vocally imitates a target sound, thereby guiding the separation process. This approach provides a more natural and accessible way for users to describe the sounds.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "only",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we leverage the sound effect generation model Sketch2Sound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for data augmentation.\nIt can generate sound effect audio samples that match textual descriptions and are temporally aligned with vocal imitation prompts.\nUsing around 12K real vocal imitation samples and their corresponding sound labels from the VimSketch dataset, we use Sketch2Sound to generate around 87K temporally aligned sound effect samples as training data for PromptSep. To better simulate real-world conditions, we apply time-shift and pitch-shift augmentations as imitation variances. We also add ambient noise, from 4.36 hours of static noise data collections&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinoshita2013reverb</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eaton2016estimation</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2021structure</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to enhance the model generalization capability.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We note that the Sketch2Sound training data does not require paired vocal imitations and sound effects.\nInstead, it relies on RMS and pitch curves to guide sound generation, without using actual imitation samples.\nThis leads to a possible exploration: whether curve-based features (RMS and pitch) or actual vocal-imitation raw waveform inputs provide more effective conditioning for audio separation. In Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S4\" style=\"font-size:90%;\" title=\"4 Results &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we address this through ablation studies.</span>\n</p>\n\n",
                "matched_terms": [
                    "effects",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We implement PromptSep using a latent diffusion model via diffusion Transformer (DiT) with three input types (text, vocal imitation, and audio mixture), following the specifications of&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024fast</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024long</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt contains three main components: (1) a pretrained variational autoencoder (VAE), following the architecture of Descript Audio Codec (DAC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, compresses 44.1 kHz mono audio samples into a sequence of continuous 128-dimensional embeddings at a temporal resolution of 40 Hz; (2) a pretrained FLAN-T5 encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raffel2019exploring</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encodes all text prompts; and (3) a DiT model is trained to generate new sequence of embeddings, which are decoded back to waveform via the VAE decoder to reconstruct target separated audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "three",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During training, we randomly drop condition signals for classifier-free guidance (CFG), with the drop rate </span>\n  <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">10\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text and mixture, but </span>\n  <math alttext=\"90\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">90</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">90\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for vocal imitation, as it gets easier to overfit through our experiments. We use a </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-prediction framework for training and diffusion probabilistic models (DPM) solvers&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025dpm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for sampling. During inference, we set the CFG scale to 1.0.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vocal Imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We curate a new dataset, VimSketchGen, consisting of </span>\n  <math alttext=\"87,171\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">87</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">171</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">87,171</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pairs of aligned vocal imitations and sound effects.\nThis dataset originally contains </span>\n  <math alttext=\"12,453\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">12</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">453</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">12,453</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vocal imitations sourced from the VimSketch dataset, and each of them is paired with </span>\n  <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">7</mn>\n      <annotation encoding=\"application/x-tex\">7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponding sound effects generated using Sketch2Sound, with different median filter sizes </span>\n  <math alttext=\"\\in\\{0,3,6,9,12,15,19\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi/>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">6</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">9</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">12</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">15</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">19</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\in\\{0,3,6,9,12,15,19\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAll audio samples in VimSketchGen are 8-second stereo tracks sampled at 44.1 kHz. We will release this dataset to support more tasks in audio research.</span>\n</p>\n\n",
                "matched_terms": [
                    "effects",
                    "all",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Specification</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; The input of PromptSep is a 10-second audio mixture by randomly combining </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events from different categories\n. The mixing signal-to-noise ratio (SNR) is uniformly sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB. A random subset of sound events in the mixture is selected as the separation target, with the only exception that if the vocal imitation is chosen as condition, its corresponding sound event alone is used as the target.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#000000;\">During training, the model is always conditioned on either text or vocal imitation, but not both simultaneously. While both conditions can be provided at inference time, their combined effect is not explored in this work and is left for future investigation.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "only",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019audiocaps</span>]</cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We follow the evaluation setup from FlowSep</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to use the AudioCaps test set of </span>\n  <math alttext=\"928\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">928</mn>\n      <annotation encoding=\"application/x-tex\">928</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio clips.\nWe treat each audio clip as the target source and mix it with a noise clip randomly selected from the test set at an SNR randomly chosen between </span>\n  <math alttext=\"-15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">15</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB and </span>\n  <math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">15</mn>\n      <annotation encoding=\"application/x-tex\">15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.\nThe first caption associated with the target audio is used as the query for separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "evaluation",
                    "setup",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ESC50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015esc</span>]</cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; It is also from the evaluation setup in FlowSep. We use the ESC50 evaluation set of </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio clips. Similarly, each clip is mixed with another randomly selected clip at an SNR of </span>\n  <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <mn mathsize=\"0.900em\">0</mn>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "esc50",
                    "evaluation",
                    "setup",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FSD-Mix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>]</cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We use the test set of FSD-Mix, which contains </span>\n  <math alttext=\"1,440\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">440</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1,440</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio mixtures.\nEach mixture consists of </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events, mixed at an SNR randomly selected between </span>\n  <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "set",
                    "wang2025soloaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Adobe Audition Sound Effects&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\" style=\"font-size:111%;\">2</span></span><a class=\"ltx_ref ltx_href ltx_font_medium\" href=\"https://www.adobe.com/products/audition/offers/adobeauditiondlcsfx.html\" style=\"font-size:111%;\" title=\"\">https://www.adobe.com/products/adobeauditiondlcsfx</a></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; It serves as a completely out-of-domain benchmark, as both PromptSep and previous baselines are not trained on any training, validation, and test sets of it. We create </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mixtures and each contains </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events randomly sampled from the test set, with an SNR between </span>\n  <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "audition",
                    "promptsep",
                    "outofdomain",
                    "sound",
                    "adobe",
                    "effects",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">VimSketchGen-Mix</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We use a subset of the VimSketchGen test split, containing </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound event samples with their vocal imitation pairs. Each target sound is mixed with 1 to 3 interference sounds randomly selected from the AudioSet test set&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemmeke2017audio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using a SNR sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our model against two generative language-queried audio separation models: FlowSep&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and SoloAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFlowSep performs separation in the latent space of a Mel-spectrogram VAE using flow matching, and reconstructs audio using a vocoder.\nIt employs FLAN-T5 for text conditioning.\nSoloAudio, on the other hand, applies a modified DiT architecture in the VAE latent space and uses CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2023large</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "models",
                    "wang2025soloaudio",
                    "soloaudio",
                    "yuan2025flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the performance of separation models using 6 objective metrics. Signal-to-distortion ratio improvement (SDRi) and L2 multi-resolution Mel-spectrogram distance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the most conventional metrics to measure the signal-level differences between the output and the groundtruth. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2024reference</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we also include CLAPScore, CLAPScore</span>\n  <sub class=\"ltx_sub\">\n    <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">A</span>\n  </sub>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Frechet Audio Distance (FAD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kilgour2018fr</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with embeddings from PANNs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2020panns</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess the generation audio quality and the semantic correlation among the text prompt, separation audio, and groundtruth audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "among",
                    "models",
                    "sdri",
                    "clapscore",
                    "wang2025soloaudio",
                    "yuan2025flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To better evaluate the separation decision error, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F1 Decision Error</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a new metric to evaluate the ability of the model to identify the correct temporal regions of a target sound. Specifically, to obtain the F1 score of decision errors, we first compute the frame-wise RMS energy on both the separated audio and the groudtruth audio. These values are then binarized using a threshold (0.01) to obtain the activity sequences, determining the sound and unsound frames. Finally, we calculate the F1 score between the predicted activity sequence and the groundtruth activity sequence.</span>\n</p>\n\n",
                "matched_terms": [
                    "sound",
                    "decision",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep achieves the best performance on nearly all metrics on ASFX, with the exception of CLAPScore, where SoloAudio surpasses us by 0.01. This highlights the strong generalization of our model, as ASFX is the only out-of-domain test set for all three models. FlowSep, trained on AudioSet, WavCaps, and VGGSound, which share similar quality and sources with AudioCaps and ESC50, obtains the best L2 Mel distance, F1 decision error, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC. SoloAudio, trained primarily on FreeSound, achieves the best results on FSD. All of AudioCaps, ESC50, and FSD are out-of-domain for PromptSep, but it yields competitive performance: achieving or nearly matching the best scores in FAD, SDRi, F1, CLAPScore, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC, and in SDRi on FSD. These results further demonstrate the strong generalization ability of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "promptsep",
                    "decision",
                    "esc50",
                    "acesc",
                    "audiocaps",
                    "soloaudio",
                    "three",
                    "fsd",
                    "outofdomain",
                    "all",
                    "results",
                    "flowsep",
                    "mel",
                    "sdri",
                    "freesound",
                    "clapscore",
                    "only",
                    "set",
                    "asfx",
                    "models",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Finally, we note that FlowSep performs poorly on SDRi. This is likely due to its separation process, which relies on vocoder to reconstruct the generated mel-spectrograms. While its output may preserve acoustic patterns and types of sound events, it can deviate substantially from the ground-truth waveform at the signal level.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "sound",
                    "sdri"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond the standard extraction setup, PromptSep also supports the sound removal. While our baselines were not explicitly trained with removal operations, they have been exposed to large-scale textual descriptions and may have implicitly learned some removal capability. Therefore, we also include their results for comparison.\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep outperforms the baselines across all datasets and metrics, with the exception of the L2 Mel distance on ACESC. These highlight the strong performance of our model in language-queried target sound removal.</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "mel",
                    "promptsep",
                    "all",
                    "sound",
                    "acesc",
                    "extraction",
                    "results",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further account for the limitations of baselines on sound removal, we design an alternative setup. Instead of directly removing the target sound, models are prompted to extract the remaining sounds by providing combined text descriptions of all non-target events, as an equivalent operation. Results under this configuration are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (gray rows, with models marked by *). PromptSep continues to achieve most of the best scores, even compared against FlowSep* and SoloAudio*. This further demonstrates the strong performance of our model in sound removal.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "promptsep",
                    "models",
                    "all",
                    "under",
                    "sound",
                    "soloaudio",
                    "results",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We further conduct a subjective evaluation for both the extraction and removal setups by following the format of DCASE2024 Task 9&#160;</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://dcase.community/challenge2024/task-language-queried-audio-source-separation\" title=\"\">https://dcase.community/challenge2024/task-language-queried-audio-source-separation</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using Relevance (REL) and Overall Sound Quality (OVL). The REL score measures how well the separated audio matches the given language query.\nThe OVL score evaluates the perceived audio quality of the output, including factors such as clarity, naturalness, and artifacts.\nBoth REL and OVL are rated by human annotators using a 5-point Likert scale.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "evaluation",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We randomly select 100 samples from ASFX test set as it is the only out-of-domain benchmark for all three models. The same mixtures and text descriptions are used across both the extraction and removal setups to ensure consistency. The total number of participants is 100, with each of samples are rated by at least 4 participants.\nResults are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.4 Objective Metrics &#8227; 3 Experiments &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nPromptSep achieves the highest scores in both REL and OVL across both setups, demonstrating its strong performance in accurately separating target sounds and maintaining high audio quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "three",
                    "asfx",
                    "promptsep",
                    "outofdomain",
                    "models",
                    "all",
                    "extraction",
                    "only",
                    "results",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To the best of our knowledge, no existing system supports vocal imitations as a conditioning input for open-domain source separation.\nWe evaluate our model on the VimSketchGen-Mix with no baseline.\nResults are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.2 Language-queried Target Sound Removal &#8227; 4 Results &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where our model achieves an SDRi of </span>\n  <math alttext=\"9.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">9.99</mn>\n      <annotation encoding=\"application/x-tex\">9.99</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB, an L2 multi-resolution Mel-spectrogram distance of </span>\n  <math alttext=\"0.92\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.92</mn>\n      <annotation encoding=\"application/x-tex\">0.92</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an F1 Decision Error of </span>\n  <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.95</mn>\n      <annotation encoding=\"application/x-tex\">0.95</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a CLAPScore</span>\n  <math alttext=\"{}_{\\text{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">A</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{A}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"0.87\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.87</mn>\n      <annotation encoding=\"application/x-tex\">0.87</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a FAD score of </span>\n  <math alttext=\"2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2.19</mn>\n      <annotation encoding=\"application/x-tex\">2.19</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese results indicate strong separation performance and demonstrate that vocal imitation is an effective conditioning signal for source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "decision",
                    "results",
                    "sdri",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluate a variant of our model that uses only pitch and RMS features extracted from the vocal imitation as the conditioning input, this setup is trained using the same median filter strategy as in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with the median filter size fixed to 8 during inference. While the pitch and RMS-based conditioning yields reasonably strong separation performance, it consistently under-performs the full vocal imitation condition across all evaluation metrics.\nWe attribute this to the complexity of the mixtures, where overlapping sounds are common; in such cases, the raw vocal imitation provides richer information than the limited pitch and RMS features alone.</span>\n</p>\n\n",
                "matched_terms": [
                    "only",
                    "all",
                    "evaluation",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep offers a unified framework for sound extraction and removal that overcomes key limitations of existing LASS systems.\nOur approach supports both sound extraction and removal within a single model.\nFurthermore, the integration of vocal imitation as a query modality addresses the ambiguity and limitations of text prompts, offering a more intuitive interface for users.\nThrough comprehensive evaluations, PromptSep demonstrates SOTA performance in sound removal and vocal-imitation-guided separation, while remaining competitive in standard LASS settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "standard",
                    "sound",
                    "promptsep"
                ]
            }
        ]
    },
    "S2.T2": {
        "source_file": "PromptSep: Generative Audio Separation via Multimodal Prompting",
        "caption": "Table 2: Results under the removal setup using negative text operators. We include a “upper-anchor” setup where models achieve the same removal effect by separating multiple target sounds as an upper bound of previous baselines (marked with * and in gray).",
        "body": "SDRi ↑\\uparrow\n\n\nL2 Mel ↓\\downarrow\n\n\nF1 Decision Error ↑\\uparrow\n\nCLAPScoreA↑\\text{CLAPScore}_{A}\\uparrow\nFADPANN↓\\text{FAD}_{\\textit{PANN}}\\downarrow\n\n\nModels\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\nACESC\nFSD\nASFX\n\n\n\nFlowSep [yuan2025flowsep]\n\n-4.45\n-12.44\n-9.53\n6.30\n13.27\n5.99\n0.76\n0.70\n0.56\n0.44\n0.55\n0.61\n25.49\n28.74\n20.26\n\n\n\nSoloAudio [wang2025soloaudio]\n\n-1.08\n-10.85\n-5.50\n12.40\n37.84\n10.87\n0.59\n0.29\n0.36\n0.30\n0.20\n0.45\n27.20\n87.79\n18.54\n\n\nPromptSep\n1.17\n-3.34\n-3.20\n6.40\n9.13\n4.86\n0.80\n0.87\n0.75\n0.54\n0.71\n0.72\n16.27\n15.99\n3.81\n\n\nFlowSep* [yuan2025flowsep]\n-4.35\n-13.14\n-9.36\n3.01\n6.64\n3.34\n0.88\n0.87\n0.73\n0.74\n0.76\n0.74\n24.37\n13.37\n19.78\n\n\nSoloAudio* [wang2025soloaudio]\n2.26\n-9.82\n-3.77\n8.60\n35.31\n8.70\n 0.74\n0.44\n0.54\n0.62\n0.32\n0.59\n23.57\n78.45\n14.20",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SDRi </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">L2 Mel </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F1 Decision Error </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><math alttext=\"\\text{CLAPScore}_{A}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m4\" intent=\":literal\"><semantics><mrow><msub><mtext mathsize=\"0.900em\">CLAPScore</mtext><mi mathsize=\"0.900em\">A</mi></msub><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{CLAPScore}_{A}\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><math alttext=\"\\text{FAD}_{\\textit{PANN}}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T2.m5\" intent=\":literal\"><semantics><mrow><msub><mtext mathsize=\"0.900em\">FAD</mtext><mtext class=\"ltx_mathvariant_italic\" mathsize=\"0.900em\">PANN</mtext></msub><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{FAD}_{\\textit{PANN}}\\downarrow</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ACESC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FSD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASFX</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FlowSep&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-4.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-12.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-9.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SoloAudio&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-10.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-5.50</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">87.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">-3.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">-3.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">9.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">16.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">15.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">FlowSep*&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>]</cite></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">-4.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">-13.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">-9.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">3.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">6.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">3.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">24.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">13.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">19.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">SoloAudio*&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>]</cite></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">2.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">-9.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">-3.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">8.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">35.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">8.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\"> 0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">23.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">78.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#9B9B9B;\">14.20</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "previous",
            "bound",
            "promptsep",
            "text",
            "decision",
            "↓downarrow",
            "target",
            "acesc",
            "wang2025soloaudio",
            "same",
            "clapscorea↑textclapscoreauparrow",
            "soloaudio",
            "fadpann↓textfadtextitpanndownarrow",
            "operators",
            "baselines",
            "fsd",
            "where",
            "include",
            "results",
            "error",
            "flowsep",
            "mel",
            "removal",
            "gray",
            "multiple",
            "sdri",
            "“upperanchor”",
            "↑uparrow",
            "negative",
            "effect",
            "sounds",
            "yuan2025flowsep",
            "asfx",
            "models",
            "under",
            "marked",
            "separating",
            "upper",
            "setup",
            "achieve"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond the standard extraction setup, PromptSep also supports the sound removal. While our baselines were not explicitly trained with removal operations, they have been exposed to large-scale textual descriptions and may have implicitly learned some removal capability. Therefore, we also include their results for comparison.\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep outperforms the baselines across all datasets and metrics, with the exception of the L2 Mel distance on ACESC. These highlight the strong performance of our model in language-queried target sound removal.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further account for the limitations of baselines on sound removal, we design an alternative setup. Instead of directly removing the target sound, models are prompted to extract the remaining sounds by providing combined text descriptions of all non-target events, as an equivalent operation. Results under this configuration are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (gray rows, with models marked by *). PromptSep continues to achieve most of the best scores, even compared against FlowSep* and SoloAudio*. This further demonstrates the strong performance of our model in sound removal.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "promptsep",
                    "text",
                    "models",
                    "removal",
                    "multiple",
                    "results",
                    "achieve"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio source separation aims to isolate specific sounds from an audio mixture.\nPrior work has approached this task across various domains, where task-specific sound sources needed to be defined, including speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2019conv</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2018supervised</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, music&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2019music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rouard2023hybrid</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and general sound events&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ochiai2020listen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023universal</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Target source extraction (TSE) is a variant where a specific source is specified by the users in the form of a conditioning signal for the model, such as class labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">delcroix2021few</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024mdx</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, reference audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seetharaman2019class</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, visual cues&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023target</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2019co</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and other modalities&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">smaragdis2009user</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bryan2014isse</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With recent breakthroughs in machine learning, language-queried audio source separation (LASS) has gradually emerged, where natural language serves as the conditioning input for target audio source separation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dong2022clipsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022separate</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024separate</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hai2024dpm</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "where",
                    "target",
                    "wang2025soloaudio",
                    "yuan2025flowsep",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional source separation methods have largely been dominated by mask prediction models that minimize point-wise losses between masked audio and target audio signals&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2019conv</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsieh2025tgif</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, masking-based models often introduce distortions, artifacts, and leakages in separation results, as maintaining mask consistency on original audio becomes particularly challenging when multiple sounds overlap. Recently, generative models like diffusion and flow-matching are alternatives to the masking-based methods for separation tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025review</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2022music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mariani2023multi</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">subakan2018generative</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, achieving higher separation audio quality. And such methods are also adopted in the LASS settings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hai2024dpm</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "multiple",
                    "target",
                    "wang2025soloaudio",
                    "results",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite promising progress, LASS systems still face two major limitations in real-world scenarios. First, most audio separation models only &#8220;extract&#8221; the target source from audio mixtures, while\ntreating separation solely as an extraction operator is restrictive. Users may also wish to remove specific sounds from an audio mixture (i.e., removal), rather than only isolating specific sounds (i.e., extraction). Supporting both extraction and removal within a single framework would better align with real-world needs. Masking-based approaches,\nhowever, struggle to generalize to such multi-operator use cases, as they often fail to deliver an accurate one-time mask for multiple sound targets for removal purpose&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kongWSCWP20</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023universal</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In contrast, generative approaches offer an alternative. It is worth exploring whether generative models can synthesize high-quality audio outputs that support both extraction and removal operators by explicitly modeling the data distribution of high-fidelity audio samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "operators",
                    "models",
                    "removal",
                    "multiple",
                    "target",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, language may not be the most appropriate and effective query for audio sources. Some textual descriptions of sounds, such as &#8220;distortion&#8221;, &#8220;dark transition&#8221;, or &#8220;light flashing&#8221;, are too abstract or ambiguous to precisely specify the target sources. Additionally, a single sound can be described in many different ways, leading to high variability in prompt length, vocabulary, and phrasing. Multiple sources in a mixture may also satisfy a given description (e.g., &#8220;distortion&#8221; could refer to any harsh or intense sounds), causing unwanted sources to be extracted or removed. Most importantly, this limitation is inherent to language itself, as sounds are naturally perceived by human ears rather than captured by textual descriptions. To overcome this limitation, previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has explored audio prompts, example samples indicating the target source, to separate similar sounds in the mixture. But this approach is also unintuitive, as users must still provide appropriate reference audio samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "previous",
                    "sounds",
                    "target",
                    "multiple"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the above limitations, we propose PromptSep, a latent diffusion model for open-vocabulary target audio source separation with multimodal cues.\nSpecifically, we first extend the standard extraction-only separation process with </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> operator.\nSecond, we incorporate </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">vocal imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as another conditioning signal beyond text prompts, where users can mimic the target sound to guide the separation target. This makes the system align better with how humans naturally describe sounds. To achieve this, we explore how to augment vocal imitation data by leveraging existing sound effect generation models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and modules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With PromptSep, users can perform either sound extraction or removal by leveraging either a textual description or a vocal imitation as a query. Vocal imitation alleviates the need to locate audio samples when the textual description is ambiguous.\nOur contributions are three-fold:</span>\n</p>\n\n",
                "matched_terms": [
                    "promptsep",
                    "text",
                    "models",
                    "removal",
                    "where",
                    "target",
                    "effect",
                    "sounds",
                    "achieve"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We extend extraction-only source separation with a conditional diffusion model and data simulation pipeline to support sound removal, offering more flexible separation operators.</span>\n</p>\n\n",
                "matched_terms": [
                    "removal",
                    "operators"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We provide a thorough evaluation and demonstrate PromptSep achieves superior performance on sound removal and vocal-imitation-guided extraction, and maintains competitive results on standard language-queried separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "removal",
                    "results",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.1 Separation Condition &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep takes an audio mixture as the primary input, along with two conditions: (1) a textual description of the separation target; and (2) a vocal imitation recording of the separation target. Both conditions can be used separately or jointly. Based on these inputs, the model outputs an audio track containing the sources specified by the conditions. In the following subsections, we introduce the construction of these conditions and the model architecture of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "promptsep",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text Prompt</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; The textual description of sounds varies in complexity. They may consist of just one keyword identifying the sound, or a long sentence containing multiple sound attributes. Consequently, text prompts can range from a single word to several sentences. To prepare the model for such a condition, we include text prompts of different lengths and descriptive styles for the same sounds during training, simulating the user inputs in real-world scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "multiple",
                    "include",
                    "same",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Moreover, we extend support for both the number of target sounds and separation operations. Previous LASS approaches typically rely on text prompts that specify a single target sound, and the models are also designed to handle only one target sound at a time. This constraint reduces practicality in real-world scenarios, where users want to extract multiple sounds simultaneously. In addition, users sometimes want to remove specific sounds rather than extract them. Indeed, such removal cases are particularly common, as it is often easier to describe the sounds to remove while preserving the remaining sounds. Previous approaches do not support this sort of interactions, to the best of our knowledge.</span>\n</p>\n\n",
                "matched_terms": [
                    "previous",
                    "text",
                    "models",
                    "removal",
                    "multiple",
                    "where",
                    "target",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this gap, we first generalize the target of a single sound source to any subset of sources in the mixture.\nThen we introduce two text-based operators, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">extraction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and use GPT-5 to generate 100 textual templates that paraphrase extraction or removal operations (50 for each).\nThe specification of these templates can be viewed at our accompaniment webpage</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://yutongwen.github.io/PromptSep/\" title=\"\">https://yutongwen.github.io/PromptSep/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "removal",
                    "target",
                    "operators"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct the data simulation pipeline to combine these templates with captions of the target sounds to generate text prompts of either extraction or removal operations. We combine them with corresponding input mixtures and output targets for the language-queried audio separation training.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "removal",
                    "target",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vocal Imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; As mentioned in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, certain sounds (e.g., &#8220;distortion&#8221; and &#8220;buzzing&#8221;) can be too abstract to describe accurately using text alone, or too ambiguous for the model to only identify the single candidate. To enable more flexible and intuitive sound specification, we introduce an additional guidance modality for separation: vocal imitation. PromptSep can be conditioned on reference audio samples in which a user vocally imitates a target sound, thereby guiding the separation process. This approach provides a more natural and accessible way for users to describe the sounds.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "sounds",
                    "promptsep",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While a few datasets, such as VimSketch&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, contain pairs of corresponding vocal imitation and sound effect samples,\nthese pairs are not temporally aligned, and thus cannot serve as a good vocal imitation condition to identify the target sound in a mixture.</span>\n</p>\n\n",
                "matched_terms": [
                    "effect",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we leverage the sound effect generation model Sketch2Sound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for data augmentation.\nIt can generate sound effect audio samples that match textual descriptions and are temporally aligned with vocal imitation prompts.\nUsing around 12K real vocal imitation samples and their corresponding sound labels from the VimSketch dataset, we use Sketch2Sound to generate around 87K temporally aligned sound effect samples as training data for PromptSep. To better simulate real-world conditions, we apply time-shift and pitch-shift augmentations as imitation variances. We also add ambient noise, from 4.36 hours of static noise data collections&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinoshita2013reverb</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eaton2016estimation</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2021structure</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to enhance the model generalization capability.</span>\n</p>\n\n",
                "matched_terms": [
                    "effect",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We implement PromptSep using a latent diffusion model via diffusion Transformer (DiT) with three input types (text, vocal imitation, and audio mixture), following the specifications of&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024fast</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024long</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt contains three main components: (1) a pretrained variational autoencoder (VAE), following the architecture of Descript Audio Codec (DAC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, compresses 44.1 kHz mono audio samples into a sequence of continuous 128-dimensional embeddings at a temporal resolution of 40 Hz; (2) a pretrained FLAN-T5 encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raffel2019exploring</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encodes all text prompts; and (3) a DiT model is trained to generate new sequence of embeddings, which are decoded back to waveform via the VAE decoder to reconstruct target separated audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "promptsep",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As we allow the sum of multiple sounds as a target, a trivial solution to the learning objective is to replicate the input mixture to achieve a deceptively low loss. A light noise perturbation to the input can prevent this case, as replicating won&#8217;t achieve low loss values.</span>\n</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "target",
                    "sounds",
                    "achieve"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During training, we randomly drop condition signals for classifier-free guidance (CFG), with the drop rate </span>\n  <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">10\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text and mixture, but </span>\n  <math alttext=\"90\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">90</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">90\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for vocal imitation, as it gets easier to overfit through our experiments. We use a </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-prediction framework for training and diffusion probabilistic models (DPM) solvers&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025dpm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for sampling. During inference, we set the CFG scale to 1.0.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound Event</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We train our model using an internal large collection of licensed sound effect datasets and publicly available, CC-licensed general audio corpora. Each sound is accompanied with multiple versions of captions varying in length.\nAs described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2\" style=\"font-size:90%;\" title=\"2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we combine the captions with the text operator templates to form the final text input. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#000000;\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.F1\" title=\"Figure 1 &#8227; 2.1 Separation Condition &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows example final text input.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "effect",
                    "multiple"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Specification</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; The input of PromptSep is a 10-second audio mixture by randomly combining </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events from different categories\n. The mixing signal-to-noise ratio (SNR) is uniformly sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB. A random subset of sound events in the mixture is selected as the separation target, with the only exception that if the vocal imitation is chosen as condition, its corresponding sound event alone is used as the target.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#000000;\">During training, the model is always conditioned on either text or vocal imitation, but not both simultaneously. While both conditions can be provided at inference time, their combined effect is not explored in this work and is left for future investigation.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "effect",
                    "promptsep",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019audiocaps</span>]</cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We follow the evaluation setup from FlowSep</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to use the AudioCaps test set of </span>\n  <math alttext=\"928\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">928</mn>\n      <annotation encoding=\"application/x-tex\">928</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio clips.\nWe treat each audio clip as the target source and mix it with a noise clip randomly selected from the test set at an SNR randomly chosen between </span>\n  <math alttext=\"-15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">15</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB and </span>\n  <math alttext=\"15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">15</mn>\n      <annotation encoding=\"application/x-tex\">15</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.\nThe first caption associated with the target audio is used as the query for separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "setup",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ESC50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015esc</span>]</cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; It is also from the evaluation setup in FlowSep. We use the ESC50 evaluation set of </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio clips. Similarly, each clip is mixed with another randomly selected clip at an SNR of </span>\n  <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <mn mathsize=\"0.900em\">0</mn>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Adobe Audition Sound Effects&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_medium\" style=\"font-size:111%;\">2</span></span><a class=\"ltx_ref ltx_href ltx_font_medium\" href=\"https://www.adobe.com/products/audition/offers/adobeauditiondlcsfx.html\" style=\"font-size:111%;\" title=\"\">https://www.adobe.com/products/adobeauditiondlcsfx</a></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; It serves as a completely out-of-domain benchmark, as both PromptSep and previous baselines are not trained on any training, validation, and test sets of it. We create </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> mixtures and each contains </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events randomly sampled from the test set, with an SNR between </span>\n  <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "previous",
                    "baselines",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">VimSketchGen-Mix</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We use a subset of the VimSketchGen test split, containing </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound event samples with their vocal imitation pairs. Each target sound is mixed with 1 to 3 interference sounds randomly selected from the AudioSet test set&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemmeke2017audio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using a SNR sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "target",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our model against two generative language-queried audio separation models: FlowSep&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and SoloAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFlowSep performs separation in the latent space of a Mel-spectrogram VAE using flow matching, and reconstructs audio using a vocoder.\nIt employs FLAN-T5 for text conditioning.\nSoloAudio, on the other hand, applies a modified DiT architecture in the VAE latent space and uses CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2023large</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "text",
                    "models",
                    "wang2025soloaudio",
                    "soloaudio",
                    "yuan2025flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the performance of separation models using 6 objective metrics. Signal-to-distortion ratio improvement (SDRi) and L2 multi-resolution Mel-spectrogram distance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the most conventional metrics to measure the signal-level differences between the output and the groundtruth. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2024reference</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we also include CLAPScore, CLAPScore</span>\n  <sub class=\"ltx_sub\">\n    <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">A</span>\n  </sub>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Frechet Audio Distance (FAD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kilgour2018fr</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with embeddings from PANNs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2020panns</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess the generation audio quality and the semantic correlation among the text prompt, separation audio, and groundtruth audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "sdri",
                    "include",
                    "wang2025soloaudio",
                    "yuan2025flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To better evaluate the separation decision error, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F1 Decision Error</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a new metric to evaluate the ability of the model to identify the correct temporal regions of a target sound. Specifically, to obtain the F1 score of decision errors, we first compute the frame-wise RMS energy on both the separated audio and the groudtruth audio. These values are then binarized using a threshold (0.01) to obtain the activity sequences, determining the sound and unsound frames. Finally, we calculate the F1 score between the predicted activity sequence and the groundtruth activity sequence.</span>\n</p>\n\n",
                "matched_terms": [
                    "decision",
                    "error",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the results for the language-queried target sound extraction setup. Each model is provided with a text description of the target sounds and evaluated on its ability to extract the sounds from a mixture. We compare PromptSep against two baselines on four benchmarks. Due to the page limitation, we aggregate results from AudioCaps and ESC50 as ACESC using a weighted sum over their metrics, proportional to the number of samples in each dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "promptsep",
                    "text",
                    "target",
                    "acesc",
                    "baselines",
                    "results",
                    "setup",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep achieves the best performance on nearly all metrics on ASFX, with the exception of CLAPScore, where SoloAudio surpasses us by 0.01. This highlights the strong generalization of our model, as ASFX is the only out-of-domain test set for all three models. FlowSep, trained on AudioSet, WavCaps, and VGGSound, which share similar quality and sources with AudioCaps and ESC50, obtains the best L2 Mel distance, F1 decision error, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC. SoloAudio, trained primarily on FreeSound, achieves the best results on FSD. All of AudioCaps, ESC50, and FSD are out-of-domain for PromptSep, but it yields competitive performance: achieving or nearly matching the best scores in FAD, SDRi, F1, CLAPScore, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC, and in SDRi on FSD. These results further demonstrate the strong generalization ability of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "asfx",
                    "mel",
                    "promptsep",
                    "fsd",
                    "models",
                    "decision",
                    "sdri",
                    "where",
                    "acesc",
                    "soloaudio",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Finally, we note that FlowSep performs poorly on SDRi. This is likely due to its separation process, which relies on vocoder to reconstruct the generated mel-spectrograms. While its output may preserve acoustic patterns and types of sound events, it can deviate substantially from the ground-truth waveform at the signal level.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "sdri"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We randomly select 100 samples from ASFX test set as it is the only out-of-domain benchmark for all three models. The same mixtures and text descriptions are used across both the extraction and removal setups to ensure consistency. The total number of participants is 100, with each of samples are rated by at least 4 participants.\nResults are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.4 Objective Metrics &#8227; 3 Experiments &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nPromptSep achieves the highest scores in both REL and OVL across both setups, demonstrating its strong performance in accurately separating target sounds and maintaining high audio quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "asfx",
                    "promptsep",
                    "text",
                    "models",
                    "removal",
                    "target",
                    "separating",
                    "same",
                    "results",
                    "sounds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To the best of our knowledge, no existing system supports vocal imitations as a conditioning input for open-domain source separation.\nWe evaluate our model on the VimSketchGen-Mix with no baseline.\nResults are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.2 Language-queried Target Sound Removal &#8227; 4 Results &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where our model achieves an SDRi of </span>\n  <math alttext=\"9.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">9.99</mn>\n      <annotation encoding=\"application/x-tex\">9.99</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB, an L2 multi-resolution Mel-spectrogram distance of </span>\n  <math alttext=\"0.92\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.92</mn>\n      <annotation encoding=\"application/x-tex\">0.92</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an F1 Decision Error of </span>\n  <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.95</mn>\n      <annotation encoding=\"application/x-tex\">0.95</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a CLAPScore</span>\n  <math alttext=\"{}_{\\text{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">A</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{A}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"0.87\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.87</mn>\n      <annotation encoding=\"application/x-tex\">0.87</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a FAD score of </span>\n  <math alttext=\"2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2.19</mn>\n      <annotation encoding=\"application/x-tex\">2.19</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese results indicate strong separation performance and demonstrate that vocal imitation is an effective conditioning signal for source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "decision",
                    "sdri",
                    "where",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluate a variant of our model that uses only pitch and RMS features extracted from the vocal imitation as the conditioning input, this setup is trained using the same median filter strategy as in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with the median filter size fixed to 8 during inference. While the pitch and RMS-based conditioning yields reasonably strong separation performance, it consistently under-performs the full vocal imitation condition across all evaluation metrics.\nWe attribute this to the complexity of the mixtures, where overlapping sounds are common; in such cases, the raw vocal imitation provides richer information than the limited pitch and RMS features alone.</span>\n</p>\n\n",
                "matched_terms": [
                    "same",
                    "setup",
                    "sounds",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep offers a unified framework for sound extraction and removal that overcomes key limitations of existing LASS systems.\nOur approach supports both sound extraction and removal within a single model.\nFurthermore, the integration of vocal imitation as a query modality addresses the ambiguity and limitations of text prompts, offering a more intuitive interface for users.\nThrough comprehensive evaluations, PromptSep demonstrates SOTA performance in sound removal and vocal-imitation-guided separation, while remaining competitive in standard LASS settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "removal",
                    "promptsep"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "PromptSep: Generative Audio Separation via Multimodal Prompting",
        "caption": "Table 3: Mean Opinion Scores (MOS) with standard error for text relevance (REL) and overall quality (OVL). Mixture and GroudTruth (GT) serve as the lower-anchor and upper-anchor results.",
        "body": "Model\nExtraction\nRemoval\n\n\n\nREL↑\\uparrow\n\n\nOVL↑\\uparrow\n\n\nREL↑\\uparrow\n\n\nOVL↑\\uparrow\n\n\n\nMixture\n\n2.96 ±\\pm 0.08\n\n\n3.55 ±\\pm 0.07\n\n\n2.42 ±\\pm 0.08\n\n\n3.26 ±\\pm 0.08\n\n\n\nGT\n\n3.94 ±\\pm 0.07\n\n\n4.17 ±\\pm 0.06\n\n\n3.27 ±\\pm 0.08\n\n\n4.06 ±\\pm 0.06\n\n\n\n\nFlowSep [yuan2025flowsep]\n\n\n3.19 ±\\pm 0.07\n\n\n3.46 ±\\pm 0.07\n\n\n2.88 ±\\pm 0.08\n\n\n3.40 ±\\pm 0.07\n\n\n\n\nSoloAudio [wang2025soloaudio]\n\n\n3.31 ±\\pm 0.08\n\n\n3.64 ±\\pm 0.07\n\n\n2.99 ±\\pm 0.09\n\n\n3.59 ±\\pm 0.07\n\n\n\nPromptSep\n3.34 ±\\pm 0.08\n3.75 ±\\pm 0.07\n3.25 ±\\pm 0.08\n3.83 ±\\pm 0.07",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Extraction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Removal</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">OVL</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">OVL</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mixture</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.96 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.55 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.42 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.26 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">GT</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.94 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.17 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.27 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.06 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.06</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FlowSep&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.19 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.46 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.88 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.40 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SoloAudio&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.31 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.64 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.99 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.59 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.34 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.75 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.25 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.83 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.07</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "upperanchor",
            "quality",
            "ovl↑uparrow",
            "promptsep",
            "text",
            "serve",
            "±pm",
            "wang2025soloaudio",
            "soloaudio",
            "standard",
            "rel",
            "mixture",
            "groudtruth",
            "ovl",
            "rel↑uparrow",
            "mean",
            "extraction",
            "results",
            "flowsep",
            "relevance",
            "model",
            "mos",
            "removal",
            "yuan2025flowsep",
            "opinion",
            "loweranchor",
            "scores",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We randomly select 100 samples from ASFX test set as it is the only out-of-domain benchmark for all three models. The same mixtures and text descriptions are used across both the extraction and removal setups to ensure consistency. The total number of participants is 100, with each of samples are rated by at least 4 participants.\nResults are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.4 Objective Metrics &#8227; 3 Experiments &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nPromptSep achieves the highest scores in both REL and OVL across both setups, demonstrating its strong performance in accurately separating target sounds and maintaining high audio quality.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "promptsep",
                    "quality",
                    "text",
                    "removal",
                    "extraction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio source separation aims to isolate specific sounds from an audio mixture.\nPrior work has approached this task across various domains, where task-specific sound sources needed to be defined, including speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2019conv</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2018supervised</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, music&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2019music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rouard2023hybrid</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and general sound events&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ochiai2020listen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023universal</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Target source extraction (TSE) is a variant where a specific source is specified by the users in the form of a conditioning signal for the model, such as class labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">delcroix2021few</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024mdx</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, reference audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seetharaman2019class</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, visual cues&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023target</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2019co</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and other modalities&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">smaragdis2009user</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bryan2014isse</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With recent breakthroughs in machine learning, language-queried audio source separation (LASS) has gradually emerged, where natural language serves as the conditioning input for target audio source separation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">dong2022clipsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022separate</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024separate</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hai2024dpm</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "mixture",
                    "wang2025soloaudio",
                    "extraction",
                    "yuan2025flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventional source separation methods have largely been dominated by mask prediction models that minimize point-wise losses between masked audio and target audio signals&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2019conv</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsieh2025tgif</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, masking-based models often introduce distortions, artifacts, and leakages in separation results, as maintaining mask consistency on original audio becomes particularly challenging when multiple sounds overlap. Recently, generative models like diffusion and flow-matching are alternatives to the masking-based methods for separation tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2025review</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2022music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mariani2023multi</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">subakan2018generative</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, achieving higher separation audio quality. And such methods are also adopted in the LASS settings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hai2024dpm</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "quality",
                    "wang2025soloaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite promising progress, LASS systems still face two major limitations in real-world scenarios. First, most audio separation models only &#8220;extract&#8221; the target source from audio mixtures, while\ntreating separation solely as an extraction operator is restrictive. Users may also wish to remove specific sounds from an audio mixture (i.e., removal), rather than only isolating specific sounds (i.e., extraction). Supporting both extraction and removal within a single framework would better align with real-world needs. Masking-based approaches,\nhowever, struggle to generalize to such multi-operator use cases, as they often fail to deliver an accurate one-time mask for multiple sound targets for removal purpose&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kongWSCWP20</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022zero</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023universal</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In contrast, generative approaches offer an alternative. It is worth exploring whether generative models can synthesize high-quality audio outputs that support both extraction and removal operators by explicitly modeling the data distribution of high-fidelity audio samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "removal",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the above limitations, we propose PromptSep, a latent diffusion model for open-vocabulary target audio source separation with multimodal cues.\nSpecifically, we first extend the standard extraction-only separation process with </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> operator.\nSecond, we incorporate </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">vocal imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as another conditioning signal beyond text prompts, where users can mimic the target sound to guide the separation target. This makes the system align better with how humans naturally describe sounds. To achieve this, we explore how to augment vocal imitation data by leveraging existing sound effect generation models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and modules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With PromptSep, users can perform either sound extraction or removal by leveraging either a textual description or a vocal imitation as a query. Vocal imitation alleviates the need to locate audio samples when the textual description is ambiguous.\nOur contributions are three-fold:</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "model",
                    "promptsep",
                    "text",
                    "removal",
                    "extraction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We extend extraction-only source separation with a conditional diffusion model and data simulation pipeline to support sound removal, offering more flexible separation operators.</span>\n</p>\n\n",
                "matched_terms": [
                    "removal",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We provide a thorough evaluation and demonstrate PromptSep achieves superior performance on sound removal and vocal-imitation-guided extraction, and maintains competitive results on standard language-queried separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "promptsep",
                    "removal",
                    "extraction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.1 Separation Condition &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep takes an audio mixture as the primary input, along with two conditions: (1) a textual description of the separation target; and (2) a vocal imitation recording of the separation target. Both conditions can be used separately or jointly. Based on these inputs, the model outputs an audio track containing the sources specified by the conditions. In the following subsections, we introduce the construction of these conditions and the model architecture of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "promptsep",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text Prompt</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; The textual description of sounds varies in complexity. They may consist of just one keyword identifying the sound, or a long sentence containing multiple sound attributes. Consequently, text prompts can range from a single word to several sentences. To prepare the model for such a condition, we include text prompts of different lengths and descriptive styles for the same sounds during training, simulating the user inputs in real-world scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Moreover, we extend support for both the number of target sounds and separation operations. Previous LASS approaches typically rely on text prompts that specify a single target sound, and the models are also designed to handle only one target sound at a time. This constraint reduces practicality in real-world scenarios, where users want to extract multiple sounds simultaneously. In addition, users sometimes want to remove specific sounds rather than extract them. Indeed, such removal cases are particularly common, as it is often easier to describe the sounds to remove while preserving the remaining sounds. Previous approaches do not support this sort of interactions, to the best of our knowledge.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "removal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this gap, we first generalize the target of a single sound source to any subset of sources in the mixture.\nThen we introduce two text-based operators, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">extraction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and use GPT-5 to generate 100 textual templates that paraphrase extraction or removal operations (50 for each).\nThe specification of these templates can be viewed at our accompaniment webpage</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://yutongwen.github.io/PromptSep/\" title=\"\">https://yutongwen.github.io/PromptSep/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "extraction",
                    "removal",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct the data simulation pipeline to combine these templates with captions of the target sounds to generate text prompts of either extraction or removal operations. We combine them with corresponding input mixtures and output targets for the language-queried audio separation training.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "removal",
                    "extraction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vocal Imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; As mentioned in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, certain sounds (e.g., &#8220;distortion&#8221; and &#8220;buzzing&#8221;) can be too abstract to describe accurately using text alone, or too ambiguous for the model to only identify the single candidate. To enable more flexible and intuitive sound specification, we introduce an additional guidance modality for separation: vocal imitation. PromptSep can be conditioned on reference audio samples in which a user vocally imitates a target sound, thereby guiding the separation process. This approach provides a more natural and accessible way for users to describe the sounds.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While a few datasets, such as VimSketch&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, contain pairs of corresponding vocal imitation and sound effect samples,\nthese pairs are not temporally aligned, and thus cannot serve as a good vocal imitation condition to identify the target sound in a mixture.</span>\n</p>\n\n",
                "matched_terms": [
                    "serve",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we leverage the sound effect generation model Sketch2Sound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for data augmentation.\nIt can generate sound effect audio samples that match textual descriptions and are temporally aligned with vocal imitation prompts.\nUsing around 12K real vocal imitation samples and their corresponding sound labels from the VimSketch dataset, we use Sketch2Sound to generate around 87K temporally aligned sound effect samples as training data for PromptSep. To better simulate real-world conditions, we apply time-shift and pitch-shift augmentations as imitation variances. We also add ambient noise, from 4.36 hours of static noise data collections&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinoshita2013reverb</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eaton2016estimation</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2021structure</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to enhance the model generalization capability.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "promptsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We implement PromptSep using a latent diffusion model via diffusion Transformer (DiT) with three input types (text, vocal imitation, and audio mixture), following the specifications of&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024fast</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024long</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt contains three main components: (1) a pretrained variational autoencoder (VAE), following the architecture of Descript Audio Codec (DAC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, compresses 44.1 kHz mono audio samples into a sequence of continuous 128-dimensional embeddings at a temporal resolution of 40 Hz; (2) a pretrained FLAN-T5 encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raffel2019exploring</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encodes all text prompts; and (3) a DiT model is trained to generate new sequence of embeddings, which are decoded back to waveform via the VAE decoder to reconstruct target separated audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "promptsep",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During training, we randomly drop condition signals for classifier-free guidance (CFG), with the drop rate </span>\n  <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">10\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text and mixture, but </span>\n  <math alttext=\"90\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">90</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">90\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for vocal imitation, as it gets easier to overfit through our experiments. We use a </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-prediction framework for training and diffusion probabilistic models (DPM) solvers&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025dpm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for sampling. During inference, we set the CFG scale to 1.0.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound Event</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We train our model using an internal large collection of licensed sound effect datasets and publicly available, CC-licensed general audio corpora. Each sound is accompanied with multiple versions of captions varying in length.\nAs described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2\" style=\"font-size:90%;\" title=\"2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we combine the captions with the text operator templates to form the final text input. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#000000;\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.F1\" title=\"Figure 1 &#8227; 2.1 Separation Condition &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows example final text input.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Specification</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; The input of PromptSep is a 10-second audio mixture by randomly combining </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events from different categories\n. The mixing signal-to-noise ratio (SNR) is uniformly sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB. A random subset of sound events in the mixture is selected as the separation target, with the only exception that if the vocal imitation is chosen as condition, its corresponding sound event alone is used as the target.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#000000;\">During training, the model is always conditioned on either text or vocal imitation, but not both simultaneously. While both conditions can be provided at inference time, their combined effect is not explored in this work and is left for future investigation.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "promptsep",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FSD-Mix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>]</cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We use the test set of FSD-Mix, which contains </span>\n  <math alttext=\"1,440\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">440</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1,440</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> audio mixtures.\nEach mixture consists of </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events, mixed at an SNR randomly selected between </span>\n  <math alttext=\"-10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "mixture",
                    "wang2025soloaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our model against two generative language-queried audio separation models: FlowSep&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and SoloAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFlowSep performs separation in the latent space of a Mel-spectrogram VAE using flow matching, and reconstructs audio using a vocoder.\nIt employs FLAN-T5 for text conditioning.\nSoloAudio, on the other hand, applies a modified DiT architecture in the VAE latent space and uses CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2023large</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "model",
                    "text",
                    "wang2025soloaudio",
                    "soloaudio",
                    "yuan2025flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the performance of separation models using 6 objective metrics. Signal-to-distortion ratio improvement (SDRi) and L2 multi-resolution Mel-spectrogram distance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the most conventional metrics to measure the signal-level differences between the output and the groundtruth. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2024reference</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we also include CLAPScore, CLAPScore</span>\n  <sub class=\"ltx_sub\">\n    <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">A</span>\n  </sub>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Frechet Audio Distance (FAD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kilgour2018fr</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with embeddings from PANNs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2020panns</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess the generation audio quality and the semantic correlation among the text prompt, separation audio, and groundtruth audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "yuan2025flowsep",
                    "quality",
                    "wang2025soloaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To better evaluate the separation decision error, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F1 Decision Error</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a new metric to evaluate the ability of the model to identify the correct temporal regions of a target sound. Specifically, to obtain the F1 score of decision errors, we first compute the frame-wise RMS energy on both the separated audio and the groudtruth audio. These values are then binarized using a threshold (0.01) to obtain the activity sequences, determining the sound and unsound frames. Finally, we calculate the F1 score between the predicted activity sequence and the groundtruth activity sequence.</span>\n</p>\n\n",
                "matched_terms": [
                    "groudtruth",
                    "model",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the results for the language-queried target sound extraction setup. Each model is provided with a text description of the target sounds and evaluated on its ability to extract the sounds from a mixture. We compare PromptSep against two baselines on four benchmarks. Due to the page limitation, we aggregate results from AudioCaps and ESC50 as ACESC using a weighted sum over their metrics, proportional to the number of samples in each dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "promptsep",
                    "mixture",
                    "text",
                    "extraction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep achieves the best performance on nearly all metrics on ASFX, with the exception of CLAPScore, where SoloAudio surpasses us by 0.01. This highlights the strong generalization of our model, as ASFX is the only out-of-domain test set for all three models. FlowSep, trained on AudioSet, WavCaps, and VGGSound, which share similar quality and sources with AudioCaps and ESC50, obtains the best L2 Mel distance, F1 decision error, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC. SoloAudio, trained primarily on FreeSound, achieves the best results on FSD. All of AudioCaps, ESC50, and FSD are out-of-domain for PromptSep, but it yields competitive performance: achieving or nearly matching the best scores in FAD, SDRi, F1, CLAPScore, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC, and in SDRi on FSD. These results further demonstrate the strong generalization ability of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "model",
                    "promptsep",
                    "quality",
                    "scores",
                    "soloaudio",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond the standard extraction setup, PromptSep also supports the sound removal. While our baselines were not explicitly trained with removal operations, they have been exposed to large-scale textual descriptions and may have implicitly learned some removal capability. Therefore, we also include their results for comparison.\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep outperforms the baselines across all datasets and metrics, with the exception of the L2 Mel distance on ACESC. These highlight the strong performance of our model in language-queried target sound removal.</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "model",
                    "promptsep",
                    "removal",
                    "extraction",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further account for the limitations of baselines on sound removal, we design an alternative setup. Instead of directly removing the target sound, models are prompted to extract the remaining sounds by providing combined text descriptions of all non-target events, as an equivalent operation. Results under this configuration are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (gray rows, with models marked by *). PromptSep continues to achieve most of the best scores, even compared against FlowSep* and SoloAudio*. This further demonstrates the strong performance of our model in sound removal.</span>\n</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "model",
                    "promptsep",
                    "text",
                    "removal",
                    "scores",
                    "soloaudio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We further conduct a subjective evaluation for both the extraction and removal setups by following the format of DCASE2024 Task 9&#160;</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://dcase.community/challenge2024/task-language-queried-audio-source-separation\" title=\"\">https://dcase.community/challenge2024/task-language-queried-audio-source-separation</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using Relevance (REL) and Overall Sound Quality (OVL). The REL score measures how well the separated audio matches the given language query.\nThe OVL score evaluates the perceived audio quality of the output, including factors such as clarity, naturalness, and artifacts.\nBoth REL and OVL are rated by human annotators using a 5-point Likert scale.</span>\n</p>\n\n",
                "matched_terms": [
                    "overall",
                    "relevance",
                    "quality",
                    "rel",
                    "removal",
                    "ovl",
                    "extraction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To the best of our knowledge, no existing system supports vocal imitations as a conditioning input for open-domain source separation.\nWe evaluate our model on the VimSketchGen-Mix with no baseline.\nResults are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.2 Language-queried Target Sound Removal &#8227; 4 Results &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where our model achieves an SDRi of </span>\n  <math alttext=\"9.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">9.99</mn>\n      <annotation encoding=\"application/x-tex\">9.99</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB, an L2 multi-resolution Mel-spectrogram distance of </span>\n  <math alttext=\"0.92\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.92</mn>\n      <annotation encoding=\"application/x-tex\">0.92</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an F1 Decision Error of </span>\n  <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.95</mn>\n      <annotation encoding=\"application/x-tex\">0.95</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a CLAPScore</span>\n  <math alttext=\"{}_{\\text{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">A</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{A}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"0.87\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.87</mn>\n      <annotation encoding=\"application/x-tex\">0.87</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a FAD score of </span>\n  <math alttext=\"2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2.19</mn>\n      <annotation encoding=\"application/x-tex\">2.19</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese results indicate strong separation performance and demonstrate that vocal imitation is an effective conditioning signal for source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep offers a unified framework for sound extraction and removal that overcomes key limitations of existing LASS systems.\nOur approach supports both sound extraction and removal within a single model.\nFurthermore, the integration of vocal imitation as a query modality addresses the ambiguity and limitations of text prompts, offering a more intuitive interface for users.\nThrough comprehensive evaluations, PromptSep demonstrates SOTA performance in sound removal and vocal-imitation-guided separation, while remaining competitive in standard LASS settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "standard",
                    "model",
                    "promptsep",
                    "text",
                    "removal",
                    "extraction"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "PromptSep: Generative Audio Separation via Multimodal Prompting",
        "caption": "Table 4: Results for the vocal imitation (Imitation) condition, along with the Pitch and RMS condition for ablation analysis.",
        "body": "Conditions\n\nSDRi ↑\\uparrow\n\n\nL2 Mel ↓\\downarrow\n\n\nF1 Decision Error ↑\\uparrow\n\nCLAPScoreA↑\\text{CLAPScore}_{A}\\uparrow\n\nFAD ↓\\downarrow\n\n\n\n\n\nImitation\n9.99\n0.92\n0.95\n0.87\n2.19\n\n\nPitch+RMS\n7.17\n3.30\n0.84\n0.71\n6.66",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Conditions</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SDRi </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">L2 Mel </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F1 Decision Error </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math alttext=\"\\text{CLAPScore}_{A}\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mrow><msub><mtext mathsize=\"0.900em\">CLAPScore</mtext><mi mathsize=\"0.900em\">A</mi></msub><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\text{CLAPScore}_{A}\\uparrow</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Imitation</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">9.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pitch+RMS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.66</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rms",
            "ablation",
            "decision",
            "↓downarrow",
            "clapscorea↑textclapscoreauparrow",
            "pitchrms",
            "conditions",
            "analysis",
            "imitation",
            "pitch",
            "results",
            "mel",
            "sdri",
            "fad",
            "↑uparrow",
            "along",
            "vocal",
            "condition",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To the best of our knowledge, no existing system supports vocal imitations as a conditioning input for open-domain source separation.\nWe evaluate our model on the VimSketchGen-Mix with no baseline.\nResults are presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.2 Language-queried Target Sound Removal &#8227; 4 Results &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where our model achieves an SDRi of </span>\n  <math alttext=\"9.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">9.99</mn>\n      <annotation encoding=\"application/x-tex\">9.99</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB, an L2 multi-resolution Mel-spectrogram distance of </span>\n  <math alttext=\"0.92\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.92</mn>\n      <annotation encoding=\"application/x-tex\">0.92</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an F1 Decision Error of </span>\n  <math alttext=\"0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.95</mn>\n      <annotation encoding=\"application/x-tex\">0.95</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a CLAPScore</span>\n  <math alttext=\"{}_{\\text{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">A</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{A}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of </span>\n  <math alttext=\"0.87\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.87</mn>\n      <annotation encoding=\"application/x-tex\">0.87</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a FAD score of </span>\n  <math alttext=\"2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2.19</mn>\n      <annotation encoding=\"application/x-tex\">2.19</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese results indicate strong separation performance and demonstrate that vocal imitation is an effective conditioning signal for source separation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "results",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the above limitations, we propose PromptSep, a latent diffusion model for open-vocabulary target audio source separation with multimodal cues.\nSpecifically, we first extend the standard extraction-only separation process with </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">removal</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> operator.\nSecond, we incorporate </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">vocal imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as another conditioning signal beyond text prompts, where users can mimic the target sound to guide the separation target. This makes the system align better with how humans naturally describe sounds. To achieve this, we explore how to augment vocal imitation data by leveraging existing sound effect generation models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and modules&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. With PromptSep, users can perform either sound extraction or removal by leveraging either a textual description or a vocal imitation as a query. Vocal imitation alleviates the need to locate audio samples when the textual description is ambiguous.\nOur contributions are three-fold:</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We incorporate vocal imitation as an additional query modality via data augmentation and conditional modules, offering a more intuitive source control than text.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.1 Separation Condition &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep takes an audio mixture as the primary input, along with two conditions: (1) a textual description of the separation target; and (2) a vocal imitation recording of the separation target. Both conditions can be used separately or jointly. Based on these inputs, the model outputs an audio track containing the sources specified by the conditions. In the following subsections, we introduce the construction of these conditions and the model architecture of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "vocal",
                    "conditions",
                    "imitation",
                    "along"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vocal Imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; As mentioned in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S1\" style=\"font-size:90%;\" title=\"1 Introduction &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, certain sounds (e.g., &#8220;distortion&#8221; and &#8220;buzzing&#8221;) can be too abstract to describe accurately using text alone, or too ambiguous for the model to only identify the single candidate. To enable more flexible and intuitive sound specification, we introduce an additional guidance modality for separation: vocal imitation. PromptSep can be conditioned on reference audio samples in which a user vocally imitates a target sound, thereby guiding the separation process. This approach provides a more natural and accessible way for users to describe the sounds.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While a few datasets, such as VimSketch&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2019vim</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, contain pairs of corresponding vocal imitation and sound effect samples,\nthese pairs are not temporally aligned, and thus cannot serve as a good vocal imitation condition to identify the target sound in a mixture.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we leverage the sound effect generation model Sketch2Sound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for data augmentation.\nIt can generate sound effect audio samples that match textual descriptions and are temporally aligned with vocal imitation prompts.\nUsing around 12K real vocal imitation samples and their corresponding sound labels from the VimSketch dataset, we use Sketch2Sound to generate around 87K temporally aligned sound effect samples as training data for PromptSep. To better simulate real-world conditions, we apply time-shift and pitch-shift augmentations as imitation variances. We also add ambient noise, from 4.36 hours of static noise data collections&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinoshita2013reverb</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eaton2016estimation</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2021structure</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to enhance the model generalization capability.</span>\n</p>\n\n",
                "matched_terms": [
                    "vocal",
                    "conditions",
                    "imitation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We note that the Sketch2Sound training data does not require paired vocal imitations and sound effects.\nInstead, it relies on RMS and pitch curves to guide sound generation, without using actual imitation samples.\nThis leads to a possible exploration: whether curve-based features (RMS and pitch) or actual vocal-imitation raw waveform inputs provide more effective conditioning for audio separation. In Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S4\" style=\"font-size:90%;\" title=\"4 Results &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we address this through ablation studies.</span>\n</p>\n\n",
                "matched_terms": [
                    "rms",
                    "ablation",
                    "vocal",
                    "imitation",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We implement PromptSep using a latent diffusion model via diffusion Transformer (DiT) with three input types (text, vocal imitation, and audio mixture), following the specifications of&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024fast</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2024long</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIt contains three main components: (1) a pretrained variational autoencoder (VAE), following the architecture of Descript Audio Codec (DAC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, compresses 44.1 kHz mono audio samples into a sequence of continuous 128-dimensional embeddings at a temporal resolution of 40 Hz; (2) a pretrained FLAN-T5 encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raffel2019exploring</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encodes all text prompts; and (3) a DiT model is trained to generate new sequence of embeddings, which are decoded back to waveform via the VAE decoder to reconstruct target separated audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For conditioning both the mixture and the vocal imitation samples,\nwe adopt an in-place addition mechanism following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024music</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wen2025user</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as they have the same latent dimensionality as the separated audio. Specifically, we apply a single MLP layer respectively to mixture and vocal imitation, and add the resulting embeddings to the noisy latent as input to DiT.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During training, we randomly drop condition signals for classifier-free guidance (CFG), with the drop rate </span>\n  <math alttext=\"10\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">10\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for text and mixture, but </span>\n  <math alttext=\"90\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">90</mn>\n        <mo mathsize=\"0.900em\">%</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">90\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for vocal imitation, as it gets easier to overfit through our experiments. We use a </span>\n  <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">v</mi>\n      <annotation encoding=\"application/x-tex\">v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-prediction framework for training and diffusion probabilistic models (DPM) solvers&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lu2025dpm</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for sampling. During inference, we set the CFG scale to 1.0.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vocal Imitation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We curate a new dataset, VimSketchGen, consisting of </span>\n  <math alttext=\"87,171\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">87</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">171</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">87,171</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pairs of aligned vocal imitations and sound effects.\nThis dataset originally contains </span>\n  <math alttext=\"12,453\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">12</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">453</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">12,453</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vocal imitations sourced from the VimSketch dataset, and each of them is paired with </span>\n  <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">7</mn>\n      <annotation encoding=\"application/x-tex\">7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponding sound effects generated using Sketch2Sound, with different median filter sizes </span>\n  <math alttext=\"\\in\\{0,3,6,9,12,15,19\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi/>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">6</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">9</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">12</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">15</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">19</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\in\\{0,3,6,9,12,15,19\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAll audio samples in VimSketchGen are 8-second stereo tracks sampled at 44.1 kHz. We will release this dataset to support more tasks in audio research.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Specification</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; The input of PromptSep is a 10-second audio mixture by randomly combining </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound events from different categories\n. The mixing signal-to-noise ratio (SNR) is uniformly sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB. A random subset of sound events in the mixture is selected as the separation target, with the only exception that if the vocal imitation is chosen as condition, its corresponding sound event alone is used as the target.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#000000;\">During training, the model is always conditioned on either text or vocal imitation, but not both simultaneously. While both conditions can be provided at inference time, their combined effect is not explored in this work and is left for future investigation.\n</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "vocal",
                    "conditions",
                    "imitation",
                    "condition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">VimSketchGen-Mix</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8196; We use a subset of the VimSketchGen test split, containing </span>\n  <math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2000</mn>\n      <annotation encoding=\"application/x-tex\">2000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sound event samples with their vocal imitation pairs. Each target sound is mixed with 1 to 3 interference sounds randomly selected from the AudioSet test set&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemmeke2017audio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using a SNR sampled between </span>\n  <math alttext=\"-3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the performance of separation models using 6 objective metrics. Signal-to-distortion ratio improvement (SDRi) and L2 multi-resolution Mel-spectrogram distance&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kumar2023high</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the most conventional metrics to measure the signal-level differences between the output and the groundtruth. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2024reference</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025flowsep</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025soloaudio</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we also include CLAPScore, CLAPScore</span>\n  <sub class=\"ltx_sub\">\n    <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">A</span>\n  </sub>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Frechet Audio Distance (FAD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kilgour2018fr</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with embeddings from PANNs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2020panns</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess the generation audio quality and the semantic correlation among the text prompt, separation audio, and groundtruth audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "sdri",
                    "fad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To better evaluate the separation decision error, we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F1 Decision Error</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a new metric to evaluate the ability of the model to identify the correct temporal regions of a target sound. Specifically, to obtain the F1 score of decision errors, we first compute the frame-wise RMS energy on both the separated audio and the groudtruth audio. These values are then binarized using a threshold (0.01) to obtain the activity sequences, determining the sound and unsound frames. Finally, we calculate the F1 score between the predicted activity sequence and the groundtruth activity sequence.</span>\n</p>\n\n",
                "matched_terms": [
                    "decision",
                    "error",
                    "rms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep achieves the best performance on nearly all metrics on ASFX, with the exception of CLAPScore, where SoloAudio surpasses us by 0.01. This highlights the strong generalization of our model, as ASFX is the only out-of-domain test set for all three models. FlowSep, trained on AudioSet, WavCaps, and VGGSound, which share similar quality and sources with AudioCaps and ESC50, obtains the best L2 Mel distance, F1 decision error, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC. SoloAudio, trained primarily on FreeSound, achieves the best results on FSD. All of AudioCaps, ESC50, and FSD are out-of-domain for PromptSep, but it yields competitive performance: achieving or nearly matching the best scores in FAD, SDRi, F1, CLAPScore, and CLAPScore</span>\n  <math alttext=\"{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">A</mi>\n      <annotation encoding=\"application/x-tex\">{A}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on ACESC, and in SDRi on FSD. These results further demonstrate the strong generalization ability of PromptSep.</span>\n</p>\n\n",
                "matched_terms": [
                    "mel",
                    "decision",
                    "sdri",
                    "fad",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond the standard extraction setup, PromptSep also supports the sound removal. While our baselines were not explicitly trained with removal operations, they have been exposed to large-scale textual descriptions and may have implicitly learned some removal capability. Therefore, we also include their results for comparison.\nAs presented in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.04623v1#S2.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 2.2 Model Architecture &#8227; 2 Method &#8227; PromptSep: Generative Audio Separation via Multimodal Prompting\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PromptSep outperforms the baselines across all datasets and metrics, with the exception of the L2 Mel distance on ACESC. These highlight the strong performance of our model in language-queried target sound removal.</span>\n</p>\n\n",
                "matched_terms": [
                    "mel",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We also evaluate a variant of our model that uses only pitch and RMS features extracted from the vocal imitation as the conditioning input, this setup is trained using the same median filter strategy as in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">garcia2025sketch2sound</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with the median filter size fixed to 8 during inference. While the pitch and RMS-based conditioning yields reasonably strong separation performance, it consistently under-performs the full vocal imitation condition across all evaluation metrics.\nWe attribute this to the complexity of the mixtures, where overlapping sounds are common; in such cases, the raw vocal imitation provides richer information than the limited pitch and RMS features alone.</span>\n</p>\n\n",
                "matched_terms": [
                    "rms",
                    "vocal",
                    "imitation",
                    "condition",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">PromptSep offers a unified framework for sound extraction and removal that overcomes key limitations of existing LASS systems.\nOur approach supports both sound extraction and removal within a single model.\nFurthermore, the integration of vocal imitation as a query modality addresses the ambiguity and limitations of text prompts, offering a more intuitive interface for users.\nThrough comprehensive evaluations, PromptSep demonstrates SOTA performance in sound removal and vocal-imitation-guided separation, while remaining competitive in standard LASS settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "imitation",
                    "vocal"
                ]
            }
        ]
    }
}