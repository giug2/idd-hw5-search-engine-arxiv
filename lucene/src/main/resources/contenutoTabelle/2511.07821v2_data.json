{
    "S2.T1": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 1: Performance comparison of CosyVoice 2 and mainstream TTS models on LibriSpeech test-clean subset.",
        "body": "Model\nWER (%)\nNMOS\nSS\n\n\nHuman\n2.66\n3.84\n0.697\n\n\nChatTTS\n6.84\n3.89\n–\n\n\nGPT-SoVITs\n5.13\n3.93\n0.405\n\n\nOpenVoice\n3.47\n3.87\n0.299\n\n\nParlerTTS\n3.16\n3.86\n–\n\n\nEmotiVoice\n3.14\n3.93\n–\n\n\nCosyVoice\n2.89\n3.93\n0.743\n\n\nCosyVoice 2\n2.47\n3.96\n0.745\n\n\nCosyVoice 2-S\n2.45\n3.90\n0.751",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">NMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Human</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.697</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">ChatTTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">GPT-SoVITs</th>\n<td class=\"ltx_td ltx_align_center\">5.13</td>\n<td class=\"ltx_td ltx_align_center\">3.93</td>\n<td class=\"ltx_td ltx_align_center\">0.405</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">OpenVoice</th>\n<td class=\"ltx_td ltx_align_center\">3.47</td>\n<td class=\"ltx_td ltx_align_center\">3.87</td>\n<td class=\"ltx_td ltx_align_center\">0.299</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ParlerTTS</th>\n<td class=\"ltx_td ltx_align_center\">3.16</td>\n<td class=\"ltx_td ltx_align_center\">3.86</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EmotiVoice</th>\n<td class=\"ltx_td ltx_align_center\">3.14</td>\n<td class=\"ltx_td ltx_align_center\">3.93</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.743</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">CosyVoice 2</span></th>\n<td class=\"ltx_td ltx_align_center\">2.47</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.96</span></td>\n<td class=\"ltx_td ltx_align_center\">0.745</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">CosyVoice 2-S</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.751</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "nmos",
            "model",
            "subset",
            "cosyvoice",
            "models",
            "tts",
            "librispeech",
            "parlertts",
            "testclean",
            "gptsovits",
            "human",
            "emotivoice",
            "wer",
            "chattts",
            "performance",
            "mainstream",
            "comparison",
            "openvoice"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T1\" title=\"Table 1 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the performance of various TTS models on the LibriSpeech test-clean subset, evaluated by word error rate (WER), naturalness mean opinion score (NMOS), and speaker similarity (SS). CosyVoice 2 and CosyVoice 2-S achieve the lowest WER and highest SS among all synthetic models, closely matching or surpassing human performance. Additionally, CosyVoice 2 attains the highest NMOS, indicating superior speech quality and naturalness. These results demonstrate that CosyVoice 2 series models offer state-of-the-art accuracy and speaker similarity for synthetic speech generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice",
                    "models",
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core contribution is the demonstration that synthetic data is not merely a substitute but a superior alternative for KWS model training. We validate SYNTTS-COMMANDS-MEDIA by training a spectrum of efficient acoustic models. The results are striking: top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese commands. This performance firmly establishes that TTS-generated datasets can effectively replace traditional, labor-intensive speech collections. By providing this scalable, high-quality data foundation and validating its efficacy, we aim to accelerate the development of more private, efficient, and intelligent on-device voice interfaces, thereby pushing the frontiers of TinyML forward.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements in neural text-to-speech (TTS) synthesis have revolutionized artificial voice generation, achieving unprecedented naturalness that often surpasses human recordings in objective metrics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017tacotron</span>]</cite>. Modern systems leverage deep learning architectures like transformer-based language models and diffusion processes to produce speech with accurate prosody, emotional expression, and speaker characteristics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2020fastspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>]</cite>. These technologies have effectively overcome the &#8221;uncanny valley&#8221; effect, with state-of-the-art models consistently scoring above 4.0 in Mean Opinion Score (MOS) evaluations<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">reddy2022dnsmos</span>]</cite>. The field has particularly advanced in zero-shot voice cloning, where systems can mimic a speaker&#8217;s voice from just seconds of reference audio while maintaining content accuracy and natural rhythm<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice2</span>]</cite> developed by Alibaba Group, represents a state-of-the-art advancement in neural text-to-speech (TTS) technology. It introduces hybrid generative modeling, combining LLM-based text-to-semantic decoding with flow-matching-based acoustic synthesis to achieve highly natural and consistent speaker output. The adoption of Finite Scalar Quantization (FSQ) replaces traditional vector quantization, resulting in 100% codebook utilization and enhanced speech detail preservation. Additionally, CosyVoice 2 supports both low-latency streaming and high-quality offline synthesis within a unified model, ensuring minimal quality loss. These architectural innovations enable CosyVoice 2 to generate synthetic speech that closely matches human recordings in naturalness, speaker similarity, and multilingual capability,making it an ideal foundation for building robust and diverse voice command datasets for smart device applications.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We cite two tables from the original CosyVoice 2 paper to provide a direct comparison of model performance and highlight the state-of-the-art results achieved by CosyVoice 2 and related TTS systems. These tables serve as a benchmark reference for subsequent analysis and dataset construction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice",
                    "tts",
                    "performance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate the advantages of CosyVoice 2, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T2\" title=\"Table 2 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes its performance compared to recent TTS models on the SEED test sets. CosyVoice 2 achieves competitive or superior results in character error rate (CER), word error rate (WER), and speaker similarity (SS) across Chinese and English test sets, highlighting its robustness and generalization ability for synthetic dataset construction.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "models",
                    "tts",
                    "wer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of CosyVoice 2 effectively addresses several critical challenges in smart device interaction research. It enables the generation of thousands of command variations with consistent speaker characteristics, eliminating the fatigue and variability associated with human voice collection. CosyVoice 2 also allows for precise control over pitch (<math alttext=\"\\pm 20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 20\\%</annotation></semantics></math>), speed (<math alttext=\"\\pm 30\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>30</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 30\\%</annotation></semantics></math>), and background noise (SNR <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"30\\,\\mathrm{dB}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>30</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>dB</mi></mrow><annotation encoding=\"application/x-tex\">30\\,\\mathrm{dB}</annotation></semantics></math>), ensuring diverse and realistic data suitable for robust model training. Furthermore, by relying on synthetic speech, it resolves privacy concerns inherent in collecting and storing human voice data, making it an ethical and scalable solution for building large-scale voice command datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected six widely adopted models known for their efficiency, architectural diversity, and proven performance in keyword spotting and audio classification tasks &#8212; carefully chosen to span a broad spectrum of model complexity, from ultra-lightweight to modern large-scale architectures. This selection enables a meaningful comparison across different design philosophies and computational constraints:</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "comparison",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MicroCNN</span>: We design a family of ultra-lightweight CNNs, named MicroCNN, inspired by the minimal architectures used in TensorFlow Lite Micro <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">microspeech2020</span>]</cite>. Our model employs depthwise separable convolutions and batch normalization to further reduce parameters while maintaining expressiveness, achieving a parameter count as low as &#160;4K (in &#8221;micro&#8221; configuration).\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DS-CNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2017hello</span>]</cite>: A depthwise separable convolution-based model (&#160;30K params), originally designed for on-device wake-word detection. It exemplifies the efficiency of factorized convolutions and remains a popular choice in embedded speech systems due to its favorable accuracy-complexity trade-off.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">TC-ResNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2019temporal</span>]</cite>: A temporal convolutional ResNet variant (&#160;68K params) that introduces residual connections and dilated convolutions for modeling long-range temporal dependencies. It reflects the adaptation of image-based residual architectures to 1D audio signals.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">CRNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moya2018convolutional</span>]</cite>: A hybrid Convolutional Recurrent Neural Network (&#160;1M params) combining CNN feature extraction with bidirectional LSTM for sequence modeling. It represents the temporal modeling paradigm, capturing both spectral and sequential patterns &#8212; crucial for command recognition where context matters.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">MobileNet V1</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">howard2017mobilenets</span>]</cite>: A classic lightweight CNN (&#160;4.3M params) based on depthwise separable convolutions. As a mobile-optimized architecture, it serves as a reference point for efficient yet accurate vision-inspired models adapted to audio classification.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">EfficientNet-B0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tan2019efficientnet</span>]</cite>: The smallest variant of the EfficientNet family (&#160;4.7M params), representing modern SOTA scaling principles. Its compound scaling strategy balances depth, width, and resolution, making it a strong performer even at small scales &#8212; ideal for evaluating whether advanced image architectures can transfer effectively to audio tasks.\nAll models were trained and tested separately on the English and Chinese subsets of the CosyVoice dataset. Performance was measured using classification accuracy and cross-entropy loss, enabling a holistic assessment of both predictive power and calibration across languages and model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive benchmark of six representative acoustic models on the SYNTTS-COMMANDS-MEDIA Dataset across both English (EN) and Chinese (ZH) subsets, with results summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S5.T7\" title=\"Table 7 &#8227; 5.2 Benchmark Results and Analysis &#8227; 5 Evaluation &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. All models are evaluated in terms of classification accuracy, cross-entropy loss, and parameter count, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that the SYNTTS-COMMANDS-MEDIA dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese, confirming the dataset&#8217;s quality and suitability for real-world deployment. Among all models, CRNN attains the best English accuracy (99.50%) and the lowest loss (0.0163), while MobileNet-V1 yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNN&#8217;s 99.50% accuracy). Interestingly, EfficientNet shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1 (97.92%), despite a higher loss&#8212;suggesting better calibration or robustness in its predictions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, lightweight models exhibit a clear accuracy&#8211;complexity trade-off. MicroCNN, with only <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>4.2K parameters, achieves 93.22% accuracy on English but drops significantly on Chinese (80.14%), highlighting the increased difficulty of modeling tonal and phonetic richness in Mandarin with ultra-compact architectures. DS-CNN and TC-ResNet, with under 70K parameters, already recover strong performance (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>96.5% in both languages), underscoring their efficiency for resource-constrained applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the benchmark establishes strong baselines across a wide spectrum of model scales&#8212;from ultra-light MicroCNN to modern EfficientNet&#8212;while revealing that moderate-complexity models (e.g., DS-CNN, TC-ResNet) already deliver near-SOTA performance with minimal resource overhead. This makes them particularly suitable for deployment on edge devices in multilingual smart environments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The high classification accuracy observed across English and Chinese commands (up to 99.5%) confirms that synthetic data does not compromise model performance. In fact, the acoustic consistency of TTS-generated speech may help models converge faster and generalize better in controlled command scenarios, particularly in the presence of channel or environmental variations. This is especially relevant for deployment on low-power chips such as the ADA100, Ceva NeuPro-Nano, and Ambiq Apollo510 Lite, where model robustness and inference efficiency are critical.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) <span class=\"ltx_text ltx_font_bold\">Open collaboration and community co-creation</span>: We are developing a dedicated public platform (to be released at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://syntts-commands.org\" title=\"\">https://syntts-commands.org</a>) where researchers, developers, and product teams can not only download curated datasets but also propose new commands, contribute synthetic voices, evaluate models, and share best practices. We also plan to release accompanying tools&#8212;including negative sample sets for out-of-vocabulary rejection, noise-augmented variants for robustness testing, and TTS configuration templates&#8212;to facilitate real-world deployment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            }
        ]
    },
    "S2.T2": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 2: Performance comparison of CosyVoice 2 and mainstream TTS models on SEED test sets",
        "body": "Model\ntest-zh\ntest-en\ntest-hard\n\n\n\nCER (%)\nSS\nWER (%)\nSS\nWER (%)\nSS\n\n\nHuman\n1.26\n0.755 (0.775)\n2.14\n0.734 (0.742)\n-\n-\n\n\nVocoder Resyn.\n1.27\n0.720\n2.17\n0.700\n-\n-\n\n\nSeed-TTS\n1.12\n0.796\n2.25\n0.762\n7.59\n0.776\n\n\nFireRedTTS\n1.51\n0.635 (0.653)\n3.82\n0.460 (0.526)\n17.45\n0.621 (0.639)\n\n\nMaskGCT\n2.27\n0.774 (0.752)\n2.62\n0.714 (0.730)\n10.27\n0.748 (0.720)\n\n\nE2 TTS (32 NFE)\n1.97\n0.730\n2.19\n0.710\n-\n-\n\n\nF5-TTS (32 NFE)\n1.56\n0.741 (0.794)\n1.83\n0.647 (0.742)\n8.67\n0.713 (0.762)\n\n\nCosyVoice\n3.63\n0.723 (0.775)\n4.29\n0.609 (0.699)\n11.75\n0.709 (0.755)\n\n\nCosyVoice 2\n1.45\n\n0.748 (0.806)\n2.57\n\n0.652 (0.736)\n6.83\n\n0.724 (0.776)\n\n\nCosyVoice 2-S\n1.45\n\n0.753 (0.812)\n2.38\n\n0.654 (0.743)\n8.08\n\n0.732 (0.785)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">test-zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">test-en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">test-hard</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\">CER (%)</td>\n<td class=\"ltx_td ltx_align_center\">SS</td>\n<td class=\"ltx_td ltx_align_center\">WER (%)</td>\n<td class=\"ltx_td ltx_align_center\">SS</td>\n<td class=\"ltx_td ltx_align_center\">WER (%)</td>\n<td class=\"ltx_td ltx_align_center\">SS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Human</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.755 (0.775)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.734 (0.742)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Vocoder Resyn.</th>\n<td class=\"ltx_td ltx_align_center\">1.27</td>\n<td class=\"ltx_td ltx_align_center\">0.720</td>\n<td class=\"ltx_td ltx_align_center\">2.17</td>\n<td class=\"ltx_td ltx_align_center\">0.700</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Seed-TTS</th>\n<td class=\"ltx_td ltx_align_center\">1.12</td>\n<td class=\"ltx_td ltx_align_center\">0.796</td>\n<td class=\"ltx_td ltx_align_center\">2.25</td>\n<td class=\"ltx_td ltx_align_center\">0.762</td>\n<td class=\"ltx_td ltx_align_center\">7.59</td>\n<td class=\"ltx_td ltx_align_center\">0.776</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FireRedTTS</th>\n<td class=\"ltx_td ltx_align_center\">1.51</td>\n<td class=\"ltx_td ltx_align_center\">0.635 (0.653)</td>\n<td class=\"ltx_td ltx_align_center\">3.82</td>\n<td class=\"ltx_td ltx_align_center\">0.460 (0.526)</td>\n<td class=\"ltx_td ltx_align_center\">17.45</td>\n<td class=\"ltx_td ltx_align_center\">0.621 (0.639)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MaskGCT</th>\n<td class=\"ltx_td ltx_align_center\">2.27</td>\n<td class=\"ltx_td ltx_align_center\">0.774 (0.752)</td>\n<td class=\"ltx_td ltx_align_center\">2.62</td>\n<td class=\"ltx_td ltx_align_center\">0.714 (0.730)</td>\n<td class=\"ltx_td ltx_align_center\">10.27</td>\n<td class=\"ltx_td ltx_align_center\">0.748 (0.720)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">E2 TTS (32 NFE)</th>\n<td class=\"ltx_td ltx_align_center\">1.97</td>\n<td class=\"ltx_td ltx_align_center\">0.730</td>\n<td class=\"ltx_td ltx_align_center\">2.19</td>\n<td class=\"ltx_td ltx_align_center\">0.710</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">F5-TTS (32 NFE)</th>\n<td class=\"ltx_td ltx_align_center\">1.56</td>\n<td class=\"ltx_td ltx_align_center\">0.741 (0.794)</td>\n<td class=\"ltx_td ltx_align_center\">1.83</td>\n<td class=\"ltx_td ltx_align_center\">0.647 (0.742)</td>\n<td class=\"ltx_td ltx_align_center\">8.67</td>\n<td class=\"ltx_td ltx_align_center\">0.713 (0.762)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center\">3.63</td>\n<td class=\"ltx_td ltx_align_center\">0.723 (0.775)</td>\n<td class=\"ltx_td ltx_align_center\">4.29</td>\n<td class=\"ltx_td ltx_align_center\">0.609 (0.699)</td>\n<td class=\"ltx_td ltx_align_center\">11.75</td>\n<td class=\"ltx_td ltx_align_center\">0.709 (0.755)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice 2</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">0.748</span> (0.806)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">0.652</span> (0.736)</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">6.83</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">0.724</span> (0.776)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">CosyVoice 2-S</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">0.753</span> (0.812)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">0.654</span> (0.743)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">8.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\">0.732</span> (0.785)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sets",
            "cosyvoice",
            "seedtts",
            "f5tts",
            "testzh",
            "nfe",
            "test",
            "tts",
            "human",
            "vocoder",
            "wer",
            "mainstream",
            "model",
            "seed",
            "cer",
            "performance",
            "maskgct",
            "testen",
            "models",
            "testhard",
            "fireredtts",
            "comparison",
            "resyn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To further demonstrate the advantages of CosyVoice 2, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T2\" title=\"Table 2 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes its performance compared to recent TTS models on the SEED test sets. CosyVoice 2 achieves competitive or superior results in character error rate (CER), word error rate (WER), and speaker similarity (SS) across Chinese and English test sets, highlighting its robustness and generalization ability for synthetic dataset construction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice",
                    "models",
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core contribution is the demonstration that synthetic data is not merely a substitute but a superior alternative for KWS model training. We validate SYNTTS-COMMANDS-MEDIA by training a spectrum of efficient acoustic models. The results are striking: top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese commands. This performance firmly establishes that TTS-generated datasets can effectively replace traditional, labor-intensive speech collections. By providing this scalable, high-quality data foundation and validating its efficacy, we aim to accelerate the development of more private, efficient, and intelligent on-device voice interfaces, thereby pushing the frontiers of TinyML forward.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements in neural text-to-speech (TTS) synthesis have revolutionized artificial voice generation, achieving unprecedented naturalness that often surpasses human recordings in objective metrics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017tacotron</span>]</cite>. Modern systems leverage deep learning architectures like transformer-based language models and diffusion processes to produce speech with accurate prosody, emotional expression, and speaker characteristics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2020fastspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>]</cite>. These technologies have effectively overcome the &#8221;uncanny valley&#8221; effect, with state-of-the-art models consistently scoring above 4.0 in Mean Opinion Score (MOS) evaluations<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">reddy2022dnsmos</span>]</cite>. The field has particularly advanced in zero-shot voice cloning, where systems can mimic a speaker&#8217;s voice from just seconds of reference audio while maintaining content accuracy and natural rhythm<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice2</span>]</cite> developed by Alibaba Group, represents a state-of-the-art advancement in neural text-to-speech (TTS) technology. It introduces hybrid generative modeling, combining LLM-based text-to-semantic decoding with flow-matching-based acoustic synthesis to achieve highly natural and consistent speaker output. The adoption of Finite Scalar Quantization (FSQ) replaces traditional vector quantization, resulting in 100% codebook utilization and enhanced speech detail preservation. Additionally, CosyVoice 2 supports both low-latency streaming and high-quality offline synthesis within a unified model, ensuring minimal quality loss. These architectural innovations enable CosyVoice 2 to generate synthetic speech that closely matches human recordings in naturalness, speaker similarity, and multilingual capability,making it an ideal foundation for building robust and diverse voice command datasets for smart device applications.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "human",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We cite two tables from the original CosyVoice 2 paper to provide a direct comparison of model performance and highlight the state-of-the-art results achieved by CosyVoice 2 and related TTS systems. These tables serve as a benchmark reference for subsequent analysis and dataset construction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice",
                    "tts",
                    "performance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T1\" title=\"Table 1 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the performance of various TTS models on the LibriSpeech test-clean subset, evaluated by word error rate (WER), naturalness mean opinion score (NMOS), and speaker similarity (SS). CosyVoice 2 and CosyVoice 2-S achieve the lowest WER and highest SS among all synthetic models, closely matching or surpassing human performance. Additionally, CosyVoice 2 attains the highest NMOS, indicating superior speech quality and naturalness. These results demonstrate that CosyVoice 2 series models offer state-of-the-art accuracy and speaker similarity for synthetic speech generation.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "models",
                    "tts",
                    "human",
                    "wer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of CosyVoice 2 effectively addresses several critical challenges in smart device interaction research. It enables the generation of thousands of command variations with consistent speaker characteristics, eliminating the fatigue and variability associated with human voice collection. CosyVoice 2 also allows for precise control over pitch (<math alttext=\"\\pm 20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 20\\%</annotation></semantics></math>), speed (<math alttext=\"\\pm 30\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>30</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 30\\%</annotation></semantics></math>), and background noise (SNR <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"30\\,\\mathrm{dB}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>30</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>dB</mi></mrow><annotation encoding=\"application/x-tex\">30\\,\\mathrm{dB}</annotation></semantics></math>), ensuring diverse and realistic data suitable for robust model training. Furthermore, by relying on synthetic speech, it resolves privacy concerns inherent in collecting and storing human voice data, making it an ethical and scalable solution for building large-scale voice command datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "sets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected six widely adopted models known for their efficiency, architectural diversity, and proven performance in keyword spotting and audio classification tasks &#8212; carefully chosen to span a broad spectrum of model complexity, from ultra-lightweight to modern large-scale architectures. This selection enables a meaningful comparison across different design philosophies and computational constraints:</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "comparison",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MicroCNN</span>: We design a family of ultra-lightweight CNNs, named MicroCNN, inspired by the minimal architectures used in TensorFlow Lite Micro <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">microspeech2020</span>]</cite>. Our model employs depthwise separable convolutions and batch normalization to further reduce parameters while maintaining expressiveness, achieving a parameter count as low as &#160;4K (in &#8221;micro&#8221; configuration).\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DS-CNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2017hello</span>]</cite>: A depthwise separable convolution-based model (&#160;30K params), originally designed for on-device wake-word detection. It exemplifies the efficiency of factorized convolutions and remains a popular choice in embedded speech systems due to its favorable accuracy-complexity trade-off.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">TC-ResNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2019temporal</span>]</cite>: A temporal convolutional ResNet variant (&#160;68K params) that introduces residual connections and dilated convolutions for modeling long-range temporal dependencies. It reflects the adaptation of image-based residual architectures to 1D audio signals.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">CRNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moya2018convolutional</span>]</cite>: A hybrid Convolutional Recurrent Neural Network (&#160;1M params) combining CNN feature extraction with bidirectional LSTM for sequence modeling. It represents the temporal modeling paradigm, capturing both spectral and sequential patterns &#8212; crucial for command recognition where context matters.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">MobileNet V1</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">howard2017mobilenets</span>]</cite>: A classic lightweight CNN (&#160;4.3M params) based on depthwise separable convolutions. As a mobile-optimized architecture, it serves as a reference point for efficient yet accurate vision-inspired models adapted to audio classification.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">EfficientNet-B0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tan2019efficientnet</span>]</cite>: The smallest variant of the EfficientNet family (&#160;4.7M params), representing modern SOTA scaling principles. Its compound scaling strategy balances depth, width, and resolution, making it a strong performer even at small scales &#8212; ideal for evaluating whether advanced image architectures can transfer effectively to audio tasks.\nAll models were trained and tested separately on the English and Chinese subsets of the CosyVoice dataset. Performance was measured using classification accuracy and cross-entropy loss, enabling a holistic assessment of both predictive power and calibration across languages and model sizes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive benchmark of six representative acoustic models on the SYNTTS-COMMANDS-MEDIA Dataset across both English (EN) and Chinese (ZH) subsets, with results summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S5.T7\" title=\"Table 7 &#8227; 5.2 Benchmark Results and Analysis &#8227; 5 Evaluation &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. All models are evaluated in terms of classification accuracy, cross-entropy loss, and parameter count, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that the SYNTTS-COMMANDS-MEDIA dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese, confirming the dataset&#8217;s quality and suitability for real-world deployment. Among all models, CRNN attains the best English accuracy (99.50%) and the lowest loss (0.0163), while MobileNet-V1 yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNN&#8217;s 99.50% accuracy). Interestingly, EfficientNet shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1 (97.92%), despite a higher loss&#8212;suggesting better calibration or robustness in its predictions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, lightweight models exhibit a clear accuracy&#8211;complexity trade-off. MicroCNN, with only <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>4.2K parameters, achieves 93.22% accuracy on English but drops significantly on Chinese (80.14%), highlighting the increased difficulty of modeling tonal and phonetic richness in Mandarin with ultra-compact architectures. DS-CNN and TC-ResNet, with under 70K parameters, already recover strong performance (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>96.5% in both languages), underscoring their efficiency for resource-constrained applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the benchmark establishes strong baselines across a wide spectrum of model scales&#8212;from ultra-light MicroCNN to modern EfficientNet&#8212;while revealing that moderate-complexity models (e.g., DS-CNN, TC-ResNet) already deliver near-SOTA performance with minimal resource overhead. This makes them particularly suitable for deployment on edge devices in multilingual smart environments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The high classification accuracy observed across English and Chinese commands (up to 99.5%) confirms that synthetic data does not compromise model performance. In fact, the acoustic consistency of TTS-generated speech may help models converge faster and generalize better in controlled command scenarios, particularly in the presence of channel or environmental variations. This is especially relevant for deployment on low-power chips such as the ADA100, Ceva NeuPro-Nano, and Ambiq Apollo510 Lite, where model robustness and inference efficiency are critical.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It should be noted that the current evaluation focuses exclusively on <em class=\"ltx_emph ltx_font_italic\">in-class command recognition</em>, where all test samples belong to a predefined command set. This design intentionally isolates the core research question&#8212;whether synthetic commands can reliably replace real speech during model training. For real-world use, however, a voice interface must also handle out-of-vocabulary inputs and environmental noise. While the current version of SYNTTS-COMMANDS-MEDIA does not include negative samples, developers can readily augment it with public audio datasets such as ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015dataset</span>]</cite> and UrbanSound8K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salamon2014dataset</span>]</cite> to improve rejection capabilities. Such a hybrid approach maintains the scalability of synthetic commands while incorporating real-world acoustic variability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, the ability to generate multilingual command sets on demand will play a crucial role in globalizing voice-enabled products. Unlike static, monolingual datasets such as Google Speech Commands, SYNTTS-COMMANDS exemplifies a dynamic and extensible data paradigm that aligns with the evolving needs of edge AI and TinyML applications. By lowering the barrier to high-quality, multilingual data, we not only accelerate model development but also foster more inclusive and accessible voice technologies.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, this work underscores a timely convergence of advanced TTS synthesis and ultra-low-power speech recognition hardware. As specialized low-power chips continue to evolve, the demand for tailored, multi-keyword command sets will grow rapidly. SYNTTS-COMMANDS provides a scalable and adaptable data foundation that aligns perfectly with this hardware trend, enabling more complex, on-device voice interactions without relying on the cloud.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "sets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(3) <span class=\"ltx_text ltx_font_bold\">Open collaboration and community co-creation</span>: We are developing a dedicated public platform (to be released at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://syntts-commands.org\" title=\"\">https://syntts-commands.org</a>) where researchers, developers, and product teams can not only download curated datasets but also propose new commands, contribute synthetic voices, evaluate models, and share best practices. We also plan to release accompanying tools&#8212;including negative sample sets for out-of-vocabulary rejection, noise-augmented variants for robustness testing, and TTS configuration templates&#8212;to facilitate real-world deployment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "sets"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 3: Comparative statistics of audio features across datasets",
        "body": "Dataset\nSamples\nDuration (s)\nSNR (dB)\nSR (Hz)\nQuality Score\n\n\n\n\nVoxCeleb1\n1,251\n29.89 ±\\pm 12.67\n17.92 ±\\pm 5.56\n16,000\n328.73 ±\\pm 126.83\n\n\nVoxCeleb2\n5,994\n27.96 ±\\pm 16.41\n16.40 ±\\pm 6.23\n16,000\n306.93 ±\\pm 164.67\n\n\nFree ST Chinese\n855\n5.44 ±\\pm 0.84\n43.31 ±\\pm 5.96\n16,000\n126.58 ±\\pm 12.94",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Dataset</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Samples</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Duration (s)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SNR (dB)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SR (Hz)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Quality Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">VoxCeleb1</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">1,251</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">29.89 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 12.67</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">17.92 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 5.56</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">16,000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">328.73 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 126.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VoxCeleb2</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">5,994</th>\n<td class=\"ltx_td ltx_align_right\">27.96 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 16.41</td>\n<td class=\"ltx_td ltx_align_right\">16.40 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 6.23</td>\n<td class=\"ltx_td ltx_align_right\">16,000</td>\n<td class=\"ltx_td ltx_align_right\">306.93 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 164.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Free ST Chinese</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\">855</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">5.44 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.84</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">43.31 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 5.96</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">16,000</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">126.58 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 12.94</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "duration",
            "snr",
            "quality",
            "across",
            "features",
            "voxceleb2",
            "comparative",
            "dataset",
            "chinese",
            "datasets",
            "voxceleb1",
            "score",
            "±pm",
            "samples",
            "audio",
            "statistics",
            "free"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "across",
                    "dataset",
                    "chinese",
                    "datasets",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the rapid advancement of TinyML, particularly in the KWS domain, is severely hampered by a critical bottleneck: the scarcity of high-quality, diverse, and domain-specific training datasets. The field still heavily relies on early benchmarks like the Google Speech Commands dataset (2018)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">warden2018speechcommands</span>]</cite>, which, while pioneering, is limited in vocabulary size and linguistic coverage. To deploy KWS systems in richer contexts&#8212;such as multimedia control, smart home management, healthcare monitoring for the elderly, or in-car commands&#8212;we need datasets with more commands, more languages, and broader application coverage.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "free",
                    "datasets",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core contribution is the demonstration that synthetic data is not merely a substitute but a superior alternative for KWS model training. We validate SYNTTS-COMMANDS-MEDIA by training a spectrum of efficient acoustic models. The results are striking: top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese commands. This performance firmly establishes that TTS-generated datasets can effectively replace traditional, labor-intensive speech collections. By providing this scalable, high-quality data foundation and validating its efficacy, we aim to accelerate the development of more private, efficient, and intelligent on-device voice interfaces, thereby pushing the frontiers of TinyML forward.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements in neural text-to-speech (TTS) synthesis have revolutionized artificial voice generation, achieving unprecedented naturalness that often surpasses human recordings in objective metrics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017tacotron</span>]</cite>. Modern systems leverage deep learning architectures like transformer-based language models and diffusion processes to produce speech with accurate prosody, emotional expression, and speaker characteristics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2020fastspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>]</cite>. These technologies have effectively overcome the &#8221;uncanny valley&#8221; effect, with state-of-the-art models consistently scoring above 4.0 in Mean Opinion Score (MOS) evaluations<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">reddy2022dnsmos</span>]</cite>. The field has particularly advanced in zero-shot voice cloning, where systems can mimic a speaker&#8217;s voice from just seconds of reference audio while maintaining content accuracy and natural rhythm<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice2</span>]</cite> developed by Alibaba Group, represents a state-of-the-art advancement in neural text-to-speech (TTS) technology. It introduces hybrid generative modeling, combining LLM-based text-to-semantic decoding with flow-matching-based acoustic synthesis to achieve highly natural and consistent speaker output. The adoption of Finite Scalar Quantization (FSQ) replaces traditional vector quantization, resulting in 100% codebook utilization and enhanced speech detail preservation. Additionally, CosyVoice 2 supports both low-latency streaming and high-quality offline synthesis within a unified model, ensuring minimal quality loss. These architectural innovations enable CosyVoice 2 to generate synthetic speech that closely matches human recordings in naturalness, speaker similarity, and multilingual capability,making it an ideal foundation for building robust and diverse voice command datasets for smart device applications.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T1\" title=\"Table 1 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the performance of various TTS models on the LibriSpeech test-clean subset, evaluated by word error rate (WER), naturalness mean opinion score (NMOS), and speaker similarity (SS). CosyVoice 2 and CosyVoice 2-S achieve the lowest WER and highest SS among all synthetic models, closely matching or surpassing human performance. Additionally, CosyVoice 2 attains the highest NMOS, indicating superior speech quality and naturalness. These results demonstrate that CosyVoice 2 series models offer state-of-the-art accuracy and speaker similarity for synthetic speech generation.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate the advantages of CosyVoice 2, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T2\" title=\"Table 2 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes its performance compared to recent TTS models on the SEED test sets. CosyVoice 2 achieves competitive or superior results in character error rate (CER), word error rate (WER), and speaker similarity (SS) across Chinese and English test sets, highlighting its robustness and generalization ability for synthetic dataset construction.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of CosyVoice 2 effectively addresses several critical challenges in smart device interaction research. It enables the generation of thousands of command variations with consistent speaker characteristics, eliminating the fatigue and variability associated with human voice collection. CosyVoice 2 also allows for precise control over pitch (<math alttext=\"\\pm 20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 20\\%</annotation></semantics></math>), speed (<math alttext=\"\\pm 30\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>30</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 30\\%</annotation></semantics></math>), and background noise (SNR <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"30\\,\\mathrm{dB}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>30</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>dB</mi></mrow><annotation encoding=\"application/x-tex\">30\\,\\mathrm{dB}</annotation></semantics></math>), ensuring diverse and realistic data suitable for robust model training. Furthermore, by relying on synthetic speech, it resolves privacy concerns inherent in collecting and storing human voice data, making it an ethical and scalable solution for building large-scale voice command datasets.</p>\n\n",
                "matched_terms": [
                    "snr",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset construction approach leverages multiple high-quality speech corpora, specifically VoxCeleb<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite>, to ensure speaker diversity and linguistic coverage. We systematically selected and processed audio samples from these primary sources to create a comprehensive foundation for command generation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "chinese",
                    "samples",
                    "audio",
                    "free"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb1</span>: VoxCeleb1 is a large-scale speaker recognition dataset collected from YouTube videos. It contains over 100,000 utterances from 1,251 celebrities, spanning a wide range of accents, ages, and recording conditions. The dataset is widely used for research in speaker identification, verification, and speech analysis due to its diversity and real-world audio scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "voxceleb1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb2</span>: VoxCeleb2 is an extension of VoxCeleb1, providing even greater diversity and scale. It consists of over one million utterances from 6,112 speakers, with improved coverage of different nationalities, languages, and acoustic environments. VoxCeleb2 is designed to support robust speaker recognition and speech processing tasks, offering high-quality audio samples sourced from online multimedia content.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "voxceleb1",
                    "voxceleb2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, due to download limitations, our dataset includes 5,994 speakers from VoxCeleb2, and all subsequent analyses and experiments are based on these samples.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "dataset",
                    "voxceleb2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Free ST Chinese Mandarin Corpus</span>: This corpus features 855 speakers, each contributing 120 utterances recorded in controlled indoor environments using mobile devices. The recordings were captured in silence, ensuring consistent audio quality suitable for our synthesis pipeline.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "features",
                    "chinese",
                    "audio",
                    "free"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality voice command samples for our Media dataset, we implemented a rigorous selection process from both the VoxCeleb1, VoxCeleb2, and Free ST Chinese Mandarin Corpus datasets. For each speaker, we analyzed all available audio files across multiple quality metrics including signal-to-noise ratio (SNR), speech ratio, duration, and a composite quality score.\nTable 1 show the comparative statistics of audio features across the datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "snr",
                    "quality",
                    "across",
                    "features",
                    "voxceleb2",
                    "comparative",
                    "dataset",
                    "chinese",
                    "datasets",
                    "voxceleb1",
                    "score",
                    "samples",
                    "audio",
                    "statistics",
                    "free"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note: All values shown as mean <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i1.p1.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> standard deviation. SR (Sampling Rate) was constant at 16 kHz across all datasets.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "across",
                    "±pm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality metric combines duration and SNR following established audio assessment principles <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2009objective</span>]</cite>, with specific parameters optimized for voice commands. The quality score <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> for each audio sample was calculated as a weighted combination of its duration and signal-to-noise ratio (SNR):</p>\n\n",
                "matched_terms": [
                    "duration",
                    "snr",
                    "quality",
                    "score",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I4.i1.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the audio duration in seconds</p>\n\n",
                "matched_terms": [
                    "duration",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The constants 5.0 and 30 are normalization factors for duration and SNR respectively</p>\n\n",
                "matched_terms": [
                    "duration",
                    "snr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each speaker ID, we selected the audio sample with the highest quality score <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p5.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as the representative &#8221;best&#8221; sample. This selection process ensures that our dataset contains high-quality command lines that are suitable for training robust voice-controlled systems.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality",
                    "dataset",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reliability and usability of the synthetic voice command dataset for model training, we implemented a comprehensive quality validation pipeline. This systematic approach combined automated screening with manual verification to identify and eliminate substandard audio samples, thereby guaranteeing that the final dataset meets high standards of audio quality and semantic accuracy for effective voice recognition model development.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "quality",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our quality screening protocol employed a bilingual ASR model (iic/speech_paraformer) to automatically transcribe all generated audio samples, followed by computing similarity scores between the transcribed text and original command texts. Samples falling below the 60% similarity threshold were flagged for detailed manual inspection. The manual evaluation focused on four critical dimensions: intelligibility (clarity and unambiguousness of speech content), completeness (absence of missing or extra words), validity (proper audio characteristics without silence or truncation), and content accuracy (faithfulness to original command text).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality assessment revealed distinct error patterns across different dataset subsets. Manual review identified several common failure modes including silent audio files, missing words, inserted words, and garbled speech output. Analysis of error distribution showed that certain command phrases were particularly challenging for the synthesis system. In Chinese samples, the wake phrase &#8221;&#23567;&#24230;&#23567;&#24230;&#8221; exhibited higher error rates, while English generations demonstrated particular vulnerability in commands containing &#8221;play&#8221; and &#8221;mute&#8221; keywords. Through this rigorous validation pipeline, we successfully identified and removed problematic samples, significantly enhancing dataset quality and ensuring a reliable foundation for voice recognition model training.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "across",
                    "dataset",
                    "chinese",
                    "samples",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This comprehensive dataset totaling 111.33 hours and 35.76 GB represents one of the largest synthetic voice command datasets available for academic research, with speaker diversity spanning multiple accent groups, age ranges, and recording conditions suitable for robust model training and evaluation. The four-fold organization enables researchers to investigate cross-lingual speaker adaptation, speaker diversity effects, and acoustic robustness across varying recording environments.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "dataset",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Technical Consistency</span>: Audio technical specifications (<span class=\"ltx_text ltx_font_typewriter\">sample_rate</span>, <span class=\"ltx_text ltx_font_typewriter\">bit_depth</span>, <span class=\"ltx_text ltx_font_typewriter\">channels</span>, <span class=\"ltx_text ltx_font_typewriter\">data_type</span>) are standardized across all samples, ensuring experimental reproducibility.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-lingual Analysis</span>: The <span class=\"ltx_text ltx_font_typewriter\">language</span> field facilitates separate or comparative analysis of Mandarin Chinese and English performance.</p>\n\n",
                "matched_terms": [
                    "comparative",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected six widely adopted models known for their efficiency, architectural diversity, and proven performance in keyword spotting and audio classification tasks &#8212; carefully chosen to span a broad spectrum of model complexity, from ultra-lightweight to modern large-scale architectures. This selection enables a meaningful comparison across different design philosophies and computational constraints:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MicroCNN</span>: We design a family of ultra-lightweight CNNs, named MicroCNN, inspired by the minimal architectures used in TensorFlow Lite Micro <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">microspeech2020</span>]</cite>. Our model employs depthwise separable convolutions and batch normalization to further reduce parameters while maintaining expressiveness, achieving a parameter count as low as &#160;4K (in &#8221;micro&#8221; configuration).\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DS-CNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2017hello</span>]</cite>: A depthwise separable convolution-based model (&#160;30K params), originally designed for on-device wake-word detection. It exemplifies the efficiency of factorized convolutions and remains a popular choice in embedded speech systems due to its favorable accuracy-complexity trade-off.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">TC-ResNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2019temporal</span>]</cite>: A temporal convolutional ResNet variant (&#160;68K params) that introduces residual connections and dilated convolutions for modeling long-range temporal dependencies. It reflects the adaptation of image-based residual architectures to 1D audio signals.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">CRNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moya2018convolutional</span>]</cite>: A hybrid Convolutional Recurrent Neural Network (&#160;1M params) combining CNN feature extraction with bidirectional LSTM for sequence modeling. It represents the temporal modeling paradigm, capturing both spectral and sequential patterns &#8212; crucial for command recognition where context matters.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">MobileNet V1</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">howard2017mobilenets</span>]</cite>: A classic lightweight CNN (&#160;4.3M params) based on depthwise separable convolutions. As a mobile-optimized architecture, it serves as a reference point for efficient yet accurate vision-inspired models adapted to audio classification.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">EfficientNet-B0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tan2019efficientnet</span>]</cite>: The smallest variant of the EfficientNet family (&#160;4.7M params), representing modern SOTA scaling principles. Its compound scaling strategy balances depth, width, and resolution, making it a strong performer even at small scales &#8212; ideal for evaluating whether advanced image architectures can transfer effectively to audio tasks.\nAll models were trained and tested separately on the English and Chinese subsets of the CosyVoice dataset. Performance was measured using classification accuracy and cross-entropy loss, enabling a holistic assessment of both predictive power and calibration across languages and model sizes.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset",
                    "across",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive benchmark of six representative acoustic models on the SYNTTS-COMMANDS-MEDIA Dataset across both English (EN) and Chinese (ZH) subsets, with results summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S5.T7\" title=\"Table 7 &#8227; 5.2 Benchmark Results and Analysis &#8227; 5 Evaluation &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. All models are evaluated in terms of classification accuracy, cross-entropy loss, and parameter count, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that the SYNTTS-COMMANDS-MEDIA dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese, confirming the dataset&#8217;s quality and suitability for real-world deployment. Among all models, CRNN attains the best English accuracy (99.50%) and the lowest loss (0.0163), while MobileNet-V1 yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNN&#8217;s 99.50% accuracy). Interestingly, EfficientNet shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1 (97.92%), despite a higher loss&#8212;suggesting better calibration or robustness in its predictions.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark results on the SYNTTS-COMMANDS-MEDIA dataset validate a significant shift in speech data creation: synthetic speech generated by modern TTS systems has matured to the point where it can effectively replace&#8212;and in some aspects surpass&#8212;human-recorded data for training voice command classifiers. Unlike traditional data collection, which involves lengthy processes of speaker recruitment, audio recording, and manual annotation, our approach enables rapid, scalable generation of linguistically diverse and acoustically consistent training samples. This methodology not only drastically reduces cost and time, but also offers fine-grained control over speaker characteristics and acoustic conditions&#8212;effectively addressing a key bottleneck in TinyML development.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The high classification accuracy observed across English and Chinese commands (up to 99.5%) confirms that synthetic data does not compromise model performance. In fact, the acoustic consistency of TTS-generated speech may help models converge faster and generalize better in controlled command scenarios, particularly in the presence of channel or environmental variations. This is especially relevant for deployment on low-power chips such as the ADA100, Ceva NeuPro-Nano, and Ambiq Apollo510 Lite, where model robustness and inference efficiency are critical.</p>\n\n",
                "matched_terms": [
                    "across",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It should be noted that the current evaluation focuses exclusively on <em class=\"ltx_emph ltx_font_italic\">in-class command recognition</em>, where all test samples belong to a predefined command set. This design intentionally isolates the core research question&#8212;whether synthetic commands can reliably replace real speech during model training. For real-world use, however, a voice interface must also handle out-of-vocabulary inputs and environmental noise. While the current version of SYNTTS-COMMANDS-MEDIA does not include negative samples, developers can readily augment it with public audio datasets such as ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015dataset</span>]</cite> and UrbanSound8K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salamon2014dataset</span>]</cite> to improve rejection capabilities. Such a hybrid approach maintains the scalability of synthetic commands while incorporating real-world acoustic variability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech using state-of-the-art TTS technology. Through comprehensive experiments across a range of efficient acoustic models&#8212;from tiny MicroCNN to modern EfficientNet&#8212;we demonstrated that classifiers trained on our synthetic data achieve top-tier accuracy, exceeding 99% on English and approaching 98% on Chinese commands. These results strongly support the thesis that synthetic speech can serve as a viable and scalable alternative to human-recorded data, effectively overcoming the data scarcity that has long hindered TinyML applications in keyword spotting.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "across",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge the creators and contributors of the VoxCeleb datasets and the Free ST Chinese Mandarin Corpus for providing the high-quality source materials that made this work possible. We also recognize the CosyVoice development team for their groundbreaking advances in voice synthesis technology.</p>\n\n",
                "matched_terms": [
                    "free",
                    "datasets",
                    "chinese"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 4: Complete dataset composition, scale and distribution",
        "body": "Subset\nSpeakers\nCommands\nUtterances\nHours\nSize (GB)\n\n\n\n\nFree-ST-Chinese\n855\n25\n21,214\n6.82\n2.19\n\n\nFree-ST-English\n855\n23\n19,228\n4.88\n1.57\n\n\nVoxCeleb1&2-Chinese\n7,245\n25\n180,331\n58.03\n18.6\n\n\nVoxCeleb1&2-English\n7,245\n23\n163,848\n41.6\n13.4\n\n\nTotal\n8,100\n48 unique\n384,621\n111.33\n35.76",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Subset</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Speakers</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Commands</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Utterances</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Size (GB)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Free-ST-Chinese</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">855</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">25</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">21,214</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">6.82</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">2.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">Free-ST-English</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">855</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">23</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">19,228</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.88</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">VoxCeleb1&amp;2-Chinese</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">7,245</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">25</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">180,331</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.03</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">18.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1pt;padding-bottom:1pt;\">VoxCeleb1&amp;2-English</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">7,245</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">23</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">163,848</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.6</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">13.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">8,100</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">48 unique</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">384,621</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">111.33</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">35.76</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "voxceleb12english",
            "composition",
            "subset",
            "utterances",
            "scale",
            "size",
            "freestenglish",
            "freestchinese",
            "distribution",
            "commands",
            "dataset",
            "unique",
            "complete",
            "total",
            "hours",
            "speakers",
            "voxceleb12chinese"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "commands",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the rapid advancement of TinyML, particularly in the KWS domain, is severely hampered by a critical bottleneck: the scarcity of high-quality, diverse, and domain-specific training datasets. The field still heavily relies on early benchmarks like the Google Speech Commands dataset (2018)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">warden2018speechcommands</span>]</cite>, which, while pioneering, is limited in vocabulary size and linguistic coverage. To deploy KWS systems in richer contexts&#8212;such as multimedia control, smart home management, healthcare monitoring for the elderly, or in-car commands&#8212;we need datasets with more commands, more languages, and broader application coverage.</p>\n\n",
                "matched_terms": [
                    "commands",
                    "dataset",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "commands",
                    "dataset",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb1</span>: VoxCeleb1 is a large-scale speaker recognition dataset collected from YouTube videos. It contains over 100,000 utterances from 1,251 celebrities, spanning a wide range of accents, ages, and recording conditions. The dataset is widely used for research in speaker identification, verification, and speech analysis due to its diversity and real-world audio scenarios.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb2</span>: VoxCeleb2 is an extension of VoxCeleb1, providing even greater diversity and scale. It consists of over one million utterances from 6,112 speakers, with improved coverage of different nationalities, languages, and acoustic environments. VoxCeleb2 is designed to support robust speaker recognition and speech processing tasks, offering high-quality audio samples sourced from online multimedia content.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "utterances",
                    "scale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, due to download limitations, our dataset includes 5,994 speakers from VoxCeleb2, and all subsequent analyses and experiments are based on these samples.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Free ST Chinese Mandarin Corpus</span>: This corpus features 855 speakers, each contributing 120 utterances recorded in controlled indoor environments using mobile devices. The recordings were captured in silence, ensuring consistent audio quality suitable for our synthesis pipeline.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality assessment revealed distinct error patterns across different dataset subsets. Manual review identified several common failure modes including silent audio files, missing words, inserted words, and garbled speech output. Analysis of error distribution showed that certain command phrases were particularly challenging for the synthesis system. In Chinese samples, the wake phrase &#8221;&#23567;&#24230;&#23567;&#24230;&#8221; exhibited higher error rates, while English generations demonstrated particular vulnerability in commands containing &#8221;play&#8221; and &#8221;mute&#8221; keywords. Through this rigorous validation pipeline, we successfully identified and removed problematic samples, significantly enhancing dataset quality and ensuring a reliable foundation for voice recognition model training.</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "commands",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final dataset consisted of 384,621 utterances of 48 commands, broken into four subsets and distributed as follows:</p>\n\n",
                "matched_terms": [
                    "commands",
                    "dataset",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This comprehensive dataset totaling 111.33 hours and 35.76 GB represents one of the largest synthetic voice command datasets available for academic research, with speaker diversity spanning multiple accent groups, age ranges, and recording conditions suitable for robust model training and evaluation. The four-fold organization enables researchers to investigate cross-lingual speaker adaptation, speaker diversity effects, and acoustic robustness across varying recording environments.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n",
                "matched_terms": [
                    "utterances",
                    "distribution",
                    "commands",
                    "dataset",
                    "speakers",
                    "unique"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset is accompanied by comprehensive metadata that facilitates systematic experimentation and reproducibility. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Metadata Organization &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> details the complete schema and description of metadata fields:</p>\n\n",
                "matched_terms": [
                    "complete",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "commands",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech using state-of-the-art TTS technology. Through comprehensive experiments across a range of efficient acoustic models&#8212;from tiny MicroCNN to modern EfficientNet&#8212;we demonstrated that classifiers trained on our synthetic data achieve top-tier accuracy, exceeding 99% on English and approaching 98% on Chinese commands. These results strongly support the thesis that synthetic speech can serve as a viable and scalable alternative to human-recorded data, effectively overcoming the data scarcity that has long hindered TinyML applications in keyword spotting.</p>\n\n",
                "matched_terms": [
                    "commands",
                    "dataset"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 5: Data partitioning statistics by language",
        "body": "Partition\nChinese Utterances\nEnglish Utterances\nTotal Utterances\nUnique Speakers\n\n\n\n\nTraining Set\n141,075\n128,172\n269,247\n11,340\n\n\nValidation Set\n30,243\n27,435\n57,678\n2,430\n\n\nTest Set\n30,227\n27,469\n57,696\n2,430\n\n\nTotal\n201,545\n183,076\n384,621\n16,200",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Partition</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Chinese Utterances</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">English Utterances</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Total Utterances</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Unique Speakers</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Training Set</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">141,075</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">128,172</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">269,247</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">11,340</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Validation Set</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">30,243</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">27,435</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">57,678</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">2,430</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Test Set</th>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">30,227</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">27,469</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">57,696</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-top:1pt;padding-bottom:1pt;\">2,430</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Total</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">201,545</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">183,076</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">384,621</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">16,200</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "language",
            "validation",
            "training",
            "utterances",
            "test",
            "partition",
            "english",
            "partitioning",
            "chinese",
            "total",
            "set",
            "data",
            "speakers",
            "unique",
            "statistics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "data",
                    "english",
                    "training",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proliferation of voice-activated assistants, marked by ubiquitous wake words like &#8221;Okay Google,&#8221; &#8221;Hey Siri,&#8221; &#8221;&#23567;&#29233;&#21516;&#23398;,&#8221; and &#8221;&#23567;&#24230;&#23567;&#24230;,&#8221; has fundamentally reshaped human-computer interaction. Traditionally, these systems rely on a cloud-centric paradigm: a low-power chip on the device detects the wake word, triggering the transmission of subsequent voice commands to powerful cloud servers for complex speech recognition and natural language processing. While effective, this approach introduces significant limitations, including latency from data transmission, high energy consumption, and growing concerns over user privacy<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aloufi2023paralinguistic</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "training",
                    "utterances",
                    "english",
                    "chinese",
                    "data",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core contribution is the demonstration that synthetic data is not merely a substitute but a superior alternative for KWS model training. We validate SYNTTS-COMMANDS-MEDIA by training a spectrum of efficient acoustic models. The results are striking: top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese commands. This performance firmly establishes that TTS-generated datasets can effectively replace traditional, labor-intensive speech collections. By providing this scalable, high-quality data foundation and validating its efficacy, we aim to accelerate the development of more private, efficient, and intelligent on-device voice interfaces, thereby pushing the frontiers of TinyML forward.</p>\n\n",
                "matched_terms": [
                    "data",
                    "english",
                    "training",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate the advantages of CosyVoice 2, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T2\" title=\"Table 2 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes its performance compared to recent TTS models on the SEED test sets. CosyVoice 2 achieves competitive or superior results in character error rate (CER), word error rate (WER), and speaker similarity (SS) across Chinese and English test sets, highlighting its robustness and generalization ability for synthetic dataset construction.</p>\n\n",
                "matched_terms": [
                    "english",
                    "test",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of CosyVoice 2 effectively addresses several critical challenges in smart device interaction research. It enables the generation of thousands of command variations with consistent speaker characteristics, eliminating the fatigue and variability associated with human voice collection. CosyVoice 2 also allows for precise control over pitch (<math alttext=\"\\pm 20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 20\\%</annotation></semantics></math>), speed (<math alttext=\"\\pm 30\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>30</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 30\\%</annotation></semantics></math>), and background noise (SNR <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"30\\,\\mathrm{dB}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>30</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>dB</mi></mrow><annotation encoding=\"application/x-tex\">30\\,\\mathrm{dB}</annotation></semantics></math>), ensuring diverse and realistic data suitable for robust model training. Furthermore, by relying on synthetic speech, it resolves privacy concerns inherent in collecting and storing human voice data, making it an ethical and scalable solution for building large-scale voice command datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb2</span>: VoxCeleb2 is an extension of VoxCeleb1, providing even greater diversity and scale. It consists of over one million utterances from 6,112 speakers, with improved coverage of different nationalities, languages, and acoustic environments. VoxCeleb2 is designed to support robust speaker recognition and speech processing tasks, offering high-quality audio samples sourced from online multimedia content.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Free ST Chinese Mandarin Corpus</span>: This corpus features 855 speakers, each contributing 120 utterances recorded in controlled indoor environments using mobile devices. The recordings were captured in silence, ensuring consistent audio quality suitable for our synthesis pipeline.</p>\n\n",
                "matched_terms": [
                    "speakers",
                    "utterances",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The English media command set includes:</p>\n\n",
                "matched_terms": [
                    "english",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Chinese media command set provides equivalent functionality:</p>\n\n",
                "matched_terms": [
                    "set",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality voice command samples for our Media dataset, we implemented a rigorous selection process from both the VoxCeleb1, VoxCeleb2, and Free ST Chinese Mandarin Corpus datasets. For each speaker, we analyzed all available audio files across multiple quality metrics including signal-to-noise ratio (SNR), speech ratio, duration, and a composite quality score.\nTable 1 show the comparative statistics of audio features across the datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "statistics",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reliability and usability of the synthetic voice command dataset for model training, we implemented a comprehensive quality validation pipeline. This systematic approach combined automated screening with manual verification to identify and eliminate substandard audio samples, thereby guaranteeing that the final dataset meets high standards of audio quality and semantic accuracy for effective voice recognition model development.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality assessment revealed distinct error patterns across different dataset subsets. Manual review identified several common failure modes including silent audio files, missing words, inserted words, and garbled speech output. Analysis of error distribution showed that certain command phrases were particularly challenging for the synthesis system. In Chinese samples, the wake phrase &#8221;&#23567;&#24230;&#23567;&#24230;&#8221; exhibited higher error rates, while English generations demonstrated particular vulnerability in commands containing &#8221;play&#8221; and &#8221;mute&#8221; keywords. Through this rigorous validation pipeline, we successfully identified and removed problematic samples, significantly enhancing dataset quality and ensuring a reliable foundation for voice recognition model training.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "english",
                    "training",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-lingual Analysis</span>: The <span class=\"ltx_text ltx_font_typewriter\">language</span> field facilitates separate or comparative analysis of Mandarin Chinese and English performance.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MicroCNN</span>: We design a family of ultra-lightweight CNNs, named MicroCNN, inspired by the minimal architectures used in TensorFlow Lite Micro <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">microspeech2020</span>]</cite>. Our model employs depthwise separable convolutions and batch normalization to further reduce parameters while maintaining expressiveness, achieving a parameter count as low as &#160;4K (in &#8221;micro&#8221; configuration).\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DS-CNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2017hello</span>]</cite>: A depthwise separable convolution-based model (&#160;30K params), originally designed for on-device wake-word detection. It exemplifies the efficiency of factorized convolutions and remains a popular choice in embedded speech systems due to its favorable accuracy-complexity trade-off.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">TC-ResNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2019temporal</span>]</cite>: A temporal convolutional ResNet variant (&#160;68K params) that introduces residual connections and dilated convolutions for modeling long-range temporal dependencies. It reflects the adaptation of image-based residual architectures to 1D audio signals.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">CRNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moya2018convolutional</span>]</cite>: A hybrid Convolutional Recurrent Neural Network (&#160;1M params) combining CNN feature extraction with bidirectional LSTM for sequence modeling. It represents the temporal modeling paradigm, capturing both spectral and sequential patterns &#8212; crucial for command recognition where context matters.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">MobileNet V1</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">howard2017mobilenets</span>]</cite>: A classic lightweight CNN (&#160;4.3M params) based on depthwise separable convolutions. As a mobile-optimized architecture, it serves as a reference point for efficient yet accurate vision-inspired models adapted to audio classification.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">EfficientNet-B0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tan2019efficientnet</span>]</cite>: The smallest variant of the EfficientNet family (&#160;4.7M params), representing modern SOTA scaling principles. Its compound scaling strategy balances depth, width, and resolution, making it a strong performer even at small scales &#8212; ideal for evaluating whether advanced image architectures can transfer effectively to audio tasks.\nAll models were trained and tested separately on the English and Chinese subsets of the CosyVoice dataset. Performance was measured using classification accuracy and cross-entropy loss, enabling a holistic assessment of both predictive power and calibration across languages and model sizes.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive benchmark of six representative acoustic models on the SYNTTS-COMMANDS-MEDIA Dataset across both English (EN) and Chinese (ZH) subsets, with results summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S5.T7\" title=\"Table 7 &#8227; 5.2 Benchmark Results and Analysis &#8227; 5 Evaluation &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. All models are evaluated in terms of classification accuracy, cross-entropy loss, and parameter count, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that the SYNTTS-COMMANDS-MEDIA dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese, confirming the dataset&#8217;s quality and suitability for real-world deployment. Among all models, CRNN attains the best English accuracy (99.50%) and the lowest loss (0.0163), while MobileNet-V1 yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNN&#8217;s 99.50% accuracy). Interestingly, EfficientNet shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1 (97.92%), despite a higher loss&#8212;suggesting better calibration or robustness in its predictions.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, lightweight models exhibit a clear accuracy&#8211;complexity trade-off. MicroCNN, with only <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>4.2K parameters, achieves 93.22% accuracy on English but drops significantly on Chinese (80.14%), highlighting the increased difficulty of modeling tonal and phonetic richness in Mandarin with ultra-compact architectures. DS-CNN and TC-ResNet, with under 70K parameters, already recover strong performance (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>96.5% in both languages), underscoring their efficiency for resource-constrained applications.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark results on the SYNTTS-COMMANDS-MEDIA dataset validate a significant shift in speech data creation: synthetic speech generated by modern TTS systems has matured to the point where it can effectively replace&#8212;and in some aspects surpass&#8212;human-recorded data for training voice command classifiers. Unlike traditional data collection, which involves lengthy processes of speaker recruitment, audio recording, and manual annotation, our approach enables rapid, scalable generation of linguistically diverse and acoustically consistent training samples. This methodology not only drastically reduces cost and time, but also offers fine-grained control over speaker characteristics and acoustic conditions&#8212;effectively addressing a key bottleneck in TinyML development.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The high classification accuracy observed across English and Chinese commands (up to 99.5%) confirms that synthetic data does not compromise model performance. In fact, the acoustic consistency of TTS-generated speech may help models converge faster and generalize better in controlled command scenarios, particularly in the presence of channel or environmental variations. This is especially relevant for deployment on low-power chips such as the ADA100, Ceva NeuPro-Nano, and Ambiq Apollo510 Lite, where model robustness and inference efficiency are critical.</p>\n\n",
                "matched_terms": [
                    "data",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It should be noted that the current evaluation focuses exclusively on <em class=\"ltx_emph ltx_font_italic\">in-class command recognition</em>, where all test samples belong to a predefined command set. This design intentionally isolates the core research question&#8212;whether synthetic commands can reliably replace real speech during model training. For real-world use, however, a voice interface must also handle out-of-vocabulary inputs and environmental noise. While the current version of SYNTTS-COMMANDS-MEDIA does not include negative samples, developers can readily augment it with public audio datasets such as ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015dataset</span>]</cite> and UrbanSound8K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salamon2014dataset</span>]</cite> to improve rejection capabilities. Such a hybrid approach maintains the scalability of synthetic commands while incorporating real-world acoustic variability.</p>\n\n",
                "matched_terms": [
                    "set",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech using state-of-the-art TTS technology. Through comprehensive experiments across a range of efficient acoustic models&#8212;from tiny MicroCNN to modern EfficientNet&#8212;we demonstrated that classifiers trained on our synthetic data achieve top-tier accuracy, exceeding 99% on English and approaching 98% on Chinese commands. These results strongly support the thesis that synthetic speech can serve as a viable and scalable alternative to human-recorded data, effectively overcoming the data scarcity that has long hindered TinyML applications in keyword spotting.</p>\n\n",
                "matched_terms": [
                    "data",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) <span class=\"ltx_text ltx_font_bold\">Linguistic and cultural inclusivity</span>: Beyond English and Chinese, we aim to incorporate major global languages (e.g., Spanish, Hindi, Arabic, Japanese, German) as well as regional dialects and code-switched utterances. This will support the development of truly inclusive voice assistants that respect linguistic diversity and local user expectations.</p>\n\n",
                "matched_terms": [
                    "english",
                    "utterances",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We envision SYNTTS-COMMANDS not as a static artifact, but as a growing, community-supported resource that bridges the gap between data scarcity and model deployment in the TinyML era. By democratizing access to high-quality, multilingual training data, we hope to spur innovation in private, efficient, and intelligent voice interfaces&#8212;paving the way for a future where micro-intelligence is seamlessly embedded into the devices that enrich our daily lives.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 6: Metadata schema and description for the voice command dataset",
        "body": "Field\n\n\nData Type\n\n\n\n\nDescription\n\n\n\n\n\n\nsource_name\n\n\nString\n\n\n\n\nData source identifier (e.g., Free_ST_Chinese, VoxCeleb12_English)\n\n\n\n\nspeaker_id\n\n\nString\n\n\n\n\nUnique speaker identifier combining source prefix and speaker code (e.g., voxceleb12_id08303)\n\n\n\n\nutterance_id\n\n\nString\n\n\n\n\nUnique identifier for each utterance (filename without extension)\n\n\n\n\nfilename\n\n\nString\n\n\n\n\nComplete filename with .wav extension\n\n\n\n\npath\n\n\nString\n\n\n\n\nRelative file path within the dataset directory structure\n\n\n\n\nlabel\n\n\nString\n\n\n\n\nText transcription of the spoken command (e.g., ”播放”, ”Play”)\n\n\n\n\nlanguage\n\n\nString\n\n\n\n\nLanguage code (ZH for Mandarin Chinese, EN for English)\n\n\n\n\nfile_size_bytes\n\n\nInteger\n\n\n\n\nFile size in bytes, ranging from 40-200 KB per utterance\n\n\n\n\nduration_seconds\n\n\nFloat\n\n\n\n\nAudio duration in seconds, typically 0.8-2.0 seconds\n\n\n\n\nsample_rate\n\n\nInteger\n\n\n\n\nSampling frequency (24,000 Hz)\n\n\n\n\nbit_depth\n\n\nInteger\n\n\n\n\nAudio bit depth (32 bits)\n\n\n\n\nchannels\n\n\nInteger\n\n\n\n\nNumber of audio channels (1 for mono)\n\n\n\n\ndata_type\n\n\nString\n\n\n\n\nAudio sample data type (float32)\n\n\n\n\naudio_format\n\n\nString\n\n\n\n\nFile format (WAV)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Field</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_text ltx_font_bold\">Data Type</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\"><span class=\"ltx_text ltx_font_bold\">Description</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">source_name</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Data source identifier (e.g., <span class=\"ltx_text ltx_font_typewriter\">Free_ST_Chinese</span>, <span class=\"ltx_text ltx_font_typewriter\">VoxCeleb12_English</span>)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">speaker_id</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Unique speaker identifier combining source prefix and speaker code (e.g., <span class=\"ltx_text ltx_font_typewriter\">voxceleb12_id08303</span>)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">utterance_id</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Unique identifier for each utterance (filename without extension)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">filename</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Complete filename with .wav extension</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">path</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Relative file path within the dataset directory structure</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">label</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Text transcription of the spoken command (e.g., &#8221;&#25773;&#25918;&#8221;, &#8221;Play&#8221;)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">language</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Language code (ZH for Mandarin Chinese, EN for English)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">file_size_bytes</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Integer</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">File size in bytes, ranging from 40-200 KB per utterance</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">duration_seconds</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Float</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Audio duration in seconds, typically 0.8-2.0 seconds</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">sample_rate</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Integer</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Sampling frequency (24,000 Hz)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">bit_depth</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Integer</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Audio bit depth (32 bits)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">channels</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Integer</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Number of audio channels (1 for mono)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">data_type</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">Audio sample data type (float32)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">audio_format</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">String</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:199.2pt;\">File format (WAV)</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "combining",
            "spoken",
            "relative",
            "source",
            "text",
            "size",
            "float",
            "seconds",
            "filename",
            "ranging",
            "float32",
            "path",
            "format",
            "utteranceid",
            "unique",
            "string",
            "each",
            "bit",
            "field",
            "voice",
            "mono",
            "audioformat",
            "within",
            "label",
            "mandarin",
            "english",
            "bits",
            "sample",
            "complete",
            "from",
            "frequency",
            "speaker",
            "”play”",
            "filesizebytes",
            "transcription",
            "language",
            "prefix",
            "duration",
            "voxceleb12id08303",
            "identifier",
            "description",
            "schema",
            "freestchinese",
            "extension",
            "speakerid",
            "”播放”",
            "dataset",
            "without",
            "structure",
            "chinese",
            "integer",
            "datatype",
            "depth",
            "sourcename",
            "typically",
            "bytes",
            "code",
            "durationseconds",
            "voxceleb12english",
            "number",
            "utterance",
            "command",
            "sampling",
            "wav",
            "samplerate",
            "bitdepth",
            "channels",
            "directory",
            "data",
            "metadata",
            "file",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The dataset is accompanied by comprehensive metadata that facilitates systematic experimentation and reproducibility. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Metadata Organization &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> details the complete schema and description of metadata fields:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "source",
                    "command",
                    "english",
                    "dataset",
                    "chinese",
                    "from",
                    "data",
                    "speaker",
                    "code",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proliferation of voice-activated assistants, marked by ubiquitous wake words like &#8221;Okay Google,&#8221; &#8221;Hey Siri,&#8221; &#8221;&#23567;&#29233;&#21516;&#23398;,&#8221; and &#8221;&#23567;&#24230;&#23567;&#24230;,&#8221; has fundamentally reshaped human-computer interaction. Traditionally, these systems rely on a cloud-centric paradigm: a low-power chip on the device detects the wake word, triggering the transmission of subsequent voice commands to powerful cloud servers for complex speech recognition and natural language processing. While effective, this approach introduces significant limitations, including latency from data transmission, high energy consumption, and growing concerns over user privacy<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aloufi2023paralinguistic</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "from",
                    "data",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, the rapid advancement of TinyML, particularly in the KWS domain, is severely hampered by a critical bottleneck: the scarcity of high-quality, diverse, and domain-specific training datasets. The field still heavily relies on early benchmarks like the Google Speech Commands dataset (2018)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">warden2018speechcommands</span>]</cite>, which, while pioneering, is limited in vocabulary size and linguistic coverage. To deploy KWS systems in richer contexts&#8212;such as multimedia control, smart home management, healthcare monitoring for the elderly, or in-car commands&#8212;we need datasets with more commands, more languages, and broader application coverage.</p>\n\n",
                "matched_terms": [
                    "field",
                    "dataset",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "command",
                    "mandarin",
                    "english",
                    "dataset",
                    "chinese",
                    "from",
                    "data",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core contribution is the demonstration that synthetic data is not merely a substitute but a superior alternative for KWS model training. We validate SYNTTS-COMMANDS-MEDIA by training a spectrum of efficient acoustic models. The results are striking: top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese commands. This performance firmly establishes that TTS-generated datasets can effectively replace traditional, labor-intensive speech collections. By providing this scalable, high-quality data foundation and validating its efficacy, we aim to accelerate the development of more private, efficient, and intelligent on-device voice interfaces, thereby pushing the frontiers of TinyML forward.</p>\n\n",
                "matched_terms": [
                    "data",
                    "english",
                    "voice",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advancements in neural text-to-speech (TTS) synthesis have revolutionized artificial voice generation, achieving unprecedented naturalness that often surpasses human recordings in objective metrics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2018natural</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017tacotron</span>]</cite>. Modern systems leverage deep learning architectures like transformer-based language models and diffusion processes to produce speech with accurate prosody, emotional expression, and speaker characteristics<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2020fastspeech</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>]</cite>. These technologies have effectively overcome the &#8221;uncanny valley&#8221; effect, with state-of-the-art models consistently scoring above 4.0 in Mean Opinion Score (MOS) evaluations<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">reddy2022dnsmos</span>]</cite>. The field has particularly advanced in zero-shot voice cloning, where systems can mimic a speaker&#8217;s voice from just seconds of reference audio while maintaining content accuracy and natural rhythm<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "field",
                    "voice",
                    "seconds",
                    "from",
                    "speaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice2</span>]</cite> developed by Alibaba Group, represents a state-of-the-art advancement in neural text-to-speech (TTS) technology. It introduces hybrid generative modeling, combining LLM-based text-to-semantic decoding with flow-matching-based acoustic synthesis to achieve highly natural and consistent speaker output. The adoption of Finite Scalar Quantization (FSQ) replaces traditional vector quantization, resulting in 100% codebook utilization and enhanced speech detail preservation. Additionally, CosyVoice 2 supports both low-latency streaming and high-quality offline synthesis within a unified model, ensuring minimal quality loss. These architectural innovations enable CosyVoice 2 to generate synthetic speech that closely matches human recordings in naturalness, speaker similarity, and multilingual capability,making it an ideal foundation for building robust and diverse voice command datasets for smart device applications.</p>\n\n",
                "matched_terms": [
                    "combining",
                    "voice",
                    "command",
                    "within",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We cite two tables from the original CosyVoice 2 paper to provide a direct comparison of model performance and highlight the state-of-the-art results achieved by CosyVoice 2 and related TTS systems. These tables serve as a benchmark reference for subsequent analysis and dataset construction.</p>\n\n",
                "matched_terms": [
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate the advantages of CosyVoice 2, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T2\" title=\"Table 2 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes its performance compared to recent TTS models on the SEED test sets. CosyVoice 2 achieves competitive or superior results in character error rate (CER), word error rate (WER), and speaker similarity (SS) across Chinese and English test sets, highlighting its robustness and generalization ability for synthetic dataset construction.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "english",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of CosyVoice 2 effectively addresses several critical challenges in smart device interaction research. It enables the generation of thousands of command variations with consistent speaker characteristics, eliminating the fatigue and variability associated with human voice collection. CosyVoice 2 also allows for precise control over pitch (<math alttext=\"\\pm 20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>20</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 20\\%</annotation></semantics></math>), speed (<math alttext=\"\\pm 30\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mrow><mn>30</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\pm 30\\%</annotation></semantics></math>), and background noise (SNR <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math>&#8211;<math alttext=\"30\\,\\mathrm{dB}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mrow><mn>30</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>dB</mi></mrow><annotation encoding=\"application/x-tex\">30\\,\\mathrm{dB}</annotation></semantics></math>), ensuring diverse and realistic data suitable for robust model training. Furthermore, by relying on synthetic speech, it resolves privacy concerns inherent in collecting and storing human voice data, making it an ethical and scalable solution for building large-scale voice command datasets.</p>\n\n",
                "matched_terms": [
                    "data",
                    "speaker",
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset construction approach leverages multiple high-quality speech corpora, specifically VoxCeleb<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite>, to ensure speaker diversity and linguistic coverage. We systematically selected and processed audio samples from these primary sources to create a comprehensive foundation for command generation.</p>\n\n",
                "matched_terms": [
                    "command",
                    "mandarin",
                    "dataset",
                    "chinese",
                    "from",
                    "speaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb1</span>: VoxCeleb1 is a large-scale speaker recognition dataset collected from YouTube videos. It contains over 100,000 utterances from 1,251 celebrities, spanning a wide range of accents, ages, and recording conditions. The dataset is widely used for research in speaker identification, verification, and speech analysis due to its diversity and real-world audio scenarios.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "from",
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VoxCeleb2</span>: VoxCeleb2 is an extension of VoxCeleb1, providing even greater diversity and scale. It consists of over one million utterances from 6,112 speakers, with improved coverage of different nationalities, languages, and acoustic environments. VoxCeleb2 is designed to support robust speaker recognition and speech processing tasks, offering high-quality audio samples sourced from online multimedia content.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "from",
                    "extension",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, due to download limitations, our dataset includes 5,994 speakers from VoxCeleb2, and all subsequent analyses and experiments are based on these samples.</p>\n\n",
                "matched_terms": [
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Free ST Chinese Mandarin Corpus</span>: This corpus features 855 speakers, each contributing 120 utterances recorded in controlled indoor environments using mobile devices. The recordings were captured in silence, ensuring consistent audio quality suitable for our synthesis pipeline.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mandarin",
                    "audio",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our command set encompasses practical voice interactions across multiple device categories and languages. The design prioritizes common user interactions with smart devices:</p>\n\n",
                "matched_terms": [
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The English media command set includes:</p>\n\n",
                "matched_terms": [
                    "english",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Playback Control</span>: &#8221;Play&#8221;, &#8221;Pause&#8221;, &#8221;Resume&#8221;, &#8221;Play from start&#8221;, &#8221;Repeat song&#8221;</p>\n\n",
                "matched_terms": [
                    "from",
                    "”play”"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Chinese media command set provides equivalent functionality:</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality voice command samples for our Media dataset, we implemented a rigorous selection process from both the VoxCeleb1, VoxCeleb2, and Free ST Chinese Mandarin Corpus datasets. For each speaker, we analyzed all available audio files across multiple quality metrics including signal-to-noise ratio (SNR), speech ratio, duration, and a composite quality score.\nTable 1 show the comparative statistics of audio features across the datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "each",
                    "duration",
                    "voice",
                    "command",
                    "mandarin",
                    "dataset",
                    "chinese",
                    "from",
                    "speaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality metric combines duration and SNR following established audio assessment principles <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2009objective</span>]</cite>, with specific parameters optimized for voice commands. The quality score <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> for each audio sample was calculated as a weighted combination of its duration and signal-to-noise ratio (SNR):</p>\n\n",
                "matched_terms": [
                    "each",
                    "duration",
                    "voice",
                    "sample",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I4.i1.p1.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the audio duration in seconds</p>\n\n",
                "matched_terms": [
                    "duration",
                    "seconds",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each speaker ID, we selected the audio sample with the highest quality score <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p5.m1\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> as the representative &#8221;best&#8221; sample. This selection process ensures that our dataset contains high-quality command lines that are suitable for training robust voice-controlled systems.</p>\n\n",
                "matched_terms": [
                    "each",
                    "command",
                    "sample",
                    "dataset",
                    "speaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reliability and usability of the synthetic voice command dataset for model training, we implemented a comprehensive quality validation pipeline. This systematic approach combined automated screening with manual verification to identify and eliminate substandard audio samples, thereby guaranteeing that the final dataset meets high standards of audio quality and semantic accuracy for effective voice recognition model development.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset",
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our quality screening protocol employed a bilingual ASR model (iic/speech_paraformer) to automatically transcribe all generated audio samples, followed by computing similarity scores between the transcribed text and original command texts. Samples falling below the 60% similarity threshold were flagged for detailed manual inspection. The manual evaluation focused on four critical dimensions: intelligibility (clarity and unambiguousness of speech content), completeness (absence of missing or extra words), validity (proper audio characteristics without silence or truncation), and content accuracy (faithfulness to original command text).</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "without",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality assessment revealed distinct error patterns across different dataset subsets. Manual review identified several common failure modes including silent audio files, missing words, inserted words, and garbled speech output. Analysis of error distribution showed that certain command phrases were particularly challenging for the synthesis system. In Chinese samples, the wake phrase &#8221;&#23567;&#24230;&#23567;&#24230;&#8221; exhibited higher error rates, while English generations demonstrated particular vulnerability in commands containing &#8221;play&#8221; and &#8221;mute&#8221; keywords. Through this rigorous validation pipeline, we successfully identified and removed problematic samples, significantly enhancing dataset quality and ensuring a reliable foundation for voice recognition model training.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "command",
                    "english",
                    "dataset",
                    "chinese",
                    "”play”",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This comprehensive dataset totaling 111.33 hours and 35.76 GB represents one of the largest synthetic voice command datasets available for academic research, with speaker diversity spanning multiple accent groups, age ranges, and recording conditions suitable for robust model training and evaluation. The four-fold organization enables researchers to investigate cross-lingual speaker adaptation, speaker diversity effects, and acoustic robustness across varying recording environments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "dataset",
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n",
                "matched_terms": [
                    "each",
                    "language",
                    "utterance",
                    "voice",
                    "mandarin",
                    "english",
                    "dataset",
                    "chinese",
                    "from",
                    "data",
                    "speaker",
                    "unique"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Traceability</span>: The <span class=\"ltx_text ltx_font_typewriter\">source_name</span> and <span class=\"ltx_text ltx_font_typewriter\">speaker_id</span> fields enable tracking of each sample&#8217;s origin, supporting analyses of source-specific performance variations.</p>\n\n",
                "matched_terms": [
                    "each",
                    "speakerid",
                    "sourcename"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Technical Consistency</span>: Audio technical specifications (<span class=\"ltx_text ltx_font_typewriter\">sample_rate</span>, <span class=\"ltx_text ltx_font_typewriter\">bit_depth</span>, <span class=\"ltx_text ltx_font_typewriter\">channels</span>, <span class=\"ltx_text ltx_font_typewriter\">data_type</span>) are standardized across all samples, ensuring experimental reproducibility.</p>\n\n",
                "matched_terms": [
                    "channels",
                    "samplerate",
                    "datatype",
                    "bitdepth",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-lingual Analysis</span>: The <span class=\"ltx_text ltx_font_typewriter\">language</span> field facilitates separate or comparative analysis of Mandarin Chinese and English performance.</p>\n\n",
                "matched_terms": [
                    "language",
                    "field",
                    "mandarin",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Control</span>: <span class=\"ltx_text ltx_font_typewriter\">file_size_bytes</span> and <span class=\"ltx_text ltx_font_typewriter\">duration_seconds</span> enable identification of anomalous recordings and ensure data quality consistency.</p>\n\n",
                "matched_terms": [
                    "data",
                    "durationseconds",
                    "filesizebytes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Flexible Access</span>: The hierarchical <span class=\"ltx_text ltx_font_typewriter\">path</span> structure allows both programmatic access through metadata and direct file system navigation.</p>\n\n",
                "matched_terms": [
                    "metadata",
                    "file",
                    "path",
                    "structure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This comprehensive metadata structure supports diverse research objectives, including cross-lingual transfer learning, speaker adaptation studies, and robustness evaluation across different recording conditions. The standardized format ensures compatibility with major deep learning frameworks and speech processing toolkits.</p>\n\n",
                "matched_terms": [
                    "metadata",
                    "speaker",
                    "format",
                    "structure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "english",
                    "dataset",
                    "voice",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected six widely adopted models known for their efficiency, architectural diversity, and proven performance in keyword spotting and audio classification tasks &#8212; carefully chosen to span a broad spectrum of model complexity, from ultra-lightweight to modern large-scale architectures. This selection enables a meaningful comparison across different design philosophies and computational constraints:</p>\n\n",
                "matched_terms": [
                    "from",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MicroCNN</span>: We design a family of ultra-lightweight CNNs, named MicroCNN, inspired by the minimal architectures used in TensorFlow Lite Micro <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">microspeech2020</span>]</cite>. Our model employs depthwise separable convolutions and batch normalization to further reduce parameters while maintaining expressiveness, achieving a parameter count as low as &#160;4K (in &#8221;micro&#8221; configuration).\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DS-CNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2017hello</span>]</cite>: A depthwise separable convolution-based model (&#160;30K params), originally designed for on-device wake-word detection. It exemplifies the efficiency of factorized convolutions and remains a popular choice in embedded speech systems due to its favorable accuracy-complexity trade-off.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">TC-ResNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2019temporal</span>]</cite>: A temporal convolutional ResNet variant (&#160;68K params) that introduces residual connections and dilated convolutions for modeling long-range temporal dependencies. It reflects the adaptation of image-based residual architectures to 1D audio signals.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">CRNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moya2018convolutional</span>]</cite>: A hybrid Convolutional Recurrent Neural Network (&#160;1M params) combining CNN feature extraction with bidirectional LSTM for sequence modeling. It represents the temporal modeling paradigm, capturing both spectral and sequential patterns &#8212; crucial for command recognition where context matters.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">MobileNet V1</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">howard2017mobilenets</span>]</cite>: A classic lightweight CNN (&#160;4.3M params) based on depthwise separable convolutions. As a mobile-optimized architecture, it serves as a reference point for efficient yet accurate vision-inspired models adapted to audio classification.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">EfficientNet-B0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tan2019efficientnet</span>]</cite>: The smallest variant of the EfficientNet family (&#160;4.7M params), representing modern SOTA scaling principles. Its compound scaling strategy balances depth, width, and resolution, making it a strong performer even at small scales &#8212; ideal for evaluating whether advanced image architectures can transfer effectively to audio tasks.\nAll models were trained and tested separately on the English and Chinese subsets of the CosyVoice dataset. Performance was measured using classification accuracy and cross-entropy loss, enabling a holistic assessment of both predictive power and calibration across languages and model sizes.</p>\n\n",
                "matched_terms": [
                    "combining",
                    "command",
                    "depth",
                    "english",
                    "dataset",
                    "chinese",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a comprehensive benchmark of six representative acoustic models on the SYNTTS-COMMANDS-MEDIA Dataset across both English (EN) and Chinese (ZH) subsets, with results summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S5.T7\" title=\"Table 7 &#8227; 5.2 Benchmark Results and Analysis &#8227; 5 Evaluation &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. All models are evaluated in terms of classification accuracy, cross-entropy loss, and parameter count, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "command",
                    "english",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that the SYNTTS-COMMANDS-MEDIA dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese, confirming the dataset&#8217;s quality and suitability for real-world deployment. Among all models, CRNN attains the best English accuracy (99.50%) and the lowest loss (0.0163), while MobileNet-V1 yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNN&#8217;s 99.50% accuracy). Interestingly, EfficientNet shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1 (97.92%), despite a higher loss&#8212;suggesting better calibration or robustness in its predictions.</p>\n\n",
                "matched_terms": [
                    "command",
                    "english",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, lightweight models exhibit a clear accuracy&#8211;complexity trade-off. MicroCNN, with only <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>4.2K parameters, achieves 93.22% accuracy on English but drops significantly on Chinese (80.14%), highlighting the increased difficulty of modeling tonal and phonetic richness in Mandarin with ultra-compact architectures. DS-CNN and TC-ResNet, with under 70K parameters, already recover strong performance (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>96.5% in both languages), underscoring their efficiency for resource-constrained applications.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark results on the SYNTTS-COMMANDS-MEDIA dataset validate a significant shift in speech data creation: synthetic speech generated by modern TTS systems has matured to the point where it can effectively replace&#8212;and in some aspects surpass&#8212;human-recorded data for training voice command classifiers. Unlike traditional data collection, which involves lengthy processes of speaker recruitment, audio recording, and manual annotation, our approach enables rapid, scalable generation of linguistically diverse and acoustically consistent training samples. This methodology not only drastically reduces cost and time, but also offers fine-grained control over speaker characteristics and acoustic conditions&#8212;effectively addressing a key bottleneck in TinyML development.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "command",
                    "dataset",
                    "data",
                    "speaker",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The high classification accuracy observed across English and Chinese commands (up to 99.5%) confirms that synthetic data does not compromise model performance. In fact, the acoustic consistency of TTS-generated speech may help models converge faster and generalize better in controlled command scenarios, particularly in the presence of channel or environmental variations. This is especially relevant for deployment on low-power chips such as the ADA100, Ceva NeuPro-Nano, and Ambiq Apollo510 Lite, where model robustness and inference efficiency are critical.</p>\n\n",
                "matched_terms": [
                    "data",
                    "command",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It should be noted that the current evaluation focuses exclusively on <em class=\"ltx_emph ltx_font_italic\">in-class command recognition</em>, where all test samples belong to a predefined command set. This design intentionally isolates the core research question&#8212;whether synthetic commands can reliably replace real speech during model training. For real-world use, however, a voice interface must also handle out-of-vocabulary inputs and environmental noise. While the current version of SYNTTS-COMMANDS-MEDIA does not include negative samples, developers can readily augment it with public audio datasets such as ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015dataset</span>]</cite> and UrbanSound8K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salamon2014dataset</span>]</cite> to improve rejection capabilities. Such a hybrid approach maintains the scalability of synthetic commands while incorporating real-world acoustic variability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, the ability to generate multilingual command sets on demand will play a crucial role in globalizing voice-enabled products. Unlike static, monolingual datasets such as Google Speech Commands, SYNTTS-COMMANDS exemplifies a dynamic and extensible data paradigm that aligns with the evolving needs of edge AI and TinyML applications. By lowering the barrier to high-quality, multilingual data, we not only accelerate model development but also foster more inclusive and accessible voice technologies.</p>\n\n",
                "matched_terms": [
                    "data",
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech using state-of-the-art TTS technology. Through comprehensive experiments across a range of efficient acoustic models&#8212;from tiny MicroCNN to modern EfficientNet&#8212;we demonstrated that classifiers trained on our synthetic data achieve top-tier accuracy, exceeding 99% on English and approaching 98% on Chinese commands. These results strongly support the thesis that synthetic speech can serve as a viable and scalable alternative to human-recorded data, effectively overcoming the data scarcity that has long hindered TinyML applications in keyword spotting.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "command",
                    "english",
                    "dataset",
                    "chinese",
                    "from",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, this work underscores a timely convergence of advanced TTS synthesis and ultra-low-power speech recognition hardware. As specialized low-power chips continue to evolve, the demand for tailored, multi-keyword command sets will grow rapidly. SYNTTS-COMMANDS provides a scalable and adaptable data foundation that aligns perfectly with this hardware trend, enabling more complex, on-device voice interactions without relying on the cloud.</p>\n\n",
                "matched_terms": [
                    "data",
                    "without",
                    "voice",
                    "command"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(1) <span class=\"ltx_text ltx_font_bold\">Domain extension</span>: We will broaden the scope of SYNTTS-COMMANDS beyond basic media-control commands (e.g., &#8220;play&#8221;, &#8220;next track&#8221;) to encompass diverse real-world voice trigger scenarios. This includes <span class=\"ltx_text ltx_font_italic\">smart home</span> commands (e.g., &#8220;turn on the light&#8221;, &#8220;open the curtain&#8221;), <span class=\"ltx_text ltx_font_italic\">in-vehicle</span> voice activations (e.g., &#8220;unlock the car&#8221;, &#8220;start the engine&#8221;, &#8220;open the trunk&#8221;), and <span class=\"ltx_text ltx_font_italic\">urgent-assistance</span> wake phrases (e.g., &#8220;help me&#8221;, &#8220;call for help&#8221;), enabling robust keyword spotting across a wider range of practical and safety-critical applications.</p>\n\n",
                "matched_terms": [
                    "extension",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) <span class=\"ltx_text ltx_font_bold\">Linguistic and cultural inclusivity</span>: Beyond English and Chinese, we aim to incorporate major global languages (e.g., Spanish, Hindi, Arabic, Japanese, German) as well as regional dialects and code-switched utterances. This will support the development of truly inclusive voice assistants that respect linguistic diversity and local user expectations.</p>\n\n",
                "matched_terms": [
                    "english",
                    "voice",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We envision SYNTTS-COMMANDS not as a static artifact, but as a growing, community-supported resource that bridges the gap between data scarcity and model deployment in the TinyML era. By democratizing access to high-quality, multilingual training data, we hope to spur innovation in private, efficient, and intelligent voice interfaces&#8212;paving the way for a future where micro-intelligence is seamlessly embedded into the devices that enrich our daily lives.</p>\n\n",
                "matched_terms": [
                    "data",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge the creators and contributors of the VoxCeleb datasets and the Free ST Chinese Mandarin Corpus for providing the high-quality source materials that made this work possible. We also recognize the CosyVoice development team for their groundbreaking advances in voice synthesis technology.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "mandarin",
                    "voice",
                    "source"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.",
        "caption": "Table 7: Benchmark Results on the SYNTTS-COMMANDS-MEDIA Dataset (EN and ZH)",
        "body": "Model\nEnglish (EN)\nChinese (ZH)\n\n\nLoss\nAccuracy\nParams\nLoss\nAccuracy\nParams\n\n\n\n\nMicroCNN\n0.2304\n0.9322\n4,189\n0.5579\n0.8014\n4,255\n\n\nDS-CNN\n0.0166\n0.9946\n30,103\n0.0677\n0.9718\n30,361\n\n\nTC-ResNet\n0.0347\n0.9887\n68,431\n0.0884\n0.9656\n68,561\n\n\nCRNN\n0.0163\n0.9950\n1,083,031\n0.0636\n0.9742\n1,083,289\n\n\nMobileNet-V1\n0.0167\n0.9950\n2,651,783\n0.0552\n0.9792\n2,653,833\n\n\nEfficientNet\n0.0182\n0.9941\n4,717,248\n0.0701\n0.9793\n4,718,274",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">English (EN)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Chinese (ZH)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Loss</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Params</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Loss</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Accuracy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Params</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MicroCNN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2304</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.9322</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4,189</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5579</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8014</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4,255</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">DS-CNN</th>\n<td class=\"ltx_td ltx_align_center\">0.0166</td>\n<td class=\"ltx_td ltx_align_center\">0.9946</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">30,103</td>\n<td class=\"ltx_td ltx_align_center\">0.0677</td>\n<td class=\"ltx_td ltx_align_center\">0.9718</td>\n<td class=\"ltx_td ltx_align_center\">30,361</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TC-ResNet</th>\n<td class=\"ltx_td ltx_align_center\">0.0347</td>\n<td class=\"ltx_td ltx_align_center\">0.9887</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">68,431</td>\n<td class=\"ltx_td ltx_align_center\">0.0884</td>\n<td class=\"ltx_td ltx_align_center\">0.9656</td>\n<td class=\"ltx_td ltx_align_center\">68,561</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CRNN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0163</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.9950</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1,083,031</td>\n<td class=\"ltx_td ltx_align_center\">0.0636</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.9742</span></td>\n<td class=\"ltx_td ltx_align_center\">1,083,289</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MobileNet-V1</th>\n<td class=\"ltx_td ltx_align_center\">0.0167</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.9950</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2,651,783</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0552</span></td>\n<td class=\"ltx_td ltx_align_center\">0.9792</td>\n<td class=\"ltx_td ltx_align_center\">2,653,833</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">EfficientNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.0182</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.9941</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">4,717,248</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.0701</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.9793</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">4,718,274</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "crnn",
            "efficientnet",
            "model",
            "accuracy",
            "params",
            "mobilenetv1",
            "english",
            "microcnn",
            "dataset",
            "chinese",
            "loss",
            "dscnn",
            "synttscommandsmedia",
            "results",
            "benchmark",
            "tcresnet"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present a comprehensive benchmark of six representative acoustic models on the SYNTTS-COMMANDS-MEDIA Dataset across both English (EN) and Chinese (ZH) subsets, with results summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S5.T7\" title=\"Table 7 &#8227; 5.2 Benchmark Results and Analysis &#8227; 5 Evaluation &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. All models are evaluated in terms of classification accuracy, cross-entropy loss, and parameter count, providing insights into the trade-offs between performance and model complexity in multilingual voice command recognition.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices. The dataset and source code are publicly available at https://github.com/lugan113/SynTTS-Commands-Official.</p>\n\n",
                "matched_terms": [
                    "model",
                    "english",
                    "dataset",
                    "chinese",
                    "accuracy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The traditional approach of collecting such datasets through human recordings is prohibitively expensive, time-consuming, and lacks scalability. This creates a pressing need for an alternative methodology. In this work, we address this challenge by leveraging state-of-the-art neural Text-to-Speech (TTS) synthesis to generate high-fidelity, scalable training data. We introduce SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech. Utilizing the CosyVoice 2 TTS model and drawing on speaker embeddings from the VoxCeleb corpus (855 speakers)<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite> (7,245 utterances), we generated a comprehensive set of commands. After rigorous manual verification, the final dataset comprises 25 Chinese and 23 English command classes, tailored for common interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core contribution is the demonstration that synthetic data is not merely a substitute but a superior alternative for KWS model training. We validate SYNTTS-COMMANDS-MEDIA by training a spectrum of efficient acoustic models. The results are striking: top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese commands. This performance firmly establishes that TTS-generated datasets can effectively replace traditional, labor-intensive speech collections. By providing this scalable, high-quality data foundation and validating its efficacy, we aim to accelerate the development of more private, efficient, and intelligent on-device voice interfaces, thereby pushing the frontiers of TinyML forward.</p>\n\n",
                "matched_terms": [
                    "model",
                    "english",
                    "chinese",
                    "synttscommandsmedia",
                    "accuracy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice2</span>]</cite> developed by Alibaba Group, represents a state-of-the-art advancement in neural text-to-speech (TTS) technology. It introduces hybrid generative modeling, combining LLM-based text-to-semantic decoding with flow-matching-based acoustic synthesis to achieve highly natural and consistent speaker output. The adoption of Finite Scalar Quantization (FSQ) replaces traditional vector quantization, resulting in 100% codebook utilization and enhanced speech detail preservation. Additionally, CosyVoice 2 supports both low-latency streaming and high-quality offline synthesis within a unified model, ensuring minimal quality loss. These architectural innovations enable CosyVoice 2 to generate synthetic speech that closely matches human recordings in naturalness, speaker similarity, and multilingual capability,making it an ideal foundation for building robust and diverse voice command datasets for smart device applications.</p>\n\n",
                "matched_terms": [
                    "model",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We cite two tables from the original CosyVoice 2 paper to provide a direct comparison of model performance and highlight the state-of-the-art results achieved by CosyVoice 2 and related TTS systems. These tables serve as a benchmark reference for subsequent analysis and dataset construction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "benchmark",
                    "dataset",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T1\" title=\"Table 1 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the performance of various TTS models on the LibriSpeech test-clean subset, evaluated by word error rate (WER), naturalness mean opinion score (NMOS), and speaker similarity (SS). CosyVoice 2 and CosyVoice 2-S achieve the lowest WER and highest SS among all synthetic models, closely matching or surpassing human performance. Additionally, CosyVoice 2 attains the highest NMOS, indicating superior speech quality and naturalness. These results demonstrate that CosyVoice 2 series models offer state-of-the-art accuracy and speaker similarity for synthetic speech generation.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further demonstrate the advantages of CosyVoice 2, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S2.T2\" title=\"Table 2 &#8227; 2.2.2 Human-Parity Performance &#8227; 2.2 CosyVoice 2: State-of-the-Art Speech Synthesis &#8227; 2 AI-Based Synthetic Speech and CosyVoice 2 &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes its performance compared to recent TTS models on the SEED test sets. CosyVoice 2 achieves competitive or superior results in character error rate (CER), word error rate (WER), and speaker similarity (SS) across Chinese and English test sets, highlighting its robustness and generalization ability for synthetic dataset construction.</p>\n\n",
                "matched_terms": [
                    "english",
                    "results",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset construction approach leverages multiple high-quality speech corpora, specifically VoxCeleb<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nagrani2017voxceleb</span>]</cite> and the Free ST Chinese Mandarin Corpus<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020free</span>]</cite>, to ensure speaker diversity and linguistic coverage. We systematically selected and processed audio samples from these primary sources to create a comprehensive foundation for command generation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure high-quality voice command samples for our Media dataset, we implemented a rigorous selection process from both the VoxCeleb1, VoxCeleb2, and Free ST Chinese Mandarin Corpus datasets. For each speaker, we analyzed all available audio files across multiple quality metrics including signal-to-noise ratio (SNR), speech ratio, duration, and a composite quality score.\nTable 1 show the comparative statistics of audio features across the datasets used in our study.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reliability and usability of the synthetic voice command dataset for model training, we implemented a comprehensive quality validation pipeline. This systematic approach combined automated screening with manual verification to identify and eliminate substandard audio samples, thereby guaranteeing that the final dataset meets high standards of audio quality and semantic accuracy for effective voice recognition model development.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our quality screening protocol employed a bilingual ASR model (iic/speech_paraformer) to automatically transcribe all generated audio samples, followed by computing similarity scores between the transcribed text and original command texts. Samples falling below the 60% similarity threshold were flagged for detailed manual inspection. The manual evaluation focused on four critical dimensions: intelligibility (clarity and unambiguousness of speech content), completeness (absence of missing or extra words), validity (proper audio characteristics without silence or truncation), and content accuracy (faithfulness to original command text).</p>\n\n",
                "matched_terms": [
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality assessment revealed distinct error patterns across different dataset subsets. Manual review identified several common failure modes including silent audio files, missing words, inserted words, and garbled speech output. Analysis of error distribution showed that certain command phrases were particularly challenging for the synthesis system. In Chinese samples, the wake phrase &#8221;&#23567;&#24230;&#23567;&#24230;&#8221; exhibited higher error rates, while English generations demonstrated particular vulnerability in commands containing &#8221;play&#8221; and &#8221;mute&#8221; keywords. Through this rigorous validation pipeline, we successfully identified and removed problematic samples, significantly enhancing dataset quality and ensuring a reliable foundation for voice recognition model training.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This comprehensive dataset totaling 111.33 hours and 35.76 GB represents one of the largest synthetic voice command datasets available for academic research, with speaker diversity spanning multiple accent groups, age ranges, and recording conditions suitable for robust model training and evaluation. The four-fold organization enables researchers to investigate cross-lingual speaker adaptation, speaker diversity effects, and acoustic robustness across varying recording environments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure rigorous evaluation and prevent data leakage, we implemented a speaker-disjoint partitioning strategy with 70%-15%-15% splits for training, validation, and testing, respectively. This approach strictly guarantees that all utterances from each speaker are exclusively assigned to a single partition, completely eliminating speaker identity leakage across subsets while preserving the natural distribution of voice commands and acoustic conditions. The partitioning was performed at the speaker level rather than the utterance level to prevent model overfitting to specific speaker characteristics and ensure genuine generalization to unseen speakers. Furthermore, recognizing the distinct phonetic inventories and linguistic structures between Mandarin Chinese and English, we maintained completely separate test sets for each language. This design enables comprehensive assessment of cross-lingual generalization capabilities and facilitates detailed analysis of potential language-specific performance variations. The language-specific partitioning also allows for targeted investigation of accent variations and pronunciation challenges unique to each linguistic context. The statistical distribution of the resulting dataset partitions is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07821v2#S4.T5\" title=\"Table 5 &#8227; 4.1 Data Partitioning Strategy &#8227; 4 Dataset Properties &#8227; SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech Citation: Lu Gan, Xi Li. SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech.\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, demonstrating balanced representation across both languages while maintaining speaker independence.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "dataset",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-lingual Analysis</span>: The <span class=\"ltx_text ltx_font_typewriter\">language</span> field facilitates separate or comparative analysis of Mandarin Chinese and English performance.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness and utility of the proposed SYNTTS-COMMANDS-MEDIA Dataset, we conducted a comprehensive benchmark evaluation using a suite of state-of-the-art acoustic models. The primary objectives of this evaluation are twofold: (1) to empirically demonstrate that the dataset supports high-accuracy classification of voice commands in both English and Chinese, thereby confirming its quality and effectiveness; and (2) to establish a foundational performance baseline for future research in voice-command recognition for resource-constrained smart devices.</p>\n\n",
                "matched_terms": [
                    "english",
                    "dataset",
                    "chinese",
                    "synttscommandsmedia",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MicroCNN</span>: We design a family of ultra-lightweight CNNs, named MicroCNN, inspired by the minimal architectures used in TensorFlow Lite Micro <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">microspeech2020</span>]</cite>. Our model employs depthwise separable convolutions and batch normalization to further reduce parameters while maintaining expressiveness, achieving a parameter count as low as &#160;4K (in &#8221;micro&#8221; configuration).\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DS-CNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2017hello</span>]</cite>: A depthwise separable convolution-based model (&#160;30K params), originally designed for on-device wake-word detection. It exemplifies the efficiency of factorized convolutions and remains a popular choice in embedded speech systems due to its favorable accuracy-complexity trade-off.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">TC-ResNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2019temporal</span>]</cite>: A temporal convolutional ResNet variant (&#160;68K params) that introduces residual connections and dilated convolutions for modeling long-range temporal dependencies. It reflects the adaptation of image-based residual architectures to 1D audio signals.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">CRNN</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moya2018convolutional</span>]</cite>: A hybrid Convolutional Recurrent Neural Network (&#160;1M params) combining CNN feature extraction with bidirectional LSTM for sequence modeling. It represents the temporal modeling paradigm, capturing both spectral and sequential patterns &#8212; crucial for command recognition where context matters.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">MobileNet V1</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">howard2017mobilenets</span>]</cite>: A classic lightweight CNN (&#160;4.3M params) based on depthwise separable convolutions. As a mobile-optimized architecture, it serves as a reference point for efficient yet accurate vision-inspired models adapted to audio classification.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">EfficientNet-B0</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tan2019efficientnet</span>]</cite>: The smallest variant of the EfficientNet family (&#160;4.7M params), representing modern SOTA scaling principles. Its compound scaling strategy balances depth, width, and resolution, making it a strong performer even at small scales &#8212; ideal for evaluating whether advanced image architectures can transfer effectively to audio tasks.\nAll models were trained and tested separately on the English and Chinese subsets of the CosyVoice dataset. Performance was measured using classification accuracy and cross-entropy loss, enabling a holistic assessment of both predictive power and calibration across languages and model sizes.</p>\n\n",
                "matched_terms": [
                    "crnn",
                    "tcresnet",
                    "model",
                    "params",
                    "english",
                    "microcnn",
                    "dataset",
                    "chinese",
                    "loss",
                    "dscnn",
                    "accuracy",
                    "efficientnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results demonstrate that the SYNTTS-COMMANDS-MEDIA dataset supports high-accuracy command recognition in both languages. Notably, the top-performing models achieve over 99.4% accuracy on English and nearly 98% on Chinese, confirming the dataset&#8217;s quality and suitability for real-world deployment. Among all models, CRNN attains the best English accuracy (99.50%) and the lowest loss (0.0163), while MobileNet-V1 yields the lowest loss on Chinese (0.0552) and competitive English performance (matching CRNN&#8217;s 99.50% accuracy). Interestingly, EfficientNet shows slightly higher Chinese accuracy (97.93%) than MobileNet-V1 (97.92%), despite a higher loss&#8212;suggesting better calibration or robustness in its predictions.</p>\n\n",
                "matched_terms": [
                    "crnn",
                    "mobilenetv1",
                    "english",
                    "dataset",
                    "chinese",
                    "loss",
                    "synttscommandsmedia",
                    "accuracy",
                    "results",
                    "efficientnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, lightweight models exhibit a clear accuracy&#8211;complexity trade-off. MicroCNN, with only <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>4.2K parameters, achieves 93.22% accuracy on English but drops significantly on Chinese (80.14%), highlighting the increased difficulty of modeling tonal and phonetic richness in Mandarin with ultra-compact architectures. DS-CNN and TC-ResNet, with under 70K parameters, already recover strong performance (<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>96.5% in both languages), underscoring their efficiency for resource-constrained applications.</p>\n\n",
                "matched_terms": [
                    "english",
                    "microcnn",
                    "chinese",
                    "dscnn",
                    "accuracy",
                    "tcresnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the benchmark establishes strong baselines across a wide spectrum of model scales&#8212;from ultra-light MicroCNN to modern EfficientNet&#8212;while revealing that moderate-complexity models (e.g., DS-CNN, TC-ResNet) already deliver near-SOTA performance with minimal resource overhead. This makes them particularly suitable for deployment on edge devices in multilingual smart environments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "microcnn",
                    "dscnn",
                    "benchmark",
                    "tcresnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our benchmark results on the SYNTTS-COMMANDS-MEDIA dataset validate a significant shift in speech data creation: synthetic speech generated by modern TTS systems has matured to the point where it can effectively replace&#8212;and in some aspects surpass&#8212;human-recorded data for training voice command classifiers. Unlike traditional data collection, which involves lengthy processes of speaker recruitment, audio recording, and manual annotation, our approach enables rapid, scalable generation of linguistically diverse and acoustically consistent training samples. This methodology not only drastically reduces cost and time, but also offers fine-grained control over speaker characteristics and acoustic conditions&#8212;effectively addressing a key bottleneck in TinyML development.</p>\n\n",
                "matched_terms": [
                    "synttscommandsmedia",
                    "results",
                    "benchmark",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The high classification accuracy observed across English and Chinese commands (up to 99.5%) confirms that synthetic data does not compromise model performance. In fact, the acoustic consistency of TTS-generated speech may help models converge faster and generalize better in controlled command scenarios, particularly in the presence of channel or environmental variations. This is especially relevant for deployment on low-power chips such as the ADA100, Ceva NeuPro-Nano, and Ambiq Apollo510 Lite, where model robustness and inference efficiency are critical.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "chinese",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It should be noted that the current evaluation focuses exclusively on <em class=\"ltx_emph ltx_font_italic\">in-class command recognition</em>, where all test samples belong to a predefined command set. This design intentionally isolates the core research question&#8212;whether synthetic commands can reliably replace real speech during model training. For real-world use, however, a voice interface must also handle out-of-vocabulary inputs and environmental noise. While the current version of SYNTTS-COMMANDS-MEDIA does not include negative samples, developers can readily augment it with public audio datasets such as ESC-50&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">piczak2015dataset</span>]</cite> and UrbanSound8K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salamon2014dataset</span>]</cite> to improve rejection capabilities. Such a hybrid approach maintains the scalability of synthetic commands while incorporating real-world acoustic variability.</p>\n\n",
                "matched_terms": [
                    "synttscommandsmedia",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented SYNTTS-COMMANDS, a multilingual voice command dataset built entirely from synthetic speech using state-of-the-art TTS technology. Through comprehensive experiments across a range of efficient acoustic models&#8212;from tiny MicroCNN to modern EfficientNet&#8212;we demonstrated that classifiers trained on our synthetic data achieve top-tier accuracy, exceeding 99% on English and approaching 98% on Chinese commands. These results strongly support the thesis that synthetic speech can serve as a viable and scalable alternative to human-recorded data, effectively overcoming the data scarcity that has long hindered TinyML applications in keyword spotting.</p>\n\n",
                "matched_terms": [
                    "english",
                    "microcnn",
                    "dataset",
                    "chinese",
                    "accuracy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">(2) <span class=\"ltx_text ltx_font_bold\">Linguistic and cultural inclusivity</span>: Beyond English and Chinese, we aim to incorporate major global languages (e.g., Spanish, Hindi, Arabic, Japanese, German) as well as regional dialects and code-switched utterances. This will support the development of truly inclusive voice assistants that respect linguistic diversity and local user expectations.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            }
        ]
    }
}