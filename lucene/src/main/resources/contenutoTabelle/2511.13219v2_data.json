{
    "S3.T1": {
        "source_file": "FoleyBench: A Benchmark For Video-to-Audio Models",
        "caption": "Table 1: Evaluation of V2A models on VGGSound test set and FoleyBench.\nWe report results across six metrics grouped into cross-modal alignment (ImageBind (IB), CLAP, De-Sync (DS)) and audio quality (FAD, IS, KLD). IB and CLAP scores are between 0 and 1, and De-Sync is measured in seconds. CLAP is only measured for models that condition on the text prompt (marked with superscript T). * Denotes evaluation on a filtered subset of the VGGSound test set. The best results for each metric are shown in bold and second-best are underlined.",
        "body": "Method\nVGGSound test set\nFoleyBench\n\n\nCross-modal alignment\nAudio quality\nCross-modal alignment\nAudio quality\n\n\n\nIB ↑\\uparrow\n\n\nCLAP ↑\\uparrow\n\n\nDS ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nIS ↑\\uparrow\n\n\nKLD ↓\\downarrow\n\n\nIB ↑\\uparrow\n\n\nCLAP ↑\\uparrow\n\n\nDS ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nIS ↑\\uparrow\n\n\nKLD ↓\\downarrow\n\n\n\n\n\n\nV-AURA [viertola2025temporally]\n\n0.276\n–\n0.654\n14.80\n10.08\n2.42\n0.237\n–\n0.716\n27.2\n6.44\n3.46\n\n\n\nDiffFoley [luo2023diff]\n\n–\n–\n–\n–\n–\n–\n0.173\n–\n0.88\n31.9\n9.26\n3.88\n\n\n\nSeeing&Hearing [xing2024seeing]\n\n0.339\n–\n1.204\n24.58\n8.58\n2.26\n0.371\n–\n1.08\n25.0\n4.80\n3.30\n\n\n\nMaskVAT [pascual2024masked]\n\n–\n–\n–\n–\n–\n–\n0.239\n–\n0.748\n19.7\n6.94\n3.22\n\n\n\nV2A-Mapper [wang2024v2a]\n\n0.226\n–\n1.225\n8.40\n12.47\n2.69\n0.189\n–\n1.09\n16.2\n8.87\n3.50\n\n\n\nSpecMaskFoleyT [zhong2025specmaskfoley]\n\n0.264\n–\n0.652\n7.425\n11.43\n1.98\n0.229\n0.191\n0.801\n19.2\n5.86\n3.08\n\n\n\nVTA-LDMT [xu2024video]\n\n–\n0.452\n–\n2.05\n10.10\n3.81\n0.221\n0.138\n1.21\n15.7\n7.27\n3.13\n\n\n\nFoleyCrafterT [zhang2024foleycrafter]\n\n0.257\n–\n1.225\n16.24\n15.68\n2.30\n0.255\n0.261\n1.15\n16.5\n9.50\n2.68\n\n\n\nLOVAT [cheng2025lova]\n\n–\n–\n–\n–\n9.73\n2.06\n0.209\n0.167\n1.15\n20.7\n7.61\n3.15\n\n\n\nCAFAT [benita2025cafa]\n\n0.210\n0.230\n0.960\n12.60\n13.45\n2.02\n0.198\n0.270\n0.825\n15.5\n7.41\n2.54\n\n\n\nMultiFoleyT [chen2025video]\n\n\n0.280*\n\n0.344*\n–\n–\n–\n1.43*\n0.144\n0.218\n0.846\n22.3\n5.38\n2.75\n\n\n\nMMAudioT [cheng2025mmaudio]\n\n0.332\n–\n0.442\n4.72\n17.40\n1.65\n0.306\n0.331\n0.447\n8.76\n11.2\n2.43",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"6\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">VGGSound test set</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">FoleyBench</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Cross-modal alignment</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Audio quality</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Cross-modal alignment</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Audio quality</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">IB&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CLAP&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">DS&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">FAD&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">IS&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">KLD&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">IB&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CLAP&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">DS&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">FAD&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">IS&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">KLD&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">V-AURA&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.276</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.654</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">14.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.237</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.716</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">27.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.46</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">DiffFoley&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2023diff</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.173</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">31.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.26</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.88</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Seeing&amp;Hearing&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.339</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.204</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">24.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.26</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.371</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">25.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.30</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MaskVAT&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pascual2024masked</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.239</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.748</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.22</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">V2A-Mapper&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024v2a</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.226</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.225</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.189</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.50</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SpecMaskFoley</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhong2025specmaskfoley</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.264</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.652</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.425</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.229</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.191</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.801</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">19.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.86</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.08</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VTA-LDM</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.452</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.221</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.138</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">15.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.27</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">FoleyCrafter</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024foleycrafter</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.257</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.225</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.24</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">15.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.255</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.261</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">9.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">LOVA</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.06</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.209</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.167</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">20.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CAFA</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.210</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.230</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.960</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.02</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.198</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.270</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.825</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">15.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">2.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MultiFoley</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025video</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">0.280</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">*</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.344<sup class=\"ltx_sup\">*</sup></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.43<sup class=\"ltx_sup\">*</sup></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.144</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.218</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.846</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">22.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.38</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.75</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MMAudio</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:80%;\">T</span></sup><span class=\"ltx_text\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.332</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.442</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">4.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">17.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">1.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">0.306</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.331</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.447</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">8.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">11.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.43</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "superscript",
            "lovat",
            "imagebind",
            "prompt",
            "secondbest",
            "cafat",
            "underlined",
            "into",
            "maskvat",
            "metric",
            "vggsound",
            "↑uparrow",
            "vaura",
            "zhong2025specmaskfoley",
            "vtaldmt",
            "crossmodal",
            "condition",
            "text",
            "denotes",
            "evaluation",
            "foleybench",
            "only",
            "best",
            "fad",
            "viertola2025temporally",
            "cheng2025lova",
            "results",
            "v2a",
            "measured",
            "seconds",
            "luo2023diff",
            "pascual2024masked",
            "marked",
            "each",
            "audio",
            "bold",
            "scores",
            "metrics",
            "specmaskfoleyt",
            "mmaudiot",
            "method",
            "wang2024v2a",
            "↓downarrow",
            "xing2024seeing",
            "desync",
            "benita2025cafa",
            "difffoley",
            "grouped",
            "foleycraftert",
            "alignment",
            "kld",
            "chen2025video",
            "test",
            "clap",
            "across",
            "filtered",
            "zhang2024foleycrafter",
            "report",
            "models",
            "set",
            "xu2024video",
            "six",
            "quality",
            "multifoleyt",
            "subset",
            "cheng2025mmaudio",
            "v2amapper",
            "between",
            "seeinghearing"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate generated audio along two key dimensions: audio quality and cross-modal alignment. For audio quality, we report three widely used metrics: Fr&#233;chet Audio Distance (FAD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fad2019</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Inception Score (IS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salimans2016inception</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Kullback&#8211;Leibler Divergence (KLD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kullback1951KL</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To compute all of these metrics, we rely on PANN embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2020panns</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which provide pretrained audio representations especially effective for non-speech, non-music audio. For alignment, we focus on three complementary metrics. CLAP Score&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laionclap2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates how well generated audio semantically matches the ground-truth textual captions. As this metric specifically measures the generated audio&#8217;s adherence to the text prompt, we only report it for models that are text-conditioned (marked with </span>\n  <sup class=\"ltx_sup\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">T</span>\n  </sup>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). ImageBind Score&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">girdhar2023imagebind</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures cross-modal similarity between audio and video. Finally, De-Sync&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">syncnet2016</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quantifies the temporal offset between audio and video streams in seconds, with lower scores corresponding to tighter synchronization. We use the AV-benchmark toolkit for evaluation </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hkchengrex/av-benchmark\" title=\"\">https://github.com/hkchengrex/av-benchmark</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present the benchmark results for several SotA V2A models on FoleyBench in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, providing a comprehensive set of reference evaluations. These scores are shown alongside those from the VGGSound Test set, which are often sparsely and inconsistently reported in prior work&#8212;a key gap our benchmark aims to resolve.\nFor FoleyBench, we find that MMAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves the best performance across nearly all metrics (second-best only on IB). Seeing and Hearing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves the best ImageBind score, which is likely because it performs gradient ascent directly on the ImageBind score at test-time, as also noted by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Interestingly, CAFA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a ControlNet-based approach, ranks second across multiple metrics. This highlights the effectiveness of adapting a strong pretrained backbone (Stable-Audio-Open </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2025stable</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which provides a full-band (44.1kHz) output, offering an advantage in audio quality metrics over models with lower-bandwidth backbones (e.g., SpecMaskGIT at 22kHz). V-AURA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> ranks second on temporal synchronization (DS), consistent with its design emphasis on fine-grained temporal alignment via high-frame-rate visual feature extraction.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-Conditioning.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The results in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> suggest that text-conditioning acts as a powerful prior, as best performing models are text conditioned. We also performed an ablation on SpecMaskFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhong2025specmaskfoley</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, evaluating it with and without its text prompt. The text-conditioned version was universally superior across every metric. On </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Action</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clips, for instance, it improves audio quality metrics (FAD: 23.18 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 19.60; IS: 4.82 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 6.02) and alignment metrics (IB: 0.188 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.222; De-Sync: 0.911s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.755s). This confirms that text prompts provide crucial guidance, helping the model select a more accurate sound and align it more precisely in time.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of <em class=\"ltx_emph ltx_font_italic\">Foley</em>&#8212;sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music&#8212;domains that lie outside the use case for Foley.\nTo address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark state-of-the-art (SotA) V2A models, evaluating them on audio quality, audio&#8211;video alignment, temporal synchronization, and audio&#8211;text consistency. Samples are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://gclef-cmu.org/foleybench\" title=\"\">https://gclef-cmu.org/foleybench</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "foleybench",
                    "evaluation",
                    "models",
                    "quality",
                    "v2a",
                    "alignment",
                    "each",
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nVideo-to-audio models, Audio-visual alignment, Foley sound synthesis</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The task of video-to-audio generation (V2A) seeks to synthesize realistic, temporally aligned audio directly from visual input, sometimes conditioned on an additional text caption for finer control. A key downstream use case in the creative industry (e.g., film, games) is Foley&#8212;the creation of sound effects such as object impacts, or ambient noises that must be both semantically appropriate and temporally synchronized with visible events. Foley is distinct from speech and music, which are typically added in other stages of production. Foley specifically refers to non-speech, non-music sound effects (like footsteps or object impacts) that are causally linked to visual actions. Recent work has rapidly advanced V2A generation to support Foley scenarios, with a diverse family of methods being developed, including autoregressive, diffusion-based, flow-matching-based, and ControlNet-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Mei2023-FoleyGen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2023diff</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025video</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "xu2024video",
                    "models",
                    "viertola2025temporally",
                    "benita2025cafa",
                    "v2a",
                    "cheng2025mmaudio",
                    "luo2023diff",
                    "audio",
                    "chen2025video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite this progress, evaluation of V2A systems remains misaligned with downstream Foley application goals. The widely used VGGSound test set&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vggsound2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, for instance, suffers from weak audio&#8211;visual correspondence, with many clips dominated by speech, music, or off-screen sounds. In our analysis, we find that 74% of these clips show poor alignment between the audio and the video, as detailed in Section &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Comparison with VGGSound-Test &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese cases lie outside the intended scope of V2A modeling, and their prevalence in existing benchmarks obscures evaluation.\nOther relevant datasets exist but are small-scale, limited in diversity, or lack comprehensive metadata.\nThree existing datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2020generating</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iashin2022sparse</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owens2016visually</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have stronger guarantees on audio-visual correspondence but only cover a dozen or fewer specific sound categories.\nThe recently proposed VisualSound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset filters VGGSound using an audio-visual similarity metric,\nbut lacks comprehensive sound category metadata, and its coverage of Foley categories is unknown.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "only",
                    "set",
                    "viertola2025temporally",
                    "v2a",
                    "alignment",
                    "between",
                    "metric",
                    "vggsound",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, current benchmarks lack systematic coverage across sound classes (such as AudioSet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audioset2017</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or the Universal Category System (UCS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">UCS</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;a sound categorization developed specifically for professional sound design use cases).\nThey lack categories for fine-grained evaluation, such as sound types (discrete events vs. continuous ambiences) and scene complexity (single-source vs. multi-source), limiting our understanding of where models succeed or fail.\nOur experiments on the VGGSound test set reveal its coverage across Foley sound categories is limited&#8212;24.3% of UCS categories have three or fewer videos in VGGSound.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "models",
                    "set",
                    "vggsound",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce FoleyBench, the first large-scale benchmark explicitly designed for evaluating Foley-style V2A: non-speech, non-music clips with visible sound sources and strong temporal correspondence between audio and video. FoleyBench comprises 5,000 curated video&#8211;audio&#8211;caption triplets, each approximately 8 to 10 seconds long, constructed through a scalable multi-stage pipeline. Each clip is automatically labeled by Gemini 2.5 Pro </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleDeepMind2025Gemini2_5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with AudioSet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audioset2017</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UCS tags, and enriched with metadata capturing source complexity (such as single vs. multi-source) and sound envelope type (such as discrete vs. continuous). We also create FoleyBench-Long&#8212;a set of 650 long videos (each 30 seconds long) to evaluate the audio generation ability of models for long-form videos. All these categories enable fine-grained analysis of model behavior across conditions.\nWe find that while MMAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves the best overall scores, other models excel in specific areas: Seeing &amp; Hearing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> obtains the best semantic audio-video alignment, V-AURA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> excels at precise temporal synchronization, and LOVA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> better preserves audio quality on long-form content. Our contributions are as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "foleybench",
                    "best",
                    "cheng2025lova",
                    "xing2024seeing",
                    "viertola2025temporally",
                    "models",
                    "set",
                    "quality",
                    "v2a",
                    "cheng2025mmaudio",
                    "seconds",
                    "alignment",
                    "each",
                    "between",
                    "audio",
                    "vaura",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">FoleyBench, a 5,000-instance, high-quality benchmark for visually grounded, non-speech, non-music audio generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Comprehensive evaluation of V2A models on audio quality, audio&#8211;visual alignment, temporal synchronization, and audio&#8211;text consistency.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models",
                    "quality",
                    "v2a",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Analyzing failure modes in existing V2A models, yielding actionable insights for future progress.</span>\n</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">FoleyBench is a large-scale benchmark designed to evaluate V2A systems in scenarios comprising video clips with precise temporal and semantic grounding. The dataset focuses on </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Foley-style clips</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which we define here as: non-speech, non-music video segments in which the sound sources are both visible on screen and temporally synchronized with their corresponding actions.\nIt consists of 5,000 curated video&#8211;audio pairs, each accompanied by a caption describing the visual scene. Importantly, each clip is accompanied by metadata that enables fine-grained analysis of model behavior. Metadata includes: AudioSet and UCS labels and attributes such as sound type (discrete events vs. continuous ambience) and source complexity (single-source vs. multi-source).</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "v2a",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Scene Detection:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Each video is segmented into scenes using automatic scene boundary detection. Scenes shorter than 8s are discarded to avoid overly short contexts.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio filtering:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We first apply YAMNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yamnet2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for frame-level classification. We use YAMNet as a fast and cost-effective method for a coarse-grained filtering of the majority of irrelevant clips.\nAny clip containing speech or music label score over 0.6 at any frame is discarded. This removes unwanted cases, even when speech/music is present for a brief interval or only present in the background. Since in-the-wild videos are dominated by speech and music, this step filters the vast majority (97.7%) of clips. We manually annotate 276 videos sampled uniformly at random from the output of this filtering and find that 130 of them were Foley-style clips, i.e.&#160;the precision of this filtering in terms of our downstream goal is&#160;47%. Most false positives were for non-speech non-music videos where the audio was not causally linked to the on screen events.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "only",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audiovisual filtering:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Remaining\nclips are evaluated using Gemini 2.5 Pro </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleDeepMind2025Gemini2_5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which judges whether the sounds are causally and temporally grounded in visible on-screen actions. For example, if the audio is clapping, the video must show visible hands clapping in sync. We pass our manually annotated set of 276 clips through this stage and observe 72% precision. This stage substantially improves the quality of the dataset by filtering out mismatched or weakly grounded cases.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "set",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We benchmark FoleyBench against a diverse set of recent V2A generation models, across a range of architectures. Our evaluation includes an autoregressive model (V-AURA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which predicts audio tokens sequentially; several diffusion-based approaches (DiffFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2023diff</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VTA-LDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, FoleyCrafter&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024foleycrafter</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and LOVA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which iteratively refine audio from noise and often emphasize temporal alignment or long-form synthesis; and adapter-based methods such as CAFA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which guide a pretrained text-to-audio generator through additional control modules. We also consider transformer-based masked prediction models (MaskVAT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pascual2024masked</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SpecMaskFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhong2025specmaskfoley</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Seeing&amp;Hearing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which adapt the MaskGIT framework to audio by filling in masked spectrogram regions and in some cases leverage test-time optimization for better grounding. We also have V2A-Mapper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024v2a</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that uses embedding translation, which projects CLIP video embeddings into the CLAP audio&#8211;text space to condition an AudioLDM backbone without extensive retraining.\nFinally, we evaluate two large-scale, jointly-trained models: MultiFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a diffusion transformer trained on internet videos and high-quality SFX libraries, and MMAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a large flow-matching model that jointly trains across video, text, and audio modalities. Together, these baselines reflect the breadth of strategies explored in V2A research&#8212;from lightweight adapters to fully joint multimodal models.</span>\n</p>\n\n",
                "matched_terms": [
                    "into",
                    "maskvat",
                    "vaura",
                    "zhong2025specmaskfoley",
                    "text",
                    "condition",
                    "evaluation",
                    "foleybench",
                    "cheng2025lova",
                    "viertola2025temporally",
                    "v2a",
                    "luo2023diff",
                    "pascual2024masked",
                    "audio",
                    "wang2024v2a",
                    "xing2024seeing",
                    "benita2025cafa",
                    "difffoley",
                    "alignment",
                    "chen2025video",
                    "seeinghearing",
                    "clap",
                    "across",
                    "xu2024video",
                    "models",
                    "set",
                    "cheng2025mmaudio",
                    "v2amapper",
                    "zhang2024foleycrafter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate FoleyBench along two axes: (1) its reliability as a Foley benchmark compared to existing datasets in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Comparison with VGGSound-Test &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and (2) the insights it provides into the behavior of V2A models in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 New Insights from FoleyBench &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "models",
                    "into",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare FoleyBench to the VGGSound test set, the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">de facto</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nstandard for evaluating V2A models, to demonstrate that it is ill-suited for Foley-style scenarios and that FoleyBench is a more reliable alternative.</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "models",
                    "set",
                    "v2a",
                    "vggsound",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset quality.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> A key limitation of VGGSound-Test is the prevalence of unsuitable content for Foley evaluation. To assess this, we applied FoleyBench&#8217;s filtering pipeline to the VGGSound test set. We find that a significant portion of clips contain speech or music, with 65.3% of the available videos being discarded by our initial audio filtering stage. After the subsequent Gemini-based audiovisual grounding filter, only 25.5% of the original available videos remain (by comparison, we estimate that 72% of the videos in our benchmark are relevant). This underscores that the majority of VGGSound is not representative of Foley-style scenarios. Another issue is availability-at present we find that only 84.3% of the original VGGSound test set remains available on Youtube. Moreover, the distribution of sound categories in VGGSound-Test is less diverse. We pass the subset of VGGSound-Test remaining after the first stage of filtration through Gemini to get the UCS category label for each clip. Using Gemini to assign UCS category labels, we find that 24.3% of categories contain three or fewer videos, compared to only 13.4% in FoleyBench. We measure the overall diversity of the distribution of VGGSound-Test over UCS categories by calculating the Shannon entropy of the distribution across categories (higher is more diverse). The Shannon entropy of the filtered subset of VGGSound-Test is 4.73 compared to 5.35 for FoleyBench.\nThis lack of suitable, diverse content makes VGGSound-test ill-suited for V2A model evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "filtered",
                    "across",
                    "evaluation",
                    "foleybench",
                    "only",
                    "set",
                    "quality",
                    "subset",
                    "v2a",
                    "each",
                    "audio",
                    "vggsound",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Correlation of metric scores.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate SotA V2A models on both FoleyBench and VGGSound test using 6 metrics: CLAP Score (audio-text similarity), ImageBind Score (audio-video similarity), and De-Sync (temporal synchronization), and three audio fidelity metrics&#8212;FAD, Inception Score (IS), and KL Divergence. For each metric, we compute Kendall&#8217;s rank correlation (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) between the scores of each model on FoleyBench and VGGSound Test.\nWe find that correlation is relatively strong for De-Sync (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.878, p = 0.006) and ImageBind Score (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.714, p = 0.030), suggesting some consistency in measuring temporal synchronization and audio-video alignment. Correlations are lower for audio fidelity metrics: FAD (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.429, p = 0.179), Inception Score (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.611, p = 0.025), and KL Divergence (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.556, p = 0.045). We do not compute the correlation for CLAP score as there is not enough data for this metric on VGGSound Test. These results highlight that FoleyBench yields meaningfully different evaluation outcomes compared to VGGSound, particularly on audio fidelity metrics where correlations are weaker.\nWe hypothesize audio fidelity metrics on our dataset may be more sensitive to performance on broad sound classes, since we have higher coverage of UCS categories.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "clap",
                    "evaluation",
                    "imagebind",
                    "foleybench",
                    "desync",
                    "fad",
                    "models",
                    "results",
                    "v2a",
                    "alignment",
                    "each",
                    "between",
                    "metric",
                    "vggsound",
                    "scores",
                    "metrics",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Discrete Events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We find that models know </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">when</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> a discrete sound happens, but not </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">what</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> it should sound like. On </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Discrete</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clips (e.g., impacts, snaps) compared to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Rest</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (e.g., continuous sound like wind, rain), models show a trade-off: temporal synchronization improves, but audio quality deteriorates. For example, MMAudio&#8217;s De-Sync improves (0.458s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.390s), but its FAD degrades severely (9.02 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 16.35), while its IS also drops (10.3 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 8.8). We observe this same pattern in other models as well: CAFA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows De-Sync improving (0.842s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.730s) while FAD degrades (16.06 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 22.15); V-AURA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows De-Sync improving (0.812s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.736s) as FAD also degrades (59.17 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 64.39). This suggests that while the visual cue for an event provides a clear temporal signal, current models fail to render the corresponding high-fidelity impact.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "desync",
                    "fad",
                    "viertola2025temporally",
                    "quality",
                    "benita2025cafa",
                    "audio",
                    "vaura"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Background Sounds.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Models perform poorly on </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Background</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sounds. When comparing </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Background</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Action</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clips, nearly all of MMAudio&#8217;s metrics degrade: FAD worsens (9.77 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 14.76), IS drops (11.98 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 4.61), and De-Sync worsens (0.405s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.636s). However, the KLD (measuring class distribution) paradoxically </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">improves</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (2.54 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 1.98). This suggests that MMAudio can successfully identify the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">general category</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of an ambient sound (e.g., &#8221;wind&#8221;) but is currently unable to render it with high fidelity or synchronize it with subtle visual cues.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "desync",
                    "fad",
                    "kld",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multisource Sounds.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> As expected, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Multi-source</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> scenes are challenging. For CAFA, moving from </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Single-source</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Multi-source</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> degrades audio quality (FAD: 16.55 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 18.25), sync (De-Sync: 0.806s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.856s), and alignment (IB: 0.202 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.191). Interestingly, MMAudio exhibits a more nuanced failure: while its signal quality and sync also degrade (FAD: 9.84 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 11.34; De-Sync: 0.436s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.467s), its semantic metrics </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">improve</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (IB: 0.296 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.324). This suggests the model&#8217;s low-quality audio &#8221;mashup&#8221; is a better semantic match for the combined visual concepts than a single, low-fidelity sound was.</span>\n</p>\n\n",
                "matched_terms": [
                    "desync",
                    "fad",
                    "quality",
                    "alignment",
                    "audio",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Long videos.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To assess the ability of V2A models to handle long-form generation, we introduce FoleyBench-Long, a collection of 650 videos each 30 seconds in length. This is a challenging task, as most V2A models are trained on clips of 10 seconds or less. Of the models we tested, only three&#8212;LOVA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VTA-LDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MMAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;could produce 30-second outputs. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 New Insights from FoleyBench &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results degrade in general compared to short videos. MMAudio remains the best at synchronization (De-Sync) and semantic alignment (IB, CLAP), but its audio quality deteriorates substantially (FAD: 8.76 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p7.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 27.5). Conversely, LOVA, which was designed for long-form synthesis, achieves the best audio quality (FAD, IS) but at the cost of weaker alignment. These results show that generating temporally consistent and high-quality audio at 30-second scales remains a significant challenge. By providing FoleyBench-Long, we aim to establish a reliable benchmark for this underexplored setting and encourage future work on long-form V2A generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "clap",
                    "best",
                    "only",
                    "cheng2025lova",
                    "fad",
                    "desync",
                    "models",
                    "xu2024video",
                    "results",
                    "quality",
                    "v2a",
                    "seconds",
                    "alignment",
                    "each",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduce FoleyBench, a benchmark for evaluating V2A generation models on Foley-style clips with precise temporal and semantic grounding. Comprising 5,000 human-validated video&#8211;audio&#8211;caption triplets, FoleyBench enables comprehensive evaluation of audio quality, alignment, and diversity. Our experiments show that widely used benchmarks often include content unsuitable for Foley evaluation, which can lead to misleading conclusions about model performance. In contrast, FoleyBench surfaces meaningfully different results compared to the popular VGGSound test set, particularly with respect to audio fidelity. Through our analysis, we uncover several insights into current model limitations. We find that models struggle to render high-fidelity discrete sounds, even when they are well synchronised; performance degrades significantly on long-form videos, with top short-clip models like MMAudio suffering a severe drop in audio quality; and text conditioning provides a critical semantic prior that models struggle to derive from video alone. By releasing this benchmark and the accompanying data pipeline, we aim to drive the development of more accurate and visually grounded V2A systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "foleybench",
                    "evaluation",
                    "models",
                    "set",
                    "quality",
                    "results",
                    "v2a",
                    "into",
                    "alignment",
                    "audio",
                    "vggsound",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">This work used Bridges2 at PSC by allocation SOC240007P from the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We thank Prem Seetharaman, Uri Nieto, and Justin Salamon (Adobe) for their valuable feedback and for running MultiFoley on our dataset, and Heng Wang (University of Sydney) and Chunghsin Yeh (Dolby) for evaluating MaskVAT and V2A-mapper on our dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "maskvat",
                    "v2amapper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">V-AURA.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Viertola et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose an autoregressive model that generates audio by sequentially predicting audio tokens, designed to capture fine-grained temporal alignment using high-frame-rate visual features.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vaura",
                    "viertola2025temporally",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DiffFoley.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Luo et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2023diff</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a diffusion-based model that synthesizes Foley sounds by iteratively denoising a random signal conditioned on visual features.</span>\n</p>\n\n",
                "matched_terms": [
                    "difffoley",
                    "luo2023diff"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">VTA-LDM.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> develop a Latent Diffusion Model (LDM) that generates audio in a compressed latent space, conditioned on both video and text inputs for improved control and efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "xu2024video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FoleyCrafter.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Zhang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024foleycrafter</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> present a diffusion model that incorporates both global video context and fine-grained frame-level features to generate highly synchronized audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "zhang2024foleycrafter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LOVA.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Cheng et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a diffusion model specifically designed for long-form video-to-audio generation, capable of producing temporally coherent audio for extended video clips.</span>\n</p>\n\n",
                "matched_terms": [
                    "cheng2025lova",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">V2A-Mapper.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Wang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024v2a</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a lightweight mapping method that projects video embeddings into the latent space of a pretrained audio generator, enabling modular video-to-audio synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "wang2024v2a",
                    "v2amapper",
                    "into",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CAFA.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Benita et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose an adapter-based approach that injects visual guidance into a pretrained text-to-audio model using a ControlNet-like architecture, enabling efficient V2A generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "into",
                    "benita2025cafa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SpecMaskFoley.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Zhong et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhong2025specmaskfoley</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> present a transformer-based model that adapts the masked prediction framework by iteratively filling in masked portions of an audio spectrogram conditioned on visual inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "zhong2025specmaskfoley"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seeing &amp; Hearing.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Xing et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose a two-stage model that first generates a descriptive text caption from the video and then uses a text-to-audio model to synthesize the corresponding sound.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "xing2024seeing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MaskVAT.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Pascual et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pascual2024masked</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> develop a transformer-based model using a masked prediction objective to generate audio spectrograms from visual inputs, adapting the MaskGIT framework for the audio domain.</span>\n</p>\n\n",
                "matched_terms": [
                    "maskvat",
                    "audio",
                    "pascual2024masked"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MultiFoley.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Chen et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a diffusion transformer-based model designed for multimodal control (text, audio, and video). It is trained on a combination of internet videos and professional, high-quality SFX libraries to produce full-bandwidth (48kHz) audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "chen2025video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMAudio.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Cheng et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a large-scale, jointly trained model using a flow-matching architecture to generate high-fidelity audio conditioned on both video and text inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "cheng2025mmaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The first prompt (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#A2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; Appendix B Prompts &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), given to Gemini 2.5 Pro&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleDeepMind2025Gemini2_5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, performs the primary filtration step. It issues an Accept/Reject verdict, accepting a video only if all sounds are visually and causally grounded while strictly filtering out any speech or music. The model also generates a one-line caption and categorizes the clip by its sound envelope (Discrete/Continuous), source complexity (Single-source/Multi-source), and acoustic focus (Background/Action), returning all information in a structured JSON.</span>\n</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#A3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; Appendix C Dataset Diversity &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the category diversity of FoleyBench against the subset of the VGGSound test set remaining after being filtered for speech and music. The comparison highlights that even after this initial filtering, the VGGSound data remains poorly suited for comprehensive evaluation due to a highly imbalanced distribution. Its content is heavily skewed and dominated by a few common sound types (such as ANIMALS and BIRDS), leaving many other Foley-relevant classes under-represented.</span>\n</p>\n\n",
                "matched_terms": [
                    "filtered",
                    "foleybench",
                    "evaluation",
                    "set",
                    "subset",
                    "vggsound",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In contrast, FoleyBench provides a visibly more uniform and balanced distribution across the UCS taxonomy, ensuring broader coverage. This visual observation is confirmed by the quantitative analysis in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Comparison with VGGSound-Test &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: FoleyBench achieves a higher Shannon entropy (5.35 vs. 4.73), indicating greater overall diversity. It also better mitigates data sparsity, with only 13.4% of its categories containing three or fewer videos, compared to 24.3% in the filtered VGGSound set. This balanced distribution ensures evaluations are not biased by a model&#8217;s performance on a handful of common sounds, offering a more reliable benchmark for general Foley synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "filtered",
                    "across",
                    "foleybench",
                    "only",
                    "set",
                    "vggsound"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "FoleyBench: A Benchmark For Video-to-Audio Models",
        "caption": "Table 2: Evaluation of V2A models on FoleyBench-Long. We report results on FoleyBench-Long and FoleyBench for models capable of generating long audios. Best results shown in bold.",
        "body": "Method\nCross-modal alignment\nAudio quality\n\n\n\nIB ↑\\uparrow\n\n\nCLAP ↑\\uparrow\n\n\nD-S ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nIS ↑\\uparrow\n\n\nKLD ↓\\downarrow\n\n\n\nFoleyBench-Long results.\n\n\n\nLOVA [cheng2025lova]\n\n0.237\n0.102\n1.20\n26.2\n5.02\n2.44\n\n\n\nVTA-LDM [xu2024video]\n\n0.147\n0.091\n1.22\n83.2\n1.27\n2.19\n\n\n\nMMAudio [cheng2025mmaudio]\n\n0.239\n0.174\n0.638\n27.5\n3.87\n2.40\n\n\nFoleyBench results.\n\n\n\nLOVA [cheng2025lova]\n\n0.209\n0.167\n1.15\n20.7\n7.61\n3.15\n\n\n\nVTA-LDM [xu2024video]\n\n0.221\n0.138\n1.21\n15.7\n7.27\n3.13\n\n\n\nMMAudio [cheng2025mmaudio]\n\n0.306\n0.331\n0.447\n8.76\n11.2\n2.43",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Cross-modal alignment</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Audio quality</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">IB&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">CLAP&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">D-S&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">FAD&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">IS&#160;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">KLD&#160;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;--ltx-fg-color:#656565;\"><span class=\"ltx_text ltx_font_italic\">FoleyBench-Long results.</span></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">LOVA&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.237</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.102</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">26.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">5.02</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VTA-LDM&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.147</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.091</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.22</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">83.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.27</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MMAudio&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.239</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.174</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.638</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">27.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.40</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"7\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;--ltx-fg-color:#656565;\"><span class=\"ltx_text ltx_font_italic\">FoleyBench results.</span></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">LOVA&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.209</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.167</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">20.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VTA-LDM&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.221</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.138</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">15.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.27</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">MMAudio&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.306</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.331</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.447</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">8.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">11.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.43</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "long",
            "↑uparrow",
            "crossmodal",
            "lova",
            "evaluation",
            "foleybench",
            "best",
            "cheng2025lova",
            "fad",
            "capable",
            "results",
            "v2a",
            "vtaldm",
            "audios",
            "audio",
            "bold",
            "foleybenchlong",
            "method",
            "↓downarrow",
            "alignment",
            "kld",
            "clap",
            "generating",
            "mmaudio",
            "report",
            "models",
            "xu2024video",
            "quality",
            "cheng2025mmaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Long videos.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To assess the ability of V2A models to handle long-form generation, we introduce FoleyBench-Long, a collection of 650 videos each 30 seconds in length. This is a challenging task, as most V2A models are trained on clips of 10 seconds or less. Of the models we tested, only three&#8212;LOVA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VTA-LDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MMAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;could produce 30-second outputs. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 New Insights from FoleyBench &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results degrade in general compared to short videos. MMAudio remains the best at synchronization (De-Sync) and semantic alignment (IB, CLAP), but its audio quality deteriorates substantially (FAD: 8.76 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p7.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 27.5). Conversely, LOVA, which was designed for long-form synthesis, achieves the best audio quality (FAD, IS) but at the cost of weaker alignment. These results show that generating temporally consistent and high-quality audio at 30-second scales remains a significant challenge. By providing FoleyBench-Long, we aim to establish a reliable benchmark for this underexplored setting and encourage future work on long-form V2A generation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of <em class=\"ltx_emph ltx_font_italic\">Foley</em>&#8212;sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music&#8212;domains that lie outside the use case for Foley.\nTo address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark state-of-the-art (SotA) V2A models, evaluating them on audio quality, audio&#8211;video alignment, temporal synchronization, and audio&#8211;text consistency. Samples are available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://gclef-cmu.org/foleybench\" title=\"\">https://gclef-cmu.org/foleybench</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "generating",
                    "evaluation",
                    "foleybench",
                    "models",
                    "quality",
                    "v2a",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nVideo-to-audio models, Audio-visual alignment, Foley sound synthesis</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The task of video-to-audio generation (V2A) seeks to synthesize realistic, temporally aligned audio directly from visual input, sometimes conditioned on an additional text caption for finer control. A key downstream use case in the creative industry (e.g., film, games) is Foley&#8212;the creation of sound effects such as object impacts, or ambient noises that must be both semantically appropriate and temporally synchronized with visible events. Foley is distinct from speech and music, which are typically added in other stages of production. Foley specifically refers to non-speech, non-music sound effects (like footsteps or object impacts) that are causally linked to visual actions. Recent work has rapidly advanced V2A generation to support Foley scenarios, with a diverse family of methods being developed, including autoregressive, diffusion-based, flow-matching-based, and ControlNet-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Mei2023-FoleyGen</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2023diff</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025video</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "xu2024video",
                    "models",
                    "v2a",
                    "cheng2025mmaudio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite this progress, evaluation of V2A systems remains misaligned with downstream Foley application goals. The widely used VGGSound test set&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vggsound2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, for instance, suffers from weak audio&#8211;visual correspondence, with many clips dominated by speech, music, or off-screen sounds. In our analysis, we find that 74% of these clips show poor alignment between the audio and the video, as detailed in Section &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Comparison with VGGSound-Test &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThese cases lie outside the intended scope of V2A modeling, and their prevalence in existing benchmarks obscures evaluation.\nOther relevant datasets exist but are small-scale, limited in diversity, or lack comprehensive metadata.\nThree existing datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2020generating</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">iashin2022sparse</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">owens2016visually</span><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have stronger guarantees on audio-visual correspondence but only cover a dozen or fewer specific sound categories.\nThe recently proposed VisualSound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset filters VGGSound using an audio-visual similarity metric,\nbut lacks comprehensive sound category metadata, and its coverage of Foley categories is unknown.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "v2a",
                    "audio",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, current benchmarks lack systematic coverage across sound classes (such as AudioSet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audioset2017</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or the Universal Category System (UCS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">UCS</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;a sound categorization developed specifically for professional sound design use cases).\nThey lack categories for fine-grained evaluation, such as sound types (discrete events vs. continuous ambiences) and scene complexity (single-source vs. multi-source), limiting our understanding of where models succeed or fail.\nOur experiments on the VGGSound test set reveal its coverage across Foley sound categories is limited&#8212;24.3% of UCS categories have three or fewer videos in VGGSound.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce FoleyBench, the first large-scale benchmark explicitly designed for evaluating Foley-style V2A: non-speech, non-music clips with visible sound sources and strong temporal correspondence between audio and video. FoleyBench comprises 5,000 curated video&#8211;audio&#8211;caption triplets, each approximately 8 to 10 seconds long, constructed through a scalable multi-stage pipeline. Each clip is automatically labeled by Gemini 2.5 Pro </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleDeepMind2025Gemini2_5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with AudioSet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audioset2017</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and UCS tags, and enriched with metadata capturing source complexity (such as single vs. multi-source) and sound envelope type (such as discrete vs. continuous). We also create FoleyBench-Long&#8212;a set of 650 long videos (each 30 seconds long) to evaluate the audio generation ability of models for long-form videos. All these categories enable fine-grained analysis of model behavior across conditions.\nWe find that while MMAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves the best overall scores, other models excel in specific areas: Seeing &amp; Hearing </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> obtains the best semantic audio-video alignment, V-AURA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> excels at precise temporal synchronization, and LOVA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> better preserves audio quality on long-form content. Our contributions are as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "lova",
                    "mmaudio",
                    "foleybench",
                    "cheng2025lova",
                    "best",
                    "models",
                    "quality",
                    "v2a",
                    "cheng2025mmaudio",
                    "long",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">FoleyBench, a 5,000-instance, high-quality benchmark for visually grounded, non-speech, non-music audio generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Comprehensive evaluation of V2A models on audio quality, audio&#8211;visual alignment, temporal synchronization, and audio&#8211;text consistency.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models",
                    "quality",
                    "v2a",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Analyzing failure modes in existing V2A models, yielding actionable insights for future progress.</span>\n</p>\n\n",
                "matched_terms": [
                    "v2a",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">FoleyBench is a large-scale benchmark designed to evaluate V2A systems in scenarios comprising video clips with precise temporal and semantic grounding. The dataset focuses on </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Foley-style clips</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which we define here as: non-speech, non-music video segments in which the sound sources are both visible on screen and temporally synchronized with their corresponding actions.\nIt consists of 5,000 curated video&#8211;audio pairs, each accompanied by a caption describing the visual scene. Importantly, each clip is accompanied by metadata that enables fine-grained analysis of model behavior. Metadata includes: AudioSet and UCS labels and attributes such as sound type (discrete events vs. continuous ambience) and source complexity (single-source vs. multi-source).</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio filtering:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We first apply YAMNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yamnet2020</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for frame-level classification. We use YAMNet as a fast and cost-effective method for a coarse-grained filtering of the majority of irrelevant clips.\nAny clip containing speech or music label score over 0.6 at any frame is discarded. This removes unwanted cases, even when speech/music is present for a brief interval or only present in the background. Since in-the-wild videos are dominated by speech and music, this step filters the vast majority (97.7%) of clips. We manually annotate 276 videos sampled uniformly at random from the output of this filtering and find that 130 of them were Foley-style clips, i.e.&#160;the precision of this filtering in terms of our downstream goal is&#160;47%. Most false positives were for non-speech non-music videos where the audio was not causally linked to the on screen events.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audiovisual filtering:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Remaining\nclips are evaluated using Gemini 2.5 Pro </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GoogleDeepMind2025Gemini2_5</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which judges whether the sounds are causally and temporally grounded in visible on-screen actions. For example, if the audio is clapping, the video must show visible hands clapping in sync. We pass our manually annotated set of 276 clips through this stage and observe 72% precision. This stage substantially improves the quality of the dataset by filtering out mismatched or weakly grounded cases.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We benchmark FoleyBench against a diverse set of recent V2A generation models, across a range of architectures. Our evaluation includes an autoregressive model (V-AURA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which predicts audio tokens sequentially; several diffusion-based approaches (DiffFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">luo2023diff</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VTA-LDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, FoleyCrafter&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024foleycrafter</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and LOVA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which iteratively refine audio from noise and often emphasize temporal alignment or long-form synthesis; and adapter-based methods such as CAFA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which guide a pretrained text-to-audio generator through additional control modules. We also consider transformer-based masked prediction models (MaskVAT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pascual2024masked</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SpecMaskFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhong2025specmaskfoley</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Seeing&amp;Hearing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which adapt the MaskGIT framework to audio by filling in masked spectrogram regions and in some cases leverage test-time optimization for better grounding. We also have V2A-Mapper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024v2a</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that uses embedding translation, which projects CLIP video embeddings into the CLAP audio&#8211;text space to condition an AudioLDM backbone without extensive retraining.\nFinally, we evaluate two large-scale, jointly-trained models: MultiFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a diffusion transformer trained on internet videos and high-quality SFX libraries, and MMAudio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a large flow-matching model that jointly trains across video, text, and audio modalities. Together, these baselines reflect the breadth of strategies explored in V2A research&#8212;from lightweight adapters to fully joint multimodal models.</span>\n</p>\n\n",
                "matched_terms": [
                    "lova",
                    "clap",
                    "mmaudio",
                    "evaluation",
                    "foleybench",
                    "cheng2025lova",
                    "models",
                    "xu2024video",
                    "v2a",
                    "vtaldm",
                    "cheng2025mmaudio",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate generated audio along two key dimensions: audio quality and cross-modal alignment. For audio quality, we report three widely used metrics: Fr&#233;chet Audio Distance (FAD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fad2019</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Inception Score (IS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">salimans2016inception</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Kullback&#8211;Leibler Divergence (KLD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kullback1951KL</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To compute all of these metrics, we rely on PANN embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2020panns</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which provide pretrained audio representations especially effective for non-speech, non-music audio. For alignment, we focus on three complementary metrics. CLAP Score&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">laionclap2023</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> evaluates how well generated audio semantically matches the ground-truth textual captions. As this metric specifically measures the generated audio&#8217;s adherence to the text prompt, we only report it for models that are text-conditioned (marked with </span>\n  <sup class=\"ltx_sup\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">T</span>\n  </sup>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). ImageBind Score&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">girdhar2023imagebind</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures cross-modal similarity between audio and video. Finally, De-Sync&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">syncnet2016</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quantifies the temporal offset between audio and video streams in seconds, with lower scores corresponding to tighter synchronization. We use the AV-benchmark toolkit for evaluation </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hkchengrex/av-benchmark\" title=\"\">https://github.com/hkchengrex/av-benchmark</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "crossmodal",
                    "clap",
                    "evaluation",
                    "report",
                    "models",
                    "fad",
                    "quality",
                    "alignment",
                    "audio",
                    "kld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate FoleyBench along two axes: (1) its reliability as a Foley benchmark compared to existing datasets in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Comparison with VGGSound-Test &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and (2) the insights it provides into the behavior of V2A models in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 New Insights from FoleyBench &#8227; 4 Results &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "models",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare FoleyBench to the VGGSound test set, the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">de facto</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nstandard for evaluating V2A models, to demonstrate that it is ill-suited for Foley-style scenarios and that FoleyBench is a more reliable alternative.</span>\n</p>\n\n",
                "matched_terms": [
                    "foleybench",
                    "models",
                    "v2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset quality.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> A key limitation of VGGSound-Test is the prevalence of unsuitable content for Foley evaluation. To assess this, we applied FoleyBench&#8217;s filtering pipeline to the VGGSound test set. We find that a significant portion of clips contain speech or music, with 65.3% of the available videos being discarded by our initial audio filtering stage. After the subsequent Gemini-based audiovisual grounding filter, only 25.5% of the original available videos remain (by comparison, we estimate that 72% of the videos in our benchmark are relevant). This underscores that the majority of VGGSound is not representative of Foley-style scenarios. Another issue is availability-at present we find that only 84.3% of the original VGGSound test set remains available on Youtube. Moreover, the distribution of sound categories in VGGSound-Test is less diverse. We pass the subset of VGGSound-Test remaining after the first stage of filtration through Gemini to get the UCS category label for each clip. Using Gemini to assign UCS category labels, we find that 24.3% of categories contain three or fewer videos, compared to only 13.4% in FoleyBench. We measure the overall diversity of the distribution of VGGSound-Test over UCS categories by calculating the Shannon entropy of the distribution across categories (higher is more diverse). The Shannon entropy of the filtered subset of VGGSound-Test is 4.73 compared to 5.35 for FoleyBench.\nThis lack of suitable, diverse content makes VGGSound-test ill-suited for V2A model evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "foleybench",
                    "quality",
                    "v2a",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Correlation of metric scores.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate SotA V2A models on both FoleyBench and VGGSound test using 6 metrics: CLAP Score (audio-text similarity), ImageBind Score (audio-video similarity), and De-Sync (temporal synchronization), and three audio fidelity metrics&#8212;FAD, Inception Score (IS), and KL Divergence. For each metric, we compute Kendall&#8217;s rank correlation (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) between the scores of each model on FoleyBench and VGGSound Test.\nWe find that correlation is relatively strong for De-Sync (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.878, p = 0.006) and ImageBind Score (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.714, p = 0.030), suggesting some consistency in measuring temporal synchronization and audio-video alignment. Correlations are lower for audio fidelity metrics: FAD (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.429, p = 0.179), Inception Score (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.611, p = 0.025), and KL Divergence (</span>\n  <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#961;</mi>\n      <annotation encoding=\"application/x-tex\">\\rho</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> = 0.556, p = 0.045). We do not compute the correlation for CLAP score as there is not enough data for this metric on VGGSound Test. These results highlight that FoleyBench yields meaningfully different evaluation outcomes compared to VGGSound, particularly on audio fidelity metrics where correlations are weaker.\nWe hypothesize audio fidelity metrics on our dataset may be more sensitive to performance on broad sound classes, since we have higher coverage of UCS categories.</span>\n</p>\n\n",
                "matched_terms": [
                    "clap",
                    "evaluation",
                    "foleybench",
                    "models",
                    "fad",
                    "results",
                    "v2a",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present the benchmark results for several SotA V2A models on FoleyBench in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, providing a comprehensive set of reference evaluations. These scores are shown alongside those from the VGGSound Test set, which are often sparsely and inconsistently reported in prior work&#8212;a key gap our benchmark aims to resolve.\nFor FoleyBench, we find that MMAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves the best performance across nearly all metrics (second-best only on IB). Seeing and Hearing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2024seeing</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> achieves the best ImageBind score, which is likely because it performs gradient ascent directly on the ImageBind score at test-time, as also noted by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Interestingly, CAFA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a ControlNet-based approach, ranks second across multiple metrics. This highlights the effectiveness of adapting a strong pretrained backbone (Stable-Audio-Open </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">evans2025stable</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which provides a full-band (44.1kHz) output, offering an advantage in audio quality metrics over models with lower-bandwidth backbones (e.g., SpecMaskGIT at 22kHz). V-AURA </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> ranks second on temporal synchronization (DS), consistent with its design emphasis on fine-grained temporal alignment via high-frame-rate visual feature extraction.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmaudio",
                    "foleybench",
                    "best",
                    "models",
                    "quality",
                    "results",
                    "v2a",
                    "cheng2025mmaudio",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Discrete Events.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We find that models know </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">when</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> a discrete sound happens, but not </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">what</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> it should sound like. On </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Discrete</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clips (e.g., impacts, snaps) compared to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Rest</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (e.g., continuous sound like wind, rain), models show a trade-off: temporal synchronization improves, but audio quality deteriorates. For example, MMAudio&#8217;s De-Sync improves (0.458s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.390s), but its FAD degrades severely (9.02 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 16.35), while its IS also drops (10.3 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 8.8). We observe this same pattern in other models as well: CAFA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2025cafa</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows De-Sync improving (0.842s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.730s) while FAD degrades (16.06 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 22.15); V-AURA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows De-Sync improving (0.812s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.736s) as FAD also degrades (59.17 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 64.39). This suggests that while the visual cue for an event provides a clear temporal signal, current models fail to render the corresponding high-fidelity impact.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "fad",
                    "quality",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Background Sounds.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Models perform poorly on </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Background</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sounds. When comparing </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Background</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Action</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clips, nearly all of MMAudio&#8217;s metrics degrade: FAD worsens (9.77 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 14.76), IS drops (11.98 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 4.61), and De-Sync worsens (0.405s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.636s). However, the KLD (measuring class distribution) paradoxically </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">improves</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (2.54 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 1.98). This suggests that MMAudio can successfully identify the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">general category</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of an ambient sound (e.g., &#8221;wind&#8221;) but is currently unable to render it with high fidelity or synchronize it with subtle visual cues.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "fad",
                    "mmaudio",
                    "kld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multisource Sounds.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> As expected, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Multi-source</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> scenes are challenging. For CAFA, moving from </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Single-source</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Multi-source</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> degrades audio quality (FAD: 16.55 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 18.25), sync (De-Sync: 0.806s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.856s), and alignment (IB: 0.202 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.191). Interestingly, MMAudio exhibits a more nuanced failure: while its signal quality and sync also degrade (FAD: 9.84 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 11.34; De-Sync: 0.436s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.467s), its semantic metrics </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">improve</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (IB: 0.296 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.324). This suggests the model&#8217;s low-quality audio &#8221;mashup&#8221; is a better semantic match for the combined visual concepts than a single, low-fidelity sound was.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmaudio",
                    "fad",
                    "quality",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-Conditioning.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The results in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> suggest that text-conditioning acts as a powerful prior, as best performing models are text conditioned. We also performed an ablation on SpecMaskFoley&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhong2025specmaskfoley</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, evaluating it with and without its text prompt. The text-conditioned version was universally superior across every metric. On </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Action</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> clips, for instance, it improves audio quality metrics (FAD: 23.18 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 19.60; IS: 4.82 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 6.02) and alignment metrics (IB: 0.188 </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.222; De-Sync: 0.911s </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p6.m4\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.755s). This confirms that text prompts provide crucial guidance, helping the model select a more accurate sound and align it more precisely in time.</span>\n</p>\n\n",
                "matched_terms": [
                    "best",
                    "models",
                    "fad",
                    "quality",
                    "results",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduce FoleyBench, a benchmark for evaluating V2A generation models on Foley-style clips with precise temporal and semantic grounding. Comprising 5,000 human-validated video&#8211;audio&#8211;caption triplets, FoleyBench enables comprehensive evaluation of audio quality, alignment, and diversity. Our experiments show that widely used benchmarks often include content unsuitable for Foley evaluation, which can lead to misleading conclusions about model performance. In contrast, FoleyBench surfaces meaningfully different results compared to the popular VGGSound test set, particularly with respect to audio fidelity. Through our analysis, we uncover several insights into current model limitations. We find that models struggle to render high-fidelity discrete sounds, even when they are well synchronised; performance degrades significantly on long-form videos, with top short-clip models like MMAudio suffering a severe drop in audio quality; and text conditioning provides a critical semantic prior that models struggle to derive from video alone. By releasing this benchmark and the accompanying data pipeline, we aim to drive the development of more accurate and visually grounded V2A systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmaudio",
                    "evaluation",
                    "foleybench",
                    "models",
                    "quality",
                    "results",
                    "v2a",
                    "alignment",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">V-AURA.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Viertola et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">viertola2025temporally</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose an autoregressive model that generates audio by sequentially predicting audio tokens, designed to capture fine-grained temporal alignment using high-frame-rate visual features.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">VTA-LDM.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xu2024video</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> develop a Latent Diffusion Model (LDM) that generates audio in a compressed latent space, conditioned on both video and text inputs for improved control and efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "xu2024video",
                    "vtaldm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LOVA.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Cheng et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025lova</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a diffusion model specifically designed for long-form video-to-audio generation, capable of producing temporally coherent audio for extended video clips.</span>\n</p>\n\n",
                "matched_terms": [
                    "lova",
                    "audio",
                    "capable",
                    "cheng2025lova"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">V2A-Mapper.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Wang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024v2a</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a lightweight mapping method that projects video embeddings into the latent space of a pretrained audio generator, enabling modular video-to-audio synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMAudio.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Cheng et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cheng2025mmaudio</span>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduce a large-scale, jointly trained model using a flow-matching architecture to generate high-fidelity audio conditioned on both video and text inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "cheng2025mmaudio",
                    "mmaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13219v2#A3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; Appendix C Dataset Diversity &#8227; FoleyBench: A Benchmark For Video-to-Audio Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the category diversity of FoleyBench against the subset of the VGGSound test set remaining after being filtered for speech and music. The comparison highlights that even after this initial filtering, the VGGSound data remains poorly suited for comprehensive evaluation due to a highly imbalanced distribution. Its content is heavily skewed and dominated by a few common sound types (such as ANIMALS and BIRDS), leaving many other Foley-relevant classes under-represented.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "foleybench"
                ]
            }
        ]
    }
}