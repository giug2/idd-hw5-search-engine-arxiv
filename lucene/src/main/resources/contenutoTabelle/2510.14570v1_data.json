{
    "S1.T1": {
        "caption": "Table 1: Five dimensions for evaluation in AudioEval.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.7pt;\"><span class=\"ltx_text ltx_align_left ltx_font_bold\">Dimension</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Definition</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.7pt;\">Content Enjoyment</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Degree of subjective enjoyment, including emotional impact and artistic expression.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.7pt;\">[0.8pt/2pt]\nContent Usefulness</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Potential usefulness of the audio for downstream applications or creative purposes.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.7pt;\">[0.8pt/2pt]\nProduction Complexity</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Level of acoustic richness and diversity of structural elements.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.7pt;\">[0.8pt/2pt]\nProduction Quality</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-bottom:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Technical fidelity of the audio, covering clarity, dynamics, and balance.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:86.7pt;\">[0.8pt/2pt]\nTextual Alignment</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Accuracy of semantic and temporal alignment with the input text.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "content",
            "semantic",
            "fidelity",
            "evaluation",
            "level",
            "artistic",
            "dynamics",
            "textual",
            "production",
            "structural",
            "downstream",
            "acoustic",
            "definition",
            "audio",
            "technical",
            "emotional",
            "diversity",
            "usefulness",
            "enjoyment",
            "input",
            "purposes",
            "accuracy",
            "text",
            "elements",
            "richness",
            "clarity",
            "covering",
            "alignment",
            "temporal",
            "applications",
            "audioeval",
            "degree",
            "08pt2pt",
            "complexity",
            "expression",
            "balance",
            "impact",
            "potential",
            "including",
            "five",
            "dimensions",
            "dimension",
            "subjective",
            "quality",
            "creative"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To address these challenges, we introduce AudioEval. As far as we know, it is the first dataset for evaluation of TTA-generated audio, enabling automated, dual-perspective, and multi-dimensional assessment. It includes 4,200 audio samples from 24 systems, with 25,200 records and 126,000 dimension-level ratings. Both experts and non-experts contribute, capturing complementary perspectives of audio perception. We extend prior evaluation framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib10\" title=\"\">10</a>]</cite> by annotating each sample along five perceptual dimensions, as summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, to establish a comprehensive evaluation protocol. Building on this dataset, we propose Qwen-DisQA, an automatic quality scoring model based on Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib13\" title=\"\">13</a>]</cite>. It jointly processes textual prompts and generated audio to predict human-like multi-dimensional ratings from expert and non-expert perspectives. Through distribution modeling, it provides more reliable and nuanced automatic evaluations.</p>\n\n",
            "<p class=\"ltx_p\">We use a 10-point Likert scale, where higher scores always indicate better performance. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> defines five perceptual dimensions, which together cover both functional utility and subjective experience.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) is rapidly advancing, with broad potential in virtual reality, accessibility, and creative media. However, evaluating TTA quality remains difficult: human ratings are costly and limited, while existing objective metrics capture only partial aspects of perceptual quality. To address this gap, we introduce AudioEval, the first large-scale TTA evaluation dataset, containing 4,200 audio samples from 24 systems with 126,000 ratings across five perceptual dimensions, annotated by both experts and non-experts. Based on this resource, we propose Qwen-DisQA, a multimodal scoring model that jointly processes text prompts and generated audio to predict human-like quality ratings. Experiments show its effectiveness in providing reliable and scalable evaluation. The dataset will be made publicly available to accelerate future research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "potential",
                    "audioeval",
                    "five",
                    "dimensions",
                    "evaluation",
                    "creative",
                    "text",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nText-to-Audio, Automatic Evaluation, Perceptual Quality Assessment</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, text-to-audio (TTA) technology has emerged as an important and rapidly evolving research area at the intersection of natural language processing and audio generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib4\" title=\"\">4</a>]</cite>. Unlike conventional text-to-speech (TTS) systems that focus on naturalness and intelligibility, TTA aims to generate diverse audio content from text, extending text-conditioned audio generation beyond speech. Consequently, TTA is expected to enable richer multimodal interaction and open broad applications in virtual reality, accessibility, and creative media.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "applications",
                    "text",
                    "creative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these rapid advances, the evaluation of TTA systems remains a significant challenge. Current practices often rely on subjective human ratings, typically reported as Mean Opinion Scores (MOS). While human judgment is considered the gold standard, this approach is expensive and time-consuming <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib5\" title=\"\">5</a>]</cite>. In parallel, objective metrics from related domains such as Frechet Inception Distance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib6\" title=\"\">6</a>]</cite> and CLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib7\" title=\"\">7</a>]</cite> have been applied to TTA evaluation. Although useful, these metrics provide a limited perspective, but do not accurately reflect perceptual quality<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib8\" title=\"\">8</a>]</cite>. Furthermore, the requirement for reference audio in some of these metrics further limits their application.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic perceptual evaluation has gained attention in the areas of synthetic speech, generated music, and general audio, underscoring both its feasibility and the critical need for reliable evaluation tools for generative models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib12\" title=\"\">12</a>]</cite>. However, predicting human perceptual quality in TTA systems presents considerable challenges. Firstly, the rapid growth of TTA methods has created a varied system landscape, which requires the collection of diverse audio data for rigorous evaluation. Moreover, the inherent complexity of audio and its prompt-driven generation necessitate attention to multiple evaluative dimensions such as aesthetic quality and textual consistency. Additionally, the broad application potential of TTA highlights the need to consider both general audiences and professional users. Together, these factors make the acquisition of reliable data and the development of effective methodologies particularly difficult.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "potential",
                    "complexity",
                    "evaluation",
                    "dimensions",
                    "textual",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">AudioEval</span>, the first multi-dimensional TTA evaluation dataset with ratings from both experts and non-experts, supporting automated evaluation task.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "audioeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop an automatic quality scoring model, <span class=\"ltx_text ltx_font_bold\">Qwen-DisQA</span>, which predicts perceptual ratings across five dimensions from text&#8211;audio pairs, capturing quality from both expert and general listener perspectives.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "quality",
                    "five"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of prompts, we assess this variation in Figure.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>: the left panel presents the distribution of prompt lengths, indicating wide lexical coverage, while the right panel visualizes prompt embeddings via t-SNE, where dispersed clusters highlight semantic diversity. Together, these analyses show that the dataset includes diverse inputs, providing a solid research basis for automatic TTA evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "diversity",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each audio sample in AudioEval is rated by three experts and three non-experts. These two groups are defined as follows.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "audioeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce bias, samples appear in random order and annotators follow standardized instructions with examples for each dimension. During evaluation, we use a consistency probe: if ratings on the same sample differ by more than two points, we discard the record. This procedure, combined with multiple raters per group, ensures reliable and consistent annotations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (top row) illustrates that audio scores cluster around the mid-range. Usefulness and enjoyment exhibit narrow, centralized distributions, indicating stable medium-level performance, whereas production quality and textual alignment are more widely spread, reflecting larger variability. Production complexity is consistently low across samples. Experts tend to assign lower ratings for complexity and enjoyment but higher ones for quality and alignment, while usefulness shows nearly identical patterns across groups.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "audio",
                    "complexity",
                    "usefulness",
                    "enjoyment",
                    "textual",
                    "production",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (bottom row) demonstrates that expert&#8211;non-expert correlations are weak at the clip level, underscoring perceptual variability, yet they improve substantially when aggregated at the system level. Agreement is strongest for usefulness and alignment, but weaker for complexity and enjoyment. Taken together, these findings reinforce the value of separating expert and non-expert ratings to capture complementary perspectives for analysis and model development.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "complexity",
                    "level",
                    "usefulness",
                    "enjoyment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the top of AudioEval, We formulate TTA quality assessment as a multi-dimensional distribution prediction task. Given a text prompt <math alttext=\"x^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(t)}</annotation></semantics></math> and generated audio <math alttext=\"x^{(a)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(a)}</annotation></semantics></math>, the goal is to predict perceptual ratings across five dimensions <math alttext=\"\\{d_{1},\\dots,d_{5}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mn>5</mn></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{d_{1},\\dots,d_{5}\\}</annotation></semantics></math> from two perspectives <math alttext=\"v\\in\\{\\text{expert},\\text{non-expert}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>expert</mtext><mo>,</mo><mtext>non-expert</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">v\\in\\{\\text{expert},\\text{non-expert}\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "audioeval",
                    "five",
                    "dimensions",
                    "text",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">Qwen-DisQA</span>, a multimodal model for automatic TTA quality assessment. As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S2.F3\" title=\"Figure 3 &#8227; 2.3 Dataset Statistics and Analysis &#8227; 2 AudioEval Dataset &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the model is built on Qwen2.5-Omni and takes as input both the text prompt <math alttext=\"x^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(t)}</annotation></semantics></math> and the generated audio <math alttext=\"x^{(a)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(a)}</annotation></semantics></math>. We design a prompt template that explicitly integrates textual and acoustic information into a unified input sequence as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S2.F3\" title=\"Figure 3 &#8227; 2.3 Dataset Statistics and Analysis &#8227; 2 AudioEval Dataset &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The fused representation is then fed into task-specific prediction heads. Concretely, Qwen-DisQA employs ten independent heads, each corresponding to one dimension-perspective pair <math alttext=\"(d,v)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(d,v)</annotation></semantics></math>.\nEach head is implemented as a linear projection layer followed by a softmax function, producing a probability distribution <math alttext=\"\\hat{P}_{d,v}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>P</mi><mo>^</mo></mover><mrow><mi>d</mi><mo>,</mo><mi>v</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{P}_{d,v}(s)</annotation></semantics></math> over discrete scores <math alttext=\"s\\in\\{1,\\dots,10\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mn>10</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s\\in\\{1,\\dots,10\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "audio",
                    "input",
                    "textual",
                    "text",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We evaluate the automatic assessment model at both the utterance level and the system level. Mean Squared Error (MSE) is employed to quantify prediction error, while Pearson&#8217;s Correlation Coefficient (PCC) is used to assess the degree of correlation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "level",
                    "degree"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of models on our proposed dataset for TTA quality assessment: (1) Zero-shot models, CLAP and Audiobox-Aesthetics, which are widely used for audio evaluation. (2) Fine-tuned pretrained encoders, representing the classical supervised paradigm. Following prior setups <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib11\" title=\"\">11</a>]</cite>, we adapt MusicEval-Baseline and Audio-CLAP-Finetune, which differ in their pretraining datasets. (3) LoRA-fine-tuned multimodal large language models (MLLMs), which include our primary method. We compare our proposed training strategy with two baselines: conventional regression (+R) and simple distribution alignment (+KL).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "alignment",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S3.T2\" title=\"Table 2 &#8227; 3.2 Model Overview &#8227; 3 Proposed Method &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we observe clear differences across model categories. Zero-shot models (CLAP, AES) perform poorly on TTA quality evaluation, as they can only provide coarse judgments and fail to capture quality differences. Traditional supervised fine-tuning methods (e.g., MusicEval-baseline, Clap-SFT) achieve moderate improvements over zero-shot baselines, but their performance remains limited and inconsistent across different dimensions and annotator groups. In contrast, large-model-based fine-tuning approaches show clear advantages. Among them, Qwen-DisQA achieves the best or comparable results on most dimensions, demonstrating stronger correlations and robustness at the utterance level.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "level",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S4.F4\" title=\"Figure 4 &#8227; 4.1 Experimental Details &#8227; 4 Experiments &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further illustrates the detailed performance of Qwen-DisQA. The model achieves significantly higher correlations and lower errors at the system level than at the utterance level, indicating more reliable capability in ranking overall system quality. Moreover, the trends across both expert and non-expert annotations remain consistent with only minor differences, which highlights the stability and generalization ability under diverse annotation conditions.</p>\n\n",
                "matched_terms": [
                    "level",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced AudioEval, the first large-scale multi-dimensional dataset for text-to-audio evaluation, annotated by both experts and non-experts across five perceptual dimensions. Building upon this resource, we proposed Qwen-DisQA, a multimodal scoring model that predicts human-like quality ratings from text&#8211;audio pairs. Experimental results demonstrate that our method achieves superior correlations and robustness compared to existing baselines, providing a reliable and scalable solution for automatic TTA evaluation.</p>\n\n",
                "matched_terms": [
                    "audioeval",
                    "five",
                    "dimensions",
                    "evaluation",
                    "quality"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Utterance-level PCC results of different systems. Models marked with “*” denote direct evaluation without fine-tuning, “†{\\dagger}” indicates fine-tuning on pretrained encoder,\nand “‡{\\ddagger}” corresponds to LoRA fine-tuning on MLLM.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_right ltx_border_tt\" colspan=\"5\">\n<span class=\"ltx_text ltx_font_bold\">Expert</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Non-Expert</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">CE</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">CU</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">PC</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">PQ</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">TA</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">CE</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">CU</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">PC</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">PQ</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">TA</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CLAP <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.338</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.381</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Audiobox-Aesthetics <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib10\" title=\"\">10</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.531</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.213</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.538</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.280</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.255</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.363</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.223</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.306</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">&#8212;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MusicEval-baseline <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib11\" title=\"\">11</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.440</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.436</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.458</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.340</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.442</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.141</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.177</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.283</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.253</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.507</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Audio-Clap-finetune <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8224;</span></sup>\n</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.503</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.533</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.470</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.516</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.521</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.338</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.428</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.428</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.477</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.531</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Qwen2.5-Omni +R <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.704</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.744</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.687</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.700</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.678</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.656</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.725</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.622</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.729</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.731</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Qwen2.5-Omni +KL <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.718</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.752</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.718</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.712</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.731</span></span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.639</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.725</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.652</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.708</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.719</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Qwen-DisQA <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.725</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.752</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.724</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.726</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">0.704</span>\n</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.671</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.735</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.652</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.738</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.742</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "musicevalbaseline",
            "marked",
            "pcc",
            "evaluation",
            "qwendisqa",
            "systems",
            "audioclapfinetune",
            "corresponds",
            "expert",
            "encoder",
            "audioboxaesthetics",
            "lora",
            "finetuning",
            "without",
            "nonexpert",
            "clap",
            "denote",
            "results",
            "indicates",
            "“‡ddagger”",
            "direct",
            "qwen25omni",
            "pretrained",
            "mllm",
            "“†dagger”",
            "model",
            "utterancelevel",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S3.T2\" title=\"Table 2 &#8227; 3.2 Model Overview &#8227; 3 Proposed Method &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we observe clear differences across model categories. Zero-shot models (CLAP, AES) perform poorly on TTA quality evaluation, as they can only provide coarse judgments and fail to capture quality differences. Traditional supervised fine-tuning methods (e.g., MusicEval-baseline, Clap-SFT) achieve moderate improvements over zero-shot baselines, but their performance remains limited and inconsistent across different dimensions and annotator groups. In contrast, large-model-based fine-tuning approaches show clear advantages. Among them, Qwen-DisQA achieves the best or comparable results on most dimensions, demonstrating stronger correlations and robustness at the utterance level.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-audio (TTA) is rapidly advancing, with broad potential in virtual reality, accessibility, and creative media. However, evaluating TTA quality remains difficult: human ratings are costly and limited, while existing objective metrics capture only partial aspects of perceptual quality. To address this gap, we introduce AudioEval, the first large-scale TTA evaluation dataset, containing 4,200 audio samples from 24 systems with 126,000 ratings across five perceptual dimensions, annotated by both experts and non-experts. Based on this resource, we propose Qwen-DisQA, a multimodal scoring model that jointly processes text prompts and generated audio to predict human-like quality ratings. Experiments show its effectiveness in providing reliable and scalable evaluation. The dataset will be made publicly available to accelerate future research.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "qwendisqa",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these rapid advances, the evaluation of TTA systems remains a significant challenge. Current practices often rely on subjective human ratings, typically reported as Mean Opinion Scores (MOS). While human judgment is considered the gold standard, this approach is expensive and time-consuming <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib5\" title=\"\">5</a>]</cite>. In parallel, objective metrics from related domains such as Frechet Inception Distance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib6\" title=\"\">6</a>]</cite> and CLAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib7\" title=\"\">7</a>]</cite> have been applied to TTA evaluation. Although useful, these metrics provide a limited perspective, but do not accurately reflect perceptual quality<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib8\" title=\"\">8</a>]</cite>. Furthermore, the requirement for reference audio in some of these metrics further limits their application.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "evaluation",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic perceptual evaluation has gained attention in the areas of synthetic speech, generated music, and general audio, underscoring both its feasibility and the critical need for reliable evaluation tools for generative models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib12\" title=\"\">12</a>]</cite>. However, predicting human perceptual quality in TTA systems presents considerable challenges. Firstly, the rapid growth of TTA methods has created a varied system landscape, which requires the collection of diverse audio data for rigorous evaluation. Moreover, the inherent complexity of audio and its prompt-driven generation necessitate attention to multiple evaluative dimensions such as aesthetic quality and textual consistency. Additionally, the broad application potential of TTA highlights the need to consider both general audiences and professional users. Together, these factors make the acquisition of reliable data and the development of effective methodologies particularly difficult.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we introduce AudioEval. As far as we know, it is the first dataset for evaluation of TTA-generated audio, enabling automated, dual-perspective, and multi-dimensional assessment. It includes 4,200 audio samples from 24 systems, with 25,200 records and 126,000 dimension-level ratings. Both experts and non-experts contribute, capturing complementary perspectives of audio perception. We extend prior evaluation framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib10\" title=\"\">10</a>]</cite> by annotating each sample along five perceptual dimensions, as summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, to establish a comprehensive evaluation protocol. Building on this dataset, we propose Qwen-DisQA, an automatic quality scoring model based on Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib13\" title=\"\">13</a>]</cite>. It jointly processes textual prompts and generated audio to predict human-like multi-dimensional ratings from expert and non-expert perspectives. Through distribution modeling, it provides more reliable and nuanced automatic evaluations.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "nonexpert",
                    "model",
                    "expert",
                    "evaluation",
                    "qwendisqa",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop an automatic quality scoring model, <span class=\"ltx_text ltx_font_bold\">Qwen-DisQA</span>, which predicts perceptual ratings across five dimensions from text&#8211;audio pairs, capturing quality from both expert and general listener perspectives.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "model",
                    "qwendisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments to explore the capabilities of different methods in the task of automatic TTA quality prediction, and demonstrated the effectiveness of the framework based on large multimodal models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset comprises 4,200 audio clips, totaling approximately 11.7 hours, which were generated by 24 representative TTA systems through inference conditioned on 451 prompts<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Some of the data are obtained from our prior work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib14\" title=\"\">14</a>]</cite>.</span></span></span>. The systems we used include AudioGen&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib15\" title=\"\">15</a>]</cite>, the AudioLDM family&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib16\" title=\"\">16</a>]</cite>, the Make-An-Audio series&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib2\" title=\"\">2</a>]</cite>, the Tango models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib17\" title=\"\">17</a>]</cite>, ConsistencyTTA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib18\" title=\"\">18</a>]</cite>, Auffusion&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib19\" title=\"\">19</a>]</cite>, MAGNeT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib20\" title=\"\">20</a>]</cite>, CTAG&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib21\" title=\"\">21</a>]</cite>, AudioLCM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib22\" title=\"\">22</a>]</cite>, LAFMA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib23\" title=\"\">23</a>]</cite>, PicoAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib24\" title=\"\">24</a>]</cite>, EzAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib25\" title=\"\">25</a>]</cite>, AudioCache&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib26\" title=\"\">26</a>]</cite>, Stable Audio Open&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib27\" title=\"\">27</a>]</cite>, SoundCTM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib28\" title=\"\">28</a>]</cite>, Lumina-T2X&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib29\" title=\"\">29</a>]</cite>, InfiniteAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib30\" title=\"\">30</a>]</cite>, FlashAudio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib31\" title=\"\">31</a>]</cite>, T2A-Feedback&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib32\" title=\"\">32</a>]</cite>, AudioX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib33\" title=\"\">33</a>]</cite>, and ARC-TTA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib34\" title=\"\">34</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (bottom row) demonstrates that expert&#8211;non-expert correlations are weak at the clip level, underscoring perceptual variability, yet they improve substantially when aggregated at the system level. Agreement is strongest for usefulness and alignment, but weaker for complexity and enjoyment. Taken together, these findings reinforce the value of separating expert and non-expert ratings to capture complementary perspectives for analysis and model development.</p>\n\n",
                "matched_terms": [
                    "nonexpert",
                    "model",
                    "expert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">Qwen-DisQA</span>, a multimodal model for automatic TTA quality assessment. As depicted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S2.F3\" title=\"Figure 3 &#8227; 2.3 Dataset Statistics and Analysis &#8227; 2 AudioEval Dataset &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the model is built on Qwen2.5-Omni and takes as input both the text prompt <math alttext=\"x^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(t)}</annotation></semantics></math> and the generated audio <math alttext=\"x^{(a)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(a)}</annotation></semantics></math>. We design a prompt template that explicitly integrates textual and acoustic information into a unified input sequence as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S2.F3\" title=\"Figure 3 &#8227; 2.3 Dataset Statistics and Analysis &#8227; 2 AudioEval Dataset &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The fused representation is then fed into task-specific prediction heads. Concretely, Qwen-DisQA employs ten independent heads, each corresponding to one dimension-perspective pair <math alttext=\"(d,v)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo>,</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(d,v)</annotation></semantics></math>.\nEach head is implemented as a linear projection layer followed by a softmax function, producing a probability distribution <math alttext=\"\\hat{P}_{d,v}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>P</mi><mo>^</mo></mover><mrow><mi>d</mi><mo>,</mo><mi>v</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{P}_{d,v}(s)</annotation></semantics></math> over discrete scores <math alttext=\"s\\in\\{1,\\dots,10\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mn>10</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s\\in\\{1,\\dots,10\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwendisqa",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Configuration.</span> Qwen-DisQA is fine-tuned on the Qwen2.5-Omni 3B model. The weights for KL and MSE losses are set to 0.8 and 1, respectively. The fine-tuning is performed using LoRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib35\" title=\"\">35</a>]</cite> for 10 epochs, and the model with the lowest validation loss is selected for testing.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model",
                    "qwendisqa",
                    "lora",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span> We evaluate the automatic assessment model at both the utterance level and the system level. Mean Squared Error (MSE) is employed to quantify prediction error, while Pearson&#8217;s Correlation Coefficient (PCC) is used to assess the degree of correlation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "pcc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate three categories of models on our proposed dataset for TTA quality assessment: (1) Zero-shot models, CLAP and Audiobox-Aesthetics, which are widely used for audio evaluation. (2) Fine-tuned pretrained encoders, representing the classical supervised paradigm. Following prior setups <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#bib.bib11\" title=\"\">11</a>]</cite>, we adapt MusicEval-Baseline and Audio-CLAP-Finetune, which differ in their pretraining datasets. (3) LoRA-fine-tuned multimodal large language models (MLLMs), which include our primary method. We compare our proposed training strategy with two baselines: conventional regression (+R) and simple distribution alignment (+KL).</p>\n\n",
                "matched_terms": [
                    "models",
                    "musicevalbaseline",
                    "pretrained",
                    "clap",
                    "audioclapfinetune",
                    "evaluation",
                    "audioboxaesthetics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14570v1#S4.F4\" title=\"Figure 4 &#8227; 4.1 Experimental Details &#8227; 4 Experiments &#8227; AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further illustrates the detailed performance of Qwen-DisQA. The model achieves significantly higher correlations and lower errors at the system level than at the utterance level, indicating more reliable capability in ranking overall system quality. Moreover, the trends across both expert and non-expert annotations remain consistent with only minor differences, which highlights the stability and generalization ability under diverse annotation conditions.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "nonexpert",
                    "model",
                    "qwendisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced AudioEval, the first large-scale multi-dimensional dataset for text-to-audio evaluation, annotated by both experts and non-experts across five perceptual dimensions. Building upon this resource, we proposed Qwen-DisQA, a multimodal scoring model that predicts human-like quality ratings from text&#8211;audio pairs. Experimental results demonstrate that our method achieves superior correlations and robustness compared to existing baselines, providing a reliable and scalable solution for automatic TTA evaluation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "evaluation",
                    "model",
                    "qwendisqa"
                ]
            }
        ]
    }
}