{
    "S3.T1": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 1: Statistics of pre-training data. Hours are shown in thousands (k).",
        "body": "Dataset\nTotal (h)\nReal (h)\nSynthetic (h)\n\n\n\n\nEnglish Interleaved\n690k\n624k\n66k\n\n\nChinese Interleaved\n952k\n876k\n76k\n\n\nUnsupervised\n2,303k\n2,303k\n0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Total (h)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Real (h)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Synthetic (h)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">English Interleaved</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">690k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">624k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">66k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Chinese Interleaved</td>\n<td class=\"ltx_td ltx_align_right\">952k</td>\n<td class=\"ltx_td ltx_align_right\">876k</td>\n<td class=\"ltx_td ltx_align_right\">76k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Unsupervised</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">2,303k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">2,303k</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "data",
            "624k",
            "876k",
            "chinese",
            "pretraining",
            "real",
            "total",
            "690k",
            "statistics",
            "2303k",
            "76k",
            "thousands",
            "952k",
            "hours",
            "dataset",
            "66k",
            "english",
            "interleaved",
            "unsupervised",
            "synthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A summary of dataset statistics is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S3.T1\" title=\"Table 1 &#8227; 3.1.1 Data Collection and Processing &#8227; 3.1 Pre-training &#8227; 3 Training Strategy &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We begin with approximately 9 million hours of real-world audio data collected from the internet. To remove non-speech content, we apply a custom voice activity detection (VAD) pipeline based on <span class=\"ltx_text ltx_font_typewriter\">pyannote</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib35\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib5\" title=\"\">2023</a>)</cite>, resulting in roughly 4 million hours of speech. These data are organized into two categories according to source type: (1) <em class=\"ltx_emph ltx_font_italic\">interleaved speech&#8211;text pre-training</em>, drawn primarily from podcasts, and (2) <em class=\"ltx_emph ltx_font_italic\">unsupervised speech pre-training</em>, drawn primarily from video content. Podcasts are chosen for interleaved pre-training because they typically provide cleaner recordings and clearer speech, enabling automatic speech recognition (ASR) systems to generate more reliable transcripts. In contrast, video sources, while more diverse and noisier, are better suited for the unsupervised pre-training setting, where robustness to challenging acoustic conditions is essential.</p>\n\n",
                "matched_terms": [
                    "interleaved",
                    "data",
                    "pretraining",
                    "unsupervised",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the interleaved task, we first apply automatic speech recognition (ASR) to obtain text transcripts. Connectionist Temporal Classification (CTC) word alignment is then used to segment the audio into random-length chunks of 3&#8211;6 seconds. Each chunk contains either the corresponding audio segment or its transcribed text, and sequences are constructed by interleaving the two modalities. For unsupervised speech pre-training, we simply use full-length audio segments without transcript.</p>\n\n",
                "matched_terms": [
                    "unsupervised",
                    "interleaved",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the low knowledge density inherent in natural speech corpora, we also synthesize additional interleaved data from high-quality text corpora. Following the approach of <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib54\" title=\"\">2024b</a>)</cite>, we use FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese. These texts are converted into audio using the CosyVoice 2 TTS system <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, producing large-scale synthetic speech&#8211;text pairs that enrich the training corpus.</p>\n\n",
                "matched_terms": [
                    "english",
                    "interleaved",
                    "data",
                    "chinese",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the second stage, we unfreeze a larger portion of the model to allow cross-modal adaptation. We experiment with three configurations: (1) unfreezing the entire model and training all parameters jointly, (2) unfreezing only the shared transformer layers while keeping the text embeddings, text-specific layers, and text LM head frozen, and (3) gradually unfreezing the shared layers in reverse order (from last to first). Since unfreezing text parameters risks degradation of textual abilities, we incorporate additional text-only pre-training data to preserve the model&#8217;s linguistic competence. Specifically, we include FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese, filtering entries with quality scores <math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "pretraining",
                    "data",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, we end up with over 1500k question&#8211;answer pairs for supervised fine-tuning, comprising approximately 650k English pairs and 860k Chinese pairs.</p>\n\n",
                "matched_terms": [
                    "total",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 2: Supervised fine-tuning datasets used in our experiments.",
        "body": "Dataset\nLanguage\nUsed Samples\n\n\n\n\nOpenHermes-2.5\nEN\n200k\n\n\nOpenHermes-2.5 (Chinese translated)\nZH\n200k\n\n\nMagpie-Llama-3.1-Pro-MT-300K-Filtered\nEN\n300k\n\n\nMagpie-Qwen2-Pro-200K-Chinese\nZH\n200k\n\n\nBAAI_OL-CC\nZH\n11.7k\n\n\nRefGPT-Fact\nZH\n50k\n\n\nCOIG-CQIA\nZH\n45k\n\n\nRuozhiba\nZH\n1.4k\n\n\nHuatuo26M-Lite\nZH\n30k\n\n\nAlign-Anything-Instruction-100K\nEN&ZH\n100k\n\n\nChinese-DeepSeek-R1-Distill-SFT\nZH\n110k\n\n\nChain-of-Thought-ShareGPT\nEN\n7.14k",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Used Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">OpenHermes-2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">EN</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">200k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OpenHermes-2.5 (Chinese translated)</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">200k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Magpie-Llama-3.1-Pro-MT-300K-Filtered</td>\n<td class=\"ltx_td ltx_align_center\">EN</td>\n<td class=\"ltx_td ltx_align_right\">300k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Magpie-Qwen2-Pro-200K-Chinese</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">200k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BAAI_OL-CC</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">11.7k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">RefGPT-Fact</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">50k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">COIG-CQIA</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">45k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ruozhiba</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">1.4k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Huatuo26M-Lite</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">30k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Align-Anything-Instruction-100K</td>\n<td class=\"ltx_td ltx_align_center\">EN&amp;ZH</td>\n<td class=\"ltx_td ltx_align_right\">100k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Chinese-DeepSeek-R1-Distill-SFT</td>\n<td class=\"ltx_td ltx_align_center\">ZH</td>\n<td class=\"ltx_td ltx_align_right\">110k</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Chain-of-Thought-ShareGPT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">EN</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">7.14k</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "datasets",
            "coigcqia",
            "117k",
            "experiments",
            "translated",
            "chinese",
            "used",
            "magpiellama31promt300kfiltered",
            "samples",
            "30k",
            "refgptfact",
            "chainofthoughtsharegpt",
            "ruozhiba",
            "14k",
            "chinesedeepseekr1distillsft",
            "enzh",
            "714k",
            "45k",
            "alignanythinginstruction100k",
            "300k",
            "dataset",
            "language",
            "200k",
            "110k",
            "huatuo26mlite",
            "finetuning",
            "50k",
            "openhermes25",
            "magpieqwen2pro200kchinese",
            "baaiolcc",
            "100k",
            "supervised",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Because high-quality supervised fine-tuning data for speech assistants are scarce in natural settings, we construct such data synthetically. Our process begins with existing open-source text-based supervised fine-tuning datasets listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Data Adaptation and Construction &#8227; 3.2 Supervised Fine-tuning &#8227; 3 Training Strategy &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.</p>\n\n",
                "matched_terms": [
                    "our",
                    "experiments",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a novel approach that enables large language models to natively model speech while largely retaining their text-based capabilities. Our method builds on a pretrained text LLM backbone but diverges from prior approaches through a modality-specific layer-splitting scheme and a frozen pretraining strategy. This design preserves the backbone&#8217;s linguistic knowledge while equipping the model with native speech understanding and generation abilities comparable to existing text-guided systems. As a result, our model can directly produce high-quality speech without relying on intermediate text representations, establishing a new paradigm for end-to-end speech-to-speech generation. Importantly, because the majority of knowledge remains in the pretrained text model, our approach avoids dependence on large-scale, knowledge-intensive speech datasets. Instead, alignment transfers reasoning, world knowledge, and generalization abilities from the text backbone to the speech modality.</p>\n\n",
                "matched_terms": [
                    "our",
                    "language",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments and ablation studies to validate the effectiveness of our approach, demonstrating advanced speech&#8211;text cross-modal alignment and textual ability preservation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "experiments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "finetuning",
                    "dataset",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, we end up with over 1500k question&#8211;answer pairs for supervised fine-tuning, comprising approximately 650k English pairs and 860k Chinese pairs.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "supervised",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the pretrained model, we conduct supervised fine-tuning on the constructed multimodal dataset for two epochs. Training is performed with the AdamW optimizer, using a cosine learning rate schedule that decays from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>. We use a batch size of 8, apply a weight decay of 0.1, and set the maximum context length to 10,240 tokens with sequence packing.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "dataset",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the experimental evaluation of our encoder and decoder components. For the encoder, a crucial aspect is the preservation of semantic information. To assess this,\nwe fine-tuned a Qwen3-0.6B model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> for ASR. Distinct from embedding-based approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib49\" title=\"\">2021</a>)</cite>, our method directly leverages discrete codebook IDs generated by various encoders as input, better aligning with the Large Language Model (LLM) paradigm. This ASR model was trained on the 960-hour Librispeech training dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib34\" title=\"\">2015</a>)</cite>. We then evaluated the corresponding Word Error Rate (WER) on the test sets (test-clean, test-other, and dev-clean). Each model was trained for 100k steps using a batch size of 128 and a learning rate of 1e-4, and we report the lowest WER achieved. Our baselines include codecs designed to capture semantic information, such as Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> and XCodec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib50\" title=\"\">2025</a>)</cite>, as well as ASR-trained codecs like GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite> and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. Ours represent our proposed streaming model, which is further fine-tuned from GLM-4-Voice.</p>\n\n",
                "matched_terms": [
                    "our",
                    "language",
                    "100k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "our",
                    "datasets",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "our",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">speech modeling ability</em>, we use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib18\" title=\"\">2023</a>)</cite> together with our in-house Chinese counterpart, zh-StoryCloze. These benchmarks test the model&#8217;s capacity to reason over and generate coherent speech continuations.</p>\n\n",
                "matched_terms": [
                    "our",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech codecs are crucial for speech large language models (SLMs) and can be grouped into two categories. Neural acoustic codecs based on (R)VQ-GAN optimize reconstruction loss to preserve fine-grained acoustic details <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib52\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib13\" title=\"\">2022</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib24\" title=\"\">2023</a>)</cite>, but their tokens often lack semantic coherence when used for language modeling. In contrast, semantic-oriented codecs adopt a single-layer VQ to encode linguistic content and recover timbre with generative modules such as conditional flow matching <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. While trading off perfect fidelity, this design yields tokens better suited for semantic modeling. We therefore follow the latter approach and further enhance it with streaming encoder&#8211;decoder modules for real-time interaction.</p>\n\n",
                "matched_terms": [
                    "language",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing models mostly depend on text guidance for speech generation. For instance, SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> integrates large language models with discrete speech representations but requires text-based prompts to guide speech generation. Similarly, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> employs a full-duplex spoken dialogue framework, generating speech tokens from a neural audio codec, yet it still necessitates text instructions for generating speech responses. Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib8\" title=\"\">2024</a>)</cite> accepts diverse audio inputs and outputs text, relying on textual prompts for speech understanding. LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib15\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> extend LLMs to process speech inputs and generate speech outputs directly, but they continue to depend on text prompts to guide the interaction. Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> fine-tunes language models to generate text and speech responses simultaneously using instruction datasets, yet the quality of both text and speech responses is limited without prior speech pre-training. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> further advances toward speech-to-speech interaction by integrating speech input and output with large language models, but it still fundamentally relies on textual supervision for alignment and instruction following. These models demonstrate progress toward speech-to-speech interaction but still require text guidance for effective performance.</p>\n\n",
                "matched_terms": [
                    "language",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on integrating speech into decoder-only LLMs has emphasized retaining text capabilities while extending to new modalities. A common strategy is to freeze most LLM parameters and train lightweight adapters. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> proposed <em class=\"ltx_emph ltx_font_italic\">Freeze-Omni</em>, which augments a frozen LLM backbone with speech encoder and decoder modules, while <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib10\" title=\"\">2025</a>)</cite> introduced <em class=\"ltx_emph ltx_font_italic\">SpeechVerse</em>, combining frozen speech and text backbones with adapters to enable zero-shot speech processing from text instructions. Beyond such frozen-backbone designs, other works adopt progressive or staged adaptation. <cite class=\"ltx_cite ltx_citemacro_citet\">Xie &amp; Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> presented <em class=\"ltx_emph ltx_font_italic\">Mini-Omni</em>, which proceeds in phases: first learning speech adapters with the LLM frozen, then performing LM-only fine-tuning to align modalities, and finally unfreezing all but the audio encoder for joint multimodal training. Together, these studies show that freezing LLM backbone helps preserve language modeling ability, while progressive unfreezing provides a pathway for more flexible and effective multimodal integration.</p>\n\n",
                "matched_terms": [
                    "language",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a large language model capable of true speech-to-speech interaction without intermediate text, advancing the state of spoken dialogue systems beyond cascaded and text-guided frameworks. Our modality-based layer-splitting and frozen pre-training strategies enable the transfer of linguistic and reasoning knowledge to speech modality from pretrained text LLMs while preserving text abilities, avoiding the degradation often observed in multimodal adaptation. Our model achieves state-of-the-art results in spoken question answering, while supporting both text and speech as native input and output modalities. This work demonstrates that end-to-end speech modeling can reach near parity with text-guided methods while overcoming their inherent limitations in latency and expressivity. Looking forward, we envision speech-native models as the foundation of future human&#8211;AI interaction, supporting seamless, multimodal dialogue across diverse languages and contexts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below are the five random samples we used to calculate the similarity score, with all text content aligned and presented in English.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "used"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 3: Evaluation results of our speech encoder",
        "body": "Model\n\n\nFrame\nRate (Hz)\n\n\nBPS\nStreaming\nWER (%) ↓\\downarrow\n\n\n\ntest-clean\ndev-clean\noverall\n\n\nMimi-8\n12.5\n1100\n×\\times\n9.65\n9.67\n14.45\n\n\nXCodec2.0\n50\n800\n×\\times\n14.17\n13.82\n20.07\n\n\nCosyvoice\n25\n300\n×\\times\n10.15\n9.64\n14.21\n\n\nCosyvoice2\n25\n325\n×\\times\n9.45\n9.42\n13.78\n\n\nGLM-4-Voice\n12.5\n175\nChunk(2s)\n6.59\n6.07\n9.17\n\n\nOurs\n12.5\n175\n✓\\checkmark\n7.89\n7.29\n10.80",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Frame</span>\n<span class=\"ltx_p\">Rate (Hz)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">BPS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Streaming</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">test-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">dev-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">overall</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Mimi-8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">XCodec2.0</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">800</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">14.17</td>\n<td class=\"ltx_td ltx_align_center\">13.82</td>\n<td class=\"ltx_td ltx_align_center\">20.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Cosyvoice</td>\n<td class=\"ltx_td ltx_align_center\">25</td>\n<td class=\"ltx_td ltx_align_center\">300</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">10.15</td>\n<td class=\"ltx_td ltx_align_center\">9.64</td>\n<td class=\"ltx_td ltx_align_center\">14.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Cosyvoice2</td>\n<td class=\"ltx_td ltx_align_center\">25</td>\n<td class=\"ltx_td ltx_align_center\">325</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">9.45</td>\n<td class=\"ltx_td ltx_align_center\">9.42</td>\n<td class=\"ltx_td ltx_align_center\">13.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GLM-4-Voice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">175</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Chunk(2s)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">9.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">12.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">175</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">10.80</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "frame",
            "cosyvoice2",
            "wer",
            "speech",
            "devclean",
            "cosyvoice",
            "streaming",
            "evaluation",
            "chunk2s",
            "rate",
            "results",
            "bps",
            "overall",
            "encoder",
            "glm4voice",
            "↓downarrow",
            "xcodec20",
            "testclean",
            "✓checkmark",
            "ours",
            "mimi8",
            "×times",
            "our",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is one of the most natural and intuitive modalities for human&#8211;computer interaction, making spoken dialogue systems a central focus of contemporary AI research. Traditional systems for spoken interaction are typically implemented using a cascaded pipeline: speech input is first transcribed into text, a text-based large language model (LLM) generates a response, and the output is subsequently converted into audio through a text-to-speech (TTS) module (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf1\" title=\"Figure 1(a) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>). While this architecture leverages the full reasoning capacity of text-based LLMs, it inevitably discards information encoded in the original speech signal and constrains the system to produce only responses that can be faithfully represented in text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end attempts such as GSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib25\" title=\"\">2021</a>)</cite> and AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib4\" title=\"\">2023</a>)</cite> demonstrated that speech could be modeled directly, but these works remained largely confined to experimental dialogue continuation tasks and faced challenges in scaling into full-featured assistants. Later work shifted toward text-guided generation as a compromise: SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> used a chain-of-modality design, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> and PSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Mitsui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib30\" title=\"\">2024</a>)</cite> achieved low-latency streaming through parallel speech&#8211;text generation. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> advanced this further by interleaving text and speech in chunk-based generation (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf2\" title=\"Figure 1(b) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), reaching near-text-level performance in streaming dialogue. Importantly, while GLM-4-Voice primarily relies on text-guided responses, it also supports direct speech generation&#8212;but its direct mode remains noticeably weaker than its text-guided counterpart.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "speech",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a novel approach that enables large language models to natively model speech while largely retaining their text-based capabilities. Our method builds on a pretrained text LLM backbone but diverges from prior approaches through a modality-specific layer-splitting scheme and a frozen pretraining strategy. This design preserves the backbone&#8217;s linguistic knowledge while equipping the model with native speech understanding and generation abilities comparable to existing text-guided systems. As a result, our model can directly produce high-quality speech without relying on intermediate text representations, establishing a new paradigm for end-to-end speech-to-speech generation. Importantly, because the majority of knowledge remains in the pretrained text model, our approach avoids dependence on large-scale, knowledge-intensive speech datasets. Instead, alignment transfers reasoning, world knowledge, and generalization abilities from the text backbone to the speech modality.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a <span class=\"ltx_text ltx_font_bold\">true speech-to-speech large language model</span> that achieves <span class=\"ltx_text ltx_font_bold\">state-of-the-art</span> performance on speech-to-speech benchmarks without relying on any intermediate text guidance. At the same time, the model natively supports both text and speech as input and output modalities, thereby narrowing the gap between spoken and written interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Transformer backbone, our design goal is to preserve the original text capabilities of LLMs while augmenting LLMs with speech understanding and generation. Existing approaches typically rely on the Depth Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> that generates multiple VQ tokens as a single input, or alternatively, expand the vocabulary to directly encode speech tokens into the input sequence <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. However, our preliminary study on speechgpt2-preview<cite class=\"ltx_cite ltx_citemacro_citep\">(Open-Moss, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib33\" title=\"\">2025</a>)</cite> revealed that the hidden-state alignment between a sentence and its corresponding speech sequence gradually deteriorates in deeper layers: while strong diagonal similarity emerges in lower layers, it vanishes in later layers.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Architecture &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, by examining the hidden-state similarity between the same spoken utterance and its corresponding text across different layers, we observe that in a 28-layer model, the similarity steadily increases in the first 11 layers, fluctuates and gradually stabilizes in the following 14 layers, and then decreases in the final 3 layers. This finding suggests that as the model is trained, the representations of text and speech become increasingly fused within the first 25 Transformer blocks, but gradually diverge in the last four layers.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by this, we introduce a modality-based layer split at the 32<sup class=\"ltx_sup\">nd</sup> block of our 36-layer Transformer. At this point, the shared hidden state is routed into modality-specific branches: one branch continues through the final four layers to predict text tokens, while the other routes into a parallel four-layer stack to predict speech tokens.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This split-then-specialize design allows the model to leverage the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> layers for joint multimodality fusion, while reserving the final layers for modality-specific generation. As a result, the architecture enhances cross-modality transfer, enabling the system to inherit the capabilities of textually-pretrained LLMs and express it natively in the speech modality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech tokenizer is designed with four key objectives:\n(1) to achieve a single-codebook, low-bitrate representation for efficient autoregressive generation and simplified context management;\n(2) to maximize semantic content in order to facilitate knowledge transfer from text to speech;\n(3) to preserve sufficient paralinguistic detail to enable faithful reconstruction of human speech; and\n(4) to support full streaming operation for low-latency processing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "streaming",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete speech tokenizers are commonly trained with reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib17\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib57\" title=\"\">2023c</a>)</cite> or self-supervised discovery methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib40\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib28\" title=\"\">2024</a>)</cite>. However, prior work has observed that tokens optimized primarily for reconstruction are often suboptimal for LLM learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite>. To address this, and following the design of CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, we adopt automatic speech recognition (ASR) as the sole training objective for our tokenizer encoder. Our encoder is further trained based on the GLM-4-Voice Tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, but we modify it to be fully causal rather than block-causal, thereby ensuring true streaming support.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "speech",
                    "our",
                    "cosyvoice",
                    "encoder",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For decoding, we adopt the flow-matching architecture introduced in CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib27\" title=\"\">2022</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. While CosyVoice 2 employs chunk-attention to improve efficiency, this mechanism introduces undesirable time delays. To mitigate this issue, we compress the chunk size, which significantly reduces latency while maintaining reconstruction quality. This modification makes our tokenizer particularly well-suited for streaming dialogue systems that demand both high fidelity and low response delay.</p>\n\n",
                "matched_terms": [
                    "our",
                    "streaming",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of pre-training is to introduce a speech modality into a pretrained text-based LLM while preserving its original text capabilities. To this end, we initialize our model from Qwen-3-8B<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> and adopt a two-stage pre-training strategy using a large-scale, high-quality speech corpus. The procedure is outlined below.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the low knowledge density inherent in natural speech corpora, we also synthesize additional interleaved data from high-quality text corpora. Following the approach of <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib54\" title=\"\">2024b</a>)</cite>, we use FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese. These texts are converted into audio using the CosyVoice 2 TTS system <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, producing large-scale synthetic speech&#8211;text pairs that enrich the training corpus.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize our model from the Qwen3-8B backbone and employ a two-stage pre-training pipeline designed to introduce the speech modality while preserving the model&#8217;s text capabilities.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we freeze all parameters of the Qwen-3-8B backbone and train only the newly introduced speech-related components, including the speech token embeddings, speech-specific transformer layers, and the speech language modeling (LM) head. This stage serves to initialize speech parameters and establish stable alignment with the pretrained text representations. Training is conducted for approximately one epoch using the AdamW optimizer with cosine learning rate scheduling. The initial learning rate is set to <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, with a batch size of 2.2M tokens, weight decay of 0.1, and a context length of 14,336 tokens.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because high-quality supervised fine-tuning data for speech assistants are scarce in natural settings, we construct such data synthetically. Our process begins with existing open-source text-based supervised fine-tuning datasets listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Data Adaptation and Construction &#8227; 3.2 Supervised Fine-tuning &#8227; 3 Training Strategy &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although LLM-based TTS systems produce highly natural speech, they are susceptible to synthesis errors. To mitigate this, we apply automatic quality filtering using SenseVoice-Small ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib1\" title=\"\">2024</a>)</cite>. Specifically, we discard entries whose ASR transcripts exhibit a word error rate (WER) <math alttext=\"\\geq 0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 0.2</annotation></semantics></math> relative to the original text.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the pretrained model, we conduct supervised fine-tuning on the constructed multimodal dataset for two epochs. Training is performed with the AdamW optimizer, using a cosine learning rate schedule that decays from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>. We use a batch size of 8, apply a weight decay of 0.1, and set the maximum context length to 10,240 tokens with sequence packing.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further strengthen cross-modal alignment between speech and text, fine-tuning incorporates four input&#8211;output modality configurations: <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>, <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, and <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>. The modality pairing is controlled by system prompts, while the underlying content remains identical across configurations. This design ensures that the model learns to handle both unimodal and cross-modal interactions, enabling it to accept text or speech as input and generate either text or speech as output within a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the experimental evaluation of our encoder and decoder components. For the encoder, a crucial aspect is the preservation of semantic information. To assess this,\nwe fine-tuned a Qwen3-0.6B model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> for ASR. Distinct from embedding-based approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib49\" title=\"\">2021</a>)</cite>, our method directly leverages discrete codebook IDs generated by various encoders as input, better aligning with the Large Language Model (LLM) paradigm. This ASR model was trained on the 960-hour Librispeech training dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib34\" title=\"\">2015</a>)</cite>. We then evaluated the corresponding Word Error Rate (WER) on the test sets (test-clean, test-other, and dev-clean). Each model was trained for 100k steps using a batch size of 128 and a learning rate of 1e-4, and we report the lowest WER achieved. Our baselines include codecs designed to capture semantic information, such as Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> and XCodec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib50\" title=\"\">2025</a>)</cite>, as well as ASR-trained codecs like GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite> and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. Ours represent our proposed streaming model, which is further fine-tuned from GLM-4-Voice.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "ours",
                    "wer",
                    "evaluation",
                    "rate",
                    "our",
                    "model",
                    "devclean",
                    "testclean",
                    "cosyvoice",
                    "encoder",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "cosyvoice",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "frame",
                    "streaming",
                    "ours",
                    "overall",
                    "wer",
                    "evaluation",
                    "rate",
                    "speech",
                    "our",
                    "mimi8",
                    "model",
                    "bps",
                    "cosyvoice",
                    "encoder",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the effectiveness of our pre-training strategy, we evaluate the resulting speech-enabled model on both speech modeling and text understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">speech modeling ability</em>, we use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib18\" title=\"\">2023</a>)</cite> together with our in-house Chinese counterpart, zh-StoryCloze. These benchmarks test the model&#8217;s capacity to reason over and generate coherent speech continuations.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dual evaluation allows us to verify that (i) the model acquires robust speech modeling abilities, while (ii) maintaining the original linguistic competence of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of our SFT model, we assess QA ability using <span class=\"ltx_text ltx_font_bold\">LLaMA-Question</span>, <span class=\"ltx_text ltx_font_bold\">Trivia QA</span>, and <span class=\"ltx_text ltx_font_bold\">Web Questions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib31\" title=\"\">2024</a>; Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib22\" title=\"\">2017</a>; Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib6\" title=\"\">2022</a>)</cite>. And the quality of generated speech is evaluated with <span class=\"ltx_text ltx_font_bold\">UTMOS</span> (MOS style evaluation) <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib39\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5.T7\" title=\"In 5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> highlight three main findings:\n(1) <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> improves both speech modeling and textual ability preservation;\n(2) <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> provides substantial additional gains;\n(3) unfreezing strategies yield relatively small differences.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the ablation confirms that modality separation and freezing text parameters during pre-training are both critical to balancing speech learning with text knowledge preservation. While different unfreezing schedules provide slight trade-offs, their impact is minor compared to the gains from <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> and <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> themselves.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech codecs are crucial for speech large language models (SLMs) and can be grouped into two categories. Neural acoustic codecs based on (R)VQ-GAN optimize reconstruction loss to preserve fine-grained acoustic details <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib52\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib13\" title=\"\">2022</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib24\" title=\"\">2023</a>)</cite>, but their tokens often lack semantic coherence when used for language modeling. In contrast, semantic-oriented codecs adopt a single-layer VQ to encode linguistic content and recover timbre with generative modules such as conditional flow matching <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. While trading off perfect fidelity, this design yields tokens better suited for semantic modeling. We therefore follow the latter approach and further enhance it with streaming encoder&#8211;decoder modules for real-time interaction.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing models mostly depend on text guidance for speech generation. For instance, SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> integrates large language models with discrete speech representations but requires text-based prompts to guide speech generation. Similarly, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> employs a full-duplex spoken dialogue framework, generating speech tokens from a neural audio codec, yet it still necessitates text instructions for generating speech responses. Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib8\" title=\"\">2024</a>)</cite> accepts diverse audio inputs and outputs text, relying on textual prompts for speech understanding. LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib15\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> extend LLMs to process speech inputs and generate speech outputs directly, but they continue to depend on text prompts to guide the interaction. Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> fine-tunes language models to generate text and speech responses simultaneously using instruction datasets, yet the quality of both text and speech responses is limited without prior speech pre-training. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> further advances toward speech-to-speech interaction by integrating speech input and output with large language models, but it still fundamentally relies on textual supervision for alignment and instruction following. These models demonstrate progress toward speech-to-speech interaction but still require text guidance for effective performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on integrating speech into decoder-only LLMs has emphasized retaining text capabilities while extending to new modalities. A common strategy is to freeze most LLM parameters and train lightweight adapters. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> proposed <em class=\"ltx_emph ltx_font_italic\">Freeze-Omni</em>, which augments a frozen LLM backbone with speech encoder and decoder modules, while <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib10\" title=\"\">2025</a>)</cite> introduced <em class=\"ltx_emph ltx_font_italic\">SpeechVerse</em>, combining frozen speech and text backbones with adapters to enable zero-shot speech processing from text instructions. Beyond such frozen-backbone designs, other works adopt progressive or staged adaptation. <cite class=\"ltx_cite ltx_citemacro_citet\">Xie &amp; Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> presented <em class=\"ltx_emph ltx_font_italic\">Mini-Omni</em>, which proceeds in phases: first learning speech adapters with the LLM frozen, then performing LM-only fine-tuning to align modalities, and finally unfreezing all but the audio encoder for joint multimodal training. Together, these studies show that freezing LLM backbone helps preserve language modeling ability, while progressive unfreezing provides a pathway for more flexible and effective multimodal integration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a large language model capable of true speech-to-speech interaction without intermediate text, advancing the state of spoken dialogue systems beyond cascaded and text-guided frameworks. Our modality-based layer-splitting and frozen pre-training strategies enable the transfer of linguistic and reasoning knowledge to speech modality from pretrained text LLMs while preserving text abilities, avoiding the degradation often observed in multimodal adaptation. Our model achieves state-of-the-art results in spoken question answering, while supporting both text and speech as native input and output modalities. This work demonstrates that end-to-end speech modeling can reach near parity with text-guided methods while overcoming their inherent limitations in latency and expressivity. Looking forward, we envision speech-native models as the foundation of future human&#8211;AI interaction, supporting seamless, multimodal dialogue across diverse languages and contexts.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement gradual layer-wise unfreezing, we assign each transformer layer its own learning rate with a <em class=\"ltx_emph ltx_font_italic\">delayed warmup&#8211;cosine</em> schedule. Let <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> denote the global training step, the model contain <math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math> layers to be unfrozen indexed by <math alttext=\"i\\in\\{0,\\dots,N-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in\\{0,\\dots,N-1\\}</annotation></semantics></math> (with <math alttext=\"i=N-1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">i=N-1</annotation></semantics></math> denoting the final layer), and define:</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 4: Evaluation results of our speech decoder",
        "body": "Model\nFrame rate\nSeed-TTS-Eval-EN\nSeed-TTS-Eval-ZH\n\n\nWER ↓\\downarrow\n\nSIM ↑\\uparrow\n\nDNSMOS ↑\\uparrow\n\nWER ↓\\downarrow\n\nSIM ↑\\uparrow\n\nDNSMOS ↑\\uparrow\n\n\n\n\n\nCosyvoice\n25hz\n10.53\n0.66\n3.07\n11.29\n0.74\n3.21\n\n\nCosyvoice2\n25hz\n4.63\n0.68\n3.09\n3.11\n0.75\n3.22\n\n\nOurs\n12.5hz\n4.14\n0.67\n3.10\n2.86\n0.73\n3.24",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\">Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\">Frame rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Seed-TTS-Eval-EN</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Seed-TTS-Eval-ZH</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">DNSMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">DNSMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Cosyvoice</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">25hz</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Cosyvoice2</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">25hz</th>\n<td class=\"ltx_td ltx_align_center\">4.63</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\">3.09</td>\n<td class=\"ltx_td ltx_align_center\">3.11</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.75</span></td>\n<td class=\"ltx_td ltx_align_center\">3.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Ours</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">12.5hz</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.24</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "frame",
            "cosyvoice2",
            "wer",
            "speech",
            "dnsmos",
            "cosyvoice",
            "↑uparrow",
            "evaluation",
            "rate",
            "results",
            "seedttsevalen",
            "125hz",
            "↓downarrow",
            "decoder",
            "sim",
            "ours",
            "25hz",
            "seedttsevalzh",
            "our",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is one of the most natural and intuitive modalities for human&#8211;computer interaction, making spoken dialogue systems a central focus of contemporary AI research. Traditional systems for spoken interaction are typically implemented using a cascaded pipeline: speech input is first transcribed into text, a text-based large language model (LLM) generates a response, and the output is subsequently converted into audio through a text-to-speech (TTS) module (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf1\" title=\"Figure 1(a) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>). While this architecture leverages the full reasoning capacity of text-based LLMs, it inevitably discards information encoded in the original speech signal and constrains the system to produce only responses that can be faithfully represented in text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a novel approach that enables large language models to natively model speech while largely retaining their text-based capabilities. Our method builds on a pretrained text LLM backbone but diverges from prior approaches through a modality-specific layer-splitting scheme and a frozen pretraining strategy. This design preserves the backbone&#8217;s linguistic knowledge while equipping the model with native speech understanding and generation abilities comparable to existing text-guided systems. As a result, our model can directly produce high-quality speech without relying on intermediate text representations, establishing a new paradigm for end-to-end speech-to-speech generation. Importantly, because the majority of knowledge remains in the pretrained text model, our approach avoids dependence on large-scale, knowledge-intensive speech datasets. Instead, alignment transfers reasoning, world knowledge, and generalization abilities from the text backbone to the speech modality.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a <span class=\"ltx_text ltx_font_bold\">true speech-to-speech large language model</span> that achieves <span class=\"ltx_text ltx_font_bold\">state-of-the-art</span> performance on speech-to-speech benchmarks without relying on any intermediate text guidance. At the same time, the model natively supports both text and speech as input and output modalities, thereby narrowing the gap between spoken and written interaction.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Transformer backbone, our design goal is to preserve the original text capabilities of LLMs while augmenting LLMs with speech understanding and generation. Existing approaches typically rely on the Depth Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> that generates multiple VQ tokens as a single input, or alternatively, expand the vocabulary to directly encode speech tokens into the input sequence <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. However, our preliminary study on speechgpt2-preview<cite class=\"ltx_cite ltx_citemacro_citep\">(Open-Moss, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib33\" title=\"\">2025</a>)</cite> revealed that the hidden-state alignment between a sentence and its corresponding speech sequence gradually deteriorates in deeper layers: while strong diagonal similarity emerges in lower layers, it vanishes in later layers.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Architecture &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, by examining the hidden-state similarity between the same spoken utterance and its corresponding text across different layers, we observe that in a 28-layer model, the similarity steadily increases in the first 11 layers, fluctuates and gradually stabilizes in the following 14 layers, and then decreases in the final 3 layers. This finding suggests that as the model is trained, the representations of text and speech become increasingly fused within the first 25 Transformer blocks, but gradually diverge in the last four layers.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by this, we introduce a modality-based layer split at the 32<sup class=\"ltx_sup\">nd</sup> block of our 36-layer Transformer. At this point, the shared hidden state is routed into modality-specific branches: one branch continues through the final four layers to predict text tokens, while the other routes into a parallel four-layer stack to predict speech tokens.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This split-then-specialize design allows the model to leverage the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> layers for joint multimodality fusion, while reserving the final layers for modality-specific generation. As a result, the architecture enhances cross-modality transfer, enabling the system to inherit the capabilities of textually-pretrained LLMs and express it natively in the speech modality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech tokenizer is designed with four key objectives:\n(1) to achieve a single-codebook, low-bitrate representation for efficient autoregressive generation and simplified context management;\n(2) to maximize semantic content in order to facilitate knowledge transfer from text to speech;\n(3) to preserve sufficient paralinguistic detail to enable faithful reconstruction of human speech; and\n(4) to support full streaming operation for low-latency processing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete speech tokenizers are commonly trained with reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib17\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib57\" title=\"\">2023c</a>)</cite> or self-supervised discovery methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib40\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib28\" title=\"\">2024</a>)</cite>. However, prior work has observed that tokens optimized primarily for reconstruction are often suboptimal for LLM learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite>. To address this, and following the design of CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, we adopt automatic speech recognition (ASR) as the sole training objective for our tokenizer encoder. Our encoder is further trained based on the GLM-4-Voice Tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, but we modify it to be fully causal rather than block-causal, thereby ensuring true streaming support.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For decoding, we adopt the flow-matching architecture introduced in CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib27\" title=\"\">2022</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. While CosyVoice 2 employs chunk-attention to improve efficiency, this mechanism introduces undesirable time delays. To mitigate this issue, we compress the chunk size, which significantly reduces latency while maintaining reconstruction quality. This modification makes our tokenizer particularly well-suited for streaming dialogue systems that demand both high fidelity and low response delay.</p>\n\n",
                "matched_terms": [
                    "our",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of pre-training is to introduce a speech modality into a pretrained text-based LLM while preserving its original text capabilities. To this end, we initialize our model from Qwen-3-8B<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> and adopt a two-stage pre-training strategy using a large-scale, high-quality speech corpus. The procedure is outlined below.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the low knowledge density inherent in natural speech corpora, we also synthesize additional interleaved data from high-quality text corpora. Following the approach of <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib54\" title=\"\">2024b</a>)</cite>, we use FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese. These texts are converted into audio using the CosyVoice 2 TTS system <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, producing large-scale synthetic speech&#8211;text pairs that enrich the training corpus.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize our model from the Qwen3-8B backbone and employ a two-stage pre-training pipeline designed to introduce the speech modality while preserving the model&#8217;s text capabilities.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we freeze all parameters of the Qwen-3-8B backbone and train only the newly introduced speech-related components, including the speech token embeddings, speech-specific transformer layers, and the speech language modeling (LM) head. This stage serves to initialize speech parameters and establish stable alignment with the pretrained text representations. Training is conducted for approximately one epoch using the AdamW optimizer with cosine learning rate scheduling. The initial learning rate is set to <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, with a batch size of 2.2M tokens, weight decay of 0.1, and a context length of 14,336 tokens.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because high-quality supervised fine-tuning data for speech assistants are scarce in natural settings, we construct such data synthetically. Our process begins with existing open-source text-based supervised fine-tuning datasets listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Data Adaptation and Construction &#8227; 3.2 Supervised Fine-tuning &#8227; 3 Training Strategy &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although LLM-based TTS systems produce highly natural speech, they are susceptible to synthesis errors. To mitigate this, we apply automatic quality filtering using SenseVoice-Small ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib1\" title=\"\">2024</a>)</cite>. Specifically, we discard entries whose ASR transcripts exhibit a word error rate (WER) <math alttext=\"\\geq 0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 0.2</annotation></semantics></math> relative to the original text.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the pretrained model, we conduct supervised fine-tuning on the constructed multimodal dataset for two epochs. Training is performed with the AdamW optimizer, using a cosine learning rate schedule that decays from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>. We use a batch size of 8, apply a weight decay of 0.1, and set the maximum context length to 10,240 tokens with sequence packing.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further strengthen cross-modal alignment between speech and text, fine-tuning incorporates four input&#8211;output modality configurations: <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>, <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, and <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>. The modality pairing is controlled by system prompts, while the underlying content remains identical across configurations. This design ensures that the model learns to handle both unimodal and cross-modal interactions, enabling it to accept text or speech as input and generate either text or speech as output within a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the experimental evaluation of our encoder and decoder components. For the encoder, a crucial aspect is the preservation of semantic information. To assess this,\nwe fine-tuned a Qwen3-0.6B model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> for ASR. Distinct from embedding-based approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib49\" title=\"\">2021</a>)</cite>, our method directly leverages discrete codebook IDs generated by various encoders as input, better aligning with the Large Language Model (LLM) paradigm. This ASR model was trained on the 960-hour Librispeech training dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib34\" title=\"\">2015</a>)</cite>. We then evaluated the corresponding Word Error Rate (WER) on the test sets (test-clean, test-other, and dev-clean). Each model was trained for 100k steps using a batch size of 128 and a learning rate of 1e-4, and we report the lowest WER achieved. Our baselines include codecs designed to capture semantic information, such as Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> and XCodec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib50\" title=\"\">2025</a>)</cite>, as well as ASR-trained codecs like GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite> and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. Ours represent our proposed streaming model, which is further fine-tuned from GLM-4-Voice.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "wer",
                    "evaluation",
                    "rate",
                    "model",
                    "decoder",
                    "our",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "decoder",
                    "sim",
                    "dnsmos",
                    "our",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "frame",
                    "ours",
                    "wer",
                    "evaluation",
                    "rate",
                    "speech",
                    "decoder",
                    "model",
                    "our",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the effectiveness of our pre-training strategy, we evaluate the resulting speech-enabled model on both speech modeling and text understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">speech modeling ability</em>, we use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib18\" title=\"\">2023</a>)</cite> together with our in-house Chinese counterpart, zh-StoryCloze. These benchmarks test the model&#8217;s capacity to reason over and generate coherent speech continuations.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dual evaluation allows us to verify that (i) the model acquires robust speech modeling abilities, while (ii) maintaining the original linguistic competence of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of our SFT model, we assess QA ability using <span class=\"ltx_text ltx_font_bold\">LLaMA-Question</span>, <span class=\"ltx_text ltx_font_bold\">Trivia QA</span>, and <span class=\"ltx_text ltx_font_bold\">Web Questions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib31\" title=\"\">2024</a>; Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib22\" title=\"\">2017</a>; Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib6\" title=\"\">2022</a>)</cite>. And the quality of generated speech is evaluated with <span class=\"ltx_text ltx_font_bold\">UTMOS</span> (MOS style evaluation) <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib39\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5.T7\" title=\"In 5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> highlight three main findings:\n(1) <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> improves both speech modeling and textual ability preservation;\n(2) <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> provides substantial additional gains;\n(3) unfreezing strategies yield relatively small differences.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on integrating speech into decoder-only LLMs has emphasized retaining text capabilities while extending to new modalities. A common strategy is to freeze most LLM parameters and train lightweight adapters. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> proposed <em class=\"ltx_emph ltx_font_italic\">Freeze-Omni</em>, which augments a frozen LLM backbone with speech encoder and decoder modules, while <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib10\" title=\"\">2025</a>)</cite> introduced <em class=\"ltx_emph ltx_font_italic\">SpeechVerse</em>, combining frozen speech and text backbones with adapters to enable zero-shot speech processing from text instructions. Beyond such frozen-backbone designs, other works adopt progressive or staged adaptation. <cite class=\"ltx_cite ltx_citemacro_citet\">Xie &amp; Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> presented <em class=\"ltx_emph ltx_font_italic\">Mini-Omni</em>, which proceeds in phases: first learning speech adapters with the LLM frozen, then performing LM-only fine-tuning to align modalities, and finally unfreezing all but the audio encoder for joint multimodal training. Together, these studies show that freezing LLM backbone helps preserve language modeling ability, while progressive unfreezing provides a pathway for more flexible and effective multimodal integration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a large language model capable of true speech-to-speech interaction without intermediate text, advancing the state of spoken dialogue systems beyond cascaded and text-guided frameworks. Our modality-based layer-splitting and frozen pre-training strategies enable the transfer of linguistic and reasoning knowledge to speech modality from pretrained text LLMs while preserving text abilities, avoiding the degradation often observed in multimodal adaptation. Our model achieves state-of-the-art results in spoken question answering, while supporting both text and speech as native input and output modalities. This work demonstrates that end-to-end speech modeling can reach near parity with text-guided methods while overcoming their inherent limitations in latency and expressivity. Looking forward, we envision speech-native models as the foundation of future human&#8211;AI interaction, supporting seamless, multimodal dialogue across diverse languages and contexts.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement gradual layer-wise unfreezing, we assign each transformer layer its own learning rate with a <em class=\"ltx_emph ltx_font_italic\">delayed warmup&#8211;cosine</em> schedule. Let <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> denote the global training step, the model contain <math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math> layers to be unfrozen indexed by <math alttext=\"i\\in\\{0,\\dots,N-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in\\{0,\\dots,N-1\\}</annotation></semantics></math> (with <math alttext=\"i=N-1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">i=N-1</annotation></semantics></math> denoting the final layer), and define:</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 5:  Evaluation result of our pre-trained model. In the table, “S.C.” refers to “StoryCloze”, “s” refers to “Spoken”, “t” refers to “Topic”. SpiritLM results are takens from Nguyen et al. (2024). The tS.C. and sS.C. results for GLM‑4‑Voice are taken from Zeng et al. (2024a), and the tS.C. and sS.C. results for Moshi are taken from Défossez et al. (2024). Chinese language evaluations are not performed on models trained only in English.",
        "body": "Model\nSpeech\nText\n\n\ntS.C.\nsS.C.\nzh-tS.C.\nzh-sS.C.\nMMLU\nCMMLU\n\n\n\n\nMoshi\n83.60\n62.70\n-\n-\n49.8\n-\n\n\nGLM-4-Voice\n82.90\n62.40\n83.27\n69.10\n57.49\n54.39\n\n\nSpiritLM\n82.90\n61.00\n-\n-\n36.90\n-\n\n\nOurs\n84.87\n63.17\n90.32\n71.94\n67.19\n69.53",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">tS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">sS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">zh-tS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">zh-sS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">MMLU</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">CMMLU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">Moshi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">83.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">62.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">49.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">GLM-4-Voice</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">82.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">62.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">83.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">69.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">57.49</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">54.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">SpiritLM</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">82.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">61.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">36.90</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">84.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">90.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">67.19</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">69.53</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "défossez",
            "taken",
            "speech",
            "chinese",
            "“topic”",
            "text",
            "evaluation",
            "only",
            "pretrained",
            "results",
            "glm‑4‑voice",
            "tsc",
            "“sc”",
            "glm4voice",
            "performed",
            "cmmlu",
            "“s”",
            "mmlu",
            "trained",
            "evaluations",
            "nguyen",
            "2024a",
            "takens",
            "zeng",
            "“t”",
            "refers",
            "zhssc",
            "from",
            "ssc",
            "spiritlm",
            "language",
            "result",
            "english",
            "ours",
            "models",
            "“storycloze”",
            "“spoken”",
            "moshi",
            "our",
            "zhtsc",
            "model",
            "not"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "speech",
                    "pretrained",
                    "results",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is one of the most natural and intuitive modalities for human&#8211;computer interaction, making spoken dialogue systems a central focus of contemporary AI research. Traditional systems for spoken interaction are typically implemented using a cascaded pipeline: speech input is first transcribed into text, a text-based large language model (LLM) generates a response, and the output is subsequently converted into audio through a text-to-speech (TTS) module (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf1\" title=\"Figure 1(a) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>). While this architecture leverages the full reasoning capacity of text-based LLMs, it inevitably discards information encoded in the original speech signal and constrains the system to produce only responses that can be faithfully represented in text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "only",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end attempts such as GSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib25\" title=\"\">2021</a>)</cite> and AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib4\" title=\"\">2023</a>)</cite> demonstrated that speech could be modeled directly, but these works remained largely confined to experimental dialogue continuation tasks and faced challenges in scaling into full-featured assistants. Later work shifted toward text-guided generation as a compromise: SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> used a chain-of-modality design, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> and PSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Mitsui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib30\" title=\"\">2024</a>)</cite> achieved low-latency streaming through parallel speech&#8211;text generation. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> advanced this further by interleaving text and speech in chunk-based generation (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf2\" title=\"Figure 1(b) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), reaching near-text-level performance in streaming dialogue. Importantly, while GLM-4-Voice primarily relies on text-guided responses, it also supports direct speech generation&#8212;but its direct mode remains noticeably weaker than its text-guided counterpart.</p>\n\n",
                "matched_terms": [
                    "text",
                    "défossez",
                    "speech",
                    "2024a",
                    "zeng",
                    "moshi",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By accepting speech directly as input, these approaches preserve paralinguistic cues such as prosody, emphasis, and emotion. Yet their reliance on intermediate text during generation creates a fundamental bottleneck: it introduces latency, reduces efficiency, and restricts expressivity, since non-verbal vocalizations (e.g., laughter, hesitation) lack natural text equivalents. In addition, because of the inherent gap between speech and text modalities, current methods often introduce speech capability at the expense of text ability, leading to a measurable degradation in the backbone&#8217;s text performance. For instance, SpiritLM<cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib32\" title=\"\">2024</a>)</cite> shows a notable drop in MMLU accuracy from 45.3 to 36.9 after incorporating speech modeling. Closing the gap between text-guided and direct speech generation is therefore critical for realizing true speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "speech",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a novel approach that enables large language models to natively model speech while largely retaining their text-based capabilities. Our method builds on a pretrained text LLM backbone but diverges from prior approaches through a modality-specific layer-splitting scheme and a frozen pretraining strategy. This design preserves the backbone&#8217;s linguistic knowledge while equipping the model with native speech understanding and generation abilities comparable to existing text-guided systems. As a result, our model can directly produce high-quality speech without relying on intermediate text representations, establishing a new paradigm for end-to-end speech-to-speech generation. Importantly, because the majority of knowledge remains in the pretrained text model, our approach avoids dependence on large-scale, knowledge-intensive speech datasets. Instead, alignment transfers reasoning, world knowledge, and generalization abilities from the text backbone to the speech modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "result",
                    "models",
                    "speech",
                    "pretrained",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a <span class=\"ltx_text ltx_font_bold\">true speech-to-speech large language model</span> that achieves <span class=\"ltx_text ltx_font_bold\">state-of-the-art</span> performance on speech-to-speech benchmarks without relying on any intermediate text guidance. At the same time, the model natively supports both text and speech as input and output modalities, thereby narrowing the gap between spoken and written interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">modality-based layer-splitting</span> and <span class=\"ltx_text ltx_font_bold\">frozen pre-training</span> that improves alignment between speech and text while mitigating the degradation of reasoning ability and world knowledge typically observed when extending LLMs to new modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For advancing toward a true speech-to-speech large language model, we add a modality-based layer-splitting to an autoregressive Transformer, enabling deep fusion of heterogeneous modalities and modality-specific generation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Transformer backbone, our design goal is to preserve the original text capabilities of LLMs while augmenting LLMs with speech understanding and generation. Existing approaches typically rely on the Depth Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> that generates multiple VQ tokens as a single input, or alternatively, expand the vocabulary to directly encode speech tokens into the input sequence <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. However, our preliminary study on speechgpt2-preview<cite class=\"ltx_cite ltx_citemacro_citep\">(Open-Moss, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib33\" title=\"\">2025</a>)</cite> revealed that the hidden-state alignment between a sentence and its corresponding speech sequence gradually deteriorates in deeper layers: while strong diagonal similarity emerges in lower layers, it vanishes in later layers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "défossez",
                    "speech",
                    "2024a",
                    "zeng",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Architecture &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, by examining the hidden-state similarity between the same spoken utterance and its corresponding text across different layers, we observe that in a 28-layer model, the similarity steadily increases in the first 11 layers, fluctuates and gradually stabilizes in the following 14 layers, and then decreases in the final 3 layers. This finding suggests that as the model is trained, the representations of text and speech become increasingly fused within the first 25 Transformer blocks, but gradually diverge in the last four layers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by this, we introduce a modality-based layer split at the 32<sup class=\"ltx_sup\">nd</sup> block of our 36-layer Transformer. At this point, the shared hidden state is routed into modality-specific branches: one branch continues through the final four layers to predict text tokens, while the other routes into a parallel four-layer stack to predict speech tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This split-then-specialize design allows the model to leverage the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> layers for joint multimodality fusion, while reserving the final layers for modality-specific generation. As a result, the architecture enhances cross-modality transfer, enabling the system to inherit the capabilities of textually-pretrained LLMs and express it natively in the speech modality.</p>\n\n",
                "matched_terms": [
                    "result",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech tokenizer is designed with four key objectives:\n(1) to achieve a single-codebook, low-bitrate representation for efficient autoregressive generation and simplified context management;\n(2) to maximize semantic content in order to facilitate knowledge transfer from text to speech;\n(3) to preserve sufficient paralinguistic detail to enable faithful reconstruction of human speech; and\n(4) to support full streaming operation for low-latency processing.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete speech tokenizers are commonly trained with reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib17\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib57\" title=\"\">2023c</a>)</cite> or self-supervised discovery methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib40\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib28\" title=\"\">2024</a>)</cite>. However, prior work has observed that tokens optimized primarily for reconstruction are often suboptimal for LLM learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite>. To address this, and following the design of CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, we adopt automatic speech recognition (ASR) as the sole training objective for our tokenizer encoder. Our encoder is further trained based on the GLM-4-Voice Tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, but we modify it to be fully causal rather than block-causal, thereby ensuring true streaming support.</p>\n\n",
                "matched_terms": [
                    "défossez",
                    "speech",
                    "trained",
                    "2024a",
                    "zeng",
                    "our",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of pre-training is to introduce a speech modality into a pretrained text-based LLM while preserving its original text capabilities. To this end, we initialize our model from Qwen-3-8B<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> and adopt a two-stage pre-training strategy using a large-scale, high-quality speech corpus. The procedure is outlined below.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "pretrained",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with approximately 9 million hours of real-world audio data collected from the internet. To remove non-speech content, we apply a custom voice activity detection (VAD) pipeline based on <span class=\"ltx_text ltx_font_typewriter\">pyannote</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib35\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib5\" title=\"\">2023</a>)</cite>, resulting in roughly 4 million hours of speech. These data are organized into two categories according to source type: (1) <em class=\"ltx_emph ltx_font_italic\">interleaved speech&#8211;text pre-training</em>, drawn primarily from podcasts, and (2) <em class=\"ltx_emph ltx_font_italic\">unsupervised speech pre-training</em>, drawn primarily from video content. Podcasts are chosen for interleaved pre-training because they typically provide cleaner recordings and clearer speech, enabling automatic speech recognition (ASR) systems to generate more reliable transcripts. In contrast, video sources, while more diverse and noisier, are better suited for the unsupervised pre-training setting, where robustness to challenging acoustic conditions is essential.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the interleaved task, we first apply automatic speech recognition (ASR) to obtain text transcripts. Connectionist Temporal Classification (CTC) word alignment is then used to segment the audio into random-length chunks of 3&#8211;6 seconds. Each chunk contains either the corresponding audio segment or its transcribed text, and sequences are constructed by interleaving the two modalities. For unsupervised speech pre-training, we simply use full-length audio segments without transcript.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the low knowledge density inherent in natural speech corpora, we also synthesize additional interleaved data from high-quality text corpora. Following the approach of <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib54\" title=\"\">2024b</a>)</cite>, we use FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese. These texts are converted into audio using the CosyVoice 2 TTS system <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, producing large-scale synthetic speech&#8211;text pairs that enrich the training corpus.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "speech",
                    "zeng",
                    "chinese",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize our model from the Qwen3-8B backbone and employ a two-stage pre-training pipeline designed to introduce the speech modality while preserving the model&#8217;s text capabilities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we freeze all parameters of the Qwen-3-8B backbone and train only the newly introduced speech-related components, including the speech token embeddings, speech-specific transformer layers, and the speech language modeling (LM) head. This stage serves to initialize speech parameters and establish stable alignment with the pretrained text representations. Training is conducted for approximately one epoch using the AdamW optimizer with cosine learning rate scheduling. The initial learning rate is set to <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, with a batch size of 2.2M tokens, weight decay of 0.1, and a context length of 14,336 tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "only",
                    "speech",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the second stage, we unfreeze a larger portion of the model to allow cross-modal adaptation. We experiment with three configurations: (1) unfreezing the entire model and training all parameters jointly, (2) unfreezing only the shared transformer layers while keeping the text embeddings, text-specific layers, and text LM head frozen, and (3) gradually unfreezing the shared layers in reverse order (from last to first). Since unfreezing text parameters risks degradation of textual abilities, we incorporate additional text-only pre-training data to preserve the model&#8217;s linguistic competence. Specifically, we include FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese, filtering entries with quality scores <math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "only",
                    "chinese",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "results",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because high-quality supervised fine-tuning data for speech assistants are scarce in natural settings, we construct such data synthetically. Our process begins with existing open-source text-based supervised fine-tuning datasets listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Data Adaptation and Construction &#8227; 3.2 Supervised Fine-tuning &#8227; 3 Training Strategy &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The adapted text is then synthesized into audio using multiple TTS systems. We primarily employ Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite> from VolcEngine. For the <em class=\"ltx_emph ltx_font_italic\">user role</em>, we generate speech with a diverse set of speaker voices to improve robustness. For the <em class=\"ltx_emph ltx_font_italic\">assistant role</em>, we always use a single consistent speaker to establish a stable and recognizable system identity. To further enhance voice diversity, naturalness, and stylistic control, we additionally employ MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib41\" title=\"\">2025</a>)</cite> to synthesize conversational datasets. By assigning different system prompts, we can vary the assistant&#8217;s speaking style and role in a controllable manner.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although LLM-based TTS systems produce highly natural speech, they are susceptible to synthesis errors. To mitigate this, we apply automatic quality filtering using SenseVoice-Small ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib1\" title=\"\">2024</a>)</cite>. Specifically, we discard entries whose ASR transcripts exhibit a word error rate (WER) <math alttext=\"\\geq 0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 0.2</annotation></semantics></math> relative to the original text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In total, we end up with over 1500k question&#8211;answer pairs for supervised fine-tuning, comprising approximately 650k English pairs and 860k Chinese pairs.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the pretrained model, we conduct supervised fine-tuning on the constructed multimodal dataset for two epochs. Training is performed with the AdamW optimizer, using a cosine learning rate schedule that decays from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>. We use a batch size of 8, apply a weight decay of 0.1, and set the maximum context length to 10,240 tokens with sequence packing.</p>\n\n",
                "matched_terms": [
                    "performed",
                    "model",
                    "from",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further strengthen cross-modal alignment between speech and text, fine-tuning incorporates four input&#8211;output modality configurations: <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>, <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, and <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>. The modality pairing is controlled by system prompts, while the underlying content remains identical across configurations. This design ensures that the model learns to handle both unimodal and cross-modal interactions, enabling it to accept text or speech as input and generate either text or speech as output within a unified framework.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the experimental evaluation of our encoder and decoder components. For the encoder, a crucial aspect is the preservation of semantic information. To assess this,\nwe fine-tuned a Qwen3-0.6B model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> for ASR. Distinct from embedding-based approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib49\" title=\"\">2021</a>)</cite>, our method directly leverages discrete codebook IDs generated by various encoders as input, better aligning with the Large Language Model (LLM) paradigm. This ASR model was trained on the 960-hour Librispeech training dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib34\" title=\"\">2015</a>)</cite>. We then evaluated the corresponding Word Error Rate (WER) on the test sets (test-clean, test-other, and dev-clean). Each model was trained for 100k steps using a batch size of 128 and a learning rate of 1e-4, and we report the lowest WER achieved. Our baselines include codecs designed to capture semantic information, such as Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> and XCodec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib50\" title=\"\">2025</a>)</cite>, as well as ASR-trained codecs like GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite> and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. Ours represent our proposed streaming model, which is further fine-tuned from GLM-4-Voice.</p>\n\n",
                "matched_terms": [
                    "language",
                    "ours",
                    "défossez",
                    "evaluation",
                    "trained",
                    "2024a",
                    "zeng",
                    "our",
                    "from",
                    "model",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "english",
                    "models",
                    "speech",
                    "chinese",
                    "our",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "ours",
                    "english",
                    "evaluation",
                    "only",
                    "speech",
                    "chinese",
                    "our",
                    "from",
                    "model",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the effectiveness of our pre-training strategy, we evaluate the resulting speech-enabled model on both speech modeling and text understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">speech modeling ability</em>, we use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib18\" title=\"\">2023</a>)</cite> together with our in-house Chinese counterpart, zh-StoryCloze. These benchmarks test the model&#8217;s capacity to reason over and generate coherent speech continuations.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">textual capability preservation</em>, we evaluate on MMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib20\" title=\"\">2021b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib19\" title=\"\">a</a>)</cite> and CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib26\" title=\"\">2024</a>)</cite>, which measure broad knowledge and reasoning across diverse subject domains.</p>\n\n",
                "matched_terms": [
                    "cmmlu",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dual evaluation allows us to verify that (i) the model acquires robust speech modeling abilities, while (ii) maintaining the original linguistic competence of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "text",
                    "evaluation",
                    "speech",
                    "pretrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of our SFT model, we assess QA ability using <span class=\"ltx_text ltx_font_bold\">LLaMA-Question</span>, <span class=\"ltx_text ltx_font_bold\">Trivia QA</span>, and <span class=\"ltx_text ltx_font_bold\">Web Questions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib31\" title=\"\">2024</a>; Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib22\" title=\"\">2017</a>; Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib6\" title=\"\">2022</a>)</cite>. And the quality of generated speech is evaluated with <span class=\"ltx_text ltx_font_bold\">UTMOS</span> (MOS style evaluation) <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib39\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further ablate different unfreezing strategies after Frozen Pre-training:\n(i) FP&#8211;Full, unfreezing all parameters at once;\n(ii) FP&#8211;Shared, unfreezing only speech&#8211;text shared layers while keeping text-specific parameters frozen;\n(iii) FP&#8211;Layerwise, gradually unfreezing shared layers from last to first.\nThe learning rate schedule for FP&#8211;Layerwise is described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#A4\" title=\"Appendix D Layer-wise Unfreeze Learning Rate Schedule &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "only",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5.T7\" title=\"In 5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> highlight three main findings:\n(1) <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> improves both speech modeling and textual ability preservation;\n(2) <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> provides substantial additional gains;\n(3) unfreezing strategies yield relatively small differences.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the ablation confirms that modality separation and freezing text parameters during pre-training are both critical to balancing speech learning with text knowledge preservation. While different unfreezing schedules provide slight trade-offs, their impact is minor compared to the gains from <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> and <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> themselves.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech codecs are crucial for speech large language models (SLMs) and can be grouped into two categories. Neural acoustic codecs based on (R)VQ-GAN optimize reconstruction loss to preserve fine-grained acoustic details <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib52\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib13\" title=\"\">2022</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib24\" title=\"\">2023</a>)</cite>, but their tokens often lack semantic coherence when used for language modeling. In contrast, semantic-oriented codecs adopt a single-layer VQ to encode linguistic content and recover timbre with generative modules such as conditional flow matching <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. While trading off perfect fidelity, this design yields tokens better suited for semantic modeling. We therefore follow the latter approach and further enhance it with streaming encoder&#8211;decoder modules for real-time interaction.</p>\n\n",
                "matched_terms": [
                    "language",
                    "défossez",
                    "models",
                    "speech",
                    "2024a",
                    "zeng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing models mostly depend on text guidance for speech generation. For instance, SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> integrates large language models with discrete speech representations but requires text-based prompts to guide speech generation. Similarly, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> employs a full-duplex spoken dialogue framework, generating speech tokens from a neural audio codec, yet it still necessitates text instructions for generating speech responses. Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib8\" title=\"\">2024</a>)</cite> accepts diverse audio inputs and outputs text, relying on textual prompts for speech understanding. LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib15\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> extend LLMs to process speech inputs and generate speech outputs directly, but they continue to depend on text prompts to guide the interaction. Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> fine-tunes language models to generate text and speech responses simultaneously using instruction datasets, yet the quality of both text and speech responses is limited without prior speech pre-training. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> further advances toward speech-to-speech interaction by integrating speech input and output with large language models, but it still fundamentally relies on textual supervision for alignment and instruction following. These models demonstrate progress toward speech-to-speech interaction but still require text guidance for effective performance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "défossez",
                    "models",
                    "speech",
                    "2024a",
                    "zeng",
                    "moshi",
                    "from",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on integrating speech into decoder-only LLMs has emphasized retaining text capabilities while extending to new modalities. A common strategy is to freeze most LLM parameters and train lightweight adapters. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> proposed <em class=\"ltx_emph ltx_font_italic\">Freeze-Omni</em>, which augments a frozen LLM backbone with speech encoder and decoder modules, while <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib10\" title=\"\">2025</a>)</cite> introduced <em class=\"ltx_emph ltx_font_italic\">SpeechVerse</em>, combining frozen speech and text backbones with adapters to enable zero-shot speech processing from text instructions. Beyond such frozen-backbone designs, other works adopt progressive or staged adaptation. <cite class=\"ltx_cite ltx_citemacro_citet\">Xie &amp; Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> presented <em class=\"ltx_emph ltx_font_italic\">Mini-Omni</em>, which proceeds in phases: first learning speech adapters with the LLM frozen, then performing LM-only fine-tuning to align modalities, and finally unfreezing all but the audio encoder for joint multimodal training. Together, these studies show that freezing LLM backbone helps preserve language modeling ability, while progressive unfreezing provides a pathway for more flexible and effective multimodal integration.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a large language model capable of true speech-to-speech interaction without intermediate text, advancing the state of spoken dialogue systems beyond cascaded and text-guided frameworks. Our modality-based layer-splitting and frozen pre-training strategies enable the transfer of linguistic and reasoning knowledge to speech modality from pretrained text LLMs while preserving text abilities, avoiding the degradation often observed in multimodal adaptation. Our model achieves state-of-the-art results in spoken question answering, while supporting both text and speech as native input and output modalities. This work demonstrates that end-to-end speech modeling can reach near parity with text-guided methods while overcoming their inherent limitations in latency and expressivity. Looking forward, we envision speech-native models as the foundation of future human&#8211;AI interaction, supporting seamless, multimodal dialogue across diverse languages and contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "language",
                    "models",
                    "speech",
                    "pretrained",
                    "results",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below are the five random samples we used to calculate the similarity score, with all text content aligned and presented in English.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate cross-modal alignment inside a Transformer backbone, we compute a similarity score at each layer <math alttext=\"i\\in\\{1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in\\{1,\\dots,L\\}</annotation></semantics></math>.\nLet the text tokens be <math alttext=\"\\{t_{1},\\ldots,t_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{1},\\ldots,t_{n}\\}</annotation></semantics></math> and the speech tokens <math alttext=\"\\{s_{1},\\ldots,s_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{1},\\ldots,s_{m}\\}</annotation></semantics></math>.\nUsing forced alignment<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib36\" title=\"\">2023</a>)</cite>, we obtain <math alttext=\"J_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m4\" intent=\":literal\"><semantics><msub><mi>J</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">J_{i}</annotation></semantics></math> alignment pairs</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"T_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m5\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{i,j}</annotation></semantics></math> is the set of aligned text tokens and <math alttext=\"S_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{i,j}</annotation></semantics></math> is the corresponding set of speech tokens (the alignment is fixed, but hidden states depend on the Transformer layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{i,t}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,t}</annotation></semantics></math> and <math alttext=\"h_{i,s}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,s}</annotation></semantics></math> denote hidden states of text token <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and speech token <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> at layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This schedule ensures that higher (later) layers are unfrozen earlier, while lower (earlier) layers remain frozen longer, enabling a controlled and stable adaptation of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "text",
                    "pretrained"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 6:  Spoken question answering evaluation results & speech quality. L./T./W. QA refer to LlamaQA, TriviaQA, and WebQA, respectively. Except for our model and GLM-4-Voice∗, results for other models in the table are taken from Zeng et al. (2024a) and Défossez et al. (2024). We follow KimiTeam et al. (2025) to normalize the answer before judging.",
        "body": "Model\nText-guided\nL. QA\nT. QA\nW. QA\nUTMOS\n\n\nS→TS\\to T\nS→SS\\to S\nS→TS\\to T\nS→SS\\to S\nS→TS\\to T\nS→SS\\to S\n\n\n\nPre-trained Model\n\n\nGLM-4-Voice\n×\\times\n64.70\n50.70\n39.10\n26.50\n32.20\n15.90\n-\n\n\nTWIST\n×\\times\n-\n4.00\n-\n-\n-\n1.50\n-\n\n\nSupervised Fine-tuned Model\n\n\nSpeechGPT\n✓\n-\n21.60\n-\n14.80\n-\n6.50\n4.00\n\n\nMoshi\n×\\times\n-\n21.00\n-\n7.30\n-\n9.20\n2.81\n\n\nMoshi\n✓\n-\n62.30\n-\n22.80\n-\n26.60\n-\n\n\nGLM-4-Voice\n✓\n74.33\n65.67\n45.90\n43.20\n39.22\n38.34\n4.25\n\n\nOurs\n×\\times\n77.33\n63.67\n45.20\n28.8\n45.9\n36.71\n4.37",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.0pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.0pt;\">Text-guided</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">L. QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">T. QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">W. QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"S\\to T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m3\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo stretchy=\"false\">&#8594;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">S\\to T</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"S\\to S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m4\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo stretchy=\"false\">&#8594;</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S\\to S</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"S\\to T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m5\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo stretchy=\"false\">&#8594;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">S\\to T</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"S\\to S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m6\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo stretchy=\"false\">&#8594;</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S\\to S</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"S\\to T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m7\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo stretchy=\"false\">&#8594;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">S\\to T</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"S\\to S\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m8\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo stretchy=\"false\">&#8594;</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S\\to S</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r\" style=\"padding:0.5pt 3.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Pre-trained Model</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">GLM-4-Voice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m9\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">64.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">50.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">39.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">26.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">32.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">15.90</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">TWIST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">4.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">1.50</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"9\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Supervised Fine-tuned Model</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">SpeechGPT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">21.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">14.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">6.50</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">4.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">Moshi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m11\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">21.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">7.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">9.20</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">2.81</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">Moshi</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">62.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">22.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">26.60</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">GLM-4-Voice</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">74.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">65.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">45.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">43.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">39.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">38.34</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">4.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m12\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">77.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">45.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">28.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">45.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">36.71</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.37</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "normalize",
            "respectively",
            "défossez",
            "kimiteam",
            "twist",
            "speech",
            "taken",
            "textguided",
            "spoken",
            "s→ssto",
            "ltw",
            "utmos",
            "answering",
            "refer",
            "evaluation",
            "question",
            "pretrained",
            "results",
            "glm4voice",
            "follow",
            "answer",
            "triviaqa",
            "llamaqa",
            "glm4voice∗",
            "2024a",
            "zeng",
            "judging",
            "from",
            "except",
            "finetuned",
            "speechgpt",
            "ours",
            "webqa",
            "models",
            "s→tsto",
            "quality",
            "before",
            "×times",
            "supervised",
            "other",
            "our",
            "moshi",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.</p>\n\n",
                "matched_terms": [
                    "answering",
                    "question",
                    "speech",
                    "pretrained",
                    "results",
                    "textguided",
                    "spoken",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is one of the most natural and intuitive modalities for human&#8211;computer interaction, making spoken dialogue systems a central focus of contemporary AI research. Traditional systems for spoken interaction are typically implemented using a cascaded pipeline: speech input is first transcribed into text, a text-based large language model (LLM) generates a response, and the output is subsequently converted into audio through a text-to-speech (TTS) module (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf1\" title=\"Figure 1(a) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>). While this architecture leverages the full reasoning capacity of text-based LLMs, it inevitably discards information encoded in the original speech signal and constrains the system to produce only responses that can be faithfully represented in text.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end attempts such as GSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib25\" title=\"\">2021</a>)</cite> and AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib4\" title=\"\">2023</a>)</cite> demonstrated that speech could be modeled directly, but these works remained largely confined to experimental dialogue continuation tasks and faced challenges in scaling into full-featured assistants. Later work shifted toward text-guided generation as a compromise: SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> used a chain-of-modality design, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> and PSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Mitsui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib30\" title=\"\">2024</a>)</cite> achieved low-latency streaming through parallel speech&#8211;text generation. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> advanced this further by interleaving text and speech in chunk-based generation (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf2\" title=\"Figure 1(b) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), reaching near-text-level performance in streaming dialogue. Importantly, while GLM-4-Voice primarily relies on text-guided responses, it also supports direct speech generation&#8212;but its direct mode remains noticeably weaker than its text-guided counterpart.</p>\n\n",
                "matched_terms": [
                    "speechgpt",
                    "défossez",
                    "speech",
                    "textguided",
                    "2024a",
                    "zeng",
                    "moshi",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By accepting speech directly as input, these approaches preserve paralinguistic cues such as prosody, emphasis, and emotion. Yet their reliance on intermediate text during generation creates a fundamental bottleneck: it introduces latency, reduces efficiency, and restricts expressivity, since non-verbal vocalizations (e.g., laughter, hesitation) lack natural text equivalents. In addition, because of the inherent gap between speech and text modalities, current methods often introduce speech capability at the expense of text ability, leading to a measurable degradation in the backbone&#8217;s text performance. For instance, SpiritLM<cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib32\" title=\"\">2024</a>)</cite> shows a notable drop in MMLU accuracy from 45.3 to 36.9 after incorporating speech modeling. Closing the gap between text-guided and direct speech generation is therefore critical for realizing true speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "textguided",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a novel approach that enables large language models to natively model speech while largely retaining their text-based capabilities. Our method builds on a pretrained text LLM backbone but diverges from prior approaches through a modality-specific layer-splitting scheme and a frozen pretraining strategy. This design preserves the backbone&#8217;s linguistic knowledge while equipping the model with native speech understanding and generation abilities comparable to existing text-guided systems. As a result, our model can directly produce high-quality speech without relying on intermediate text representations, establishing a new paradigm for end-to-end speech-to-speech generation. Importantly, because the majority of knowledge remains in the pretrained text model, our approach avoids dependence on large-scale, knowledge-intensive speech datasets. Instead, alignment transfers reasoning, world knowledge, and generalization abilities from the text backbone to the speech modality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "pretrained",
                    "textguided",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a <span class=\"ltx_text ltx_font_bold\">true speech-to-speech large language model</span> that achieves <span class=\"ltx_text ltx_font_bold\">state-of-the-art</span> performance on speech-to-speech benchmarks without relying on any intermediate text guidance. At the same time, the model natively supports both text and speech as input and output modalities, thereby narrowing the gap between spoken and written interaction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Transformer backbone, our design goal is to preserve the original text capabilities of LLMs while augmenting LLMs with speech understanding and generation. Existing approaches typically rely on the Depth Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> that generates multiple VQ tokens as a single input, or alternatively, expand the vocabulary to directly encode speech tokens into the input sequence <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. However, our preliminary study on speechgpt2-preview<cite class=\"ltx_cite ltx_citemacro_citep\">(Open-Moss, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib33\" title=\"\">2025</a>)</cite> revealed that the hidden-state alignment between a sentence and its corresponding speech sequence gradually deteriorates in deeper layers: while strong diagonal similarity emerges in lower layers, it vanishes in later layers.</p>\n\n",
                "matched_terms": [
                    "défossez",
                    "speech",
                    "2024a",
                    "zeng",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Architecture &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, by examining the hidden-state similarity between the same spoken utterance and its corresponding text across different layers, we observe that in a 28-layer model, the similarity steadily increases in the first 11 layers, fluctuates and gradually stabilizes in the following 14 layers, and then decreases in the final 3 layers. This finding suggests that as the model is trained, the representations of text and speech become increasingly fused within the first 25 Transformer blocks, but gradually diverge in the last four layers.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by this, we introduce a modality-based layer split at the 32<sup class=\"ltx_sup\">nd</sup> block of our 36-layer Transformer. At this point, the shared hidden state is routed into modality-specific branches: one branch continues through the final four layers to predict text tokens, while the other routes into a parallel four-layer stack to predict speech tokens.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This split-then-specialize design allows the model to leverage the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> layers for joint multimodality fusion, while reserving the final layers for modality-specific generation. As a result, the architecture enhances cross-modality transfer, enabling the system to inherit the capabilities of textually-pretrained LLMs and express it natively in the speech modality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech tokenizer is designed with four key objectives:\n(1) to achieve a single-codebook, low-bitrate representation for efficient autoregressive generation and simplified context management;\n(2) to maximize semantic content in order to facilitate knowledge transfer from text to speech;\n(3) to preserve sufficient paralinguistic detail to enable faithful reconstruction of human speech; and\n(4) to support full streaming operation for low-latency processing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete speech tokenizers are commonly trained with reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib17\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib57\" title=\"\">2023c</a>)</cite> or self-supervised discovery methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib40\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib28\" title=\"\">2024</a>)</cite>. However, prior work has observed that tokens optimized primarily for reconstruction are often suboptimal for LLM learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite>. To address this, and following the design of CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, we adopt automatic speech recognition (ASR) as the sole training objective for our tokenizer encoder. Our encoder is further trained based on the GLM-4-Voice Tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, but we modify it to be fully causal rather than block-causal, thereby ensuring true streaming support.</p>\n\n",
                "matched_terms": [
                    "défossez",
                    "speech",
                    "2024a",
                    "zeng",
                    "our",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For decoding, we adopt the flow-matching architecture introduced in CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib27\" title=\"\">2022</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. While CosyVoice 2 employs chunk-attention to improve efficiency, this mechanism introduces undesirable time delays. To mitigate this issue, we compress the chunk size, which significantly reduces latency while maintaining reconstruction quality. This modification makes our tokenizer particularly well-suited for streaming dialogue systems that demand both high fidelity and low response delay.</p>\n\n",
                "matched_terms": [
                    "our",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of pre-training is to introduce a speech modality into a pretrained text-based LLM while preserving its original text capabilities. To this end, we initialize our model from Qwen-3-8B<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> and adopt a two-stage pre-training strategy using a large-scale, high-quality speech corpus. The procedure is outlined below.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pretrained",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with approximately 9 million hours of real-world audio data collected from the internet. To remove non-speech content, we apply a custom voice activity detection (VAD) pipeline based on <span class=\"ltx_text ltx_font_typewriter\">pyannote</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib35\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib5\" title=\"\">2023</a>)</cite>, resulting in roughly 4 million hours of speech. These data are organized into two categories according to source type: (1) <em class=\"ltx_emph ltx_font_italic\">interleaved speech&#8211;text pre-training</em>, drawn primarily from podcasts, and (2) <em class=\"ltx_emph ltx_font_italic\">unsupervised speech pre-training</em>, drawn primarily from video content. Podcasts are chosen for interleaved pre-training because they typically provide cleaner recordings and clearer speech, enabling automatic speech recognition (ASR) systems to generate more reliable transcripts. In contrast, video sources, while more diverse and noisier, are better suited for the unsupervised pre-training setting, where robustness to challenging acoustic conditions is essential.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the low knowledge density inherent in natural speech corpora, we also synthesize additional interleaved data from high-quality text corpora. Following the approach of <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib54\" title=\"\">2024b</a>)</cite>, we use FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese. These texts are converted into audio using the CosyVoice 2 TTS system <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, producing large-scale synthetic speech&#8211;text pairs that enrich the training corpus.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech",
                    "zeng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize our model from the Qwen3-8B backbone and employ a two-stage pre-training pipeline designed to introduce the speech modality while preserving the model&#8217;s text capabilities.</p>\n\n",
                "matched_terms": [
                    "our",
                    "from",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we freeze all parameters of the Qwen-3-8B backbone and train only the newly introduced speech-related components, including the speech token embeddings, speech-specific transformer layers, and the speech language modeling (LM) head. This stage serves to initialize speech parameters and establish stable alignment with the pretrained text representations. Training is conducted for approximately one epoch using the AdamW optimizer with cosine learning rate scheduling. The initial learning rate is set to <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, with a batch size of 2.2M tokens, weight decay of 0.1, and a context length of 14,336 tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the second stage, we unfreeze a larger portion of the model to allow cross-modal adaptation. We experiment with three configurations: (1) unfreezing the entire model and training all parameters jointly, (2) unfreezing only the shared transformer layers while keeping the text embeddings, text-specific layers, and text LM head frozen, and (3) gradually unfreezing the shared layers in reverse order (from last to first). Since unfreezing text parameters risks degradation of textual abilities, we incorporate additional text-only pre-training data to preserve the model&#8217;s linguistic competence. Specifically, we include FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese, filtering entries with quality scores <math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results",
                    "supervised",
                    "from",
                    "except"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because high-quality supervised fine-tuning data for speech assistants are scarce in natural settings, we construct such data synthetically. Our process begins with existing open-source text-based supervised fine-tuning datasets listed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S3.T2\" title=\"Table 2 &#8227; 3.2.1 Data Adaptation and Construction &#8227; 3.2 Supervised Fine-tuning &#8227; 3 Training Strategy &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the GPT-5 API to transform question&#8211;answer pairs into formats suitable for speech representation. This process involves (i) converting non-vocal content such as mathematical expressions, tables, or Markdown into TTS-compatible forms, and (ii) filtering out instances that cannot be effectively rendered as speech (e.g., long code dumps or dense <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> passages). Adaptation also improves data quality by shortening excessively long responses to make them more appropriate for spoken delivery, correcting obvious factual errors, and suggesting suitable emotional tones for TTS synthesis. The prompt for the adaptation process can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#A1\" title=\"Appendix A Prompt for Supervised Fine-tuning Text Adaptation &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spoken",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The adapted text is then synthesized into audio using multiple TTS systems. We primarily employ Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite> from VolcEngine. For the <em class=\"ltx_emph ltx_font_italic\">user role</em>, we generate speech with a diverse set of speaker voices to improve robustness. For the <em class=\"ltx_emph ltx_font_italic\">assistant role</em>, we always use a single consistent speaker to establish a stable and recognizable system identity. To further enhance voice diversity, naturalness, and stylistic control, we additionally employ MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib41\" title=\"\">2025</a>)</cite> to synthesize conversational datasets. By assigning different system prompts, we can vary the assistant&#8217;s speaking style and role in a controllable manner.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although LLM-based TTS systems produce highly natural speech, they are susceptible to synthesis errors. To mitigate this, we apply automatic quality filtering using SenseVoice-Small ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib1\" title=\"\">2024</a>)</cite>. Specifically, we discard entries whose ASR transcripts exhibit a word error rate (WER) <math alttext=\"\\geq 0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 0.2</annotation></semantics></math> relative to the original text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the pretrained model, we conduct supervised fine-tuning on the constructed multimodal dataset for two epochs. Training is performed with the AdamW optimizer, using a cosine learning rate schedule that decays from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>. We use a batch size of 8, apply a weight decay of 0.1, and set the maximum context length to 10,240 tokens with sequence packing.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "supervised",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further strengthen cross-modal alignment between speech and text, fine-tuning incorporates four input&#8211;output modality configurations: <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>, <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, and <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>. The modality pairing is controlled by system prompts, while the underlying content remains identical across configurations. This design ensures that the model learns to handle both unimodal and cross-modal interactions, enabling it to accept text or speech as input and generate either text or speech as output within a unified framework.</p>\n\n",
                "matched_terms": [
                    "answer",
                    "question",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the experimental evaluation of our encoder and decoder components. For the encoder, a crucial aspect is the preservation of semantic information. To assess this,\nwe fine-tuned a Qwen3-0.6B model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> for ASR. Distinct from embedding-based approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib49\" title=\"\">2021</a>)</cite>, our method directly leverages discrete codebook IDs generated by various encoders as input, better aligning with the Large Language Model (LLM) paradigm. This ASR model was trained on the 960-hour Librispeech training dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib34\" title=\"\">2015</a>)</cite>. We then evaluated the corresponding Word Error Rate (WER) on the test sets (test-clean, test-other, and dev-clean). Each model was trained for 100k steps using a batch size of 128 and a learning rate of 1e-4, and we report the lowest WER achieved. Our baselines include codecs designed to capture semantic information, such as Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> and XCodec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib50\" title=\"\">2025</a>)</cite>, as well as ASR-trained codecs like GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite> and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. Ours represent our proposed streaming model, which is further fine-tuned from GLM-4-Voice.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "défossez",
                    "evaluation",
                    "2024a",
                    "zeng",
                    "our",
                    "from",
                    "model",
                    "finetuned",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "quality",
                    "our",
                    "from",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "ours",
                    "evaluation",
                    "speech",
                    "quality",
                    "other",
                    "our",
                    "from",
                    "model",
                    "finetuned",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the effectiveness of our pre-training strategy, we evaluate the resulting speech-enabled model on both speech modeling and text understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">speech modeling ability</em>, we use StoryCloze <cite class=\"ltx_cite ltx_citemacro_citep\">(Hassid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib18\" title=\"\">2023</a>)</cite> together with our in-house Chinese counterpart, zh-StoryCloze. These benchmarks test the model&#8217;s capacity to reason over and generate coherent speech continuations.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dual evaluation allows us to verify that (i) the model acquires robust speech modeling abilities, while (ii) maintaining the original linguistic competence of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "model",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of our SFT model, we assess QA ability using <span class=\"ltx_text ltx_font_bold\">LLaMA-Question</span>, <span class=\"ltx_text ltx_font_bold\">Trivia QA</span>, and <span class=\"ltx_text ltx_font_bold\">Web Questions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib31\" title=\"\">2024</a>; Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib22\" title=\"\">2017</a>; Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib6\" title=\"\">2022</a>)</cite>. And the quality of generated speech is evaluated with <span class=\"ltx_text ltx_font_bold\">UTMOS</span> (MOS style evaluation) <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib39\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "quality",
                    "utmos",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5.T7\" title=\"In 5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> highlight three main findings:\n(1) <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> improves both speech modeling and textual ability preservation;\n(2) <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> provides substantial additional gains;\n(3) unfreezing strategies yield relatively small differences.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the ablation confirms that modality separation and freezing text parameters during pre-training are both critical to balancing speech learning with text knowledge preservation. While different unfreezing schedules provide slight trade-offs, their impact is minor compared to the gains from <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> and <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> themselves.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech codecs are crucial for speech large language models (SLMs) and can be grouped into two categories. Neural acoustic codecs based on (R)VQ-GAN optimize reconstruction loss to preserve fine-grained acoustic details <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib52\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib13\" title=\"\">2022</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib24\" title=\"\">2023</a>)</cite>, but their tokens often lack semantic coherence when used for language modeling. In contrast, semantic-oriented codecs adopt a single-layer VQ to encode linguistic content and recover timbre with generative modules such as conditional flow matching <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. While trading off perfect fidelity, this design yields tokens better suited for semantic modeling. We therefore follow the latter approach and further enhance it with streaming encoder&#8211;decoder modules for real-time interaction.</p>\n\n",
                "matched_terms": [
                    "follow",
                    "défossez",
                    "models",
                    "speech",
                    "2024a",
                    "zeng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing models mostly depend on text guidance for speech generation. For instance, SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> integrates large language models with discrete speech representations but requires text-based prompts to guide speech generation. Similarly, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> employs a full-duplex spoken dialogue framework, generating speech tokens from a neural audio codec, yet it still necessitates text instructions for generating speech responses. Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib8\" title=\"\">2024</a>)</cite> accepts diverse audio inputs and outputs text, relying on textual prompts for speech understanding. LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib15\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> extend LLMs to process speech inputs and generate speech outputs directly, but they continue to depend on text prompts to guide the interaction. Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> fine-tunes language models to generate text and speech responses simultaneously using instruction datasets, yet the quality of both text and speech responses is limited without prior speech pre-training. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> further advances toward speech-to-speech interaction by integrating speech input and output with large language models, but it still fundamentally relies on textual supervision for alignment and instruction following. These models demonstrate progress toward speech-to-speech interaction but still require text guidance for effective performance.</p>\n\n",
                "matched_terms": [
                    "speechgpt",
                    "défossez",
                    "models",
                    "speech",
                    "quality",
                    "2024a",
                    "spoken",
                    "zeng",
                    "moshi",
                    "from",
                    "glm4voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on integrating speech into decoder-only LLMs has emphasized retaining text capabilities while extending to new modalities. A common strategy is to freeze most LLM parameters and train lightweight adapters. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> proposed <em class=\"ltx_emph ltx_font_italic\">Freeze-Omni</em>, which augments a frozen LLM backbone with speech encoder and decoder modules, while <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib10\" title=\"\">2025</a>)</cite> introduced <em class=\"ltx_emph ltx_font_italic\">SpeechVerse</em>, combining frozen speech and text backbones with adapters to enable zero-shot speech processing from text instructions. Beyond such frozen-backbone designs, other works adopt progressive or staged adaptation. <cite class=\"ltx_cite ltx_citemacro_citet\">Xie &amp; Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> presented <em class=\"ltx_emph ltx_font_italic\">Mini-Omni</em>, which proceeds in phases: first learning speech adapters with the LLM frozen, then performing LM-only fine-tuning to align modalities, and finally unfreezing all but the audio encoder for joint multimodal training. Together, these studies show that freezing LLM backbone helps preserve language modeling ability, while progressive unfreezing provides a pathway for more flexible and effective multimodal integration.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a large language model capable of true speech-to-speech interaction without intermediate text, advancing the state of spoken dialogue systems beyond cascaded and text-guided frameworks. Our modality-based layer-splitting and frozen pre-training strategies enable the transfer of linguistic and reasoning knowledge to speech modality from pretrained text LLMs while preserving text abilities, avoiding the degradation often observed in multimodal adaptation. Our model achieves state-of-the-art results in spoken question answering, while supporting both text and speech as native input and output modalities. This work demonstrates that end-to-end speech modeling can reach near parity with text-guided methods while overcoming their inherent limitations in latency and expressivity. Looking forward, we envision speech-native models as the foundation of future human&#8211;AI interaction, supporting seamless, multimodal dialogue across diverse languages and contexts.</p>\n\n",
                "matched_terms": [
                    "answering",
                    "question",
                    "speech",
                    "models",
                    "pretrained",
                    "results",
                    "textguided",
                    "spoken",
                    "our",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#A2.F4\" title=\"Figure 4 &#8227; B.1 HeatMaps &#8227; Appendix B Similarity Details &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the similarity maps of all layers for sample 0. It can be observed that the similarity diagonal begins to appear at layer 7, becomes clearly noticeable by layer 11, and after layer 24, the similarity of other tokens gradually increases at layers 25 and 26, while at layer 27, all similarities drop significantly. This pattern is consistent with the observations in our Similarity Score figure.</p>\n\n",
                "matched_terms": [
                    "our",
                    "other"
                ]
            }
        ]
    },
    "S5.T7": {
        "source_file": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "caption": "Table 7: Ablation study on pre-training strategy.\nFP: Frozen Pretrain (text parameters frozen during pretrain).\nFP–Full: all parameters unfrozen after Frozen Pretrain.\nFP–Layerwise: shared layers gradually unfrozen from last to first.\nFP–Shared: only speech–text shared layers unfrozen, text-specific remain frozen.\nNF: No Frozen Pretrain (all parameters trained directly).\nNF–NoSplit: NF without Modality-Based Layer Split, i.e., speech tokens added directly into text vocab without modality-specific layers. All models are trained for around 2 epochs on the pre-training speech dataset.",
        "body": "Model\nSpeech\nText\n\n\ntS.C.\nsS.C.\nzh-tS.C.\nzh-sS.C.\nMMLU\nCMMLU\n\n\n\n\nFP–Full\n85.20\n63.12\n90.21\n72.10\n66.50\n69.15\n\n\nFP–Layerwise\n84.77\n62.64\n90.11\n71.51\n68.82\n69.26\n\n\nFP–Shared\n83.27\n63.50\n90.11\n72.69\n67.26\n69.27\n\n\nNF\n77.66\n56.60\n88.51\n67.56\n62.11\n64.11\n\n\nNF–NoSplit\n77.12\n55.80\n88.72\n67.02\n60.97\n63.73\n\n\nQwen3-8B\n-\n-\n-\n-\n76.60\n77.35",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding:0.5pt 3.0pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">tS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">sS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">zh-tS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">zh-sS.C.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">MMLU</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column\" style=\"padding:0.5pt 3.0pt;\">CMMLU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">FP&#8211;Full</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">85.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">63.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">90.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">72.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">66.50</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">69.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">FP&#8211;Layerwise</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">84.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">62.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">90.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">71.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">68.82</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">69.26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">FP&#8211;Shared</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">83.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">90.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">72.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">67.26</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">69.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">NF</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">77.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">56.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">88.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">67.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">62.11</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">64.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 3.0pt;\">NF&#8211;NoSplit</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">77.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">55.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">88.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 3.0pt;\">67.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">60.97</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:0.5pt 3.0pt;\">63.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">Qwen3-8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">76.60</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:0.5pt 3.0pt;\">77.35</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "textspecific",
            "parameters",
            "speech",
            "study",
            "shared",
            "fp–layerwise",
            "into",
            "pretraining",
            "split",
            "speech–text",
            "nf–nosplit",
            "text",
            "modalityspecific",
            "gradually",
            "remain",
            "only",
            "without",
            "epochs",
            "vocab",
            "around",
            "fp–full",
            "tsc",
            "layer",
            "strategy",
            "cmmlu",
            "ablation",
            "fp–shared",
            "added",
            "mmlu",
            "during",
            "trained",
            "unfrozen",
            "zhssc",
            "directly",
            "from",
            "tokens",
            "first",
            "dataset",
            "ssc",
            "pretrain",
            "all",
            "qwen38b",
            "models",
            "model",
            "last",
            "after",
            "layers",
            "zhtsc",
            "modalitybased",
            "frozen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5.T7\" title=\"In 5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> highlight three main findings:\n(1) <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> improves both speech modeling and textual ability preservation;\n(2) <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> provides substantial additional gains;\n(3) unfreezing strategies yield relatively small differences.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "strategy",
                    "speech",
                    "without",
                    "modalitybased",
                    "pretraining",
                    "directly",
                    "model",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is one of the most natural and intuitive modalities for human&#8211;computer interaction, making spoken dialogue systems a central focus of contemporary AI research. Traditional systems for spoken interaction are typically implemented using a cascaded pipeline: speech input is first transcribed into text, a text-based large language model (LLM) generates a response, and the output is subsequently converted into audio through a text-to-speech (TTS) module (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf1\" title=\"Figure 1(a) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>). While this architecture leverages the full reasoning capacity of text-based LLMs, it inevitably discards information encoded in the original speech signal and constrains the system to produce only responses that can be faithfully represented in text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "only",
                    "speech",
                    "into",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early end-to-end attempts such as GSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Lakhotia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib25\" title=\"\">2021</a>)</cite> and AudioLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Borsos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib4\" title=\"\">2023</a>)</cite> demonstrated that speech could be modeled directly, but these works remained largely confined to experimental dialogue continuation tasks and faced challenges in scaling into full-featured assistants. Later work shifted toward text-guided generation as a compromise: SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> used a chain-of-modality design, while Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> and PSLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Mitsui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib30\" title=\"\">2024</a>)</cite> achieved low-latency streaming through parallel speech&#8211;text generation. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> advanced this further by interleaving text and speech in chunk-based generation (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S1.F1.sf2\" title=\"Figure 1(b) &#8227; Figure 1 &#8227; 1 Introduction &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), reaching near-text-level performance in streaming dialogue. Importantly, while GLM-4-Voice primarily relies on text-guided responses, it also supports direct speech generation&#8212;but its direct mode remains noticeably weaker than its text-guided counterpart.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "into",
                    "speech–text",
                    "directly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By accepting speech directly as input, these approaches preserve paralinguistic cues such as prosody, emphasis, and emotion. Yet their reliance on intermediate text during generation creates a fundamental bottleneck: it introduces latency, reduces efficiency, and restricts expressivity, since non-verbal vocalizations (e.g., laughter, hesitation) lack natural text equivalents. In addition, because of the inherent gap between speech and text modalities, current methods often introduce speech capability at the expense of text ability, leading to a measurable degradation in the backbone&#8217;s text performance. For instance, SpiritLM<cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib32\" title=\"\">2024</a>)</cite> shows a notable drop in MMLU accuracy from 45.3 to 36.9 after incorporating speech modeling. Closing the gap between text-guided and direct speech generation is therefore critical for realizing true speech-to-speech interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "mmlu",
                    "during",
                    "after",
                    "directly",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce a novel approach that enables large language models to natively model speech while largely retaining their text-based capabilities. Our method builds on a pretrained text LLM backbone but diverges from prior approaches through a modality-specific layer-splitting scheme and a frozen pretraining strategy. This design preserves the backbone&#8217;s linguistic knowledge while equipping the model with native speech understanding and generation abilities comparable to existing text-guided systems. As a result, our model can directly produce high-quality speech without relying on intermediate text representations, establishing a new paradigm for end-to-end speech-to-speech generation. Importantly, because the majority of knowledge remains in the pretrained text model, our approach avoids dependence on large-scale, knowledge-intensive speech datasets. Instead, alignment transfers reasoning, world knowledge, and generalization abilities from the text backbone to the speech modality.</p>\n\n",
                "matched_terms": [
                    "text",
                    "modalityspecific",
                    "strategy",
                    "models",
                    "speech",
                    "without",
                    "pretraining",
                    "directly",
                    "from",
                    "model",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a <span class=\"ltx_text ltx_font_bold\">true speech-to-speech large language model</span> that achieves <span class=\"ltx_text ltx_font_bold\">state-of-the-art</span> performance on speech-to-speech benchmarks without relying on any intermediate text guidance. At the same time, the model natively supports both text and speech as input and output modalities, thereby narrowing the gap between spoken and written interaction.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">modality-based layer-splitting</span> and <span class=\"ltx_text ltx_font_bold\">frozen pre-training</span> that improves alignment between speech and text while mitigating the degradation of reasoning ability and world knowledge typically observed when extending LLMs to new modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "pretraining",
                    "modalitybased",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments and ablation studies to validate the effectiveness of our approach, demonstrating advanced speech&#8211;text cross-modal alignment and textual ability preservation.</p>\n\n",
                "matched_terms": [
                    "ablation",
                    "speech–text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For advancing toward a true speech-to-speech large language model, we add a modality-based layer-splitting to an autoregressive Transformer, enabling deep fusion of heterogeneous modalities and modality-specific generation.</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "model",
                    "modalitybased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Transformer backbone, our design goal is to preserve the original text capabilities of LLMs while augmenting LLMs with speech understanding and generation. Existing approaches typically rely on the Depth Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib14\" title=\"\">2024</a>)</cite> that generates multiple VQ tokens as a single input, or alternatively, expand the vocabulary to directly encode speech tokens into the input sequence <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. However, our preliminary study on speechgpt2-preview<cite class=\"ltx_cite ltx_citemacro_citep\">(Open-Moss, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib33\" title=\"\">2025</a>)</cite> revealed that the hidden-state alignment between a sentence and its corresponding speech sequence gradually deteriorates in deeper layers: while strong diagonal similarity emerges in lower layers, it vanishes in later layers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gradually",
                    "speech",
                    "study",
                    "into",
                    "layers",
                    "directly",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Architecture &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, by examining the hidden-state similarity between the same spoken utterance and its corresponding text across different layers, we observe that in a 28-layer model, the similarity steadily increases in the first 11 layers, fluctuates and gradually stabilizes in the following 14 layers, and then decreases in the final 3 layers. This finding suggests that as the model is trained, the representations of text and speech become increasingly fused within the first 25 Transformer blocks, but gradually diverge in the last four layers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gradually",
                    "speech",
                    "trained",
                    "last",
                    "layers",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by this, we introduce a modality-based layer split at the 32<sup class=\"ltx_sup\">nd</sup> block of our 36-layer Transformer. At this point, the shared hidden state is routed into modality-specific branches: one branch continues through the final four layers to predict text tokens, while the other routes into a parallel four-layer stack to predict speech tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "modalityspecific",
                    "speech",
                    "shared",
                    "into",
                    "layers",
                    "split",
                    "modalitybased",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This split-then-specialize design allows the model to leverage the first <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> layers for joint multimodality fusion, while reserving the final layers for modality-specific generation. As a result, the architecture enhances cross-modality transfer, enabling the system to inherit the capabilities of textually-pretrained LLMs and express it natively in the speech modality.</p>\n\n",
                "matched_terms": [
                    "modalityspecific",
                    "speech",
                    "layers",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech tokenizer is designed with four key objectives:\n(1) to achieve a single-codebook, low-bitrate representation for efficient autoregressive generation and simplified context management;\n(2) to maximize semantic content in order to facilitate knowledge transfer from text to speech;\n(3) to preserve sufficient paralinguistic detail to enable faithful reconstruction of human speech; and\n(4) to support full streaming operation for low-latency processing.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete speech tokenizers are commonly trained with reconstruction objectives&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib17\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib57\" title=\"\">2023c</a>)</cite> or self-supervised discovery methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib40\" title=\"\">2024</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib28\" title=\"\">2024</a>)</cite>. However, prior work has observed that tokens optimized primarily for reconstruction are often suboptimal for LLM learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite>. To address this, and following the design of CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, we adopt automatic speech recognition (ASR) as the sole training objective for our tokenizer encoder. Our encoder is further trained based on the GLM-4-Voice Tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, but we modify it to be fully causal rather than block-causal, thereby ensuring true streaming support.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of pre-training is to introduce a speech modality into a pretrained text-based LLM while preserving its original text capabilities. To this end, we initialize our model from Qwen-3-8B<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> and adopt a two-stage pre-training strategy using a large-scale, high-quality speech corpus. The procedure is outlined below.</p>\n\n",
                "matched_terms": [
                    "text",
                    "strategy",
                    "speech",
                    "into",
                    "pretraining",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin with approximately 9 million hours of real-world audio data collected from the internet. To remove non-speech content, we apply a custom voice activity detection (VAD) pipeline based on <span class=\"ltx_text ltx_font_typewriter\">pyannote</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib35\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib5\" title=\"\">2023</a>)</cite>, resulting in roughly 4 million hours of speech. These data are organized into two categories according to source type: (1) <em class=\"ltx_emph ltx_font_italic\">interleaved speech&#8211;text pre-training</em>, drawn primarily from podcasts, and (2) <em class=\"ltx_emph ltx_font_italic\">unsupervised speech pre-training</em>, drawn primarily from video content. Podcasts are chosen for interleaved pre-training because they typically provide cleaner recordings and clearer speech, enabling automatic speech recognition (ASR) systems to generate more reliable transcripts. In contrast, video sources, while more diverse and noisier, are better suited for the unsupervised pre-training setting, where robustness to challenging acoustic conditions is essential.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pretraining",
                    "into",
                    "speech–text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the interleaved task, we first apply automatic speech recognition (ASR) to obtain text transcripts. Connectionist Temporal Classification (CTC) word alignment is then used to segment the audio into random-length chunks of 3&#8211;6 seconds. Each chunk contains either the corresponding audio segment or its transcribed text, and sequences are constructed by interleaving the two modalities. For unsupervised speech pre-training, we simply use full-length audio segments without transcript.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "without",
                    "into",
                    "pretraining",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the low knowledge density inherent in natural speech corpora, we also synthesize additional interleaved data from high-quality text corpora. Following the approach of <cite class=\"ltx_cite ltx_citemacro_citet\">Zeng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib54\" title=\"\">2024b</a>)</cite>, we use FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese. These texts are converted into audio using the CosyVoice 2 TTS system <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>, producing large-scale synthetic speech&#8211;text pairs that enrich the training corpus.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "into",
                    "speech–text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize our model from the Qwen3-8B backbone and employ a two-stage pre-training pipeline designed to introduce the speech modality while preserving the model&#8217;s text capabilities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "qwen38b",
                    "speech",
                    "pretraining",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the first stage, we freeze all parameters of the Qwen-3-8B backbone and train only the newly introduced speech-related components, including the speech token embeddings, speech-specific transformer layers, and the speech language modeling (LM) head. This stage serves to initialize speech parameters and establish stable alignment with the pretrained text representations. Training is conducted for approximately one epoch using the AdamW optimizer with cosine learning rate scheduling. The initial learning rate is set to <math alttext=\"4\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-4}</annotation></semantics></math>, with a batch size of 2.2M tokens, weight decay of 0.1, and a context length of 14,336 tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "parameters",
                    "all",
                    "qwen38b",
                    "only",
                    "speech",
                    "layers",
                    "tokens",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the second stage, we unfreeze a larger portion of the model to allow cross-modal adaptation. We experiment with three configurations: (1) unfreezing the entire model and training all parameters jointly, (2) unfreezing only the shared transformer layers while keeping the text embeddings, text-specific layers, and text LM head frozen, and (3) gradually unfreezing the shared layers in reverse order (from last to first). Since unfreezing text parameters risks degradation of textual abilities, we incorporate additional text-only pre-training data to preserve the model&#8217;s linguistic competence. Specifically, we include FineWeb-Edu <cite class=\"ltx_cite ltx_citemacro_citep\">(Lozhkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib29\" title=\"\">2024</a>)</cite> for English and Chinese FineWeb-Edu V2.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib51\" title=\"\">2025</a>)</cite> for Chinese, filtering entries with quality scores <math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "textspecific",
                    "parameters",
                    "gradually",
                    "all",
                    "only",
                    "shared",
                    "last",
                    "layers",
                    "pretraining",
                    "from",
                    "model",
                    "frozen",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Stage 2 training is conducted for two epochs on the same speech dataset used in Stage 1, combined with 0.1 epoch of text-only pre-training data. Hyperparameters are largely consistent with Stage 1, except that the learning rate is reduced (decaying from <math alttext=\"6\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6\\times 10^{-6}</annotation></semantics></math>) and the batch size is increased to 2.8M tokens to account for the additional text data. In practice, the three configurations achieve comparable results. For simplicity, we adopt configuration (1) as the default initialization for subsequent supervised fine-tuning. A detailed ablation study is provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#S5\" title=\"5 Ablation Study &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "ablation",
                    "speech",
                    "study",
                    "tokens",
                    "pretraining",
                    "from",
                    "epochs",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the GPT-5 API to transform question&#8211;answer pairs into formats suitable for speech representation. This process involves (i) converting non-vocal content such as mathematical expressions, tables, or Markdown into TTS-compatible forms, and (ii) filtering out instances that cannot be effectively rendered as speech (e.g., long code dumps or dense <span class=\"ltx_text ltx_LaTeX_logo\" style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span class=\"ltx_text\" style=\"position:relative; bottom:0.4ex;font-variant:small-caps;;\">a</span>T<span class=\"ltx_text\" style=\"position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;\">e</span>X</span> passages). Adaptation also improves data quality by shortening excessively long responses to make them more appropriate for spoken delivery, correcting obvious factual errors, and suggesting suitable emotional tones for TTS synthesis. The prompt for the adaptation process can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#A1\" title=\"Appendix A Prompt for Supervised Fine-tuning Text Adaptation &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The adapted text is then synthesized into audio using multiple TTS systems. We primarily employ Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite> from VolcEngine. For the <em class=\"ltx_emph ltx_font_italic\">user role</em>, we generate speech with a diverse set of speaker voices to improve robustness. For the <em class=\"ltx_emph ltx_font_italic\">assistant role</em>, we always use a single consistent speaker to establish a stable and recognizable system identity. To further enhance voice diversity, naturalness, and stylistic control, we additionally employ MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib41\" title=\"\">2025</a>)</cite> to synthesize conversational datasets. By assigning different system prompts, we can vary the assistant&#8217;s speaking style and role in a controllable manner.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "speech",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although LLM-based TTS systems produce highly natural speech, they are susceptible to synthesis errors. To mitigate this, we apply automatic quality filtering using SenseVoice-Small ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(An et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib1\" title=\"\">2024</a>)</cite>. Specifically, we discard entries whose ASR transcripts exhibit a word error rate (WER) <math alttext=\"\\geq 0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 0.2</annotation></semantics></math> relative to the original text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the pretrained model, we conduct supervised fine-tuning on the constructed multimodal dataset for two epochs. Training is performed with the AdamW optimizer, using a cosine learning rate schedule that decays from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>. We use a batch size of 8, apply a weight decay of 0.1, and set the maximum context length to 10,240 tokens with sequence packing.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "from",
                    "model",
                    "epochs",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further strengthen cross-modal alignment between speech and text, fine-tuning incorporates four input&#8211;output modality configurations: <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, <span class=\"ltx_text ltx_font_italic\">speech question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>, <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> speech answer</span>, and <span class=\"ltx_text ltx_font_italic\">text question <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text answer</span>. The modality pairing is controlled by system prompts, while the underlying content remains identical across configurations. This design ensures that the model learns to handle both unimodal and cross-modal interactions, enabling it to accept text or speech as input and generate either text or speech as output within a unified framework.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the experimental evaluation of our encoder and decoder components. For the encoder, a crucial aspect is the preservation of semantic information. To assess this,\nwe fine-tuned a Qwen3-0.6B model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib47\" title=\"\">2025</a>)</cite> for ASR. Distinct from embedding-based approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib49\" title=\"\">2021</a>)</cite>, our method directly leverages discrete codebook IDs generated by various encoders as input, better aligning with the Large Language Model (LLM) paradigm. This ASR model was trained on the 960-hour Librispeech training dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib34\" title=\"\">2015</a>)</cite>. We then evaluated the corresponding Word Error Rate (WER) on the test sets (test-clean, test-other, and dev-clean). Each model was trained for 100k steps using a batch size of 128 and a learning rate of 1e-4, and we report the lowest WER achieved. Our baselines include codecs designed to capture semantic information, such as Mimi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> and XCodec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib50\" title=\"\">2025</a>)</cite>, as well as ASR-trained codecs like GLM-4-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite> and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>)</cite>. Ours represent our proposed streaming model, which is further fine-tuned from GLM-4-Voice.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "directly",
                    "from",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate our decoder, we utilize the Seed-TTS-Eval benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib2\" title=\"\">2024</a>)</cite>, employing its standard English and Chinese test datasets. We assess\nintelligibility (WER), speaker similarity (SIM), and speech quality (DNSMOS). Speaker similarity (SIM) is\ncomputed as the cosine similarity between WavLM-TDNN embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib7\" title=\"\">2022</a>)</cite> of the prompt and generated speech. WER is measured using whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib37\" title=\"\">2023</a>)</cite> for non-Chinese languages and\nparaformer-zh for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib16\" title=\"\">2022</a>)</cite>. Additionally, we incorporate the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib38\" title=\"\">2022</a>)</cite> metric to assess the perceived quality of the generated speech. Since our decoder is fine-tuned from CosyVoice 2, we benchmark its performance against the CosyVoice series models.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation demonstrates the robust performance of Our codec across both encoder and decoder components. For the encoder, Ours achieve an overall Word Error Rate (WER) of 10.80%. While this is slightly higher than the 9.17% of GLM-4-Voice, it is important to note that GLM-4-Voice operates with 2-second processing blocks rather than pure streaming. While being a full streaming architecture, Our model achieves a competitive WER. Furthermore, Our encoder significantly surpass other non-streaming codecs like Mimi-8 (14.45%) and CosyVoice 2 (13.78%), despite its lower BPS and frame rate. Consequently, our decoder, fine-tuned from CosyVoice 2, benefits from this enhanced capture of semantic information. Even at a lower frame rate, Our decoder achieves better intelligibility (lower WER) and perceived speech quality on both English and Chinese benchmarks compared to CosyVoice 2, with only a marginal trade-off in speaker similarity..</p>\n\n",
                "matched_terms": [
                    "only",
                    "speech",
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the effectiveness of our pre-training strategy, we evaluate the resulting speech-enabled model on both speech modeling and text understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "strategy",
                    "speech",
                    "pretraining",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <em class=\"ltx_emph ltx_font_italic\">textual capability preservation</em>, we evaluate on MMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib20\" title=\"\">2021b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib19\" title=\"\">a</a>)</cite> and CMMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib26\" title=\"\">2024</a>)</cite>, which measure broad knowledge and reasoning across diverse subject domains.</p>\n\n",
                "matched_terms": [
                    "cmmlu",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dual evaluation allows us to verify that (i) the model acquires robust speech modeling abilities, while (ii) maintaining the original linguistic competence of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of our SFT model, we assess QA ability using <span class=\"ltx_text ltx_font_bold\">LLaMA-Question</span>, <span class=\"ltx_text ltx_font_bold\">Trivia QA</span>, and <span class=\"ltx_text ltx_font_bold\">Web Questions</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Nachmani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib31\" title=\"\">2024</a>; Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib22\" title=\"\">2017</a>; Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib6\" title=\"\">2022</a>)</cite>. And the quality of generated speech is evaluated with <span class=\"ltx_text ltx_font_bold\">UTMOS</span> (MOS style evaluation) <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib39\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study the effect of two key components in our pre-training pipeline: <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> and <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "study",
                    "pretraining",
                    "split",
                    "modalitybased",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first compare a naive baseline without either strategy (NF&#8211;NoSplit) against a variant that introduces <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> but trains all parameters directly (NF), isolating the benefit of modality separation. Next, we evaluate the effect of <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> by comparing NF with FP&#8211;Full, where text parameters are frozen during pre-training and then unfrozen.</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "strategy",
                    "parameters",
                    "all",
                    "without",
                    "during",
                    "unfrozen",
                    "pretraining",
                    "split",
                    "directly",
                    "fp–full",
                    "modalitybased",
                    "nf–nosplit",
                    "frozen",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further ablate different unfreezing strategies after Frozen Pre-training:\n(i) FP&#8211;Full, unfreezing all parameters at once;\n(ii) FP&#8211;Shared, unfreezing only speech&#8211;text shared layers while keeping text-specific parameters frozen;\n(iii) FP&#8211;Layerwise, gradually unfreezing shared layers from last to first.\nThe learning rate schedule for FP&#8211;Layerwise is described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#A4\" title=\"Appendix D Layer-wise Unfreeze Learning Rate Schedule &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "textspecific",
                    "parameters",
                    "gradually",
                    "fp–shared",
                    "all",
                    "only",
                    "shared",
                    "from",
                    "fp–layerwise",
                    "last",
                    "layers",
                    "after",
                    "pretraining",
                    "fp–full",
                    "speech–text",
                    "frozen",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the ablation confirms that modality separation and freezing text parameters during pre-training are both critical to balancing speech learning with text knowledge preservation. While different unfreezing schedules provide slight trade-offs, their impact is minor compared to the gains from <span class=\"ltx_text ltx_font_italic\">Modality-based Layer Split</span> and <span class=\"ltx_text ltx_font_italic\">Frozen Pre-training</span> themselves.</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "parameters",
                    "ablation",
                    "speech",
                    "during",
                    "pretraining",
                    "split",
                    "from",
                    "modalitybased",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech codecs are crucial for speech large language models (SLMs) and can be grouped into two categories. Neural acoustic codecs based on (R)VQ-GAN optimize reconstruction loss to preserve fine-grained acoustic details <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib52\" title=\"\">2021</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib13\" title=\"\">2022</a>; Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib24\" title=\"\">2023</a>)</cite>, but their tokens often lack semantic coherence when used for language modeling. In contrast, semantic-oriented codecs adopt a single-layer VQ to encode linguistic content and recover timbre with generative modules such as conditional flow matching <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib12\" title=\"\">2024</a>; Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite>. While trading off perfect fidelity, this design yields tokens better suited for semantic modeling. We therefore follow the latter approach and further enhance it with streaming encoder&#8211;decoder modules for real-time interaction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "into",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing models mostly depend on text guidance for speech generation. For instance, SpeechGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib55\" title=\"\">2023a</a>)</cite> integrates large language models with discrete speech representations but requires text-based prompts to guide speech generation. Similarly, Moshi <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib11\" title=\"\">2024</a>)</cite> employs a full-duplex spoken dialogue framework, generating speech tokens from a neural audio codec, yet it still necessitates text instructions for generating speech responses. Qwen-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib8\" title=\"\">2024</a>)</cite> accepts diverse audio inputs and outputs text, relying on textual prompts for speech understanding. LLaMA-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Fang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib15\" title=\"\">2025</a>)</cite> and Freeze-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> extend LLMs to process speech inputs and generate speech outputs directly, but they continue to depend on text prompts to guide the interaction. Mini-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie &amp; Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> fine-tunes language models to generate text and speech responses simultaneously using instruction datasets, yet the quality of both text and speech responses is limited without prior speech pre-training. GLM-4-Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib53\" title=\"\">2024a</a>)</cite> further advances toward speech-to-speech interaction by integrating speech input and output with large language models, but it still fundamentally relies on textual supervision for alignment and instruction following. These models demonstrate progress toward speech-to-speech interaction but still require text guidance for effective performance.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "speech",
                    "without",
                    "pretraining",
                    "directly",
                    "from",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on integrating speech into decoder-only LLMs has emphasized retaining text capabilities while extending to new modalities. A common strategy is to freeze most LLM parameters and train lightweight adapters. For instance, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib44\" title=\"\">2025b</a>)</cite> proposed <em class=\"ltx_emph ltx_font_italic\">Freeze-Omni</em>, which augments a frozen LLM backbone with speech encoder and decoder modules, while <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib10\" title=\"\">2025</a>)</cite> introduced <em class=\"ltx_emph ltx_font_italic\">SpeechVerse</em>, combining frozen speech and text backbones with adapters to enable zero-shot speech processing from text instructions. Beyond such frozen-backbone designs, other works adopt progressive or staged adaptation. <cite class=\"ltx_cite ltx_citemacro_citet\">Xie &amp; Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib45\" title=\"\">2024</a>)</cite> presented <em class=\"ltx_emph ltx_font_italic\">Mini-Omni</em>, which proceeds in phases: first learning speech adapters with the LLM frozen, then performing LM-only fine-tuning to align modalities, and finally unfreezing all but the audio encoder for joint multimodal training. Together, these studies show that freezing LLM backbone helps preserve language modeling ability, while progressive unfreezing provides a pathway for more flexible and effective multimodal integration.</p>\n\n",
                "matched_terms": [
                    "text",
                    "strategy",
                    "parameters",
                    "all",
                    "speech",
                    "into",
                    "from",
                    "frozen",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced a large language model capable of true speech-to-speech interaction without intermediate text, advancing the state of spoken dialogue systems beyond cascaded and text-guided frameworks. Our modality-based layer-splitting and frozen pre-training strategies enable the transfer of linguistic and reasoning knowledge to speech modality from pretrained text LLMs while preserving text abilities, avoiding the degradation often observed in multimodal adaptation. Our model achieves state-of-the-art results in spoken question answering, while supporting both text and speech as native input and output modalities. This work demonstrates that end-to-end speech modeling can reach near parity with text-guided methods while overcoming their inherent limitations in latency and expressivity. Looking forward, we envision speech-native models as the foundation of future human&#8211;AI interaction, supporting seamless, multimodal dialogue across diverse languages and contexts.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "speech",
                    "without",
                    "modalitybased",
                    "pretraining",
                    "from",
                    "model",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#A2.F4\" title=\"Figure 4 &#8227; B.1 HeatMaps &#8227; Appendix B Similarity Details &#8227; MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the similarity maps of all layers for sample 0. It can be observed that the similarity diagonal begins to appear at layer 7, becomes clearly noticeable by layer 11, and after layer 24, the similarity of other tokens gradually increases at layers 25 and 26, while at layer 27, all similarities drop significantly. This pattern is consistent with the observations in our Similarity Score figure.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "gradually",
                    "all",
                    "after",
                    "layers",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below are the five random samples we used to calculate the similarity score, with all text content aligned and presented in English.</p>\n\n",
                "matched_terms": [
                    "text",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate cross-modal alignment inside a Transformer backbone, we compute a similarity score at each layer <math alttext=\"i\\in\\{1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in\\{1,\\dots,L\\}</annotation></semantics></math>.\nLet the text tokens be <math alttext=\"\\{t_{1},\\ldots,t_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t_{1},\\ldots,t_{n}\\}</annotation></semantics></math> and the speech tokens <math alttext=\"\\{s_{1},\\ldots,s_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{s_{1},\\ldots,s_{m}\\}</annotation></semantics></math>.\nUsing forced alignment<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00499v2#bib.bib36\" title=\"\">2023</a>)</cite>, we obtain <math alttext=\"J_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m4\" intent=\":literal\"><semantics><msub><mi>J</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">J_{i}</annotation></semantics></math> alignment pairs</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"T_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m5\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{i,j}</annotation></semantics></math> is the set of aligned text tokens and <math alttext=\"S_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{i,j}</annotation></semantics></math> is the corresponding set of speech tokens (the alignment is fixed, but hidden states depend on the Transformer layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{i,t}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,t}</annotation></semantics></math> and <math alttext=\"h_{i,s}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,s}</annotation></semantics></math> denote hidden states of text token <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and speech token <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> at layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, <math alttext=\"\\mathrm{DTW}_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m1\" intent=\":literal\"><semantics><msub><mi>DTW</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathrm{DTW}_{i,j}</annotation></semantics></math> measures the mean similarity along the DTW-optimal path for pair <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m2\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> at layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, <math alttext=\"\\mathrm{BG}_{i,j}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m4\" intent=\":literal\"><semantics><msub><mi>BG</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathrm{BG}_{i,j}</annotation></semantics></math> normalizes against similarities with non-aligned text tokens, and <math alttext=\"\\mathrm{SS}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m5\" intent=\":literal\"><semantics><msub><mi>SS</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathrm{SS}_{i}</annotation></semantics></math> means Similarity Score of layer i quantifies the relative strength of text&#8211;speech alignment at Transformer layer <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p6.m7\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> serving as a global coefficient averaged over all pairs and layers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "layer",
                    "all",
                    "layers",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To implement gradual layer-wise unfreezing, we assign each transformer layer its own learning rate with a <em class=\"ltx_emph ltx_font_italic\">delayed warmup&#8211;cosine</em> schedule. Let <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> denote the global training step, the model contain <math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math> layers to be unfrozen indexed by <math alttext=\"i\\in\\{0,\\dots,N-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in\\{0,\\dots,N-1\\}</annotation></semantics></math> (with <math alttext=\"i=N-1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">i=N-1</annotation></semantics></math> denoting the final layer), and define:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "unfrozen",
                    "model",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> is the per-layer delay (in steps), <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> the global step at which all layers have reached <math alttext=\"\\eta_{\\min}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mi>min</mi></msub><annotation encoding=\"application/x-tex\">\\eta_{\\min}</annotation></semantics></math>, <math alttext=\"\\eta_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">\\eta_{\\max}</annotation></semantics></math> the peak learning rate, and <math alttext=\"r=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m9\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">r=0.1</annotation></semantics></math> the final decay ratio.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>: layer index (<math alttext=\"N-1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">N-1</annotation></semantics></math> = last layer).</p>\n\n",
                "matched_terms": [
                    "layer",
                    "last"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i6.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>: global step when all layers reach <math alttext=\"\\eta_{\\min}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i6.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mi>min</mi></msub><annotation encoding=\"application/x-tex\">\\eta_{\\min}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This schedule ensures that higher (later) layers are unfrozen earlier, while lower (earlier) layers remain frozen longer, enabling a controlled and stable adaptation of the pretrained text backbone.</p>\n\n",
                "matched_terms": [
                    "text",
                    "remain",
                    "unfrozen",
                    "layers",
                    "frozen"
                ]
            }
        ]
    }
}