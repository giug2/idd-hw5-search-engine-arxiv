<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation</title>
<!--Generated on Tue Sep 23 21:23:39 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2509.19592v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S1" title="In Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2" title="In Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.SS1" title="In 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Background: Multi-codebook Neural Audio Codecs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.SS2" title="In 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Base Model: Koel-TTS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.SS3" title="In 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Local Transformer for Iterative Prediction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.SS3.SSS1" title="In 2.3 Local Transformer for Iterative Prediction ‣ 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Autoregressive Local Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.SS3.SSS2" title="In 2.3 Local Transformer for Iterative Prediction ‣ 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>MaskGIT Local Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.SS4" title="In 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Frame Stacking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3" title="In Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.SS1" title="In 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Training and Sampling Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.SS2" title="In 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Evaluation Criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.SS3" title="In 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.SS3.SSS1" title="In 3.3 Results ‣ 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Iterative Sampling and FD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.SS3.SSS2" title="In 3.3 Results ‣ 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Local Transformer and Frame Stacking</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S4" title="In Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Frame-Stacked Local Transformers for Efficient
<br class="ltx_break"/>Multi-Codebook Speech Generation</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Speech generation models based on large language models (LLMs) typically operate on discrete acoustic codes, which differ fundamentally from text tokens due to their multi-codebook structure. At each timestep, models must predict <math alttext="N" class="ltx_Math" display="inline" id="m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebook entries jointly, introducing dependencies that challenge simple parallel prediction approaches. Parallel prediction assumes independence among codebooks, yielding efficient decoding but often at the cost of reduced fidelity. To address this, hierarchical strategies employ a local transformer (LT) to refine predictions and capture intra-timestep dependencies. In this work, we systematically investigate two LT architectures: an autoregressive transformer that generates codebooks sequentially, and a MaskGIT-based transformer that performs iterative masked prediction. Both designs further enable frame stacking, where the primary transformer predicts multiple frames jointly, and the LT decodes their codebooks, offering improvements in speed without compromising perceptual quality. Through extensive analysis, we characterize the tradeoffs between parallel and iterative sampling strategies across different throughput and quality regimes. Finally, we propose practical guidelines for selecting decoding strategies based on deployment priorities such as computational efficiency and synthesis fidelity<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Audio demo page: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://frame-stacking-lt.github.io" style="font-size:70%;" title="">https://frame-stacking-lt.github.io</a></span></span></span>
.</p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span class="ltx_text ltx_font_upright">— </span></span>
Speech LLMs, Local Transformer, Autoregressive TTS</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Recent advances in large language models (LLMs) for speech generation, such as text-to-speech (TTS) systems, have enabled highly natural speech synthesis by predicting sequences of discrete acoustic codes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib5" title="">5</a>]</cite>. Unlike text generation, which produces a one-dimensional sequence of tokens, acoustic representations are typically structured as a matrix of shape <math alttext="[T,N]" class="ltx_Math" display="inline" id="S1.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mi>T</mi><mo>,</mo><mi>N</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[T,N]</annotation></semantics></math>, where <math alttext="T" class="ltx_Math" display="inline" id="S1.p1.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> denotes the number of timesteps and <math alttext="N" class="ltx_Math" display="inline" id="S1.p1.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> the number of codebooks. Therefore, each timestep requires the prediction of multiple codebook entries. This structural difference introduces challenges unique to speech generation: while text tokens are conditionally dependent only along the temporal dimension, acoustic tokens also exhibit inter-codebook dependencies within each timestep.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">A common baseline for this problem is <em class="ltx_emph ltx_font_italic">parallel prediction</em>, in which all <math alttext="N" class="ltx_Math" display="inline" id="S1.p2.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebooks of a timestep are predicted simultaneously under an independence assumption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>]</cite>. While computationally efficient, this approach neglects the rich dependencies among the codebooks, often leading to a degradation in synthesis quality. To mitigate this limitation, alternative decoding strategies introduce <em class="ltx_emph ltx_font_italic">hierarchical modeling</em> through an auxiliary local transformer (LT), tasked with capturing intra-timestep dependencies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib6" title="">6</a>]</cite>. In such approaches, a primary transformer predicts coarse acoustic embeddings across timesteps, and a secondary LT refines the prediction by modeling the structured codebooks corresponding to each frame.
This hierarchical transformer setup with an LT has also been successful in diffusion-based autoregressive models that predict continuous codec representations instead of discrete codes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib7" title="">7</a>]</cite>.
Note that unlike some alternate approaches with multiple transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib5" title="">5</a>]</cite>, the LT operates within a single frame of acoustic codes instead of multiple frames.
Alternatively, the delay pattern <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib8" title="">8</a>]</cite> partly addresses the issues with parallel prediction. However, it does not capture the complete dependencies amongst the tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">This work examines two principal design choices for the local transformer: (i) an <em class="ltx_emph ltx_font_italic">autoregressive (AR) LT</em>, which generates codebooks sequentially by conditioning each prediction on previously generated codebooks, and (ii) a <em class="ltx_emph ltx_font_italic">MaskGIT-inspired LT</em>, which employs iterative masked prediction to jointly model codebooks. Both methods allow for the integration of <em class="ltx_emph ltx_font_italic">frame stacking</em>, where the primary transformer predicts multiple consecutive frames, and the LT resolves the corresponding codebooks. This hierarchical strategy offers the potential to substantially increase generation speed while maintaining perceptual quality, striking a balance between throughput and fidelity.
In this study, we systematically analyze the tradeoffs of parallel versus iterative sampling strategies for acoustic code prediction in speech generation LLMs.
<span class="ltx_text ltx_font_bold">Our contributions are as follows:</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">We study two iterative multi-codebook prediction heads—autoregressive and MaskGIT—for LLM-based speech synthesis models. We demonstrate that both of these techniques outperform a parallel prediction head in terms of audio quality and target speaker similarity for zero-shot TTS models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">When combined with frame stacking, we demonstrate that a model equipped with an AR LT can achieve 2.1x faster throughput while improving Fréchet Distances and preserving intelligibility and speaker similarity and neural MOS compared to a parallel prediction baseline without frame stacking. This frame-stacking approach allows us to significantly improve throughput of the TTS model without having to retrain a lower frame rate speech codec.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p">We distill practical guidelines for selecting among these approaches, offering insights into how design choices can be aligned with deployment requirements such as real-time synthesis or offline batch generation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background: Multi-codebook Neural Audio Codecs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Neural audio codecs represent audio as a sequence of discrete tokens arranged in a two-dimensional structure of size <math alttext="(T,N)" class="ltx_Math" display="inline" id="S2.SS1.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T,N)</annotation></semantics></math>, where <math alttext="T" class="ltx_Math" display="inline" id="S2.SS1.p1.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the number of temporal frames and <math alttext="N" class="ltx_Math" display="inline" id="S2.SS1.p1.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is the number of codebooks per frame. Each audio frame is thus described by a set of <math alttext="N" class="ltx_Math" display="inline" id="S2.SS1.p1.m4" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebook entries.
A key challenge arises because these codebooks are not statistically independent. For Residual Vector Quantization (RVQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib9" title="">9</a>]</cite> this is true by construction, since higher codebooks are constructed from the residual of lower ones. But even for Finite Scalar Quantization (FSQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib10" title="">10</a>]</cite> there is nothing in the training objective directly enforcing independence between the codebooks.
Conventional parallel prediction strategies assume independence among the <math alttext="N" class="ltx_Math" display="inline" id="S2.SS1.p1.m5" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebooks, enabling efficient decoding but often producing artifacts due to mismatches across codebooks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>]</cite>. This motivates the use of iterative prediction strategies, where codebooks within a frame are decoded in a dependent manner, better capturing the intra-frame structure.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Base Model: Koel-TTS</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">Our study builds on Koel-TTS, an encoder-decoder TTS system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>]</cite>. The encoder processes linguistic inputs, while the decoder autoregressively predicts discrete acoustic tokens (Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S2.F1" title="Figure 1 ‣ 2.4 Frame Stacking ‣ 2 Methodology ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_tag">1</span></a>). For audio tokenization we use a 21.5 frames per second, 8-codebook NanoCodec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib11" title="">11</a>]</cite>. Koel-TTS predicts all <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebooks at each timestep in parallel. While efficient, this parallel decoding inherits the limitations discussed above, that is, the intra-frame dependencies are ignored.
Consequently, the generated speech suffers from reduced fidelity, which we empirically demonstrate in our experiments (Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3" title="3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_tag">3</span></a>).
To address this, we augment Koel-TTS with an LT that explicitly models intra-frame dependencies, enabling iterative prediction of the codebooks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Local Transformer for Iterative Prediction</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p">The LT operates at the frame level and refines the predictions of the primary decoder. Given the hidden state output by the primary decoder at a given frame, the LT predicts codebooks iteratively, either autoregressively or through MaskGIT.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Autoregressive Local Transformer</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p">In the autoregressive variant, the LT generates the <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebook entries sequentially within each frame. Conditioned on previously generated codebooks and the hidden state from the primary decoder, the LT predicts the next codebook until all <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> entries are decoded. This formulation assumes causal dependencies from codebooks <math alttext="1" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.m3" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> to <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.m4" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> which is true by construction for RVQs. However, even for FSQs, we find that this approach yields better results than parallel prediction, albeit at the cost of increased latency proportional to <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS1.p1.m5" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>MaskGIT Local Transformer</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p">In the MaskGIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib12" title="">12</a>]</cite> variant, the LT starts from a fully masked sequence and progressively unmasks it through iterative prediction. The LT is configured with non-causal self-attention. The input sequence is initialized with the hidden state followed by <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> mask embeddings. After each forward pass, a subset of predictions is retained and unmasked, and this partially unmasked sequence is fed into the next iteration. After
<math alttext="P" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> iterations, all <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> codebooks are decoded. Importantly, <math alttext="P" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.m4" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> can be smaller than <math alttext="N" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p1.m5" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>, allowing multiple tokens to be predicted in parallel, thereby speeding up inference and allowing a flexible tradeoff between speed and quality. In addition to its speed benefits, this scheme also enables <em class="ltx_emph ltx_font_italic">bidirectional</em> dependency modeling between codebooks in the frame, without constraining decoding to a particular codebook order (unlike the AR LT). The main drawback is that dependencies between codebooks that get unmasked in the same iteration are not captured, i.e. the modeling of the joint probability distribution is <em class="ltx_emph ltx_font_italic">approximate</em>, as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Frame Stacking</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p">The hierarchical setup of the primary decoder and LT gives us an opportunity to offload work to the LT and let the primary decoder operate at a lower frame rate, further improving efficiency. We call this <em class="ltx_emph ltx_font_italic">frame stacking</em>. Concretely, the primary decoder is trained to predict <math alttext="S" class="ltx_Math" display="inline" id="S2.SS4.p1.m1" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> frames, i.e. <math alttext="S\times N" class="ltx_Math" display="inline" id="S2.SS4.p1.m2" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">S\times N</annotation></semantics></math> codebooks, in one step. We pass the hidden state from the primary decoder into the LT. The LT then predicts all <math alttext="S\times N" class="ltx_Math" display="inline" id="S2.SS4.p1.m3" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">S\times N</annotation></semantics></math> codebooks. We study both variants of the LT combined with frame-stacking. We use separate embedding tables for codebooks at different frame indices within the frame stack to disambiguate between them; these embeddings are shared between the primary decoder and the LT. At the input to the primary decoder they are averaged across both the frame stack and codebooks. The frame-stacking setup is faster than the baseline for two reasons. First, the LT is much <em class="ltx_emph ltx_font_italic">smaller</em> than the primary decoder. Second, <em class="ltx_emph ltx_font_italic">it operates on a short sequence</em> of up to  <math alttext="N+1" class="ltx_Math" display="inline" id="S2.SS4.p1.m4" intent=":literal"><semantics><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N+1</annotation></semantics></math> elements, compared to the primary decoder which must self-attend to the entire generation history and also cross-attend to the text encoder. These speed benefits increase with the stacking factor <math alttext="S" class="ltx_Math" display="inline" id="S2.SS4.p1.m5" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>, as explored in section <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3" title="3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="435" id="S2.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 1</span>: </span><span class="ltx_text" style="font-size:90%;">Model Architecture</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1001" id="S3.F2.sf1.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">UTMOSv2 (unseen speakers)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1000" id="S3.F2.sf2.g1" src="x3.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">WER (unseen speakers)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1000" id="S3.F2.sf3.g1" src="x4.png" width="829"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">SSIM (seen speakers)</span></figcaption>
</figure>
</div>
<figure class="ltx_figure ltx_align_center" id="S3.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1000" id="S3.F2.sf4.g1" src="x5.png" width="829"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(d)</span> </span><span class="ltx_text" style="font-size:90%;">SSIM (unseen speakers)</span></figcaption>
</figure><div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel">
<figure class="ltx_figure ltx_align_center" id="S3.F2.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="280" id="S3.F2.sf5.g1" src="x6.png" width="233"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(e)</span> </span><span class="ltx_text" style="font-size:90%;">Fréchet Distance (unseen speakers)</span></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="S3.F2.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="281" id="S3.F2.sf6.g1" src="x7.png" width="232"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(f)</span> </span><span class="ltx_text" style="font-size:90%;">Inference Speed</span></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="S3.F2.sf7">
<div class="ltx_inline-block ltx_minipage ltx_transformed_outer" style="width:243.1pt;height:265.2pt;vertical-align:-130.0pt;"><span class="ltx_transformed_inner" style="transform:translate(8.1pt,-5.0pt) scale(1.03892003603974,1.03892003603974) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;">Eval Set</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;">Stack Factor</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">LT Type</td>
<td class="ltx_td ltx_align_center ltx_border_tt">WER(%)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.F2.sf7.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">SSIM<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.F2.sf7.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">FD<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.F2.sf7.m3" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt">UTMOSv2 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.F2.sf7.m4" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"><span class="ltx_text ltx_align_center"></span> <span class="ltx_text ltx_align_center">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Seen</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Speakers</span></span>
</span></span> <span class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"><span class="ltx_text ltx_align_center">1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">none</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m5" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.796 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m6" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.089 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m7" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.54 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m8" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">MaskGIT</td>
<td class="ltx_td ltx_align_center">1.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m9" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.2</td>
<td class="ltx_td ltx_align_center">0.807 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m10" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">0.050 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m11" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">3.67 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m12" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">AR</td>
<td class="ltx_td ltx_align_center">1.2 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m13" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.3</td>
<td class="ltx_td ltx_align_center">0.810 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m14" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center">0.049 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m15" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">3.66 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m16" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"><span class="ltx_text ltx_align_center">2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">none</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m17" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.754 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m18" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.161 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m19" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.47 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m20" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">MaskGIT</td>
<td class="ltx_td ltx_align_center">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m21" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.3</td>
<td class="ltx_td ltx_align_center">0.790 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m22" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.001</td>
<td class="ltx_td ltx_align_center">0.055 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m23" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.001</td>
<td class="ltx_td ltx_align_center">3.63 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m24" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">AR</td>
<td class="ltx_td ltx_align_center">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m25" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.4</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">0.799</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m26" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">0.057 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m27" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">3.70 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m28" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"><span class="ltx_text ltx_align_center">4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">none</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m29" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.676 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m30" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.281 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m31" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.004</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.27 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m32" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">MaskGIT</td>
<td class="ltx_td ltx_align_center">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m33" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.2</td>
<td class="ltx_td ltx_align_center">0.769 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m34" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">0.061 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m35" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">3.45 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m36" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">AR</td>
<td class="ltx_td ltx_align_center">1.2 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m37" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">0.779</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m38" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.001</td>
<td class="ltx_td ltx_align_center">0.060 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m39" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">3.68</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m40" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"><span class="ltx_text ltx_align_center"></span> <span class="ltx_text ltx_align_center">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Unseen</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_center">Speakers</span></span>
</span></span> <span class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"><span class="ltx_text ltx_align_center">1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">none</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.2 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m41" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.765 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m42" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.001</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.086 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m43" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.57 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m44" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">MaskGIT</td>
<td class="ltx_td ltx_align_center">1.5 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m45" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.4</td>
<td class="ltx_td ltx_align_center">0.777 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m46" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.005</td>
<td class="ltx_td ltx_align_center">0.063 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m47" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.004</td>
<td class="ltx_td ltx_align_center">3.68 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m48" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">AR</td>
<td class="ltx_td ltx_align_center">1.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m49" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.3</td>
<td class="ltx_td ltx_align_center">0.784 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m50" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">0.054</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m51" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.003</td>
<td class="ltx_td ltx_align_center">3.66 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m52" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"><span class="ltx_text ltx_align_center">2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">none</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.2 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m53" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.695 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m54" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.005</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.144 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m55" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.46 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m56" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">MaskGIT</td>
<td class="ltx_td ltx_align_center">1.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m57" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.3</td>
<td class="ltx_td ltx_align_center">0.741 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m58" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">0.053</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m59" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.001</td>
<td class="ltx_td ltx_align_center">3.63 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m60" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">AR</td>
<td class="ltx_td ltx_align_center">1.0 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m61" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">0.757</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m62" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">0.056 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m63" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">3.70 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m64" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"><span class="ltx_text ltx_align_center">4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">none</td>
<td class="ltx_td ltx_align_center ltx_border_t">1.5 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m65" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.545 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m66" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.004</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.312 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m67" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.004</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.22 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m68" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center">MaskGIT</td>
<td class="ltx_td ltx_align_center">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m69" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center">0.624 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m70" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.005</td>
<td class="ltx_td ltx_align_center">0.071 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m71" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center">3.41 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m72" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:34.1pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span class="ltx_inline-block ltx_align_top">
<span class="ltx_p" style="width:25.6pt;"></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb">AR</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m73" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb">
<span class="ltx_text ltx_font_bold">0.642</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m74" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.002</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.070 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m75" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.004</td>
<td class="ltx_td ltx_align_center ltx_border_bb">
<span class="ltx_text ltx_font_bold">3.71</span> <math alttext="\pm" class="ltx_Math" display="inline" id="S3.F2.sf7.m76" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.05</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(g)</span> </span><span class="ltx_text" style="font-size:90%;">Metrics on LibriTTS. We report 95% CIs. Bold indicates the best mean score; no bolding is applied if CIs overlap. </span></figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 2</span>: </span><span class="ltx_text" style="font-size:90%;">Evaluation Results on LibriTTS. 1x, 2x, 4x in the model names refer to the number of frames stacked. (a) LT models achieve same or better UTMOSv2 scores than baseline, except the 4x-stacked MaskGit LT. (b) WERs are similar for all models, with CIs overlapping. (c)(d) SSIMs: at 1x stacking, LT models have an advantage; at 2x, LT models are similar to baseline; at 4x, LT models are still usable for seen speakers, but with reduced robustness to unseen speakers. (e) FDs: LT models consistently exhibit a strong advantage over parallel sampling. (f) Significant inference speedup from frame stacking. (g) Metrics table.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Training and Sampling Setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">We base our experiments on the TTS model described in Koel-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>]</cite>. To keep the number of transformer layers fixed between the baseline and LT models, we add 4 transformer layers to the baseline’s decoder such that it has 16 layers while the LT-based models have 12 in main decoder and 4 in the LT, for the same total of 16 layers. All transformer decoder layers (primary and LT) have dimension 768 and 12 self-attention heads. We experiment with stacking factors of 1 (no stacking), 2 and 4. We train all models on the same 18k hours of data as in the Koel-TTS paper for 220k steps with the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib14" title="">14</a>]</cite>. For sampling, we use the same CFG, top-k, and temperature setup as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>]</cite>. For models with a MaskGIT-based LT we use 3 sampling steps and choose which tokens to unmask using <span class="ltx_text ltx_font_italic">purity sampling</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Criteria</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">We evaluate the model on the same seen and unseen-speaker subsets of LibriTTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib17" title="">17</a>]</cite> as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib3" title="">3</a>]</cite>, containing 180 utterances each. To assess text adherence, we report the Word Error Rate (<span class="ltx_text ltx_font_bold">WER</span>) computed with the Parakeet-TDT-1.1b ASR model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib18" title="">18</a>]</cite>. For <em class="ltx_emph ltx_font_italic">Speaker Similarity</em> (<span class="ltx_text ltx_font_bold">SSIM</span>) , we extract speaker embeddings using TitaNet-Large <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib19" title="">19</a>]</cite> and compute the cosine similarity between the generated and reference (context) audio. Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib20" title="">20</a>]</cite> and others, we compute a Fréchet Distance (<span class="ltx_text ltx_font_bold">FD</span>) in the codec’s 32-dimensional embedding space. We use model-generated frames as the generated distribution and the ground truth LibriTTS codec frames of the same utterances as the real distribution. This metric measures how closely the distribution of generated and real codec frames match, capturing both fidelity and diversity. If a sampling method yields implausible codebook token combinations, this metric should detect it. We report speech quality using <span class="ltx_text ltx_font_bold">UTMOSv2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#bib.bib21" title="">21</a>]</cite>, a state-of-the-art neural estimator of naturalness mean opinion scores (MOS). Inference speed (i.e., throughput) is reported relative to the baseline’s speed.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>To ensure our quality comparisons were fair, the baseline model was designed with 16 decoder layers to match the total layer count (12+4) of the LT models. While this establishes an equitable footing for quality metrics, it slightly inflates speedup figures. A baseline with only 12 layers would be roughly 14% faster, which would proportionally lower the speedup figures we report. Nevertheless, the fundamental conclusion that frame-stacked models offer significant throughput gains remains valid.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Results</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.F2.sf7" title="In Figure 2 ‣ 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_tag">2(g)</span></a> provides a full set of evaluation metrics on LibriTTS.
In all reported results, <em class="ltx_emph ltx_font_italic">baseline</em> denotes the non-stacked model (1x-stacked) with no LT, parallel-sampled across all codebooks of a single frame. Speeds are reported in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.F2.sf6" title="In Figure 2 ‣ 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_tag">2(f)</span></a>. We next review these results.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Iterative Sampling and FD</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19592v1#S3.F2.sf5" title="In Figure 2 ‣ 3 Experiments ‣ Frame-Stacked Local Transformers for Efficient Multi-Codebook Speech Generation"><span class="ltx_text ltx_ref_tag">2(e)</span></a> shows the FDs for unseen speakers (trends for seen speakers are similar). At each stacking factor, the LT-based models have a lower (better) FD than the no-LT (parallel-sampled) model. In fact, remarkably, all LT-based models, at all stacking factors, have a lower FD than all parallel models, even the unstacked one. This result supports the hypothesis that iterative sampling generates a distribution that is closer to the ground truth than parallel sampling.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Local Transformer and Frame Stacking</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">At a frame stacking factor of 1</span>, SSIMs and MOS estimates for both LT-based models outperform baseline for both seen and unseen speakers. WERs are slightly higher for LT models compared to baseline, but the difference is not statistically significant. Speeds are similar across the three models. Overall, the LT-based models are a better choice at this stacking factor, producing better SSIM, MOS and FD at similar speed.
<span class="ltx_text ltx_font_bold">At a frame stacking factor of 2</span>, we start observing major speed benefits from frame stacking. The AR LT is <span class="ltx_text ltx_font_bold">2.1x</span> faster and the MaskGIT LT <span class="ltx_text ltx_font_bold">3.1x</span> faster than the unstacked parallel baseline. SSIMs are similar to baseline for seen speakers and nearly as good for unseen speakers. MOS scores are better or within CI of baseline. If we try to <em class="ltx_emph ltx_font_italic">parallel</em>-sample from a 2x-stacked model, the sampling starts to degrade substantially, with FDs in particular worsening by 67% for unseen speakers and MOS decreasing. This is not surprising as parallel sampling two frames, rather than one, can only exacerbate the issues with parallel sampling discussed earlier. Overall, even at a stacking factor of 2, the LT-based models are a better choice than the baseline. <span class="ltx_text ltx_font_bold">At a frame stacking factor of 4,</span> there are large speedups of 2.9x (LT) and 5.5x (MaskGIT) vs baseline but with some cost in quality and robustness: SSIMs drop a little for seen speakers but substantially for unseen speakers. WER confidence intervals remain overlapping with baseline. FDs remain better than baseline. MOS for the AR LT remains similar to baseline but for MaskGit there is a significant drop. We attribute this drop to our use of only 3 sampling steps—a constraint likely too severe for sampling <math alttext="8\times 4=32" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.m1" intent=":literal"><semantics><mrow><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>4</mn></mrow><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">8\times 4=32</annotation></semantics></math> tokens—and expect that relaxing it could mitigate the degradation. Parallel sampling breaks at this stacking factor, with large large degradations in both FD and SSIM. Based on these results, <span class="ltx_text ltx_font_bold">we propose the following practical guidelines:</span></p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p">When quality is the primary consideration, the non-frame-stacked configuration with an autoregressive local transformer is recommended.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p">A good balance between quality and complexity can be achieved by using frame stacking factor of 2 with an autoregressive LT. This works as well as baseline but is 2.1x faster. MaskGIT also achieves good performance at a 3.1x speedup.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p">When aggressively seeking speedup, and not needing zero-shot functionality, use a high stacking factor (e.g. 4) with a LT (either AR or MaskGIT).</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">We study iterative multi-codebook prediction methods using Local Transformers for TTS models and provide a comparative analysis against parallel prediction. We demonstrate that iterative prediction is important to capture intra-codebook dependencies, improving audio fidelity. LTs also enable frame-stacking in the primary decoder, which substantially improves the throughput without having to retrain a lower frame rate codec model.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Sanyuan Chen, Shujie Liu, Long Zhou, Yanqing Liu, Xu Tan, Jinyu Li, Sheng Zhao, Yao Qian, and Furu Wei,

</span>
<span class="ltx_bibblock">“VALL-E 2: Neural codec language models are human parity zero-shot text to speech synthesizers,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv:2406.05370</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Paarth Neekhara, Shehzeen Hussain, Subhankar Ghosh, Jason Li, and Boris Ginsburg,

</span>
<span class="ltx_bibblock">“Improving robustness of LLM-based speech synthesis by learning monotonic alignment,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">Interspeech</span>, 2024, pp. 3425–3429.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Mikyas T Desta, Roy Fejgin, Rafael Valle, and Jason Li,

</span>
<span class="ltx_bibblock">“Koel-TTS: Enhancing LLM based speech generation with preference alignment and classifier free guidance,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">EMNLP</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Haohan Guo, Xuankai Chang, Jiatong Shi, Jiang Bian, Zhou Zhao, et al.,

</span>
<span class="ltx_bibblock">“UniAudio: Towards universal audio generation with large language models,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">ICML</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka,

</span>
<span class="ltx_bibblock">“SpeechX: Neural codec language model as a versatile speech transformer,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour,

</span>
<span class="ltx_bibblock">“Moshi: a speech-text foundation model for real-time dialogue,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv:2410.00037</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, et al.,

</span>
<span class="ltx_bibblock">“Vibevoice technical report,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv:2508.19205</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez,

</span>
<span class="ltx_bibblock">“Simple and controllable music generation,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol. 36, pp. 47704–47720, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi,

</span>
<span class="ltx_bibblock">“Soundstream: An end-to-end neural audio codec,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, vol. 30, pp. 495–507, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen,

</span>
<span class="ltx_bibblock">“Finite scalar quantization: Vq-vae made simple,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.15505</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Edresson Casanova, Paarth Neekhara, Ryan Langman, Shehzeen Hussain, Subhankar Ghosh, Xuesong Yang, Ante Jukic, Jason Li, and Boris Ginsburg,

</span>
<span class="ltx_bibblock">“NanoCodec: Towards high-quality ultra fast speech LLM inference,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">Interspeech</span>, 2025, pp. 5028–5032.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman,

</span>
<span class="ltx_bibblock">“MaskGIT: Masked generative image transformer,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">ICCV</span>, 2022, pp. 11315–11325.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, and Matthieu Cord,

</span>
<span class="ltx_bibblock">“Halton scheduler for masked generative image transformer,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">International Conference on Learning Representations (ICLR)</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter,

</span>
<span class="ltx_bibblock">“Decoupled weight decay regularization,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">International Conference on Learning Representations (ICLR)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen,

</span>
<span class="ltx_bibblock">“Improved vector quantized diffusion models,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv:2205.16007</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jaewoong Lee, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Yunji Kim, Jin-Hwa Kim, Jung-Woo Ha, and Sung Ju Hwang,

</span>
<span class="ltx_bibblock">“Text-conditioned sampling framework for text-to-image generation with masked generative models,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">ICCV</span>, 2023, pp. 23252–23262.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu,

</span>
<span class="ltx_bibblock">“Libritts: A corpus derived from librispeech for text-to-speech,”

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.02882</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Dima Rekesh, Nithin Rao Koluguri, Samuel Kriman, Somshubra Majumdar, Vahid Noroozi, He Huang, Oleksii Hrinchuk, Krishna Puvvada, Ankur Kumar, Jagadeesh Balam, et al.,

</span>
<span class="ltx_bibblock">“Fast conformer with linearly scalable attention for efficient speech recognition,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span>. IEEE, 2023, pp. 1–8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Nithin Rao Koluguri, Taejin Park, and Boris Ginsburg,

</span>
<span class="ltx_bibblock">“Titanet: Neural model for speaker representation with 1d depth-wise separable convolutions and global context,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">ICASSP</span>. IEEE, 2022, pp. 8102–8106.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serrà,

</span>
<span class="ltx_bibblock">“Masked generative video-to-audio transformers with enhanced synchronicity,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>. Springer, 2024, pp. 247–264.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Kaito Baba, Wataru Nakata, Yuki Saito, and Hiroshi Saruwatari,

</span>
<span class="ltx_bibblock">“The t05 system for the VoiceMOS Challenge 2024: Transfer learning from deep image classifier to naturalness MOS prediction of high-quality synthetic speech,”

</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_font_italic">IEEE Spoken Language Technology Workshop (SLT)</span>, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 23 21:23:39 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
