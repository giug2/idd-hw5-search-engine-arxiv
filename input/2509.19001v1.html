<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS</title>
<!--Generated on Tue Sep 23 13:20:16 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2509.19001v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S1" title="In HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S2" title="In HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>METHODOLOGY</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S2.SS1" title="In 2 METHODOLOGY ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Speech Token Codec with Content- and Prompt-Preference Token Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S2.SS2" title="In 2 METHODOLOGY ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>LLM’s Hierarchical Decoding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3" title="In HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>EXPERIMENTS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.SS1" title="In 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.SS2" title="In 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.SS2.SSS1" title="In 3.2 Experimental Results ‣ 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Comparison with Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.SS2.SSS2" title="In 3.2 Experimental Results ‣ 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Ablation Study on Preference Tokens</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.SS2.SSS3" title="In 3.2 Experimental Results ‣ 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Ablation Study on Hierarchical Decoding Strategy</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S4" title="In HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>CONCLUSION</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at  <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://xxh333.github.io/" title="">https://xxh333.github.io/</a>.</p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span class="ltx_text ltx_font_upright">— </span></span>
Text-to-Speech, Large Language Model, Speech Tokenizer, Controllable Synthesis</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="188" id="S1.F1.g1" src="mot.png" width="338"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold">Fig. 1</span>: </span>(a) shows a previous framework where LLM predicts a monolithic speech token sequence. (b) illustrates our proposed HD-PPT, which guides the LLM to model speech hierarchically by extracting content- and prompt-preference tokens from the speech tokens and jointly modeling them.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Recently, the naturalness of TTS models has achieved substantial progress <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib3" title="">3</a>]</cite>. However, the precise control of human-like speech synthesis remains a central challenge in Text-to-Speech (TTS), with expressive attributes like prosody, emotion, and timbre. To solve this problem, the instruction-based Text-to-Speech (Instruct-TTS) paradigm is proposed, aiming to generate high-quality speech that precisely adheres to descriptive natural language prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">Current approaches fall largely into two main categories: explicit style encoding methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib9" title="">9</a>]</cite> and Large Language Model (LLM)-driven methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib12" title="">12</a>]</cite>. Although explicit style encoding methods can achieve basic prompt control, they are limited by their inefficient structure and coarse-grained control. In contrast, LLM-based methods offer a more flexible architecture and stronger control for interpreting nuanced textual instructions. Despite their promise, these methods often struggle with precision and robustness, particularly when faced with complex or subtle prompts. This is primarily because they directly map the style information from the text instructions onto the speech tokens, making fine-grained control difficult, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">1</span></a>(a). In addition, they treat speech tokens as a monolithic sequence. We posit that this limitation stems from a fundamental hierarchical mismatch: they attempt to map a single-level text instruction directly onto multilevel speech tokens. This approach overlooks the inherently hierarchical nature of speech, which involves three types of information: linguistic, paralinguistic, and extralinguistic, corresponding to spoken content, prosody/emotion, and speaker/scenario, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:208.1pt;">
<p class="ltx_p ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="223" id="S1.F2.g1" src="codec.png" width="291"/></p>
<p class="ltx_p ltx_align_center">(a) Speech Token Codec</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:208.1pt;">
<p class="ltx_p ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="258" id="S1.F2.g2" src="llm.png" width="393"/></p>
<p class="ltx_p ltx_align_center">(b) An overview of the proposed HD-PPT</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Figure (a) illustrates the speech token codec, which extracts content- and prompt-preference tokens from speech tokens. Figure (b) shows the overall architecture of HD-PPT, comprising the hierarchical LLM and a subsequent vocoder.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">To resolve this hierarchical mismatch, we reframe the synthesis task from monolithic generation to a structured process. We propose <span class="ltx_text ltx_font_bold">HD-PPT</span>, a framework for <span class="ltx_text ltx_font_bold">H</span>ierarchical <span class="ltx_text ltx_font_bold">D</span>ecoding of Content- and <span class="ltx_text ltx_font_bold">P</span>rompt-<span class="ltx_text ltx_font_bold">P</span>reference <span class="ltx_text ltx_font_bold">T</span>okens for Instruct-TTS. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">1</span></a>(b), our approach is founded on two key innovations designed to bridge the gap between instruction and audio. To enable fine-grained control, we introduce a novel speech token codec. Jointly supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib14" title="">14</a>]</cite>, it distinguishes between prompt-preference tokens to capture fine-grained style and content-preference tokens to anchor semantics. To bridge the hierarchical gap, we design a hierarchical decoding strategy. This guides the LLM to generate these representations sequentially: first establishing the semantic foundation, then layering stylistic details, and finally rendering the complete acoustic representation. This structured generation process dramatically enhances the model’s ability to execute instructions with precision and fidelity.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">In summary, our main contributions are as follows.
1) A novel speech codec to extract and differentiate content- and prompt-preference tokens, providing a fine-grained intermediate modeling target for the LLM.
2) A hierarchical decoding strategy that aligns generation with the intrinsic structure of speech, guiding the LLM to render audio hierarchically to improve complex instruction execution.
3) Extensive validation of our method’s effectiveness, demonstrating state-of-the-art performance in both naturalness and control accuracy.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>METHODOLOGY</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">The HD-PPT framework consists of three main components, as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">1</span></a>: 1) a speech token codec designed to extract content- and prompt-preference tokens from the speech token; 2) an LLM with a hierarchical decoder, which receives natural language instructions and generates various token sequences in a structured manner; and 3) a vocoder, which synthesizes the final waveform from the generated speech tokens and speaker embeddings. The core design principle is to transform speech synthesis from predicting an undifferentiated acoustic sequence into a structured hierarchical generation process.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speech Token Codec with Content- and Prompt-Preference Token Extraction</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">To effectively extract fine-grained preference representations from speech, we designed a speech token codec based on finite-scalar quantization (FSQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib15" title="">15</a>]</cite>, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">2</span></a>(a). The model is optimized via a combination of a reconstruction loss and two auxiliary supervision tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">The codec employs a transformer-based architecture. A preference token extractor first encodes the input speech tokens (from the pre-trained CosyVoice2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib10" title="">10</a>]</cite> tokenizer) into a continuous representation <math alttext="Z" class="ltx_Math" display="inline" id="S2.SS1.p2.m1" intent=":literal"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>. Subsequently, two independent FSQ modules quantize <math alttext="Z" class="ltx_Math" display="inline" id="S2.SS1.p2.m2" intent=":literal"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math> into distinct, discrete preference tokens. Finally, a causal transformer-based speech token combiner fuses these preference tokens to reconstruct the original speech tokens. This causal design enforces temporal alignment between the representations. In addition, slight random noise is injected during training to enhance robustness.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p">To ensure that the preference tokens capture distinct speech attributes, we impose specific supervision mechanisms. The content-preference tokens are supervised by an ASR task, using a Whisper-Small decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib16" title="">16</a>]</cite> to predict text, thus exposing them to semantic information. The prompt-preference tokens are supervised by a CLAP-based contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib14" title="">14</a>]</cite> to capture prosody and emotion. A cross-attention module maps these tokens to a fixed-length embedding. This embedding is trained to maximize cosine similarity with the text embedding of the corresponding prompt (from a pre-trained RoBERTa-base model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib17" title="">17</a>]</cite>), while minimizing similarity to embeddings of noncorresponding prompts. This objective compels the prompt-preference tokens to encode fine-grained stylistic attributes correlated with the prompt.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p">The total loss is a weighted sum of the reconstruction, ASR, and CLAP losses:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{total}=L_{rec}+\lambda_{asr}L_{asr}+\lambda_{clap}L_{clap}" class="ltx_Math" display="block" id="S2.E1.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow></msub><mo>=</mo><mrow><msub><mi>L</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>c</mi></mrow></msub><mo>+</mo><mrow><msub><mi>λ</mi><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>L</mi><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>L</mi><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">L_{total}=L_{rec}+\lambda_{asr}L_{asr}+\lambda_{clap}L_{clap}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p">where <math alttext="L_{rec}" class="ltx_Math" display="inline" id="S2.SS1.p5.m1" intent=":literal"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>c</mi></mrow></msub><annotation encoding="application/x-tex">L_{rec}</annotation></semantics></math> is the cross-entropy loss for reconstruction, and the weights <math alttext="\lambda_{asr}" class="ltx_Math" display="inline" id="S2.SS1.p5.m2" intent=":literal"><semantics><msub><mi>λ</mi><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{asr}</annotation></semantics></math> and <math alttext="\lambda_{clap}" class="ltx_Math" display="inline" id="S2.SS1.p5.m3" intent=":literal"><semantics><msub><mi>λ</mi><mrow><mi>c</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{clap}</annotation></semantics></math> are set to 2.0 and 0.8, respectively. Through this joint optimization strategy, the codec effectively learns to extract different preference representations tailored for hierarchical synthesis.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>LLM’s Hierarchical Decoding</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">With the preference speech tokens established, we leverage an LLM to generate them from textual instructions. We chose Qwen2.5-0.5B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib18" title="">18</a>]</cite> as backbone, paired with a lightweight transformer decoder to perform hierarchical generation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">The LLM auto-regressively generates a sequence of hidden states <math alttext="T_{h}" class="ltx_Math" display="inline" id="S2.SS2.p2.m1" intent=":literal"><semantics><msub><mi>T</mi><mi>h</mi></msub><annotation encoding="application/x-tex">T_{h}</annotation></semantics></math> based on input text <math alttext="T_{t}" class="ltx_Math" display="inline" id="S2.SS2.p2.m2" intent=":literal"><semantics><msub><mi>T</mi><mi>t</mi></msub><annotation encoding="application/x-tex">T_{t}</annotation></semantics></math>. At each step, the hidden state is fed into the lightweight hierarchical decoder to sequentially predict the tokens. Assume that the content-preference tokens, prompt-preference tokens, and speech tokens are <math alttext="T_{c}" class="ltx_Math" display="inline" id="S2.SS2.p2.m3" intent=":literal"><semantics><msub><mi>T</mi><mi>c</mi></msub><annotation encoding="application/x-tex">T_{c}</annotation></semantics></math>, <math alttext="T_{p}" class="ltx_Math" display="inline" id="S2.SS2.p2.m4" intent=":literal"><semantics><msub><mi>T</mi><mi>p</mi></msub><annotation encoding="application/x-tex">T_{p}</annotation></semantics></math>, and <math alttext="T_{s}" class="ltx_Math" display="inline" id="S2.SS2.p2.m5" intent=":literal"><semantics><msub><mi>T</mi><mi>s</mi></msub><annotation encoding="application/x-tex">T_{s}</annotation></semantics></math>, respectively. Additionally, let <math alttext="\theta_{LM}" class="ltx_Math" display="inline" id="S2.SS2.p2.m6" intent=":literal"><semantics><msub><mi>θ</mi><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow></msub><annotation encoding="application/x-tex">\theta_{LM}</annotation></semantics></math>, <math alttext="\theta_{HD}" class="ltx_Math" display="inline" id="S2.SS2.p2.m7" intent=":literal"><semantics><msub><mi>θ</mi><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow></msub><annotation encoding="application/x-tex">\theta_{HD}</annotation></semantics></math> represent the parameters of the LLM and the decoder. The generation process at step <math alttext="j" class="ltx_Math" display="inline" id="S2.SS2.p2.m8" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> is as follows:</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Content Foundation</span>. The modified LLM generates a hidden state <math alttext="T_{h,j}" class="ltx_Math" display="inline" id="S2.SS2.p3.m1" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{h,j}</annotation></semantics></math>, which is then passed to the hierarchical decoder to produce the content-preference token <math alttext="T_{c,j}" class="ltx_Math" display="inline" id="S2.SS2.p3.m2" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>c</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{c,j}</annotation></semantics></math>, thereby establishing a semantic basis for the current timestep <math alttext="j" class="ltx_Math" display="inline" id="S2.SS2.p3.m3" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_gather ltx_eqn_table" id="S4.EGx1">
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\displaystyle p(T_{h,j}|T_{t};\theta_{LM})=p(T_{h,j}|T_{t},T_{s,:j})" class="ltx_Math" display="block" id="S2.E2.m1" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>s</mi><mo>,</mo><mrow><mi></mi><mo lspace="0.278em" rspace="0.278em">:</mo><mi>j</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle p(T_{h,j}|T_{t};\theta_{LM})=p(T_{h,j}|T_{t},T_{s,:j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\displaystyle p(T_{c,j}|T_{t};\theta_{LM},\theta_{HD})=p(T_{c,j}|T_{h,j})" class="ltx_Math" display="block" id="S2.E3.m1" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>c</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow></msub><mo>,</mo><msub><mi>θ</mi><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>c</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle p(T_{c,j}|T_{t};\theta_{LM},\theta_{HD})=p(T_{c,j}|T_{h,j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Style Rendering</span>. Subsequently, the model renders stylistic attributes. The prompt-preference token <math alttext="T_{p,j}" class="ltx_Math" display="inline" id="S2.SS2.p4.m1" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{p,j}</annotation></semantics></math> is predicted by the decoder, conditioned on both the hidden state <math alttext="T_{h,j}" class="ltx_Math" display="inline" id="S2.SS2.p4.m2" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{h,j}</annotation></semantics></math> and the newly generated content token <math alttext="T_{c,j}" class="ltx_Math" display="inline" id="S2.SS2.p4.m3" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>c</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{c,j}</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(T_{p,j}|T_{t};\theta_{LM},\theta_{HD})=p(T_{p,j}|T_{h,j},T_{c,j})" class="ltx_Math" display="block" id="S2.E4.m1" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>p</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow></msub><mo>,</mo><msub><mi>θ</mi><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>p</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>c</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p(T_{p,j}|T_{t};\theta_{LM},\theta_{HD})=p(T_{p,j}|T_{h,j},T_{c,j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Final Token Generation</span>. Finally, with both semantic and stylistic foundations in place, the complete speech token <math alttext="T_{s,j}" class="ltx_Math" display="inline" id="S2.SS2.p5.m1" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>s</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{s,j}</annotation></semantics></math> is predicted by fusing all prior information:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(T_{s,j}|T_{t};\theta_{LM},\theta_{HD})=p(T_{s,j}|T_{h,j},T_{c,j},T_{p,j})" class="ltx_Math" display="block" id="S2.E5.m1" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>s</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mi>t</mi></msub><mo>;</mo><msub><mi>θ</mi><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi></mrow></msub><mo>,</mo><msub><mi>θ</mi><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>T</mi><mrow><mi>s</mi><mo>,</mo><mi>j</mi></mrow></msub><mo fence="false">|</mo><mrow><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>c</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>p</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p(T_{s,j}|T_{t};\theta_{LM},\theta_{HD})=p(T_{s,j}|T_{h,j},T_{c,j},T_{p,j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p">The resulting speech token <math alttext="T_{s,j}" class="ltx_Math" display="inline" id="S2.SS2.p6.m1" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>s</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">T_{s,j}</annotation></semantics></math> is then fed back to the modified LLM to generate the next hidden state <math alttext="T_{h,j+1}" class="ltx_Math" display="inline" id="S2.SS2.p6.m2" intent=":literal"><semantics><msub><mi>T</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub><annotation encoding="application/x-tex">T_{h,j+1}</annotation></semantics></math> for the next timestep.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p">To ensure that the model robustly learns this hierarchical process, we employ two regularization strategies during training. First, we introduce stochasticity by probabilistically masking the hidden states and prompt tokens, and by concatenating token logits with the token embeddings as input to the lightweight decoder. These interventions compel the model to integrate the information from all available sources rather than relying on a single one. Second, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">2</span></a>(b), an auxiliary linear layer is added to directly project the LLM’s hidden states into the speech tokens, ensuring that its internal representations remain acoustically grounded.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold">Table 1</span>: </span>Comparison of results on TextrolSpeech and EmoVoice-DB test sets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:330.1pt;height:71.7pt;vertical-align:-34.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.4pt,13.1pt) scale(0.732192133514265,0.732192133514265) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold">Subjective</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Objective</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">MOS-N ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">MOS-S ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">DNSMOS ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">EMO-SIM ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">WER ↓</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PromptStyle</th>
<td class="ltx_td ltx_align_center ltx_border_t">2.674 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.145</td>
<td class="ltx_td ltx_align_center ltx_border_t">2.420 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m2" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.147</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.68</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.529</td>
<td class="ltx_td ltx_align_center ltx_border_t">17.92%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">PromptTTS</th>
<td class="ltx_td ltx_align_center">2.920 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m3" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.137</td>
<td class="ltx_td ltx_align_center">2.601 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m4" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.148</td>
<td class="ltx_td ltx_align_center">3.65</td>
<td class="ltx_td ltx_align_center">0.588</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">4.38%</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">CosyVoice</th>
<td class="ltx_td ltx_align_center">3.240 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m5" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.138</td>
<td class="ltx_td ltx_align_center">3.028 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m6" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.149</td>
<td class="ltx_td ltx_align_center">3.77</td>
<td class="ltx_td ltx_align_center">0.635</td>
<td class="ltx_td ltx_align_center">6.10%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">CosyVoice2</th>
<td class="ltx_td ltx_align_center">3.920 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m7" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.112</td>
<td class="ltx_td ltx_align_center">3.885 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m8" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.116</td>
<td class="ltx_td ltx_align_center">3.83</td>
<td class="ltx_td ltx_align_center">0.714</td>
<td class="ltx_td ltx_align_center">5.71%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">EmoVoice-PP</th>
<td class="ltx_td ltx_align_center">3.694 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m9" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.123</td>
<td class="ltx_td ltx_align_center">3.594 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m10" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.128</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">3.87</span></td>
<td class="ltx_td ltx_align_center">0.613</td>
<td class="ltx_td ltx_align_center">8.56%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">HD-PPT (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">4.108 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m11" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.105</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">4.167 <math alttext="\pm" class="ltx_Math" display="inline" id="S2.T1.m12" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.103</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">3.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">0.753</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">5.18%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>EXPERIMENTS</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">1) Datasets and Baselines.</span> We conducted experiments on two public datasets to ensure a comprehensive evaluation: TextrolSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib11" title="">11</a>]</cite> for fine-grained style control and EmoVoice-DB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib6" title="">6</a>]</cite> for emotional control. All audio was resampled to 24kHz. We compared HD-PPT against two categories of baselines: 1) <span class="ltx_text ltx_font_bold">Explicit style encoding</span>: PromptTTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib4" title="">4</a>]</cite> and PromptStyle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib8" title="">8</a>]</cite>; and 2) <span class="ltx_text ltx_font_bold">LLM-driven</span>: CosyVoice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib1" title="">1</a>]</cite>, EmoVoice-PP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib6" title="">6</a>]</cite>, and our main baseline, CosyVoice2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib10" title="">10</a>]</cite>, which represents one of the current state-of-the-art systems in Instruct-TTS.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">2) Evaluation metrics</span>. Our evaluation employed a combination of subjective and objective metrics. For subjective tests, 18 participants rated speech naturalness (MOS-N) and stylistic consistency (MOS-S) with the instructional text on a 5-point Likert scale. For objective evaluation, we used three metrics. We used the CV3-Eval toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib20" title="">20</a>]</cite> to obtain perceptual quality through the Deep Noise Suppression Mean Opinion Score (DNSMOS) and the word error rate (WER). Furthermore, emotional similarity (EMO-SIM) was quantified by the cosine similarity of the emotional feature vectors between real and synthesized audio, extracted using the pre-trained emotion2vec-plus-large model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">3) Implementation details</span>. We first trained our speech token codec, which consists of a 5-layer conformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib22" title="">22</a>]</cite> extractor and a 4-layer causal transformer combiner. The FSQ codebook sizes for the prompt- and content-preference tokens were set to 64 and 1296, respectively, both operating at a rate of 25Hz. The codec was trained for 50 epochs on 4 NVIDIA 4090 GPUs using the AdamW optimizer with a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S3.SS1.p3.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-4}</annotation></semantics></math>. Following this, we trained the modified LLM, which uses Qwen2.5-0.5B as its backbone. For this model, we employed a lightweight 2-layer auto-regressive transformer with a fixed length of 3 as the hierarchical Decoder. The LLM was trained for 16 epochs on the same hardware using the AdamW optimizer, but with a learning rate of <math alttext="1\times 10^{-5}" class="ltx_Math" display="inline" id="S3.SS1.p3.m2" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-5}</annotation></semantics></math>. For the final audio generation, we used the official pre-trained vocoder from CosyVoice2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib10" title="">10</a>]</cite>, which combines a flow-matching model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib23" title="">23</a>]</cite> and HifiGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Results</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Comparison with Baselines</h4>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold">Table 2</span>: </span>Ablation study on preference tokens.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:184.9pt;height:48.2pt;vertical-align:-22.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.8pt,12.7pt) scale(0.65453779398777,0.65453779398777) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">DNSMOS ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">EMO-SIM ↑</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">WER ↓</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">w/o Content-Pref.</th>
<td class="ltx_td ltx_align_center ltx_border_t">3.76</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.742</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">8.04%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o Prompt-Pref.</th>
<td class="ltx_td ltx_align_center">3.76</td>
<td class="ltx_td ltx_align_center">0.728</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">5.49%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o Dual-Pref.</th>
<td class="ltx_td ltx_align_center">3.73</td>
<td class="ltx_td ltx_align_center">0.716</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">10.10%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o Instruct Text</th>
<td class="ltx_td ltx_align_center">3.78</td>
<td class="ltx_td ltx_align_center">0.605</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">5.44%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">Proposed</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">3.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">0.753</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">5.18%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S2.T1" title="Table 1 ‣ 2.2 LLM’s Hierarchical Decoding ‣ 2 METHODOLOGY ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">1</span></a> presents a comprehensive comparison between HD-PPT and the five baseline models on the combined test sets of TextrolSpeech and EmoVoice-DB. HD-PPT achieves superior performance across the board. In subjective tests, it received the highest MOS-N and MOS-S scores, which prove its excellent naturalness and stylistic consistency. Objectively, it achieved the best EMO-SIM score for controllable emotional expression. These high scores directly validate that our hierarchical structure improved instruction adherence and stylistic control. Furthermore, HD-PPT also achieved a competitive DNSMOS and the second lowest WER, demonstrating its ability to generate high-fidelity and intelligible speech.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Ablation Study on Preference Tokens</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p">To validate the efficacy of preference tokens, we conducted ablation experiments across four variants: 1) <span class="ltx_text ltx_font_bold">w/o Content-Pref.</span>: removing content-preference tokens from the decoding process; 2) <span class="ltx_text ltx_font_bold">w/o Prompt-Pref.</span>: removing prompt-preference tokens; 3) <span class="ltx_text ltx_font_bold">w/o Dual-Pref.</span>: bypassing both preference tokens; and 4) <span class="ltx_text ltx_font_bold">w/o Instruct Text</span>: generating speech without the style prompt. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.T2" title="Table 2 ‣ 3.2.1 Comparison with Baselines ‣ 3.2 Experimental Results ‣ 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">2</span></a>, removing either preference token led to a performance drop. The removal of content-preference tokens caused a significant increase in WER, highlighting their role in maintaining semantic integrity. The absence of prompt-preference tokens led to a notable decrease in EMO-SIM, underscoring their necessity for stylistic nuances. When both were removed, all metrics degraded, confirming the importance of our structured intermediate representations. Furthermore, the drastic drop in EMO-SIM without instruction text proves that the model’s stylistic control is directly derived from the prompt rather than dataset bias.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Ablation Study on Hierarchical Decoding Strategy</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p">We evaluated our hierarchical decoding strategy against two alternatives: 1) <span class="ltx_text ltx_font_bold">Parallel</span>: predicting all three token types (content, prompt, speech) simultaneously from the LLM’s hidden state. 2) <span class="ltx_text ltx_font_bold">Single-step</span>: directly predicting the final speech tokens, bypassing the intermediate preference tokens. Results in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19001v1#S3.T3" title="Table 3 ‣ 3.2.3 Ablation Study on Hierarchical Decoding Strategy ‣ 3.2 Experimental Results ‣ 3 EXPERIMENTS ‣ HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for instruction-based TTS"><span class="ltx_text ltx_ref_tag">3</span></a> show that our hierarchical approach outperforms both. The suboptimal results of the parallel approach demonstrated that an explicit conditional dependency is needed for effective output structuring. The weaker performance of the single-step model further affirmed the need for structured intermediate representations. These findings confirmed that the sequential, layer-by-layer process of our hierarchical strategy was crucial for its success, enabling precise control via the preference extraction process. Importantly, this structured approach only added a 34% inference latency overhead to the LLM component compared to the single-step baseline.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold">Table 3</span>: </span>Ablation study on hierarchical decoding strategy.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:204.9pt;height:38.5pt;vertical-align:-17.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.3pt,5.5pt) scale(0.777561912040565,0.777561912040565) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">DNSMOS ↑</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">EMO-SIM ↑</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">WER ↓</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Parallel</th>
<td class="ltx_td ltx_align_center ltx_border_t">3.76</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.736</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5.99%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Single-step</th>
<td class="ltx_td ltx_align_center">3.80</td>
<td class="ltx_td ltx_align_center">0.713</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">5.93%</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">Hierarchical</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">3.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">0.753</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">5.18%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>CONCLUSION</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">In this paper, we introduced HD-PPT, a novel framework for Instruct-TTS that resolves the hierarchical mismatch between textual instructions and speech signals. By employing a specialized codec to extract dual preference tokens from speech tokens and a hierarchical decoding strategy to generate them sequentially, our method significantly enhances fine-grained control and expressiveness. Extensive experiments demonstrated that HD-PPT outperforms state-of-the-art baselines in both instruction adherence and speech naturalness. The results validate that aligning the generative process with the intrinsic structure of speech is a robust paradigm for precise and controllable synthesis. Future work could explore extending this hierarchical approach to other domains, such as singing voice synthesis or cross-lingual TTS.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhihao Du, Qian Chen, Shiliang Zhang, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2407.05407</span><span class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Xinsheng Wang, Mingqi Jiang, Ziyang Ma, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2503.01710</span><span class="ltx_text" style="font-size:90%;">, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Bowen Zhang, Congchao Guo, Geng Yang, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Minimax-speech: Intrinsic zero-shot text-to-speech with a learnable speaker encoder,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2505.07916</span><span class="ltx_text" style="font-size:90%;">, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Prompttts: Controllable text-to-speech with text descriptions,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1–5.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Dongchao Yang, Songxiang Liu, Rongjie Huang, Chao Weng, and Helen Meng,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span class="ltx_text" style="font-size:90%;">, vol. 32, pp. 2913–2925, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Guanrou Yang, Chen Yang, Qian Chen, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Emovoice: Llm-based emotional text-to-speech model with freestyle text prompting,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2504.12867</span><span class="ltx_text" style="font-size:90%;">, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yichong Leng, ZHifang Guo, Kai Shen, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Prompttts 2: Describing and generating voices with text prompt,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Representation Learning</span><span class="ltx_text" style="font-size:90%;">, B. Kim, Y. Yue, S. Chaudhuri, K. Fragkiadaki, M. Khan, and Y. Sun, Eds., 2024, vol. 2024, pp. 57672–57688.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Guanghou Liu, Yongmao Zhang, Yi Lei, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Promptstyle: Controllable style transfer for text-to-speech with natural language descriptions,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</span><span class="ltx_text" style="font-size:90%;">, vol. 2023-August, pp. 4888–4892, 2023,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">Publisher Copyright: © 2023 International Speech Communication Association. All rights reserved.; 24rd Annual Conference of the International Speech Communication Association, INTERSPEECH 2023 ; Conference date: 20-08-2023 Through 24-08-2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Shengpeng Ji, Qian Chen, Wen Wang, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“ControlSpeech: Towards simultaneous and independent zero-shot speaker cloning and zero-shot language style control,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span><span class="ltx_text" style="font-size:90%;">, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, Eds., Vienna, Austria, July 2025, pp. 6966–6981, Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhihao Du, Yuxuan Wang, Qian Chen, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Cosyvoice 2: Scalable streaming speech synthesis with large language models,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2412.10117</span><span class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Shengpeng Ji, Jialong Zuo, Minghui Fang, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Textrolspeech: A text style control speech corpus with codec language text-to-speech models,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" style="font-size:90%;">. IEEE, 2024, pp. 10301–10305.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yixuan Zhou, Xiaoyu Qin, Zeyu Jin, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Voxinstruct: Expressive human instruction-to-speech generation with unified multilingual codec language modelling,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 32nd ACM International Conference on Multimedia</span><span class="ltx_text" style="font-size:90%;">, 2024, pp. 554–563.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Hui Lu, Xixin Wu, Zhiyong Wu, and Helen Meng,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Speechtriplenet: End-to-end disentangled speech representation learning for content, timbre and prosody,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 31st ACM International Conference on Multimedia</span><span class="ltx_text" style="font-size:90%;">, 2023, pp. 2829–2837.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Clap learning audio concepts from natural language supervision,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1–5.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Finite scalar quantization: Vq-vae made simple,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Representation Learning</span><span class="ltx_text" style="font-size:90%;">, B. Kim, Y. Yue, S. Chaudhuri, K. Fragkiadaki, M. Khan, and Y. Sun, Eds., 2024, vol. 2024, pp. 51772–51783.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Tao Xu, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Robust speech recognition via large-scale weak supervision,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" style="font-size:90%;">. PMLR, 2023, pp. 28492–28518.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Roberta: A robustly optimized bert pretraining approach,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.11692</span><span class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
An Yang, Baosong Yang, Beichen Zhang, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Qwen2.5 technical report,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span class="ltx_text" style="font-size:90%;">, vol. abs/2412.15115, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Zhihao Du, Changfeng Gao, Yuxuan Wang, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Cosyvoice 3: Towards in-the-wild speech generation via scaling-up and post-training,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2505.17589</span><span class="ltx_text" style="font-size:90%;">, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Changfeng Gao, Zhihao Du, and Shiliang Zhang,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Differentiable reward optimization for llm based tts system,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2507.05911</span><span class="ltx_text" style="font-size:90%;">, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“emotion2vec: Self-supervised pre-training for speech emotion representation,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Findings of the Association for Computational Linguistics: ACL 2024</span><span class="ltx_text" style="font-size:90%;">, Lun-Wei Ku, Andre Martins, and Vivek Srikumar, Eds., Bangkok, Thailand, Aug. 2024, pp. 15747–15760, Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Anmol Gulati, James Qin, Chung-Cheng Chiu, et al.,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Conformer: Convolution-augmented transformer for speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech 2020</span><span class="ltx_text" style="font-size:90%;">, 2020, pp. 5036–5040.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Flow matching for generative modeling,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning Representations</span><span class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">“Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" style="font-size:90%;">, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 17022–17033, Curran Associates, Inc.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 23 13:20:16 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
