<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>WolBanking77: Wolof Banking Speech Intent Classification Dataset</title>
<!--Generated on Fri Oct 24 19:22:16 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2509.19271v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S1" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S2" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S2.SS1" title="In 2 Related work ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Existing datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S2.SS2" title="In 2 Related work ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Summary of contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data collection</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.SS1" title="In 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Textual data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.SS2" title="In 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Audio recordings and transcriptions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S4" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Tasks &amp; Settings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S4.SS1" title="In 4 Tasks &amp; Settings ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Automatic Speech Recognition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S4.SS2" title="In 4 Tasks &amp; Settings ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Intent Detection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Dataset evaluation &amp; Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS1" title="In 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Intent detection models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1" title="In 5.1 Intent detection models ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Zero-shot classification with WolBanking77</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS1.SSS2" title="In 5.1 Intent detection models ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Few-shot classification with WolBanking77</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS2" title="In 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>ASR System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS3" title="In 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Model training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS4" title="In 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Results and Discussions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S6" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Long-term Support and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S7" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Technical Appendices and Supplementary Material</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS1" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>WolBanking77 supplementary details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS2" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Linguistic variation in the Wolof language across Senegal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS3" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Speakers age distribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS4" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Illiteracy in Senegal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS5" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5 </span>Transportation data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS6" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS7" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.7 </span>Zero-shot classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS8" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.8 </span>Text prompt for Llama3.2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS9" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.9 </span>Reference scores from classic Machine Learning Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS10" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.10 </span>Pretrained Sentence Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS11" title="In 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.11 </span>Consent form</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S9" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S10" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Ethics Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S11" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Wolof language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12" title="In WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12 </span>Datasheets for WolBanking77</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS1" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.1 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS2" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.2 </span>Composition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS3" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.3 </span>Collection Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS4" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.4 </span>Preprocessing/cleaning/labeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS5" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.5 </span>Uses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS6" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.6 </span>Distribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12.SS7" title="In 12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12.7 </span>Maintenance</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">WolBanking77: Wolof Banking Speech Intent Classification Dataset</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Abdou Karim Kandji<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup> Frédéric Precioso<sup class="ltx_sup">2</sup> Cheikh Ba<sup class="ltx_sup">1</sup> Samba Ndiaye<sup class="ltx_sup">3</sup> Augustin Ndione<sup class="ltx_sup">3</sup>
<br class="ltx_break"/>
<br class="ltx_break"/><sup class="ltx_sup">1</sup>University of Gaston Berger (UGB), Saint-Louis, Senegal 
<br class="ltx_break"/><sup class="ltx_sup">2</sup>Inria, Université Côte d’Azur (UniCA), Maasai team, Nice, France 
<br class="ltx_break"/><sup class="ltx_sup">3</sup>Cheikh Anta Diop University, Dakar, Senegal
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">{kandji.abdou-karim1,cheikh2.ba}@ugb.edu.sn
<br class="ltx_break"/>frederic.precioso@univ-cotedazur.fr
<br class="ltx_break"/>{samba.ndiaye,augustin.ndione}@ucad.edu.sn</span>
</span><span class="ltx_author_notes"><span class="ltx_text" style="font-size:70%;">Corresponding author</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90% of the population, while the national illiteracy rate remains at of 42%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset’s contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: <a class="ltx_ref ltx_href" href="https://github.com/abdoukarim/wolbanking77" title="">wolbanking77</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Wolof is spoken in Senegal, Gambia and Mauritania by more than 10 million people. Most of the population in Senegal speaks Wolof (around 90%) <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://www.axl.cefan.ulaval.ca/afrique/senegal.htm" title="">https://www.axl.cefan.ulaval.ca/afrique/senegal.htm</a></span></span></span> which is essentially a language of communication between several ethnic groups. More details on Wolof are presented in the appendix section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S11" title="11 Wolof language ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">The lack of digital resources for Wolof motivated us to build the WolBanking77 dataset. Wolof, being an oral language, it is essential to establish voice assistants that allow the population access to digital services and help address challenges such as enhancing financial inclusion and access to digital public services. The trade sector, for instance, which plays an important role in the economy of African countries, is predominantly driven by the informal economy <cite class="ltx_cite ltx_citemacro_citep">[Ouattara et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib63" title="">2025</a>]</cite>. We can also cite the agriculture, livestock, fishing, and transport sectors, which are all related to commercial activity and generate more employment compared to the formal sector <cite class="ltx_cite ltx_citemacro_citep">[Martínez and Short, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib51" title="">2022</a>]</cite>. Recent advances in artificial intelligence provide an good opportunity for these populations to benefit from digital technology. They can gain better control over financial transactions and minimize the risk of fraud due to language barriers. In fact, the language barrier often compel business owners in the informal sector to rely on a third party to consult their account balances or make payments on their behalf. This potentially exposes them to the risk of fraud <cite class="ltx_cite ltx_citemacro_citep">[Anthony et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib8" title="">2024</a>, Ouattara et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib63" title="">2025</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">However, to build such a virtual assistant, it is necessary to understand human requests in order to provide appropriate responses. In Natural Language Processing (NLP), the field that deals with the understanding of human language, is Natural Language Understanding (NLU). In our case, we focus on both text and speech given that our objective is to work with African languages rooted in oral traditions. Before doing an NLU task, the first step is to understand the information contained in speech using Spoken Language Understanding (SLU). According to the World Bank <cite class="ltx_cite ltx_citemacro_citet">Bank [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib11" title="">2022</a>]</cite>, the rate of adult illiterate (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS4" title="8.4 Illiteracy in Senegal ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.4</span></a>) population in Senegal is 42%, which means that to facilitate access to a virtual assistant for these populations, it is essential to offer voice processing solutions. To determine the intent of a user request, <cite class="ltx_cite ltx_citemacro_citet">Coucke et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib22" title="">2018</a>]</cite> frame the problem as an Intent Detection (ID) classification task, performed either directly from text or from audio transcriptions.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">Models based on neural networks has emerged in recent years in the field of ID <cite class="ltx_cite ltx_citemacro_citep">[Gerz et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib34" title="">2021</a>, Krishnan et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib45" title="">2021</a>, Si et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib72" title="">2023b</a>]</cite>. However, very few datasets in African languages <cite class="ltx_cite ltx_citemacro_citet">Alexis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib5" title="">2022</a>], Mastel et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib52" title="">2023</a>], Moghe et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib55" title="">2023</a>], Mwongela et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib57" title="">2023</a>], Kasule et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib44" title="">2024</a>]</cite> have been explored. Even less in the field of e-banking for Sub-Saharan languages. Most existing datasets are in English <cite class="ltx_cite ltx_citemacro_citet">Coucke et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib22" title="">2018</a>]</cite>, as their development requires significant financial and human resources. In this paper, we present an intent classification dataset in Wolof, an African and Sub-Saharan language, based on the Banking77 dataset <cite class="ltx_cite ltx_citemacro_citet">Casanueva et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib15" title="">2020</a>]</cite> which originally consists of 13,083 customer service queries labeled with 77 intents. In addition, our work also draws on the MINDS-14 dataset <cite class="ltx_cite ltx_citemacro_citet">Gerz et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib34" title="">2021</a>]</cite>, which contains 14 intents derived from a commercial e-banking system and includes an audio component.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Existing datasets</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Several datasets, both monolingual and multilingual, for NLU conversational question-answering system, have been published in the last decade. For instance, The Chatbot Corpus dataset composed of questions and answers and also The StackExchange Corpus based on the StackExchange platform both available on GitHub <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://github.com/sebischair/NLU-Evaluation-Corpora" title="">https://github.com/sebischair/NLU-Evaluation-Corpora</a></span></span></span> and evaluated by <cite class="ltx_cite ltx_citemacro_citep">[Braun et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib13" title="">2017</a>]</cite>. More recently, MINDS-14 <cite class="ltx_cite ltx_citemacro_citet">Gerz et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib34" title="">2021</a>]</cite> a multilingual dataset in the field of e-banking has been made publicly available. When considering multilingual intent classification benchmarking, we can mention for instance the MultiATIS++ dataset <cite class="ltx_cite ltx_citemacro_citet">Xu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib82" title="">2020</a>]</cite>, which pertains to the aviation field, and has been expanded upon the original English ATIS dataset <cite class="ltx_cite ltx_citemacro_citet">Price [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib67" title="">1990</a>]</cite> to six additional languages. More recently, the XTREME-S benchmark <cite class="ltx_cite ltx_citemacro_citet">Alexis et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib5" title="">2022</a>]</cite> aims at evaluating several tasks such as speech recognition, translation, classification and retrieval. XTREME-S brings together several datasets covering 102 various languages, including 20 languages of Sub-Saharan Africa.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">With regard to the Wolof language, datasets have been published in the literature, particularly in the field of NLP and Automatic Speech Recognition (ASR). For example, some datasets include Wolof texts such as afriqa dataset <cite class="ltx_cite ltx_citemacro_citet">Ogundepo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib60" title="">2023</a>]</cite> which deals with the question answering (QA) task, masakhaner versions 1 and 2 <cite class="ltx_cite ltx_citemacro_citet">Adelani et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib2" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib3" title="">2022b</a>]</cite> which targets the Name Entity Recognition (NER) task, UD_Wolof-WTB <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="https://github.com/UniversalDependencies/UD_Wolof-WTB" title="">https://github.com/UniversalDependencies/UD_Wolof-WTB</a></span></span></span> or even MasakhaPOS <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href" href="https://github.com/masakhane-io/lacuna_pos_ner" title="">https://github.com/masakhane-io/lacuna_pos_ner</a></span></span></span> which addresses the part-of-speech tagging (POS) task. In addition, several datasets for the machine translation (MT) task such as OPUS, <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_href" href="https://opus.nlpl.eu/" title="">https://opus.nlpl.eu/</a></span></span></span> FLORES 200 <cite class="ltx_cite ltx_citemacro_citet">team et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib74" title="">2022</a>]</cite>, NTREX-128 <cite class="ltx_cite ltx_citemacro_citet">Federmann et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib28" title="">2022</a>]</cite> and MAFAND-MT <cite class="ltx_cite ltx_citemacro_citep">[Adelani et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib1" title="">2022a</a>]</cite>. In the field of ASR, several datasets containing Wolof have also been published, including ALFFA <cite class="ltx_cite ltx_citemacro_citet">Gauthier et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib31" title="">2016</a>]</cite>, fleurs <cite class="ltx_cite ltx_citemacro_citet">Conneau et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib21" title="">2023</a>]</cite> and more recently KALLAAMA <cite class="ltx_cite ltx_citemacro_citet">Gauthier et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib32" title="">2024</a>]</cite>. The cited datasets cover fairly general domains, such as news and religion. However, to date, there is only one text dataset dedicated to the banking sector named INJONGO <cite class="ltx_cite ltx_citemacro_citet">Yu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib83" title="">2025</a>]</cite> (with other domains such as home, kitchen and dining, travel and utility). This dataset includes slot-filling and intent classification tasks for 16 African languages including Wolof.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p">The authors <cite class="ltx_cite ltx_citemacro_citet">Casanueva et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib15" title="">2020</a>]</cite> explored few-shot learning scenario in addition to introducing the Banking77 dataset, which contains 77 intents and 13,083 examples. ArBanking77 <cite class="ltx_cite ltx_citemacro_citet">Jarrar et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib42" title="">2023</a>]</cite> is the Arabic language version of the Banking77 dataset <cite class="ltx_cite ltx_citemacro_citet">Casanueva et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib15" title="">2020</a>]</cite> in which the authors conducted simulations in low-resource settings scenario by training their model on a subset of the dataset. Other recent contributions to low-resource languages have been made, the authors <cite class="ltx_cite ltx_citemacro_citet">Mastel et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib52" title="">2023</a>]</cite> used Google Cloud Translation API to translate the ATIS dataset <cite class="ltx_cite ltx_citemacro_citet">Price [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib67" title="">1990</a>]</cite> in Kinyarwanda and Swahili which are languages spoken by approximately 100 million people in East Africa. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Moghe et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib55" title="">2023</a>]</cite> introduced the MULTI3NLU++ dataset for several languages including Amharic. <cite class="ltx_cite ltx_citemacro_citet">Kasule et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib44" title="">2024</a>]</cite> proposed a voice command dataset containing 20 intents in Luganda, designed for deployment on IoT devices.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Summary of contributions</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">In this work, 9,791 customer service queries from the Banking77 dataset <cite class="ltx_cite ltx_citemacro_citet">Casanueva et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib15" title="">2020</a>]</cite> was translated to French and Wolof by a team of linguistic experts from the Centre de Linguistique Appliquée de Dakar (CLAD). Additionally, this work introduces another dataset based on MINDS-14 <cite class="ltx_cite ltx_citemacro_citet">Gerz et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib34" title="">2021</a>]</cite>, in which each query is paired with audio recordings from multiple speakers with diverse accents and ages (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS3" title="8.3 Speakers age distribution ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.3</span></a>). The dataset includes 10 intents represented with both text and audio examples. The open source tool Lig-Aikuma <cite class="ltx_cite ltx_citemacro_citet">Blachon et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib12" title="">2016</a>]</cite> was used to produce the voice recordings. Our contributions include:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p">An audio dataset with 263 sentences covering 10 intents in the banking and transport domains, including diverse voices and accents, making it the first of its kind in the Wolof language at the time of writing.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p">A text dataset with 9,791 sentences translated from English to French and Wolof covering 77 intents in banking domain.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p">The results obtained with state-of-the-art models in various tasks such as Automatic Speech Recognition (ASR) and Intent Detection (ID). We also provide training and evaluation code to support the reproducibility of experimental benchmarks.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p">A dataset documentation (datasheets) for WolBanking77.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">All datasets and source codes are released under a CC BY 4.0 license to stimulate research in the field of NLP for low-resource African languages.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data collection</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">In this section, we present the main information on the creation of WolBanking77, a more detailed description can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S12" title="12 Datasheets for WolBanking77 ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Textual data</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">The text dataset contains a total of 9,791 sentences and 77 intents from the English train set of Banking77 <cite class="ltx_cite ltx_citemacro_citet">Casanueva et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib15" title="">2020</a>]</cite>, manually translated to French and Wolof thanks to a team of linguistic experts from the Centre de Linguistique Appliquée de Dakar (CLAD).
The Wolof version was translated and localized according to the local context, for instance, "<span class="ltx_text ltx_font_italic">ATM</span>" and "<span class="ltx_text ltx_font_italic">app</span>" translated as "<span class="ltx_text ltx_font_italic">GAB</span>" and "<span class="ltx_text ltx_font_italic">aplikaasiyo<span class="ltx_ERROR undefined">\tipaencoding</span>N</span>", see the example in figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.F1" title="Figure 1 ‣ 3.1 Textual data ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="S3.F1.g1" src="figures/wrong_amount_of_cash_received.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A query translated in French and Wolof</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">Duplicated sentences translated in Wolof was removed for the ID task to avoid many-to-one translations from English to Wolof. Two versions of the dataset are reported on table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.T2" title="Table 2 ‣ 3.1 Textual data ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">2</span></a>. The first version is comprised of 5k samples which is a sub-sample of the second version of 9,791 samples. Train set is 80% of both versions and test set represents 20%. Note that intents are unbalanced, the most represented intent has a frequency of 200 while the least represented one has a frequency of 24.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p">Some statistics were collected on the data translated into French and Wolof. Statistics on table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.T2" title="Table 2 ‣ 3.1 Textual data ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">2</span></a> show a slightly higher number of words per query for French (83) compared to Wolof (81).</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S3.T2.fig1" style="width:216.8pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 1: </span>WolBanking77 dataset statistics</figcaption>
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_right ltx_border_tt">WOLOF</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt">FRENCH</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_t">Min Word Count</td>
<td class="ltx_td ltx_align_right ltx_border_t">2</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right">Max Word Count</td>
<td class="ltx_td ltx_align_right">81</td>
<td class="ltx_td ltx_nopad_r ltx_align_right">83</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right">Mean Word Count</td>
<td class="ltx_td ltx_align_right">12.22</td>
<td class="ltx_td ltx_nopad_r ltx_align_right">12.47</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_bb">Median Word Count</td>
<td class="ltx_td ltx_align_right ltx_border_bb">10</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">10</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" id="S3.T2.fig2" style="width:216.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span>WolBanking77 dataset split</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">SPLIT</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt">5k sample</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt">all</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_t">TRAIN SET</td>
<td class="ltx_td ltx_align_right ltx_border_t">4,000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">7,832</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right ltx_border_bb">TEST SET</td>
<td class="ltx_td ltx_align_right ltx_border_bb">1,000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">1,959</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Audio recordings and transcriptions</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">The audio corpus based on MINDS-14 <cite class="ltx_cite ltx_citemacro_citet">Gerz et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib34" title="">2021</a>]</cite> consists of 186 queries and 77 responses, initially translated from French into English by a team of linguistic experts from the Centre de Linguistique Appliquée de Dakar (CLAD). The dataset also includes a phonetic transcription of the Wolof text to facilitate the correct pronunciation of sentences by different speakers. Additional intents was added to the initial MINDS-14 dataset, namely: <span class="ltx_text ltx_font_italic">OPEN_ACCOUNT</span>, for information related to opening a bank account, <span class="ltx_text ltx_font_italic">BUS_RESERVATION</span>, for booking a transport bus (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS5" title="8.5 Transportation data ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.5</span></a>), <span class="ltx_text ltx_font_italic">TECHNICAL_VISIT</span> for scheduling a vehicle inspection appointment. <span class="ltx_text ltx_font_italic">TRANSFER_MONEY</span> for the intent to transfer money and <span class="ltx_text ltx_font_italic">AMOUNT</span> to specify the amount to be transferred. See table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.SS2" title="3.2 Audio recordings and transcriptions ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">3.2</span></a> for the complete list of intents in the audio dataset.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.F2" title="Figure 2 ‣ 3.2 Audio recordings and transcriptions ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">2</span></a> shows top 5 most frequent words in the datastet after excluding stopwords. The dataset contains 272 unique words and 3,204 audio clips. The audio clips are in WAV format, single channel, with a sampling rate of 16 kHz. Sentences have an average duration of 4,815 ms. The intents <span class="ltx_text ltx_font_italic">’CASH_DEPOSIT’</span>, <span class="ltx_text ltx_font_italic">’FREEZE’</span>, <span class="ltx_text ltx_font_italic">’LATEST_TRANSACTIONS’</span> contain the longest queries, whereas <span class="ltx_text ltx_font_italic">’AMOUNT’</span>, <span class="ltx_text ltx_font_italic">’BUS_RESERVATION’</span>, <span class="ltx_text ltx_font_italic">’OPEN_ACCOUNT’</span> and <span class="ltx_text ltx_font_italic">’TRANSFER_MONEY’</span> generally have shorter query durations. The total duration of the dataset is approximately 4 hours and 17 minutes.
<span class="ltx_inline-logical-block ltx_minipage ltx_align_top" style="width:179.4pt;">
<span class="ltx_table ltx_align_center" id="S3.T3">
<span class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>List of intents.</span>
</span>
<span class="ltx_para" id="S3.SS2.p2.p1">
<span class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span class="ltx_tr">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:80%;">intent</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:80%;">domain</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr">
<span class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text" style="font-size:80%;">BALANCE</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t"><span class="ltx_text" style="font-size:80%;">e-banking</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">CASH_DEPOSIT</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">FREEZE</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">LATEST_TRANSACTIONS</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">PAY_BILL</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">OPEN_ACCOUNT</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">TRANSFER_MONEY</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">AMOUNT</span></span>
<span class="ltx_td ltx_nopad_r"></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="font-size:80%;">BUS_RESERVATION</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_left"><span class="ltx_text" style="font-size:80%;">transport</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text" style="font-size:80%;">TECHNICAL_VISIT</span></span>
<span class="ltx_td ltx_nopad_r ltx_border_bb"></span></span>
</span>
</span>
</span></span>
<span class="ltx_inline-logical-block ltx_minipage ltx_align_top" style="width:155.2pt;">
<span class="ltx_para" id="S3.SS2.p2.p2"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="128.04" id="S3.SS2.p2.pic1" overflow="visible" version="1.1" viewbox="0 0 232.39 128.04" width="232.39"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,128.04) matrix(1 0 0 -1 0 0) translate(0.19,0) translate(0,32.6) matrix(0.7 0.0 0.0 0.7 -0.19 -32.6)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 1 0 0) translate(47.62,0) translate(0,57.79)"><clippath id="pgfcp1"><path d="M -47.35 -208.07 L 284.09 -208.07 L 284.09 320.22 L -47.35 320.22"></path></clippath><g clip-path="url(#pgfcp1)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M 0 -14.17 L 0 -8.26 M 59.19 -14.17 L 59.19 -8.26 M 118.37 -14.17 L 118.37 -8.26 M 177.56 -14.17 L 177.56 -8.26 M 236.74 -14.17 L 236.74 -8.26" style="fill:none"></path></g><g></g></g><path d="M -47.35 -11.22 L 281.32 -11.22" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 281.32 -11.22)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.7071 0.7071 -0.7071 0.7071 -15.07 -33.99)"><foreignobject height="6.73" overflow="visible" style="--fo_width :1.95em;--fo_height:0.61em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.73)" width="21.47"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" style="font-size:70%;">kont</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.7071 0.7071 -0.7071 0.7071 39.64 -38.47)"><foreignobject height="6.73" overflow="visible" style="--fo_width :2.49em;--fo_height:0.61em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.73)" width="27.49"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" style="font-size:70%;">xaalis</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.7071 0.7071 -0.7071 0.7071 84.11 -51.85)"><foreignobject height="8.72" overflow="visible" style="--fo_width :4.27em;--fo_height:0.61em;--fo_depth :0.18em;" transform="matrix(1 0 0 -1 0 6.78)" width="47.07"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" style="font-size:70%;">bëggoona</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.7071 0.7071 -0.7071 0.7071 150.04 -45.11)"><foreignobject height="8.72" overflow="visible" style="--fo_width :3.46em;--fo_height:0.61em;--fo_depth :0.18em;" transform="matrix(1 0 0 -1 0 6.78)" width="38.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" style="font-size:70%;">jëflante</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.7071 0.7071 -0.7071 0.7071 221.3 -32.56)"><foreignobject height="4.17" overflow="visible" style="--fo_width :1.85em;--fo_height:0.38em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 4.17)" width="20.42"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" style="font-size:70%;">xam</span></span></span></foreignobject></g><clippath id="pgfcp2"><path d="M -47.35 -11.22 L 284.09 -11.22 L 284.09 123.37 L -47.35 123.37 Z"></path></clippath><g clip-path="url(#pgfcp2)"><g color="#0000FF" fill="#B3B3FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;"><path d="M -11.07 -11.22 h 22.14 v 123.37 h -22.14 Z M 48.12 -11.22 h 22.14 v 40.73 h -22.14 Z M 107.3 -11.22 h 22.14 v 31.88 h -22.14 Z M 166.49 -11.22 h 22.14 v 14.17 h -22.14 Z M 225.67 -11.22 h 22.14 v 11.22 h -22.14 Z"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 -5.52 115.66)"><foreignobject height="6.24" overflow="visible" style="--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.24)" width="11.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="65" class="ltx_Math" display="inline" id="S3.SS2.p2.pic1.m1" intent=":literal"><semantics><mn mathsize="0.700em">65</mn><annotation encoding="application/x-tex">65</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 53.67 33.02)"><foreignobject height="6.24" overflow="visible" style="--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.24)" width="11.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="37" class="ltx_Math" display="inline" id="S3.SS2.p2.pic1.m2" intent=":literal"><semantics><mn mathsize="0.700em">37</mn><annotation encoding="application/x-tex">37</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 112.86 24.17)"><foreignobject height="6.24" overflow="visible" style="--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.24)" width="11.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="34" class="ltx_Math" display="inline" id="S3.SS2.p2.pic1.m3" intent=":literal"><semantics><mn mathsize="0.700em">34</mn><annotation encoding="application/x-tex">34</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 172.04 6.46)"><foreignobject height="6.24" overflow="visible" style="--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.24)" width="11.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="28" class="ltx_Math" display="inline" id="S3.SS2.p2.pic1.m4" intent=":literal"><semantics><mn mathsize="0.700em">28</mn><annotation encoding="application/x-tex">28</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 231.23 3.51)"><foreignobject height="6.24" overflow="visible" style="--fo_width :0.97em;--fo_height:0.55em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 6.24)" width="11.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="27" class="ltx_Math" display="inline" id="S3.SS2.p2.pic1.m5" intent=":literal"><semantics><mn mathsize="0.700em">27</mn><annotation encoding="application/x-tex">27</annotation></semantics></math></span></span></foreignobject></g></g></g></svg></span>
</span>
<span class="ltx_figure ltx_align_center" id="S3.F2">
<span class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Top 5 most frequent words: kont (account), xaalis (money), bëggoona (I wanted to), jëflante (operation), xam (know).</span>
</span></span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">The dataset was collected with the participation of students from Cheikh Anta Diop University in Dakar (UCAD), specifically from the Faculty of Letters and Human Sciences. Each participant recorded their voice using the elicitation mode of the Lig-Aikuma software <cite class="ltx_cite ltx_citemacro_citep">[Blachon et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib12" title="">2016</a>]</cite>. A total of 186 utterances were recorded by participants in a controlled environment. The elicitation mode of the Lig-Aikuma software was installed on an Android tablet. At the start of each session, information about the participant are requested, including native language, region of origin (see figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.F3" title="Figure 3 ‣ 3.2 Audio recordings and transcriptions ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">3</span></a>), gender, year of birth and name. This information is subsequently stored as metadata on the tablet’s memory card. Note that, the scores presented in figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3.F3" title="Figure 3 ‣ 3.2 Audio recordings and transcriptions ‣ 3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">3</span></a> are not representative of the population of each region. We tried to cover various ethnic groups in the country in order to have diverse accents and dialects as described in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS2" title="8.2 Linguistic variation in the Wolof language across Senegal ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.2</span></a>. Additional details on demographic data and distribution can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS1" title="8.1 WolBanking77 supplementary details ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p">To anonymize participant names, the system generates a user_id to replace the actual names (further details on the ethical collection and processing of personal data are provided in Appendices <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS11" title="8.11 Consent form ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.11</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S10" title="10 Ethics Statement ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">10</span></a>). Next, a text file containing the sentences to be pronounced is selected. During recording, sentences were presented one at a time, with the option to cancel if a mispronunciation occurred at any stage. In terms of gender diversity, a total of 31 recording sessions were conducted, including 14 males, 14 females and 3 unspecified. The text and audio data were split with 80% allocated for train set and 20% for test set. Prior to data splitting, preprocessing steps were applied, including the removal of punctuation marks and numbers, and conversion of text to lowercase. Corrupted audio files were also removed from the dataset.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="224.54" id="S3.F3.pic1" overflow="visible" version="1.1" viewbox="0 0 298.58 224.54" width="298.58"><g stroke="#000000" style="--ltx-stroke-color:#000000;" transform="translate(0,224.54) matrix(1 0 0 -1 0 0) translate(158.96,0) translate(0,114.15)"><g fill="#BFBFBF" stroke-linejoin="round" stroke-width="0.8pt" style="--ltx-fill-color:#BFBFBF;"><path d="M 0 0 L 82.68 0 C 82.68 29.54 66.92 56.83 41.34 71.6 C 27.55 79.56 11.75 83.37 -4.15 82.57 Z"></path></g><g fill="#000000" stroke-width="0.4pt" style="--ltx-fill-color:#000000;"><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 75.21 78.62)"><foreignobject height="9.61" overflow="visible" style="--fo_width :2.68em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="37.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Dakar</span></span></foreignobject></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M 57.94 60.54 L 78.38 73.74" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 14.64 29.58)"><foreignobject height="11.15" overflow="visible" style="--fo_width :2.61em;--fo_height:0.75em;--fo_depth :0.06em;" transform="matrix(1 0 0 -1 0 10.38)" width="36.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">25.8%</span></span></foreignobject></g><g fill="#E0E0E0" stroke-linejoin="round" stroke-width="0.8pt" style="--ltx-fill-color:#E0E0E0;"><path d="M 0 0 L -4.15 82.57 C -26.77 81.43 -47.94 71.07 -62.7 53.89 Z"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -102.07 96.17)"><foreignobject height="9.61" overflow="visible" style="--fo_width :3.77em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="52.12"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Diourbel</span></span></foreignobject></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M -37.34 74.9 L -61.51 91.28" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -43.55 47.21)"><foreignobject height="11.15" overflow="visible" style="--fo_width :2.61em;--fo_height:0.75em;--fo_depth :0.06em;" transform="matrix(1 0 0 -1 0 10.38)" width="36.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">12.9%</span></span></foreignobject></g><g fill="#BFBFBF" stroke-linejoin="round" stroke-width="0.8pt" style="--ltx-fill-color:#BFBFBF;"><path d="M 0 0 L -62.7 53.89 C -81.95 31.49 -87.79 0.52 -78.02 -27.35 C -72.75 -42.38 -63.26 -55.57 -50.67 -65.33 Z"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -154.35 -24.44)"><foreignobject height="9.61" overflow="visible" style="--fo_width :3.53em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="48.81"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Kaolack</span></span></foreignobject></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M -83.23 -8.53 L -101.03 -12.74" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -65.28 -9.57)"><foreignobject height="11.15" overflow="visible" style="--fo_width :2.61em;--fo_height:0.75em;--fo_depth :0.06em;" transform="matrix(1 0 0 -1 0 10.38)" width="36.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">25.8%</span></span></foreignobject></g><g fill="#F0F0F0" stroke-linejoin="round" stroke-width="0.8pt" style="--ltx-fill-color:#F0F0F0;"><path d="M 0 0 L -50.67 -65.33 C -41.75 -72.25 -31.5 -77.27 -20.56 -80.08 Z"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -86.96 -105.48)"><foreignobject height="12.15" overflow="visible" style="--fo_width :2.68em;--fo_height:0.68em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.46)" width="37.09"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Louga</span></span></foreignobject></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M -37.34 -75.09 L -55.81 -91.14" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -42.37 -61.49)"><foreignobject height="11.15" overflow="visible" style="--fo_width :2.11em;--fo_height:0.75em;--fo_depth :0.06em;" transform="matrix(1 0 0 -1 0 10.38)" width="29.21"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">6.5%</span></span></foreignobject></g><g fill="#CFCFCF" stroke-linejoin="round" stroke-width="0.8pt" style="--ltx-fill-color:#CFCFCF;"><path d="M 0 0 L -20.56 -80.08 C 13.02 -88.7 48.42 -75.45 68.09 -46.9 Z"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 40.33 -109.54)"><foreignobject height="9.46" overflow="visible" style="--fo_width :3.14em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="43.43"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Matam</span></span></foreignobject></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M 29.95 -78.23 L 50.44 -95.19" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 0.39 -54.11)"><foreignobject height="11.15" overflow="visible" style="--fo_width :2.61em;--fo_height:0.75em;--fo_depth :0.06em;" transform="matrix(1 0 0 -1 0 10.38)" width="36.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">19.4%</span></span></foreignobject></g><g fill="#E6E6E6" stroke-linejoin="round" stroke-width="0.8pt" style="--ltx-fill-color:#E6E6E6;"><path d="M 0 0 L 68.09 -46.9 C 77.69 -32.96 82.78 -16.41 82.68 0.52 Z"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 101.88 -44.65)"><foreignobject height="9.61" overflow="visible" style="--fo_width :2.39em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="33.13"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Thies</span></span></foreignobject></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M 79.99 -24.69 L 96.99 -31.39" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 43.24 -22.6)"><foreignobject height="11.15" overflow="visible" style="--fo_width :2.11em;--fo_height:0.75em;--fo_depth :0.06em;" transform="matrix(1 0 0 -1 0 10.38)" width="29.21"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">9.7%</span></span></foreignobject></g></g></g></svg></span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Senegal region repartition by speaker</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Tasks &amp; Settings</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Automatic Speech Recognition</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">Automatic Speech Recognition (ASR) refers to the technology that enables a model to recognize and convert spoken language into text. ASR systems are widely used in various applications such as voice assistants, transcription services and customer service automation. Significant research has been conducted in this area, including Listen Attend and Spell by <cite class="ltx_cite ltx_citemacro_citet">Chan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib16" title="">2016</a>]</cite>, an end-to-end speech recognition system based on a sequence-to-sequence neural network. <cite class="ltx_cite ltx_citemacro_citet">Graves et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib35" title="">2006</a>]</cite> proposed Connectionist Temporal Classification (CTC) model used for training deep neural networks in speech recognition as well as other sequential problems where there is no explicit alignment information between the input and output.
More recently, the NVIDIA team <cite class="ltx_cite ltx_citemacro_citet">Żelasko et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib84" title="">2025</a>]</cite> developed the Canary Flash model, a variant of Canary models <cite class="ltx_cite ltx_citemacro_citet">Puvvada et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib68" title="">2024</a>]</cite> notable for being both multilingual and multitask. This model achieved state-of-the-art results on several benchmarks, including ASR.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p">Another state-of-the-art model developed by Microsoft and named Phi-4-multimodal-instruct <cite class="ltx_cite ltx_citemacro_citet">Microsoft et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib54" title="">2025</a>]</cite> has recently been released to address ASR tasks, as well as vision and text applications. Phi-4-multimodal achieves an average WER score of 6.14% and currently ranks second on Huggingface’s OpenASR leaderboard <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard" title="">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span>. To evaluate ASR models, Word Error Rate (WER) scores are reported in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS4" title="5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Intent Detection</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">Intent Detection (ID) is an NLP task that involves classifying a sentence to identify its underlying intent. Several contributions have been made in the field. For instance, <cite class="ltx_cite ltx_citemacro_citet">Gerz et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib34" title="">2021</a>]</cite> published MINDS-14, the first training and evaluation dataset for the ID task using spoken data. It includes 14 intents derived from e-banking domain, with spoken examples in 14 different languages. <cite class="ltx_cite ltx_citemacro_citet">Si et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib71" title="">2023a</a>]</cite> introduced SpokenWOZ, a large-scale speech-text dataset for spoken Task-Oriented Dialogue in 8 domains and 249 hours of audio. <cite class="ltx_cite ltx_citemacro_citet">Casanueva et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib15" title="">2020</a>]</cite> published BANKING77, a dataset comprising 13,083 examples across 77 intents in banking domain.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p">In this article, we aim to assess the challenging potential of our dataset to better highlight its relevance to the community by evaluating downstream tasks using progressively sophisticated ML Workflows. We first consider classic machine learning algorithms such as k-nearest neighbor (KNN), support vector machine (SVM), linear regression (LR) and naive bayes (NB) as initial baselines. The results are reported in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS9" title="8.9 Reference scores from classic Machine Learning Techniques ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.9</span></a>. We then consider slightly more sophisticated approaches, using the LASER sentence encoder <cite class="ltx_cite ltx_citemacro_citet">Heffernan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib39" title="">2022</a>]</cite> pre-trained on several languages, including Wolof, for sentence encoding, followed by standard classification models such as Multi-Layer Perception (MLP) and Convolution Neural Networks (CNN). Results are reported in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS10" title="8.10 Pretrained Sentence Encoder ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.10</span></a>. Subsequently, more advanced models based on BERT are evaluated in zero-shot-classification and few-shot-classification mode, and the same models are fine-tuned on the WolBanking77 dataset for comparison. Finally, several benchmarks of recent Small Language Models (SLM) such as <span class="ltx_text ltx_font_bold">Llama-3.2-3B-Instruct</span> <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_href" href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" title="">https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct</a></span></span></span> and <span class="ltx_text ltx_font_bold">Llama-3.2-1B-Instruct</span> <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_href" href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct" title="">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a></span></span></span> are presented. To evaluate ID models, <span class="ltx_text ltx_font_italic">F1</span>, <span class="ltx_text ltx_font_italic">precision</span> and <span class="ltx_text ltx_font_italic">recall</span> metrics are reported in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS4" title="5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Dataset evaluation &amp; Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">In this section, pre-trained models in multiple languages, including African languages, are presented for ID and ASR tasks. Details of the hyperparameter settings are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS6" title="8.6 Hyperparameters ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.6</span></a>. The results demonstrate the value of the WolBanking77 dataset for the community, highlighting its potential to improve ID task in Wolof and to serve as a basis for extending approaches to other low-resource languages.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Intent detection models</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p">The next sections present and detail the pre-trained models used in the ID task. Experiments for zero and few shot classification are also discussed.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Zero-shot classification with WolBanking77</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p">Zero-shot text classification is a NLP task in which a model, trained on labeled data from specific classes, can classify text from entirely new, unseen classes without additional training. In this experiments WolBanking77 provides the new, unseen dataset. To simulate Zero-shot text classification (more details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS7" title="8.7 Zero-shot classification ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.7</span></a>), following pre-trained models are considered:</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">BERT base (uncased)</span>: A transformers model pre-trained on English introduced by <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib24" title="">2019</a>]</cite>, using a Masked Language Modeling (MLM) objective.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Afro-xlmr-large</span>: Based on XLM-R-large <cite class="ltx_cite ltx_citemacro_citet">Conneau et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib20" title="">2020</a>]</cite> using MLM objective, this model was published by <cite class="ltx_cite ltx_citemacro_citet">Alabi et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib4" title="">2022</a>]</cite> and pre-trained on 17 African languages but not Wolof, while still demonstrating in the original paper good scores for NER task on Wolof.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">AfroLM_active_learning</span>: Based on XLM-RoBERTa (XLM-R) using MLM objective <cite class="ltx_cite ltx_citemacro_citet">Ogueji et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib59" title="">2021</a>]</cite>, this model was published by <cite class="ltx_cite ltx_citemacro_citep">[Dossou et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib27" title="">2022</a>]</cite>. AfroLM_active_learning is a Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages including Wolof.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">mDeBERTa-v3-base-mnli-xnli</span>: Based on DeBERTaV3-base <cite class="ltx_cite ltx_citemacro_citet">He et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib38" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib37" title="">2021</a>]</cite>, this model was published by <cite class="ltx_cite ltx_citemacro_citet">Moritz et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib56" title="">2022</a>]</cite> and pre-trained on the CC100 multilingual dataset <cite class="ltx_cite ltx_citemacro_citet">Conneau et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib20" title="">2020</a>], Wenzek et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib80" title="">2020</a>]</cite> with 100 different languages including Wolof. This model can perform Natural Language Inference (NLI) on 100 languages.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">AfriTeVa V2 Base</span>: A multilingual T5 model released by <cite class="ltx_cite ltx_citemacro_citet">Oladipo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib61" title="">2023</a>]</cite> and pre-trained on WURA dataset <cite class="ltx_cite ltx_citemacro_citet">Oladipo et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib61" title="">2023</a>]</cite> covering 16 African Languages not including Wolof.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p7">
<p class="ltx_p">F1-score metrics for Zero-shot text classification are reported in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS4" title="5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Few-shot classification with WolBanking77</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p">Given the results obtained in zero-shot classification, it is relevant to leverage manually annotated data to improve the generalization capacity of existing models on the dataset presented in this article, WolBanking77. All pre-trained models cited in section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1" title="5.1.1 Zero-shot classification with WolBanking77 ‣ 5.1 Intent detection models ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5.1.1</span></a> are selected for the few-shot classification to measure the gap between large pre-trained models with and without a small number of Wolof samples from WolBanking77 used for fine-tuning. F1-score metrics for few-shot text classification are reported in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS4" title="5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>ASR System</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">State-of-the-art ASR are selected as baseline models, pre-trained on multiple languages with the possibility of fine-tuning them for a specific language and domain. A description of these models, with additional details, is provided below:</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Canary Flash</span>: Canary Flash <cite class="ltx_cite ltx_citemacro_citet">Żelasko et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib84" title="">2025</a>]</cite> has been pre-trained on several languages (English, German, French, Spanish) and on various tasks such as ASR and translation. Canary Flash is based on Canary <cite class="ltx_cite ltx_citemacro_citet">Puvvada et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib68" title="">2024</a>]</cite> that is an encoder-decoder model whose encoder is based on the FastConformer model <cite class="ltx_cite ltx_citemacro_citet">Rekesh et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib70" title="">2023</a>]</cite> and the decoder on the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">[Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib77" title="">2017</a>]</cite>. Canary has the distinctive feature of concatenating tokenizers <cite class="ltx_cite ltx_citemacro_citet">Dhawan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib25" title="">2023</a>]</cite> from different languages using SentencePiece.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_href" href="https://github.com/google/sentencepiece" title="">Google Sentencepiece Tokenizer</a></span></span></span> In this work, English, Spanish, French and Wolof languages are concatenated with Canary’s Tokenizer. These tokens are then transformed into token embedding before being fed into the Transformer decoder. In addition to the tokens of each language, Canary uses 1,152 special tokens representation. At the time of writing, Canary has three variants: canary-1b, canary-1b-flash, and canary-180m-flash. Canary-1b-flash version is chosen for the experiments because of its multilingual support. The canary-1b-flash model was trained on 85K hours of speech data, including 31K hours of public data (FLEURS <cite class="ltx_cite ltx_citemacro_citet">Conneau et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib21" title="">2023</a>]</cite>, CoVOST v2 <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib79" title="">2021b</a>]</cite>) and the remainder on private data. The Mozilla CommonVoice 12 dataset <cite class="ltx_cite ltx_citemacro_citet">Ardila et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib9" title="">2020</a>]</cite> was used as validation data for each language.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Phi-4-multimodal-instruct</span>: Phi-4-multimodal is a multimodal Small Language Model (SLM) supporting image, text and audio within a single model. It can handle multiple modalities without interference thanks to the Mixture of LoRAs <cite class="ltx_cite ltx_citemacro_citep">[Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib41" title="">2022</a>]</cite>. To enable multilingual inputs and outputs, the tiktoken tokenizer <span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_href" href="https://github.com/openai/tiktoken" title="">Tiktoken Tokenizer</a></span></span></span> is used with a vocabulary size of approximately 200K tokens. The model is based on a Transformer decoder <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib77" title="">2017</a>]</cite> and supports a context length of 128K based on LongRopE <cite class="ltx_cite ltx_citemacro_citep">[Ding et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib26" title="">2024</a>]</cite>. For the speech/audio modality, several modules have been introduced, including an audio encoder composed of 3 CNN layers and 24 Conformer blocks <cite class="ltx_cite ltx_citemacro_citep">[Gulati et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib36" title="">2020</a>]</cite>. To map 1024-dimensional audio features to the 3072-dimensional text embedding space, 2 MLP layers are used in the Audio Projector module. Note that LoRA<sub class="ltx_sub"><span class="ltx_text ltx_font_italic">A</span></sub> was used to all attention and MLP layers with a rank of 320. Phi-4-multimodal was trained on 2M hours of private speech-text pairs in 8 languages. A second post-training phase using Supervised Fine Tuning (SFT) on speech/audio data pairs was then conducted. For the ASR task, this included 20k hours of private data and 20k hours of public data across 8 languages. The SFT data follows the format below:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="542.12" id="S5.SS2.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 542.12" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,542.12) matrix(1 0 0 -1 0 0)"><g fill="#808080" fill-opacity="1.0" style="--ltx-fill-color:#808080;"><path d="M 0 10.67 L 0 531.45 C 0 537.35 4.78 542.12 10.67 542.12 L 589.33 542.12 C 595.22 542.12 600 537.35 600 531.45 L 600 10.67 C 600 4.78 595.22 0 589.33 0 L 10.67 0 C 4.78 0 0 4.78 0 10.67 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 0.83 10.67 L 0.83 531.45 C 0.83 536.89 5.24 541.29 10.67 541.29 L 589.33 541.29 C 594.76 541.29 599.17 536.89 599.17 531.45 L 599.17 10.67 C 599.17 5.24 594.76 0.83 589.33 0.83 L 10.67 0.83 C 5.24 0.83 0.83 5.24 0.83 10.67 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.07 524.29)"><foreignobject color="#000000" height="524.29" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :41.47em;--fo_height:0.64em;--fo_depth :37.25em;" transform="matrix(1 0 0 -1 0 8.92)" width="573.86"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:41.47em;">
<span class="ltx_listing ltx_lstlisting ltx_listing"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,PHx1c2VyfD48YXVkaW8+e3Rhc2sgcHJvbXB0fTx8ZW5kfD48fGFzc2lzdGFudHw+e2xhYmVsfTx8ZW5kfD4=">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx1"><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">user</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">|&gt;&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">audio</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">&gt;{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">task</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">prompt</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">}&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">end</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">|&gt;&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">assistant</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">|&gt;{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">label</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">}&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">end</span><span class="ltx_text ltx_font_typewriter" style="font-size:90%;">|&gt;</span>
</span>
</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p">The task prompt designates the specific task on which the model is to be fine-tuned.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p">Phi-4-multimodal outperforms nvidia/canary-1b-flash <cite class="ltx_cite ltx_citemacro_citet">Żelasko et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib84" title="">2025</a>]</cite>, WhisperV3 1.5B <cite class="ltx_cite ltx_citemacro_citet">Radford et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib69" title="">2023</a>]</cite>, SeamlessM4T-V2 2.3B <cite class="ltx_cite ltx_citemacro_citet">Communication et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib19" title="">2023</a>]</cite>, Qwen2-audio 8B <cite class="ltx_cite ltx_citemacro_citet">Chu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib18" title="">2024</a>]</cite>, Gemini-2.0-Flash <cite class="ltx_cite ltx_citemacro_citet">Team et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib73" title="">2023</a>]</cite> and GPT-4o <cite class="ltx_cite ltx_citemacro_citet">OpenAI et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib62" title="">2024</a>]</cite> on the OpenASR dataset leaderboard <span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard" title="">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a></span></span></span> with a WER score of 6.14.</p>
</div>
<div class="ltx_para" id="S5.SS2.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Distil-whisper-large-v3.5</span>: Distil-whisper-large-v3.5 is based on Whisper introduced by <cite class="ltx_cite ltx_citemacro_citet">Radford et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib69" title="">2023</a>]</cite>, which was trained through supervised learning on 680,000 hours of labeled audio data. The authors demonstrated that models trained with this scale could generalize to any dataset through zero-shot learning, meaning they can adapt without requiring fine-tuning for specific datasets. The training data covered 97 different languages. Several versions of Whisper have been released in recent years, including version 3 on which the distil-whisper-large-v3.5 model was built by knowledge-distillation following the methodology described by <cite class="ltx_cite ltx_citemacro_citet">Gandhi et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib30" title="">2023</a>]</cite>. Distil-whisper-large-v3.5 was trained on 98k hours of diverse filtered datasets such as Common Voice <cite class="ltx_cite ltx_citemacro_citet">Ardila et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib9" title="">2020</a>]</cite>, LibriSpeech <cite class="ltx_cite ltx_citemacro_citet">Panayotov et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib64" title="">2015</a>]</cite> , VoxPopuli <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib78" title="">2021a</a>]</cite> , TED-LIUM <cite class="ltx_cite ltx_citemacro_citet">Hernandez et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib40" title="">2018</a>]</cite>, People’s Speech <cite class="ltx_cite ltx_citemacro_citet">Galvez et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib29" title="">2021</a>]</cite>, GigaSpeech <cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib17" title="">2021</a>]</cite>, AMI <cite class="ltx_cite ltx_citemacro_citet">Carletta et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib14" title="">2006</a>]</cite>, and Yodas <cite class="ltx_cite ltx_citemacro_citep">[Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib48" title="">2023</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Model training</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p">All models presented in this article are based on <span class="ltx_text ltx_font_italic">Pytorch</span> library <cite class="ltx_cite ltx_citemacro_citep">[Paszke et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib65" title="">2019</a>]</cite>. All experiments were conducted on Runpod <span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_href" href="https://www.runpod.io/" title="">https://www.runpod.io/</a></span></span></span> and Kaggle.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_href" href="https://www.kaggle.com/" title="">https://www.kaggle.com/</a></span></span></span> P100 GPU (16GB VRAM) was used to finetune BERT-Base and mDeBERTa-v3-base. RTX 4090 GPU for Afro-xlmr-large, AfroLM_active_learning, AfriteVa V2 and Llama-3.2 models. For Zero-shot and Few-shot classification, RTX 2000 Ada GPU (16 GB VRAM) was used. Few-shot classification was performed using <span class="ltx_text ltx_font_italic">SetFit huggingface</span> library <cite class="ltx_cite ltx_citemacro_citep">[Tunstall et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib76" title="">2022</a>]</cite>. <span class="ltx_text ltx_font_italic">2-shots</span> refers to 2 samples per intent while <span class="ltx_text ltx_font_italic">8-shots</span> represents 8 samples per intent. All pre-trained text models are hosted on <span class="ltx_text ltx_font_italic">huggingface platform</span>, they were fine-tuned using <span class="ltx_text ltx_font_italic">transformers</span> library <cite class="ltx_cite ltx_citemacro_citet">Wolf et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib81" title="">2020</a>]</cite> except for Llama-3.2 which was fine-tuned using <span class="ltx_text ltx_font_italic">torchtune</span> library <cite class="ltx_cite ltx_citemacro_citep">[torchtune maintainers, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib75" title="">2024</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p">MPL and CNN models were trained from scratch for 200 epochs. The MLP model consists of a single 800-dimensional hidden layer, a ReLU activation layer, and a fully connected output layer. For CNN, a Conv1d layer followed by a ReLU activation layer and a fully connected layer for output classification. The Wolbanking77 data was structured in the form of a prompt, as illustrated in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS8" title="8.8 Text prompt for Llama3.2 ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.8</span></a>, to enable the Llama-3.2 model to generate the correct intent.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p">Canary Flash and Phi-4-multimodal-instruct ASR models were fine-tuned on the Wolbanking77 audio dataset using the <span class="ltx_text ltx_font_italic">NVIDIA NEMO</span> framework <cite class="ltx_cite ltx_citemacro_citet">Kuchaiev et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib46" title="">2019</a>]</cite> and <span class="ltx_text ltx_font_italic">Huggingface Transformers</span> library, respectively, on an A100 SXM GPU (80GB VRAM). In contrast, Distil-whisper-large-v3.5 was fine-tuned using the <span class="ltx_text ltx_font_italic">Huggingface Transformers</span> framework on an RTX 2000 Ada GPU (16 GB VRAM). All ASR models were fine-tuned for 1,000 steps.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Results and Discussions</h3>
<figure class="ltx_figure" id="S5.F4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Zero-shot, few-shot and fine-tuning (FT) results (in % for F1-score) of multilingual pre-trained models on WolBanking77.</figcaption><span class="ltx_inline-block"><svg class="ltx_picture" height="200.61" id="S5.F4.pic1" overflow="visible" version="1.1" viewbox="0 0 430.72 200.61" width="430.72"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,200.61) matrix(1 0 0 -1 0 0) translate(39.37,0) translate(0,39.52)"><g class="ltx_centering" transform="matrix(0.8 0.0 0.0 0.8 -39.37 -39.52)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 1 0 0) translate(105.62,0) translate(0,69.47)"><clippath id="pgfcp3"><path d="M -253.26 -20.07 L -253.26 153.88 L 629.35 153.88 L 629.35 -20.07"></path></clippath><g clip-path="url(#pgfcp3)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M -56.41 0 L 432.5 0 M -56.41 66.91 L 432.5 66.91 M -56.41 133.81 L 432.5 133.81" style="fill:none"></path></g><g></g></g><clippath id="pgfcp4"><path d="M -56.41 -216.92 L 432.5 -216.92 L 432.5 350.73 L -56.41 350.73"></path></clippath><g clip-path="url(#pgfcp4)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M 0 -20.07 L 0 -14.17 M 94.02 -20.07 L 94.02 -14.17 M 188.05 -20.07 L 188.05 -14.17 M 282.07 -20.07 L 282.07 -14.17 M 376.09 -20.07 L 376.09 -14.17 M 0 153.88 L 0 147.98 M 94.02 153.88 L 94.02 147.98 M 188.05 153.88 L 188.05 147.98 M 282.07 153.88 L 282.07 147.98 M 376.09 153.88 L 376.09 147.98" style="fill:none"></path></g><g></g></g><clippath id="pgfcp5"><path d="M -253.26 -20.07 L -253.26 153.88 L 629.35 153.88 L 629.35 -20.07"></path></clippath><g clip-path="url(#pgfcp5)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M -56.41 0 L -50.51 0 M -56.41 66.91 L -50.51 66.91 M -56.41 133.81 L -50.51 133.81 M 432.5 0 L 426.6 0 M 432.5 66.91 L 426.6 66.91 M 432.5 133.81 L 426.6 133.81" style="fill:none"></path></g><g></g></g><path d="M -56.41 -20.07 L -56.41 153.88 L 432.5 153.88 L 432.5 -20.07 L -56.41 -20.07 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -35.59 -34.41)"><foreignobject height="9.46" overflow="visible" style="--fo_width :5.23em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="72.34"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">BERT-Base</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 59.6 -34.57)"><foreignobject height="9.61" overflow="visible" style="--fo_width :4.98em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="68.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">AfroXLMR</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 163.91 -34.57)"><foreignobject height="9.61" overflow="visible" style="--fo_width :3.49em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="48.28"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">AfroLM</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 236.52 -34.41)"><foreignobject height="9.46" overflow="visible" style="--fo_width :6.75em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="93.4"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">mDeBERTa-v3</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 343.02 -34.57)"><foreignobject height="9.61" overflow="visible" style="--fo_width :4.84em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="66.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">AfritevaV2</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -68.22 -4.46)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="0" class="ltx_Math" display="inline" id="S5.F4.pic1.m1" intent=":literal"><mn>0</mn></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -75.14 62.45)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="50" class="ltx_Math" display="inline" id="S5.F4.pic1.m2" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -82.06 129.35)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="20.76"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="100" class="ltx_Math" display="inline" id="S5.F4.pic1.m3" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math></span></span></foreignobject></g><clippath id="pgfcp6"><path d="M -56.41 -20.07 L 432.5 -20.07 L 432.5 153.88 L -56.41 153.88 Z"></path></clippath><g clip-path="url(#pgfcp6)"><g color="#0000FF" fill="#B3B3FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;"><path d="M -43.52 0 h 19.69 v 1.34 h -19.69 Z M 50.5 0 h 19.69 v 1.34 h -19.69 Z M 144.52 0 h 19.69 v 0.27 h -19.69 Z M 238.55 0 h 19.69 v 4.01 h -19.69 Z M 332.57 0 h 19.69 v 1.34 h -19.69 Z"></path></g><g></g><g color="#FF0000" fill="#FFB3B3" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FFB3B3;--ltx-fg-color:#FF0000;"><path d="M -21.07 0 h 19.69 v 26.76 h -19.69 Z M 72.95 0 h 19.69 v 34.79 h -19.69 Z M 166.98 0 h 19.69 v 22.75 h -19.69 Z M 261 0 h 19.69 v 13.38 h -19.69 Z M 355.02 0 h 19.69 v 18.73 h -19.69 Z"></path></g><g></g><g color="#734D26" fill="#ECD9C6" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#ECD9C6;--ltx-fg-color:#734D26;"><path d="M 1.38 0 h 19.69 v 61.55 h -19.69 Z M 95.41 0 h 19.69 v 62.89 h -19.69 Z M 189.43 0 h 19.69 v 54.86 h -19.69 Z M 283.45 0 h 19.69 v 46.83 h -19.69 Z M 377.48 0 h 19.69 v 37.47 h -19.69 Z"></path></g><g></g><g color="#000000" fill="#808080" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#808080;--ltx-fg-color:#000000;"><path d="M 23.84 0 h 19.69 v 96.34 h -19.69 Z M 117.86 0 h 19.69 v 105.71 h -19.69 Z M 211.88 0 h 19.69 v 96.34 h -19.69 Z M 305.91 0 h 19.69 v 92.33 h -19.69 Z M 399.93 0 h 19.69 v 73.6 h -19.69 Z"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 -37.14 6.23)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="1" class="ltx_Math" display="inline" id="S5.F4.pic1.m4" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 56.89 6.23)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="1" class="ltx_Math" display="inline" id="S5.F4.pic1.m5" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 144.37 5.16)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.44em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="19.99"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="0.2" class="ltx_Math" display="inline" id="S5.F4.pic1.m6" intent=":literal"><semantics><mn>0.2</mn><annotation encoding="application/x-tex">0.2</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 244.93 8.9)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="3" class="ltx_Math" display="inline" id="S5.F4.pic1.m7" intent=":literal"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 338.95 6.23)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="1" class="ltx_Math" display="inline" id="S5.F4.pic1.m8" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 -18.14 31.65)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="20" class="ltx_Math" display="inline" id="S5.F4.pic1.m9" intent=":literal"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 75.88 39.68)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="26" class="ltx_Math" display="inline" id="S5.F4.pic1.m10" intent=":literal"><semantics><mn>26</mn><annotation encoding="application/x-tex">26</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 169.9 27.64)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="17" class="ltx_Math" display="inline" id="S5.F4.pic1.m11" intent=":literal"><semantics><mn>17</mn><annotation encoding="application/x-tex">17</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 263.92 18.27)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="10" class="ltx_Math" display="inline" id="S5.F4.pic1.m12" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 357.95 23.62)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="14" class="ltx_Math" display="inline" id="S5.F4.pic1.m13" intent=":literal"><semantics><mn>14</mn><annotation encoding="application/x-tex">14</annotation></semantics></math></span></span></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#734D26;--ltx-fg-color:#734D26;" transform="matrix(1.0 0.0 0.0 1.0 4.31 66.44)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="46" class="ltx_Math" display="inline" id="S5.F4.pic1.m14" intent=":literal"><semantics><mn mathcolor="#734D26" style="--ltx-fg-color:#734D26;">46</mn><annotation encoding="application/x-tex">46</annotation></semantics></math></span></span></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#734D26;--ltx-fg-color:#734D26;" transform="matrix(1.0 0.0 0.0 1.0 98.33 67.78)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="47" class="ltx_Math" display="inline" id="S5.F4.pic1.m15" intent=":literal"><semantics><mn mathcolor="#734D26" style="--ltx-fg-color:#734D26;">47</mn><annotation encoding="application/x-tex">47</annotation></semantics></math></span></span></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#734D26;--ltx-fg-color:#734D26;" transform="matrix(1.0 0.0 0.0 1.0 192.35 59.75)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="41" class="ltx_Math" display="inline" id="S5.F4.pic1.m16" intent=":literal"><semantics><mn mathcolor="#734D26" style="--ltx-fg-color:#734D26;">41</mn><annotation encoding="application/x-tex">41</annotation></semantics></math></span></span></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#734D26;--ltx-fg-color:#734D26;" transform="matrix(1.0 0.0 0.0 1.0 286.38 51.72)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="35" class="ltx_Math" display="inline" id="S5.F4.pic1.m17" intent=":literal"><semantics><mn mathcolor="#734D26" style="--ltx-fg-color:#734D26;">35</mn><annotation encoding="application/x-tex">35</annotation></semantics></math></span></span></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#734D26;--ltx-fg-color:#734D26;" transform="matrix(1.0 0.0 0.0 1.0 380.4 42.36)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="28" class="ltx_Math" display="inline" id="S5.F4.pic1.m18" intent=":literal"><semantics><mn mathcolor="#734D26" style="--ltx-fg-color:#734D26;">28</mn><annotation encoding="application/x-tex">28</annotation></semantics></math></span></span></foreignobject></g><g color="#000000" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;--ltx-fg-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 26.76 101.23)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="72" class="ltx_Math" display="inline" id="S5.F4.pic1.m19" intent=":literal"><semantics><mn>72</mn><annotation encoding="application/x-tex">72</annotation></semantics></math></span></span></foreignobject></g><g color="#000000" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;--ltx-fg-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 120.78 110.6)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="79" class="ltx_Math" display="inline" id="S5.F4.pic1.m20" intent=":literal"><semantics><mn>79</mn><annotation encoding="application/x-tex">79</annotation></semantics></math></span></span></foreignobject></g><g color="#000000" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;--ltx-fg-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 214.81 101.23)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="72" class="ltx_Math" display="inline" id="S5.F4.pic1.m21" intent=":literal"><semantics><mn>72</mn><annotation encoding="application/x-tex">72</annotation></semantics></math></span></span></foreignobject></g><g color="#000000" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;--ltx-fg-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 308.83 97.22)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="69" class="ltx_Math" display="inline" id="S5.F4.pic1.m22" intent=":literal"><semantics><mn>69</mn><annotation encoding="application/x-tex">69</annotation></semantics></math></span></span></foreignobject></g><g color="#000000" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;--ltx-fg-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 402.85 78.48)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="55" class="ltx_Math" display="inline" id="S5.F4.pic1.m23" intent=":literal"><semantics><mn>55</mn><annotation encoding="application/x-tex">55</annotation></semantics></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.0 1.0 -1.0 0.0 -91.56 41.58)"><foreignobject height="9.46" overflow="visible" style="--fo_width :3.66em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="50.66"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">F1-score</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 134.37 167.07)"><foreignobject height="9.61" overflow="visible" style="--fo_width :7.76em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="107.35"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Scores on 5k data</span></span></foreignobject></g><g fill="#FFFFFF" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#FFFFFF;"><path d="M 76.38 -69.19 h 223.34 v 22.75 h -223.34 Z"></path></g><g fill="#FFFFFF" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#FFFFFF;" transform="matrix(1.0 0.0 0.0 1.0 80.53 -57.82)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#0000FF" fill="#B3B3FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 -1 13.01 0) translate(21.26,0) matrix(1.0 0.0 0.0 1.0 -18.49 -3.77)"><foreignobject height="9.61" overflow="visible" style="--fo_width :2.67em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="36.98"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">0-shot</span></span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FFB3B3" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FFB3B3;--ltx-fg-color:#FF0000;" transform="matrix(1 0 0 -1 55.52 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 -1 68.52 0) translate(23.98,0) matrix(1.0 0.0 0.0 1.0 -21.22 -3.77)"><foreignobject height="9.61" overflow="visible" style="--fo_width :3.07em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="42.43"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">2-shots</span></span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#734D26" fill="#ECD9C6" stroke="#734D26" style="--ltx-stroke-color:#734D26;--ltx-fill-color:#ECD9C6;--ltx-fg-color:#734D26;" transform="matrix(1 0 0 -1 116.49 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 -1 129.5 0) translate(23.98,0) matrix(1.0 0.0 0.0 1.0 -21.22 -3.77)"><foreignobject height="9.61" overflow="visible" style="--fo_width :3.07em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="42.43"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">8-shots</span></span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#000000" fill="#808080" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#808080;--ltx-fg-color:#000000;" transform="matrix(1 0 0 -1 177.47 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 -1 190.47 0) translate(12.28,0) matrix(1.0 0.0 0.0 1.0 -9.51 -3.69)"><foreignobject height="9.46" overflow="visible" style="--fo_width :1.38em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="19.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">FT</span></span></foreignobject></g></g></g></g></g></g></g></svg></span>
</figure>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.F4" title="Figure 4 ‣ 5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">4</span></a> shows the weighted average F1-score results for zero-shot and few-shot classification. AfroXLMR model outperforms all multilingual pre-trained models in few-shot and fine-tuning settings (with an F1-scores of 79% on 5k samples), followed by BERT-Base and AfroLM. This results also show that existing models, even when pre-trained on Wolof, do not perform well and that our dataset presents specific challenges.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Zero-shot and fine-tuning (FT) results (in %) on multilingual pre-trained models.</figcaption><span class="ltx_inline-block"><svg class="ltx_picture" height="166.54" id="S5.F5.pic1" overflow="visible" version="1.1" viewbox="0 0 367.73 166.54" width="367.73"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,166.54) matrix(1 0 0 -1 0 0) translate(39.37,0) translate(0,34.8)"><g class="ltx_centering" transform="matrix(0.8 0.0 0.0 0.8 -39.37 -34.8)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 1 0 0) translate(96.54,0) translate(0,59.02)"><clippath id="pgfcp7"><path d="M -244.18 -15.53 L -244.18 119.05 L 559.7 119.05 L 559.7 -15.53"></path></clippath><g clip-path="url(#pgfcp7)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt" style="--ltx-stroke-color:#BFBFBF;--ltx-fill-color:#BFBFBF;--ltx-fg-color:#BFBFBF;"><path d="M -47.33 0 L 362.85 0 M -47.33 51.76 L 362.85 51.76 M -47.33 103.53 L 362.85 103.53" style="fill:none"></path></g><g></g></g><clippath id="pgfcp8"><path d="M -47.33 -212.38 L 362.85 -212.38 L 362.85 315.91 L -47.33 315.91"></path></clippath><g clip-path="url(#pgfcp8)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M 0 -15.53 L 0 -9.62 M 78.88 -15.53 L 78.88 -9.62 M 157.76 -15.53 L 157.76 -9.62 M 236.64 -15.53 L 236.64 -9.62 M 315.52 -15.53 L 315.52 -9.62 M 0 119.05 L 0 113.15 M 78.88 119.05 L 78.88 113.15 M 157.76 119.05 L 157.76 113.15 M 236.64 119.05 L 236.64 113.15 M 315.52 119.05 L 315.52 113.15" style="fill:none"></path></g><g></g></g><clippath id="pgfcp9"><path d="M -244.18 -15.53 L -244.18 119.05 L 559.7 119.05 L 559.7 -15.53"></path></clippath><g clip-path="url(#pgfcp9)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt" style="--ltx-stroke-color:#808080;--ltx-fill-color:#808080;--ltx-fg-color:#808080;"><path d="M -47.33 0 L -41.42 0 M -47.33 51.76 L -41.42 51.76 M -47.33 103.53 L -41.42 103.53 M 362.85 0 L 356.94 0 M 362.85 51.76 L 356.94 51.76 M 362.85 103.53 L 356.94 103.53" style="fill:none"></path></g><g></g></g><path d="M -47.33 -15.53 L -47.33 119.05 L 362.85 119.05 L 362.85 -15.53 L -47.33 -15.53 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -35.59 -29.87)"><foreignobject height="9.46" overflow="visible" style="--fo_width :5.23em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="72.34"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">BERT-Base</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 44.46 -30.03)"><foreignobject height="9.61" overflow="visible" style="--fo_width :4.98em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="68.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">AfroXLMR</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 133.62 -30.03)"><foreignobject height="9.61" overflow="visible" style="--fo_width :3.49em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="48.28"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">AfroLM</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 191.09 -29.87)"><foreignobject height="9.46" overflow="visible" style="--fo_width :6.75em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="93.4"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">mDeBERTa-v3</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 282.45 -30.03)"><foreignobject height="9.61" overflow="visible" style="--fo_width :4.84em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="66.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">AfritevaV2</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -59.13 -4.46)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="0" class="ltx_Math" display="inline" id="S5.F5.pic1.m1" intent=":literal"><mn>0</mn></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -66.05 47.3)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="50" class="ltx_Math" display="inline" id="S5.F5.pic1.m2" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -72.97 99.07)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="20.76"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="100" class="ltx_Math" display="inline" id="S5.F5.pic1.m3" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math></span></span></foreignobject></g><clippath id="pgfcp10"><path d="M -47.33 -15.53 L 362.85 -15.53 L 362.85 119.05 L -47.33 119.05 Z"></path></clippath><g clip-path="url(#pgfcp10)"><g color="#0000FF" fill="#B3B3FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;"><path d="M -21.07 0 h 19.69 v 2.38 h -19.69 Z M 57.81 0 h 19.69 v 2.07 h -19.69 Z M 136.69 0 h 19.69 v 0.41 h -19.69 Z M 215.57 0 h 19.69 v 1.55 h -19.69 Z M 294.45 0 h 19.69 v 0.52 h -19.69 Z"></path></g><g></g><g color="#FF0000" fill="#FFB3B3" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FFB3B3;--ltx-fg-color:#FF0000;"><path d="M 1.38 0 h 19.69 v 56.94 h -19.69 Z M 80.26 0 h 19.69 v 59.01 h -19.69 Z M 159.14 0 h 19.69 v 57.97 h -19.69 Z M 238.02 0 h 19.69 v 53.83 h -19.69 Z M 316.91 0 h 19.69 v 51.76 h -19.69 Z"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 -21.22 7.27)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.44em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="19.99"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="2.3" class="ltx_Math" display="inline" id="S5.F5.pic1.m4" intent=":literal"><semantics><mn>2.3</mn><annotation encoding="application/x-tex">2.3</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 64.19 6.96)"><foreignobject height="8.92" overflow="visible" style="--fo_width :0.5em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="6.92"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="2" class="ltx_Math" display="inline" id="S5.F5.pic1.m5" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 136.54 5.3)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.44em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="19.99"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="0.4" class="ltx_Math" display="inline" id="S5.F5.pic1.m6" intent=":literal"><semantics><mn>0.4</mn><annotation encoding="application/x-tex">0.4</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 215.42 6.44)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.44em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="19.99"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="1.5" class="ltx_Math" display="inline" id="S5.F5.pic1.m7" intent=":literal"><semantics><mn>1.5</mn><annotation encoding="application/x-tex">1.5</annotation></semantics></math></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;" transform="matrix(1.0 0.0 0.0 1.0 294.3 5.41)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1.44em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="19.99"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.F5.pic1.m8" intent=":literal"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 4.31 61.83)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="55" class="ltx_Math" display="inline" id="S5.F5.pic1.m9" intent=":literal"><semantics><mn>55</mn><annotation encoding="application/x-tex">55</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 83.19 63.9)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="57" class="ltx_Math" display="inline" id="S5.F5.pic1.m10" intent=":literal"><semantics><mn>57</mn><annotation encoding="application/x-tex">57</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 162.07 62.86)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="56" class="ltx_Math" display="inline" id="S5.F5.pic1.m11" intent=":literal"><semantics><mn>56</mn><annotation encoding="application/x-tex">56</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 240.95 58.72)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="52" class="ltx_Math" display="inline" id="S5.F5.pic1.m12" intent=":literal"><semantics><mn>52</mn><annotation encoding="application/x-tex">52</annotation></semantics></math></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;" transform="matrix(1.0 0.0 0.0 1.0 319.83 56.65)"><foreignobject height="8.92" overflow="visible" style="--fo_width :1em;--fo_height:0.64em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 8.92)" width="13.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><math alttext="50" class="ltx_Math" display="inline" id="S5.F5.pic1.m13" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(0.0 1.0 -1.0 0.0 -82.47 26.43)"><foreignobject height="9.46" overflow="visible" style="--fo_width :3.66em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="50.66"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">F1-score</span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 93.82 134.94)"><foreignobject height="12.3" overflow="visible" style="--fo_width :9.24em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="127.88"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">Scores on all samples</span></span></foreignobject></g><g fill="#FFFFFF" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#FFFFFF;"><path d="M 107.07 -58.75 h 101.39 v 22.75 h -101.39 Z"></path></g><g fill="#FFFFFF" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#FFFFFF;" transform="matrix(1.0 0.0 0.0 1.0 111.22 -47.37)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#0000FF" fill="#B3B3FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#B3B3FF;--ltx-fg-color:#0000FF;" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 -1 13.01 0) translate(21.26,0) matrix(1.0 0.0 0.0 1.0 -18.49 -3.77)"><foreignobject height="9.61" overflow="visible" style="--fo_width :2.67em;--fo_height:0.69em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.61)" width="36.98"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">0-shot</span></span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FFB3B3" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FFB3B3;--ltx-fg-color:#FF0000;" transform="matrix(1 0 0 -1 55.52 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1 0 0 -1 68.52 0) translate(12.28,0) matrix(1.0 0.0 0.0 1.0 -9.51 -3.69)"><foreignobject height="9.46" overflow="visible" style="--fo_width :1.38em;--fo_height:0.68em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 9.46)" width="19.03"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">FT</span></span></foreignobject></g></g></g></g></g></g></g></svg></span>
</figure>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p">Experiments were conducted on the entire WolBanking77 dataset and reported in figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.F5" title="Figure 5 ‣ 5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5</span></a>. Similar to the experiments performed on the 5k sub-sample of WolBanking77, results shows that AfroXLMR achieves slightly better scores compared to BERT-Base and AfroLM. These findings highlight that ID task for low-resource languages remains a challenging task, even for state-of-the-art small language models, as shown in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:slm</span>.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Precision, recall and f1-score scores for Small Language Models on WolBanking77. Best models are highlighted in lightgray.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">5k samples</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">All samples</span></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Llama-3.2-1B-Instruct</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.74</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.45</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.46</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span>lightgray Llama-3.2-3B-Instruct</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.55</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p">A comparison was also conducted for ASR models on the audio part of WolBanking77 dataset. Special characters were removed from the text, and all text was converted to lowercase. We can observe from the results with 4 hours of speech data in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.T5" title="Table 5 ‣ 5.4 Results and Discussions ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5</span></a> that, Canary-1b-flash with a WER score of 0.59% outperforms Phi-4-multimodal-instruct (WER 3.1%) and more particularly Distil-whisper-large-v3.5 (WER 4.63%). All pre-trained ASR models were fine-tuned on WolBanking77 audio dataset for 1000 steps. The results indicate that strong performance can be achieved with relatively little data, which is promising for WolBanking77 and other low-resource language contexts.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Word Error Rates (WER) with 4 hours of speech. Training time is reported in minutes.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td class="ltx_td ltx_align_left ltx_border_tt">Parameters</td>
<td class="ltx_td ltx_align_left ltx_border_tt">Training time</td>
<td class="ltx_td ltx_align_left ltx_border_tt">Steps</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt">WER</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Phi-4-multimodal-instruct</td>
<td class="ltx_td ltx_align_left ltx_border_t">5.6B</td>
<td class="ltx_td ltx_align_left ltx_border_t">32</td>
<td class="ltx_td ltx_align_left ltx_border_t">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">3.1%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Distil-whisper-large-v3.5</td>
<td class="ltx_td ltx_align_left">756M</td>
<td class="ltx_td ltx_align_left">44</td>
<td class="ltx_td ltx_align_left">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right">4.63%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">
<span class="ltx_ERROR undefined">\rowcolor</span>lightgray <span class="ltx_text ltx_font_bold">Canary-1b-flash</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb">1B</td>
<td class="ltx_td ltx_align_left ltx_border_bb">20</td>
<td class="ltx_td ltx_align_left ltx_border_bb">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb"><span class="ltx_text ltx_font_bold">0.59%</span></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Long-term Support and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">Long term support includes updates to the text and audio recordings, improvements to the simulation setup script, and potential bug fixes. Free access is provided to various versions of the dataset and source code under a CC BY 4.0 license to accelerate research in the field. It is planned to add more audio recordings in diverse environments to enhance model robustness. Text data corresponding to possible responses for each intent will be shared for potential use in Text-To-Speech applications. Additionally, the dataset can be annotated for slot-filling tasks.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p">Access to financial services or public transportation services can be facilitated for illiterate people by providing them with voice interfaces in their language of communication. In this paper, an Intent Detection dataset is presented in Wolof language in two modalities which are voice and text. Additionally, dataset description and benchmarks are presented. WolBanking77 is published and shared freely under CC BY 4.0 license along with the code and datasheets <cite class="ltx_cite ltx_citemacro_citet">Gebru et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib33" title="">2021</a>]</cite> with the aim of inspiring low budget research into low-resource languages.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p">This work is supported by the Partnership for Skills in Applied Sciences, Engineering and Technology (PASET) - Regional Scholarship and Innovation Fund (RSIF) under Grant No.:B8501L10014. We also thank the entire CLAD team and in particular: Professor Souleymane Faye and all the translators who participated in this project.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. [2022a]</span>
<span class="ltx_bibblock">
David Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel
Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen
Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong,
Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre
Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair
Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed
Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,
Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste
Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris
Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula,
and Sam Manthalu.

</span>
<span class="ltx_bibblock">A few thousand translations go a long way! leveraging pre-trained
models for African news translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 3053–3070, Seattle, United States, July
2022a. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.naacl-main.223</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.223" title="">https://aclanthology.org/2022.naacl-main.223</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. [2021]</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia
Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti
Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H.
Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu
Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie
Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo,
Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin
Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi,
Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde,
Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor
Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane
MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde,
Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou,
Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin,
Tendai Marengereke, and Salomey Osei.

</span>
<span class="ltx_bibblock">MasakhaNER: Named entity recognition for African languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:1116–1131, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl_a_00416</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.tacl-1.66/" title="">https://aclanthology.org/2021.tacl-1.66/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. [2022b]</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani,
Michael Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba O. Alabi,
Shamsuddeen H. Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa
Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy
Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor,
Fatoumata Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo,
Catherine Gitau, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne,
Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Marivate, Elvis Mboning,
Tajuddeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-Nabende,
Neo L. Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Adeyemi,
Gilles Q. Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana
Moteu Ngoli, and Dietrich Klakow.

</span>
<span class="ltx_bibblock">MasakhaNER 2.0: Africa-centric transfer learning for named
entity recognition.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
<em class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing</em>, pages 4488–4508, Abu Dhabi, United Arab Emirates,
December 2022b. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.emnlp-main.298</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.298/" title="">https://aclanthology.org/2022.emnlp-main.298/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alabi et al. [2022]</span>
<span class="ltx_bibblock">
Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow.

</span>
<span class="ltx_bibblock">Adapting Pre-trained Language Models to African Languages via
Multilingual Adaptive Fine-Tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on
Computational Linguistics</em>, pages 4336–4349, Gyeongju, Republic of
Korea, October 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexis et al. [2022]</span>
<span class="ltx_bibblock">
Conneau Alexis, Bapna Ankur, Zhang Yu, Ma Min, von Platen Patrick, Lozhkov
Anton, Cherry Colin, Jia Ye, Rivera Clara, Kale Mihir, van Esch Daan,
Axelrod Vera, Khanuja Simran, Clark Jonathan, Firat Orhan, Auli Michael,
Ruder Sebastian, Riesa Jason, and Johnson Melvin.

</span>
<span class="ltx_bibblock">XTREME-S: Evaluating Cross-lingual Speech Representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2022</em>, pages 3248–3252, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">{10.21437/Interspeech.2022-10007}</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ANSD [2013]</span>
<span class="ltx_bibblock">
ANSD.

</span>
<span class="ltx_bibblock">Recensement général de la population et de l’habitat, de
l’agriculture et de l’elevage.

</span>
<span class="ltx_bibblock">Technical report, Agence Nationale de la Statistique et de la
Démographie, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ANSD [2021]</span>
<span class="ltx_bibblock">
ANSD.

</span>
<span class="ltx_bibblock">Enquête harmonisée sur les conditions de vie des ménages (ehcvm).

</span>
<span class="ltx_bibblock">Technical report, Agence Nationale de la Statistique et de la
Démographie, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthony et al. [2024]</span>
<span class="ltx_bibblock">
Aubra Anthony, Nanjira Sambuli, and Lakshmee Sharma.

</span>
<span class="ltx_bibblock">Security and Trust in Africa’s Digital Financial Inclusion
Landscape.

</span>
<span class="ltx_bibblock">Technical report, Carnegie Endowment for International Peace, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. [2020]</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael
Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock">In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache,
Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi
Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion
Moreno, Jan Odijk, and Stelios Piperidis, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the
Twelfth Language Resources and Evaluation Conference</em>, pages 4218–4222,
Marseille, France, May 2020. European Language Resources Association.

</span>
<span class="ltx_bibblock">ISBN 979-10-95546-34-4.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.lrec-1.520/" title="">https://aclanthology.org/2020.lrec-1.520/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe and Schwenk [2019]</span>
<span class="ltx_bibblock">
Mikel Artetxe and Holger Schwenk.

</span>
<span class="ltx_bibblock">Margin-based parallel corpus mining with multilingual sentence
embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3197–3203, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1309</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1309" title="">https://aclanthology.org/P19-1309</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bank [2022]</span>
<span class="ltx_bibblock">
World Bank.

</span>
<span class="ltx_bibblock">World Bank Open Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">World Bank Open Data</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.worldbank.org" title="">https://data.worldbank.org</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blachon et al. [2016]</span>
<span class="ltx_bibblock">
David Blachon, Elodie Gauthier, Laurent Besacier, Guy-Noël Kouarata, Martine
Adda-Decker, and Annie Rialland.

</span>
<span class="ltx_bibblock">Parallel Speech Collection for Under-resourced Language
Studies Using the Lig-Aikuma Mobile Device App.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Procedia Computer Science</em>, 81:61–66, January 2016.

</span>
<span class="ltx_bibblock">ISSN 1877-0509.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.procs.2016.04.030</span>.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1877050916300448" title="">https://www.sciencedirect.com/science/article/pii/S1877050916300448</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun et al. [2017]</span>
<span class="ltx_bibblock">
Daniel Braun, Adrian Hernandez Mendez, Florian Matthes, and Manfred Langen.

</span>
<span class="ltx_bibblock">Evaluating Natural Language Understanding Services for
Conversational Question Answering Systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue</em>, pages 174–185, Saarbrücken, Germany, August
2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/W17-5522</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W17-5522" title="">https://aclanthology.org/W17-5522</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carletta et al. [2006]</span>
<span class="ltx_bibblock">
Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot,
Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa
Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes Lisowska, Iain McCowan,
Wilfried Post, Dennis Reidsma, and Pierre Wellner.

</span>
<span class="ltx_bibblock">The ami meeting corpus: A pre-announcement.

</span>
<span class="ltx_bibblock">In Steve Renals and Samy Bengio, editors, <em class="ltx_emph ltx_font_italic">Machine Learning for
Multimodal Interaction</em>, pages 28–39, Berlin, Heidelberg, 2006. Springer
Berlin Heidelberg.

</span>
<span class="ltx_bibblock">ISBN 978-3-540-32550-5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casanueva et al. [2020]</span>
<span class="ltx_bibblock">
Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and
Ivan Vulić.

</span>
<span class="ltx_bibblock">Efficient Intent Detection with Dual Sentence Encoders.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Natural Language
Processing for Conversational AI</em>, pages 38–45, Online, July 2020.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.nlp4convai-1.5</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. [2016]</span>
<span class="ltx_bibblock">
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Listen, attend and spell: A neural network for large vocabulary
conversational speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 4960–4964, 2016.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP.2016.7472621</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021]</span>
<span class="ltx_bibblock">
Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng,
Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur,
Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing
Wang, Zhao You, and Zhiyong Yan.

</span>
<span class="ltx_bibblock">Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of
transcribed audio.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2021</em>, pages 3670–3674, 08 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2021-1965</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. [2024]</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong
Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen2-audio technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.10759</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.10759" title="">https://arxiv.org/abs/2407.10759</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication et al. [2023]</span>
<span class="ltx_bibblock">
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli,
David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong,
Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht,
Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek,
Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis,
Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes,
Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi,
Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan
Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna
Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin
Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur
Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee,
Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah
Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler
Wang.

</span>
<span class="ltx_bibblock">Seamlessm4t: Massively multilingual &amp; multimodal machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11596</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.11596" title="">https://arxiv.org/abs/2308.11596</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. [2020]</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Unsupervised Cross-lingual Representation Learning at Scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics</em>, pages 8440–8451, Online,
July 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.747</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. [2023]</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth
Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna.

</span>
<span class="ltx_bibblock">Fleurs: Few-shot learning evaluation of universal representations of
speech.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</em>, pages
798–805, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/SLT54892.2023.10023141</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coucke et al. [2018]</span>
<span class="ltx_bibblock">
Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier,
David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone,
Thibaut Lavril, Maël Primet, and Joseph Dureau.

</span>
<span class="ltx_bibblock">Snips Voice Platform: an embedded Spoken Language
Understanding system for private-by-design voice interfaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.10190</em>, December 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.1805.10190</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1805.10190" title="">http://arxiv.org/abs/1805.10190</a>.

</span>
<span class="ltx_bibblock">arXiv:1805.10190 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">countryeconomy [2023]</span>
<span class="ltx_bibblock">
countryeconomy.

</span>
<span class="ltx_bibblock">Senegal - Literacy rate | countryeconomy.com, 2023.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://countryeconomy.com/demography/literacy-rate/senegal" title="">https://countryeconomy.com/demography/literacy-rate/senegal</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2019]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors,
<em class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1423</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1423/" title="">https://aclanthology.org/N19-1423/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhawan et al. [2023]</span>
<span class="ltx_bibblock">
Kunal Dhawan, Kdimating Rekesh, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Unified Model for Code-Switching Speech Recognition and
Language Identification Based on Concatenated Tokenizer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 6th Workshop on Computational
Approaches to Linguistic Code-Switching</em>, pages 74–82, Singapore,
December 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2024]</span>
<span class="ltx_bibblock">
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang
Xu, Fan Yang, and Mao Yang.

</span>
<span class="ltx_bibblock">Longrope: extending llm context window beyond 2 million tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 41st International Conference on Machine
Learning</em>, ICML’24. JMLR.org, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dossou et al. [2022]</span>
<span class="ltx_bibblock">
Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei,
Abigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris
Emezue.

</span>
<span class="ltx_bibblock">AfroLM: A Self-Active Learning-based Multilingual Pretrained
Language Model for 23 African Languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on Simple and
Efficient Natural Language Processing (SustaiNLP)</em>, pages 52–64, Abu
Dhabi, United Arab Emirates (Hybrid), December 2022. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.sustainlp-1.11</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Federmann et al. [2022]</span>
<span class="ltx_bibblock">
Christian Federmann, Tom Kocmi, and Ying Xin.

</span>
<span class="ltx_bibblock">NTREX-128 – news test references for MT evaluation of 128
languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the First Workshop on Scaling Up Multilingual
Evaluation</em>, pages 21–24, Online, nov 2022. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.sumeval-1.4" title="">https://aclanthology.org/2022.sumeval-1.4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galvez et al. [2021]</span>
<span class="ltx_bibblock">
Daniel Galvez, Greg Diamos, Juan Manuel Ciro Torres, Juan Felipe Cerón,
Keith Achorn, Anjali Gopi, David Kanter, Max Lam, Mark Mazumder, and
Vijay Janapa Reddi.

</span>
<span class="ltx_bibblock">The people’s speech: A large-scale diverse english
speech recognition dataset for commercial usage.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1)</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=R8CwidgJ0yT" title="">https://openreview.net/forum?id=R8CwidgJ0yT</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi et al. [2023]</span>
<span class="ltx_bibblock">
Sanchit Gandhi, Patrick von Platen, and Alexander M. Rush.

</span>
<span class="ltx_bibblock">Distil-whisper: Robust knowledge distillation via large-scale pseudo
labelling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.00430</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gauthier et al. [2016]</span>
<span class="ltx_bibblock">
Elodie Gauthier, Laurent Besacier, Sylvie Voisin, Michael Melese, and
Uriel Pascal Elingui.

</span>
<span class="ltx_bibblock">Collecting resources in sub-Saharan African languages for
automatic speech recognition: a case study of Wolof.

</span>
<span class="ltx_bibblock">In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi,
Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion
Moreno, Jan Odijk, and Stelios Piperidis, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the
Tenth International Conference on Language Resources and Evaluation
(LREC’16)</em>, pages 3863–3867, Portorož, Slovenia, May 2016. European
Language Resources Association (ELRA).

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/L16-1611/" title="">https://aclanthology.org/L16-1611/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gauthier et al. [2024]</span>
<span class="ltx_bibblock">
Elodie Gauthier, Aminata Ndiaye, and Abdoulaye Guissé.

</span>
<span class="ltx_bibblock">Kallaama: A transcribed speech dataset about agriculture in the three
most widely spoken languages in senegal.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Fifth workshop on Resources for African
Indigenous Languages (RAIL 2024)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru et al. [2021]</span>
<span class="ltx_bibblock">
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
Hanna Wallach, Hal Daumé III, and Kate Crawford.

</span>
<span class="ltx_bibblock">Datasheets for datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Commun. ACM</em>, 64(12):86–92, November
2021.

</span>
<span class="ltx_bibblock">ISSN 0001-0782.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3458723</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3458723" title="">https://doi.org/10.1145/3458723</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerz et al. [2021]</span>
<span class="ltx_bibblock">
Daniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek Mondal, Michał Lis, Eshan
Singhal, Nikola Mrkšić, Tsung-Hsien Wen, and Ivan Vulić.

</span>
<span class="ltx_bibblock">Multilingual and cross-lingual intent detection from spoken data.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
Wen-tau Yih, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing</em>, pages 7468–7475, Online and Punta
Cana, Dominican Republic, November 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.591</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.591/" title="">https://aclanthology.org/2021.emnlp-main.591/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. [2006]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence
data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 23rd International Conference on Machine
Learning</em>, ICML ’06, page 369–376, New York, NY, USA, 2006. Association for
Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 1595933832.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/1143844.1143891</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1143844.1143891" title="">https://doi.org/10.1145/1143844.1143891</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulati et al. [2020]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu,
Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented transformer for speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2020</em>, pages 5036–5040, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2020-3015</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2021]</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Deberta: Decoding-enhanced bert with disentangled attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=XPZIaotutsD" title="">https://openreview.net/forum?id=XPZIaotutsD</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2023]</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen.

</span>
<span class="ltx_bibblock">DeBERTav3: Improving deBERTa using ELECTRA-style pre-training
with gradient-disentangled embedding sharing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=sE7-XhLxHA" title="">https://openreview.net/forum?id=sE7-XhLxHA</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heffernan et al. [2022]</span>
<span class="ltx_bibblock">
Kevin Heffernan, Onur Çelebi, and Holger Schwenk.

</span>
<span class="ltx_bibblock">Bitext mining using distilled sentence representations for
low-resource languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2022</em>, pages 2101–2112, Abu Dhabi, United Arab Emirates, December
2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.findings-emnlp.154</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.findings-emnlp.154" title="">https://aclanthology.org/2022.findings-emnlp.154</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hernandez et al. [2018]</span>
<span class="ltx_bibblock">
François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko,
and Yannick Estève.

</span>
<span class="ltx_bibblock">Ted-lium 3: Twice as much data and corpus repartition for experiments
on speaker adaptation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Speech and Computer</em>, pages 198–208. Springer International
Publishing, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2022]</span>
<span class="ltx_bibblock">
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">https://openreview.net/forum?id=nZeVKeeFYf9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jarrar et al. [2023]</span>
<span class="ltx_bibblock">
Mustafa Jarrar, Ahmet Birim, Mohammed Khalilia, Mustafa Erden, and Sana Ghanem.

</span>
<span class="ltx_bibblock">ArBanking77: Intent detection neural model and a new dataset in
modern and dialectical Arabic.

</span>
<span class="ltx_bibblock">In Hassan Sawaf, Samhaa El-Beltagy, Wajdi Zaghouani, Walid Magdy,
Ahmed Abdelali, Nadi Tomeh, Ibrahim Abu Farha, Nizar Habash, Salam Khalifa,
Amr Keleg, Hatem Haddad, Imed Zitouni, Khalil Mrini, and Rawan Almatham,
editors, <em class="ltx_emph ltx_font_italic">Proceedings of ArabicNLP 2023</em>, pages 276–287, Singapore
(Hybrid), December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.arabicnlp-1.22</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.arabicnlp-1.22/" title="">https://aclanthology.org/2023.arabicnlp-1.22/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KANDJI et al. [2025]</span>
<span class="ltx_bibblock">
Abdou Karim KANDJI, Frederic Precioso, Cheikh Ba, Samba Ndiaye, and Augustin
Ndione.

</span>
<span class="ltx_bibblock">Wolbanking77, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/dsv/11828279" title="">https://www.kaggle.com/dsv/11828279</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasule et al. [2024]</span>
<span class="ltx_bibblock">
John Trevor Kasule, Sudi Murindanyi, Elvis Mugume, and Andrew Katumba.

</span>
<span class="ltx_bibblock">Luganda Speech Intent Recognition for IoT Applications.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">5th Workshop on African Natural Language Processing</em>, April
2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=GBq2FsT2Us" title="">https://openreview.net/forum?id=GBq2FsT2Us</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnan et al. [2021]</span>
<span class="ltx_bibblock">
Jitin Krishnan, Antonios Anastasopoulos, Hemant Purohit, and Huzefa Rangwala.

</span>
<span class="ltx_bibblock">Multilingual code-switching for zero-shot cross-lingual intent
prediction and slot filling.

</span>
<span class="ltx_bibblock">In Duygu Ataman, Alexandra Birch, Alexis Conneau, Orhan Firat,
Sebastian Ruder, and Gozde Gul Sahin, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the 1st
Workshop on Multilingual Representation Learning</em>, pages 211–223, Punta
Cana, Dominican Republic, November 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.mrl-1.18</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mrl-1.18/" title="">https://aclanthology.org/2021.mrl-1.18/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuchaiev et al. [2019]</span>
<span class="ltx_bibblock">
Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris
Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook,
Patrice Castonguay, Mariya Popova, Jocelyn Huang, and Jonathan M. Cohen.

</span>
<span class="ltx_bibblock">Nemo: a toolkit for building ai applications using neural modules.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.09577</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1909.09577" title="">https://arxiv.org/abs/1909.09577</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leclerc [2023]</span>
<span class="ltx_bibblock">
Jacques Leclerc.

</span>
<span class="ltx_bibblock">&lt;&lt;Langues principales du Sénégal&gt;&gt; dans Sénégal &amp;
L’aménagement linguistique dans le monde, Québec, CEFAN,
Université Laval.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota,
and Shinji Watanabe.

</span>
<span class="ltx_bibblock">Yodas: Youtube-oriented dataset for audio and speech.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU)</em>, pages 1–8. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liaw et al. [2018]</span>
<span class="ltx_bibblock">
Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez,
and Ion Stoica.

</span>
<span class="ltx_bibblock">Tune: A research platform for distributed model selection and
training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.05118</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">macrotrends [2022]</span>
<span class="ltx_bibblock">
macrotrends.

</span>
<span class="ltx_bibblock">Senegal Literacy Rate | Historical Chart &amp;
Data, 2022.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.macrotrends.net/global-metrics/countries/sen/senegal/literacy-rate" title="">https://www.macrotrends.net/global-metrics/countries/sen/senegal/literacy-rate</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martínez and Short [2022]</span>
<span class="ltx_bibblock">
Lina Martínez and John Rennie Short.

</span>
<span class="ltx_bibblock">The informal city: Exploring the variety of the street vending
economy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Sustainability</em>, 14(12), 2022.

</span>
<span class="ltx_bibblock">ISSN 2071-1050.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/su14127213</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2071-1050/14/12/7213" title="">https://www.mdpi.com/2071-1050/14/12/7213</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mastel et al. [2023]</span>
<span class="ltx_bibblock">
Pierrette MAHORO Mastel, Ester Namara, Aime Munezero, Richard Kagame, Zihan
Wang, Allan Anzagira, Akshat Gupta, and Jema David Ndibwile.

</span>
<span class="ltx_bibblock">NATURAL LANGUAGE UNDERSTANDING FOR AFRICAN LANGUAGES.

</span>
<span class="ltx_bibblock">April 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gWuvdFMqHM" title="">https://openreview.net/forum?id=gWuvdFMqHM</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mbodj [2014]</span>
<span class="ltx_bibblock">
Chérif Mbodj.

</span>
<span class="ltx_bibblock">L’activité terminologique au Sénégal.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">RINT</em>, 11:3–8, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft et al. [2025]</span>
<span class="ltx_bibblock">
Microsoft, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla,
Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary,
Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun
Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit
Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud
Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen
Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong
Luo, Piyush Madan, Vadim Mazalov, Arindam Mitra, Ali Mousavi, Anh Nguyen,
Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu,
Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal,
Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang,
Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang,
Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan
Zhang, and Xiren Zhou.

</span>
<span class="ltx_bibblock">Phi-4-Mini Technical Report: Compact yet Powerful
Multimodal Language Models via Mixture-of-LoRAs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.01743</em>, March 2025.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2503.01743</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moghe et al. [2023]</span>
<span class="ltx_bibblock">
Nikita Moghe, Evgeniia Razumovskaia, Liane Guillou, Ivan Vulić, Anna
Korhonen, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Multi3NLU++: A multilingual, multi-intent, multi-domain dataset
for natural language understanding in task-oriented dialogue.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,
<em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>,
pages 3732–3755, Toronto, Canada, July 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-acl.230</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-acl.230/" title="">https://aclanthology.org/2023.findings-acl.230/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moritz et al. [2022]</span>
<span class="ltx_bibblock">
Laurer Moritz, Atteveldt Wouter van, Andreu Casas, and Kasper Welbers.

</span>
<span class="ltx_bibblock">Less Annotating, More Classifying – Addressing the
Data Scarcity Issue of Supervised Machine Learning with Deep
Transfer Learning and BERT-NLI.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mwongela et al. [2023]</span>
<span class="ltx_bibblock">
Stanslaus Mwongela, Jay Patel, Sathy Rajasekharan, Laura Wotton, Mohamed Ahmed,
Gilles Hacheme, Bernard Shibwabo, and Julius Butime.

</span>
<span class="ltx_bibblock">Data-efficient learning for healthcare queries in low-resource and
code mixed language settings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ndione [2013]</span>
<span class="ltx_bibblock">
Augustin Ndione.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Contribution à Une Étude de La Différence Entre La
Réduplication et La Répétition En Français et En
Wolof</em>.

</span>
<span class="ltx_bibblock">These de doctorat, Tours, November 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogueji et al. [2021]</span>
<span class="ltx_bibblock">
Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Small data? no problem! exploring the viability of pretrained
multilingual language models for low-resourced languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 1st Workshop on Multilingual
Representation Learning</em>, pages 116–126, Punta Cana, Dominican Republic,
November 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.mrl-1.11</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.mrl-1.11/" title="">https://aclanthology.org/2021.mrl-1.11/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogundepo et al. [2023]</span>
<span class="ltx_bibblock">
Odunayo Ogundepo, Tajuddeen R. Gwadabe, Clara E. Rivera, Jonathan H. Clark,
Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure F. P. Dossou, Abdou Aziz
Diop, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani,
Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Njoroge Kahira,
Shamsuddeen Hassan Muhammad, Akintunde Oladipo, Abraham Toluwase Owodunni,
Atnafu Lambebo Tonja, Iyanuoluwa Shode, Akari Asai, Tunde Oluwaseyi Ajayi,
Clemencia Siro, Steven Arthur, Mofetoluwa Adeyemi, Orevaoghene Ahia,
Anuoluwapo Aremu, Oyinkansola Awosan, Chiamaka Chukwuneke, Bernard Opoku,
Awokoya Ayodele, Verrah Otiende, Christine Mwase, Boyd Sinkala,
Andre Niyongabo Rubungo, Daniel A. Ajisafe, Emeka Felix Onwuegbuzia, Habib
Mbow, Emile Niyomutabazi, Eunice Mukonde, Falalu Ibrahim Lawan, Ibrahim Said
Ahmad, Jesujoba O. Alabi, Martin Namukombo, Mbonu Chinedu, Mofya Phiri, Neo
Putini, Ndumiso Mngoma, Priscilla A. Amouk, Ruqayya Nasir Iro, and Sonia
Adhiambo.

</span>
<span class="ltx_bibblock">AfriQA: Cross-lingual open-retrieval question answering for
African languages.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <em class="ltx_emph ltx_font_italic">Findings
of the Association for Computational Linguistics: EMNLP 2023</em>, pages
14957–14972, Singapore, December 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-emnlp.997</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-emnlp.997/" title="">https://aclanthology.org/2023.findings-emnlp.997/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oladipo et al. [2023]</span>
<span class="ltx_bibblock">
Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Owodunni,
Odunayo Ogundepo, David Adelani, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Better quality pre-training data and t5 models for African
languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing</em>, pages 158–168, Singapore, December 2023.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.11" title="">https://aclanthology.org/2023.emnlp-main.11</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. [2024]</span>
<span class="ltx_bibblock">
OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya
Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford,
Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex
Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin,
Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau,
Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin
Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej
Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu,
Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela
Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley
Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben
Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth
Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng,
Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu,
Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary
Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern,
Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong
Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina
Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer,
Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin
Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David
Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev
Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay,
Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric
Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene
Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso,
Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff
Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu,
Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun,
Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang,
Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian
Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever,
Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub
Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros,
Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei,
Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica
Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang,
Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes
Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato,
Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld,
Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee,
Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy
Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen,
Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar,
Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher
Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka,
Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier,
Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric
Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat
Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh,
Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan
Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen,
Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang,
Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo
de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira
Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia
Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan
LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak,
Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora
Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe,
Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul
McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter
Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe
Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora,
Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar
Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby,
Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael,
Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi
Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini
Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger,
Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto,
Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan,
Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer,
Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan,
Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas
Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom
Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters,
Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad
Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu,
Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng,
Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov.

</span>
<span class="ltx_bibblock">Gpt-4o system card.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.21276</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2410.21276" title="">https://arxiv.org/abs/2410.21276</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouattara et al. [2025]</span>
<span class="ltx_bibblock">
Maimouna Ouattara, Abdoul Kader Kaboré, Jacques Klein, and Tegawendé F.
Bissyandé.

</span>
<span class="ltx_bibblock">Bridging literacy gaps in African informal business management with
low-resource conversational agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the First Workshop on Language Models for
Low-Resource Languages</em>, pages 193–203, Abu Dhabi, United Arab Emirates,
January 2025. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2025.loreslm-1.15/" title="">https://aclanthology.org/2025.loreslm-1.15/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. [2015]</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE
International Conference on</em>, pages 5206–5210. IEEE, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. [2019]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.01703</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1912.01703" title="">https://arxiv.org/abs/1912.01703</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perrin [2005]</span>
<span class="ltx_bibblock">
Loïc-Michel Perrin.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Des représentations du temps en wolof</em>.

</span>
<span class="ltx_bibblock">PhD thesis, Université Paris-Diderot - Paris VII, May 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Price [1990]</span>
<span class="ltx_bibblock">
P. J. Price.

</span>
<span class="ltx_bibblock">Evaluation of Spoken Language Systems: the ATIS Domain.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Speech and Natural Language: Proceedings of a
Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990</em>,
1990.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/H90-1020" title="">https://aclanthology.org/H90-1020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puvvada et al. [2024]</span>
<span class="ltx_bibblock">
Krishna Puvvada, Piotr Żelasko, He Huang, Oleksii Hrinchuk, Nithin Koluguri,
Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly
Lavrukhin, Jagadeesh Balam, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Less is more: Accurate speech recognition &amp; translation without
web-scale data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2024</em>, pages 3964–3968, 09 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2024-2294</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2023]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine
Learning</em>, ICML’23. JMLR.org, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rekesh et al. [2023]</span>
<span class="ltx_bibblock">
Dima Rekesh, Nithin Rao Koluguri, Samuel Kriman, Somshubra Majumdar, Vahid
Noroozi, He Huang, Oleksii Hrinchuk, Krishna Puvvada, Ankur Kumar, Jagadeesh
Balam, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Fast Conformer With Linearly Scalable Attention For Efficient
Speech Recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU)</em>, pages 1–8, December 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ASRU57964.2023.10389701</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. [2023a]</span>
<span class="ltx_bibblock">
Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu
Li, Rui Yan, Fei Huang, and Yongbin Li.

</span>
<span class="ltx_bibblock">SpokenWOZ: A large-scale speech-text benchmark for spoken
task-oriented dialogue agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=viktK3nO5b" title="">https://openreview.net/forum?id=viktK3nO5b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. [2023b]</span>
<span class="ltx_bibblock">
Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu
Li, Rui Yan, Fei Huang, and Yongbin Li.

</span>
<span class="ltx_bibblock">SpokenWOZ: A Large-Scale Speech-Text Benchmark for
Spoken Task-Oriented Dialogue Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
36:39088–39118, December 2023b.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/7b16688a2b053a1b01474ab5c78ce662-Abstract-Datasets_and_Benchmarks.html" title="">https://proceedings.neurips.cc/paper_files/paper/2023/hash/7b16688a2b053a1b01474ab5c78ce662-Abstract-Datasets_and_Benchmarks.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. [2023]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican,
et al.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.11805" title="">https://arxiv.org/abs/2312.11805</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">team et al. [2022]</span>
<span class="ltx_bibblock">
Nllb team, Marta Ruiz Costa-jussà, James Cross, Onur cCelebi, Maha Elbayad,
Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Alison Youngblood,
Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon L.
Spruit, C. Tran, Pierre Yves Andrews, Necip Fazil Ayan, Shruti Bhosale,
Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm’an,
Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem,
Holger Schwenk, and Jeff Wang.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2207.04672, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:250425961" title="">https://api.semanticscholar.org/CorpusID:250425961</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">torchtune maintainers [2024]</span>
<span class="ltx_bibblock">
torchtune maintainers.

</span>
<span class="ltx_bibblock">torchtune: Pytorch’s finetuning library, April 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https//github.com/pytorch/torchtune" title="">https//github.com/pytorch/torchtune</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunstall et al. [2022]</span>
<span class="ltx_bibblock">
Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat, Moshe
Wasserblat, and Oren Pereg.

</span>
<span class="ltx_bibblock">Efficient few-shot learning without prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.11055</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.11055" title="">https://arxiv.org/abs/2209.11055</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021a]</span>
<span class="ltx_bibblock">
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel
Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux.

</span>
<span class="ltx_bibblock">VoxPopuli: A large-scale multilingual speech corpus for
representation learning, semi-supervised learning and interpretation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 993–1003,
Online, August 2021a. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.80" title="">https://aclanthology.org/2021.acl-long.80</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021b]</span>
<span class="ltx_bibblock">
Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino.

</span>
<span class="ltx_bibblock">Covost 2 and massively multilingual speech translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2021</em>, pages 2247–2251, 2021b.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2021-2027</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et al. [2020]</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary,
Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from
Web Crawl Data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Twelfth Language Resources and
Evaluation Conference</em>, pages 4003–4012, Marseille, France, May 2020.
European Language Resources Association.

</span>
<span class="ltx_bibblock">ISBN 979-10-95546-34-4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. [2020]</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander M. Rush.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1910.03771" title="">https://arxiv.org/abs/1910.03771</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2020]</span>
<span class="ltx_bibblock">
Weijia Xu, Batool Haider, and Saab Mansour.

</span>
<span class="ltx_bibblock">End-to-End Slot Alignment and Recognition for
Cross-Lingual NLU.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>, pages 5052–5063,
Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.410</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.410" title="">https://aclanthology.org/2020.emnlp-main.410</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2025]</span>
<span class="ltx_bibblock">
Hao Yu, Jesujoba Oluwadara Alabi, Andiswa Bukula, Jian Yun Zhuang,
En-Shiun Annie Lee, Tadesse Kebede Guge, Israel Abebe Azime, Happy Buzaaba,
Blessing Kudzaishe Sibanda, Godson Koffi Kalipe, Jonathan Mukiibi, Salomon
Kabongo Kabenamualu, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu,
Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb,
Dietrich Klakow, and David Ifeoluwa Adelani.

</span>
<span class="ltx_bibblock">INJONGO: A multicultural intent detection and slot-filling dataset
for 16 African languages.

</span>
<span class="ltx_bibblock">In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher
Pilehvar, editors, <em class="ltx_emph ltx_font_italic">Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages
9429–9452, Vienna, Austria, July 2025. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">ISBN 979-8-89176-251-0.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2025.acl-long.464</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2025.acl-long.464/" title="">https://aclanthology.org/2025.acl-long.464/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Żelasko et al. [2025]</span>
<span class="ltx_bibblock">
Piotr Żelasko, Kunal Dhawan, Daniel Galvez, Krishna C. Puvvada, Ankita Pasad,
Nithin Rao Koluguri, Ke Hu, Vitaly Lavrukhin, Jagadeesh Balam, and Boris
Ginsburg.

</span>
<span class="ltx_bibblock">Training and Inference Efficiency of Encoder-Decoder Speech
Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.05931v2</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.05931v2" title="">https://arxiv.org/abs/2503.05931v2</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Technical Appendices and Supplementary Material</h2>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>WolBanking77 supplementary details</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Is the Wolof language as used in Mauritania and Gambia substantially distinct from the Senegalese variety, such that the demonstrated performance on the audio dataset may not generalise to these Wolof-speaking populations?</span></p>
</div>
<div class="ltx_para" id="S8.SS1.p2">
<p class="ltx_p">The Wolof language used in Mauritania and Gambia is not substantially different. However, the differences are to be seen rather in linguistic evolutions and in particular with the borrowings from foreign languages present, in particular Arabic and English. These differences may pose a problem for the generalization of models trained on the audio data that do not yet cover these variants of Wolof. This is also present in the accent of the Wolof locutors which highly differs under the effect of the official language (English or Arabic). This has a strong impact on Wolof words pronunciation.</p>
</div>
<div class="ltx_para" id="S8.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">It would also be useful to know the intersection of age and literacy in the population. If older speakers, for example, were proportionally less literate in the language, this might argue for strengthening the representation of older voices in the dataset, given that this development is aimed at improving accessibility through voice-interactive technology.</span></p>
</div>
<div class="ltx_para" id="S8.SS1.p4">
<p class="ltx_p">According to statistics (from Agence Nationale de la Statistique et de la Démographie <cite class="ltx_cite ltx_citemacro_citet">ANSD [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib6" title="">2013</a>]</cite>), the literacy rate is higher among the younger populations of 10-14 years old and 15-19 years old, with 58.1% and 64.1% respectively. However, the literacy rate decreases among older populations over 70 years of age (between 13% and 20%). In view of these figures, we intend to take the representation of the older population in future versions of the dataset.</p>
</div>
<div class="ltx_para" id="S8.SS1.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Is the digital infrastructure (mobile or desktop internet services) sufficient in all regions to support the deployment of the proposed future technology?</span></p>
</div>
<div class="ltx_para" id="S8.SS1.p6">
<p class="ltx_p">The mobile phone and internet coverage is now very high in Senegal, the WhatsApp application for example is used throughout the country.</p>
</div>
<div class="ltx_para" id="S8.SS1.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Among the 77 text intents and 10 speech intents, is there any association between these intents? Has the possibility of multiple intents existing in one sample been considered?</span></p>
</div>
<div class="ltx_para" id="S8.SS1.p8">
<p class="ltx_p">The text intent dataset and the speech intent dataset have been independently created. However, it could be possible to associate some speech intents with text intents such as BALANCE, CASH_DEPOSIT, FREEZE and TRANSFER_MONEY. It is indeed possible that a sample can belong to several intents, however this possibility has not been considered in this work.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Linguistic variation in the Wolof language across Senegal</h3>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p">Wolof has several dialects, including Bawol, Kajoor, Jolof, and Jander; This dialectal diversity does not prevent inter-understanding. However, there is a lesser degree of inter-understanding between these varieties and that spoken by the Lebous (an ethnic group living mainly in the Cape Verde peninsula, i.e. in certain localities in the Dakar region).</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3 </span>Speakers age distribution</h3>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p">The age distribution of the participants are presented in table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.T6" title="Table 6 ‣ 8.3 Speakers age distribution ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="S8.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Age distribution of the speakers</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Stat</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Age</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">mean</th>
<td class="ltx_td ltx_align_center ltx_border_t">26</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">std</th>
<td class="ltx_td ltx_align_center">4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">min</th>
<td class="ltx_td ltx_align_center">22</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">25%</th>
<td class="ltx_td ltx_align_center">23</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">50%</th>
<td class="ltx_td ltx_align_center">25</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">75%</th>
<td class="ltx_td ltx_align_center">27</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">max</th>
<td class="ltx_td ltx_align_center ltx_border_bb">36</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S8.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.4 </span>Illiteracy in Senegal</h3>
<div class="ltx_para" id="S8.SS4.p1">
<p class="ltx_p">We use the word “illiterate” in the sense of not being able to understand the official language, which is French. “ In Senegal, <cite class="ltx_cite ltx_citemacro_citet">ANSD [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib7" title="">2021</a>]</cite> reports an overall illiteracy rate of 48,2%, reaching 62,7% in rural area. Literacy rate relates to the official language of a country. In Senegal, the official language is French but is seldom spoken by the population in their daily lives. Senegalese people primarily use their native languages or Wolof, as a vehicular language, to communicate.” <cite class="ltx_cite ltx_citemacro_citep">[Gauthier et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib32" title="">2024</a>]</cite></p>
</div>
<div class="ltx_para" id="S8.SS4.p2">
<p class="ltx_p">The school dropout rate is high in Senegal and this has resulted in a significant number of people who went to school and left without acquiring basic skills in the official language (French). As a result, in our situation, basic skills in French are lacking for more than half of the Senegalese population. One of the solutions has been to resort to literacy in national languages with satisfactory results with the experiments in Pulaar carried out by NGO TOSTAN and the examples of functional literacy carried out by UNESCO. Despite this, a large segment of the population still remains in a situation where basic linguistic skills, either in the official language or in one of the national languages, are acquired very little or not at all.</p>
</div>
<div class="ltx_para" id="S8.SS4.p3">
<p class="ltx_p">Literacy, considering the various languages present, but also the formal and informal systems, is progressing in Senegal even if it remains below 60% for adults according to statistics from UNESCO or the World Bank <cite class="ltx_cite ltx_citemacro_citep">[macrotrends, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib50" title="">2022</a>]</cite>. According to these same sources, we note that among young people aged 15 to 24, the rate is higher and is around 78%, a sign of this progression. It is clear that there is a disparity on two levels, firstly, at the gender level, in fact adult women are far more victims of illiteracy than men of the same age group and also, urban areas suffer less than rural areas (UNESCO) <cite class="ltx_cite ltx_citemacro_citep">[countryeconomy, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib23" title="">2023</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5 </span>Transportation data</h3>
<div class="ltx_para" id="S8.SS5.p1">
<p class="ltx_p">The main goal for creating this dataset is to design a speech chatbot for Wolof, in order to equip illiterate people from Senegal with an AI tool allowing them to access banking services, as well as buying a transit pass from public transportation, etc.</p>
</div>
<div class="ltx_para" id="S8.SS5.p2">
<p class="ltx_p">Adding transportation data, is thus not only a way to evaluate the model’s ability to recognize out of context intents, but also to integrate complementary services into a mobile money application. Beyond making financial transactions with mobile money, it is possible to integrate other services such as transportation or bill payments (electricity, water, etc.). The fields are different but the kind of services (and so the sentences) we want to address are pretty similar in all of them.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.6 </span>Hyperparameters</h3>
<figure class="ltx_table" id="S8.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Hyperparameters for NLP models. mDeBERTa-v3* and AfritevaV2 have same hyperparameters.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt">bert-base-uncased</td>
<td class="ltx_td ltx_align_center ltx_border_tt">AfroXLMR</td>
<td class="ltx_td ltx_align_center ltx_border_tt">mDeBERTa-v3*</td>
<td class="ltx_td ltx_align_center ltx_border_tt">AfroLM</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Llama3.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Learning Rate</td>
<td class="ltx_td ltx_align_center ltx_border_t">2e-05</td>
<td class="ltx_td ltx_align_center ltx_border_t">2e-05</td>
<td class="ltx_td ltx_align_center ltx_border_t">2e-05</td>
<td class="ltx_td ltx_align_center ltx_border_t">2e-05</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">3e-4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Train Batch Size</td>
<td class="ltx_td ltx_align_center">32</td>
<td class="ltx_td ltx_align_center">4</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_align_center">16</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Eval Batch Size</td>
<td class="ltx_td ltx_align_center">32</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Warmup ratio</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_align_center">0.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"># epochs</td>
<td class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">20</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S8.SS6.p1">
<p class="ltx_p">NLP models hyperparameters are reported on table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.T7" title="Table 7 ‣ 8.6 Hyperparameters ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">7</span></a>, AdamW_torch_fused is the default optimizer for NLP models except for Llama3.2 that is AdamW.</p>
</div>
<figure class="ltx_table" id="S8.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Hyperparameters for ASR models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt">Canary-1b-flash</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Distil-whisper-large-v3.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">Phi-4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Learning Rate</td>
<td class="ltx_td ltx_align_center ltx_border_t">3e-4</td>
<td class="ltx_td ltx_align_center ltx_border_t">1e-5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">1e-4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Train Batch Size</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">16</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Eval Batch Size</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_align_center">8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Gradient Accumulation Steps</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Lr Scheduler Warmup Steps</td>
<td class="ltx_td ltx_align_center">2500</td>
<td class="ltx_td ltx_align_center">500</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Optimizer</td>
<td class="ltx_td ltx_align_center">AdamW</td>
<td class="ltx_td ltx_align_center">AdamW</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">AdamW_torch</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"># steps</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1000</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1000</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">1000</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S8.SS6.p2">
<p class="ltx_p">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.T8" title="Table 8 ‣ 8.6 Hyperparameters ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8</span></a> shows the hyperparameters used to train Phi-4-multimodal-instruct, Distil-whisper-large-v3.5 and Canary-1b-flash. Hyperparameters has been chosen by following HuggingFace <cite class="ltx_cite ltx_citemacro_citet">Wolf et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib81" title="">2020</a>]</cite> and NVIDIA Nemo <cite class="ltx_cite ltx_citemacro_citet">Kuchaiev et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib46" title="">2019</a>]</cite> documentations. AdamW has been used as a default optimizer.</p>
</div>
<figure class="ltx_table" id="S8.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Best hyperparameters for LASER3+CNN.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">LASER3+CNN</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Learning Rate</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.013364046097018405</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Last size of non classification layer</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">128</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Batch Size</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">2</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S8.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.7 </span>Zero-shot classification</h3>
<div class="ltx_para" id="S8.SS7.p1">
<p class="ltx_p">We consider the models listed in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S5.SS1.SSS1" title="5.1.1 Zero-shot classification with WolBanking77 ‣ 5.1 Intent detection models ‣ 5 Dataset evaluation &amp; Experiments ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">5.1.1</span></a> for zero-shot classification. We use the pre-trained Masked Language Modeling (MLM) models (i.e. encoder models) on the datasets mentioned in this section.</p>
</div>
<div class="ltx_para" id="S8.SS7.p2">
<p class="ltx_p">We then use these models to embed each sentence and to project it onto the embedding of the intent classes for WolBanking dataset without additional training.</p>
</div>
<div class="ltx_para" id="S8.SS7.p3">
<p class="ltx_p">To run a model in zero-shot mode, we use the huggingface framework with a pipeline configured as "zero-shot-classification". This pipeline can be used to classify sequences into any of the specified class names. In this case, the pipeline uses the pre-trained model to perform inference and retrieve the logits at the model output. A softmax function is then applied to the logits to generate the probabilities of each class.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.8 </span>Text prompt for Llama3.2</h3>
<div class="ltx_para" id="S8.SS8.p1">
<p class="ltx_p">Following prompt format was used to train the Llama3.2 model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.SS8.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="1175.78" id="S8.SS8.p2.pic1" overflow="visible" version="1.1" viewbox="0 0 600 1175.78" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,1175.78) matrix(1 0 0 -1 0 0)"><g fill="#666666" fill-opacity="1.0" style="--ltx-fill-color:#666666;"><path d="M 0 12.92 L 0 1162.86 C 0 1169.99 5.78 1175.78 12.92 1175.78 L 587.08 1175.78 C 594.22 1175.78 600 1169.99 600 1162.86 L 600 12.92 C 600 5.78 594.22 0 587.08 0 L 12.92 0 C 5.78 0 0 5.78 0 12.92 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.11 12.92 L 1.11 1162.86 C 1.11 1169.38 6.39 1174.67 12.92 1174.67 L 587.08 1174.67 C 593.61 1174.67 598.89 1169.38 598.89 1162.86 L 598.89 12.92 C 598.89 6.39 593.61 1.11 587.08 1.11 L 12.92 1.11 C 6.39 1.11 1.11 6.39 1.11 12.92 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 10.58 1156.28)"><foreignobject color="#000000" height="1154.62" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :41.83em;--fo_height:0.64em;--fo_depth :82.8em;" transform="matrix(1 0 0 -1 0 8.92)" width="578.84"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:41.83em;">
<span class="ltx_listing ltx_lstlisting ltx_listing"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,Q2xhc3NpZnkgdGhlIHRleHQgZm9yIG9uZSBvZiB0aGUgY2F0ZWdvcmllczoKCjx0ZXh0Pgp7dGV4dH0KPC90ZXh0PgoKQ2hvb3NlIGZyb20gb25lIG9mIHRoZSBjYXRlZ29yeToKe2NsYXNzZXN9Ck9ubHkgY2hvb3NlIG9uZSBjYXRlZ29yeSwgdGhlIG1vc3QgYXBwcm9wcmlhdGUgb25lLiBSZXBseSBvbmx5IHdpdGggdGhlIGNhdGVnb3J5Lg==">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx2"><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">Classify</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">one</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">categories</span><span class="ltx_text ltx_font_typewriter">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx3">
</span>
<span class="ltx_listingline" id="lstnumberx4"><span class="ltx_text ltx_font_typewriter">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">text</span><span class="ltx_text ltx_font_typewriter">&gt;</span>
</span>
<span class="ltx_listingline" id="lstnumberx5"><span class="ltx_text ltx_font_typewriter">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">text</span><span class="ltx_text ltx_font_typewriter">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx6"><span class="ltx_text ltx_font_typewriter">&lt;/</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">text</span><span class="ltx_text ltx_font_typewriter">&gt;</span>
</span>
<span class="ltx_listingline" id="lstnumberx7">
</span>
<span class="ltx_listingline" id="lstnumberx8"><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">Choose</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">one</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">category</span><span class="ltx_text ltx_font_typewriter">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx9"><span class="ltx_text ltx_font_typewriter">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">classes</span><span class="ltx_text ltx_font_typewriter">}</span>
</span>
<span class="ltx_listingline" id="lstnumberx10"><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">Only</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">choose</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">one</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">category</span><span class="ltx_text ltx_font_typewriter">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">most</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">appropriate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">one</span><span class="ltx_text ltx_font_typewriter">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">Reply</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">only</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter">category</span><span class="ltx_text ltx_font_typewriter">.</span>
</span>
</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para" id="S8.SS8.p3">
<p class="ltx_p">{text} is a variable that can be one of the following sentences for example:</p>
</div>
<div class="ltx_para" id="S8.SS8.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Dama wara yónnee xaalis, ndax mën naa jëfandikoo sama kàrtu kredi?</span> (<span class="ltx_text ltx_font_italic">I need to transfer some money, can I use my credit card?</span>)</p>
</div>
<div class="ltx_para" id="S8.SS8.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Jotuma xaalis bi ma ñu ma waroon a delloo.</span> (<span class="ltx_text ltx_font_italic">I have not received a refund.</span>)</p>
</div>
<div class="ltx_para" id="S8.SS8.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Ci lan ngeeni jëfandikoo kàrt yi ñuy sànni?</span> (<span class="ltx_text ltx_font_italic">What do you use disposable cards on?</span>)</p>
</div>
<div class="ltx_para" id="S8.SS8.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Ndax amna anam buma mëna toppe lii?</span> (<span class="ltx_text ltx_font_italic">Was there a way for me to get tracking for that?</span>)</p>
</div>
<div class="ltx_para" id="S8.SS8.p8">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Ban diir la wara am ngir yonnee xaalis Etats-Uni?</span> (<span class="ltx_text ltx_font_italic">How long does it take for deliver to the US?</span>)</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.9 </span>Reference scores from classic Machine Learning Techniques</h3>
<div class="ltx_para" id="S8.SS9.p1">
<p class="ltx_p">Machine learning models such as KNN, SVM, Logistic Regression (LR) and Naive Bayes (NB) with Bag-Of-Words are used as baselines as well as CNN and MPL with LASER3 as sentence encoder. Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:ml</span> shows ML baselines comparison. Results shows that Bag of Words combined with Linear Regression outperforms the other ML models with an F1-score of 53%. However, LR and SVM achieve same results (F1-score of 68%) on 5k split.</p>
</div>
<figure class="ltx_table" id="S8.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>ML baselines models precision, recall and f1-score results on WolBanking77. Best models are highlighted in lightgray.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">5k samples</span></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">All samples</span></th>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">F1</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BoW+KNN</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.31</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">BoW+SVM</th>
<td class="ltx_td ltx_align_center">0.70</td>
<td class="ltx_td ltx_align_center">0.69</td>
<td class="ltx_td ltx_align_center">0.68</td>
<td class="ltx_td ltx_align_center">0.49</td>
<td class="ltx_td ltx_align_center">0.48</td>
<td class="ltx_td ltx_align_center">0.48</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span class="ltx_ERROR undefined">\rowcolor</span>lightgray BoW+LR</th>
<td class="ltx_td ltx_align_center">0.70</td>
<td class="ltx_td ltx_align_center">0.69</td>
<td class="ltx_td ltx_align_center">0.68</td>
<td class="ltx_td ltx_align_center">0.54</td>
<td class="ltx_td ltx_align_center">0.53</td>
<td class="ltx_td ltx_align_center">0.53</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">BoW+NB</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.54</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.26</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S8.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.10 </span>Pretrained Sentence Encoder</h3>
<div class="ltx_para" id="S8.SS10.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">LASER3:</span> <cite class="ltx_cite ltx_citemacro_citet">Heffernan et al. [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib39" title="">2022</a>]</cite> propose an improved version of LASER by training the model on multiple languages with the goal of encoding them within a shared representation space. Their method involves training a teacher-student model that combines supervised and self-supervised approaches with the aim of training the model on low-resource languages. This approach makes it possible to cover 50 African languages, including Wolof. Teacher-student training is employed to avoid training a new model from scratch each time a new language needs to be encoded. Some of the African languages originate from the Masakhane project <span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_href" href="https://www.masakhane.io/" title="">https://www.masakhane.io/</a></span></span></span> as well as the EMNLP’22 workshop.<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_href" href="https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html" title="">https://www.statmt.org/wmt22/large-scale-multilingual-translation-task.html</a></span></span></span> The authors made some modifications to the LASER architecture such as replacing BPE with SPM (SentencePiece Model), an unsupervised text tokenizer and detokenizer that does not rely on language-specific pre or postprocessing, and adding an upsampling step for low-resource languages. The LASER sentence encoder trained on the public OPUS corpus <span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_href" href="https://opus.nlpl.eu/" title="">https://opus.nlpl.eu/</a></span></span></span> is used as the teacher model and renamed by <span class="ltx_text ltx_font_bold">LASER2</span>, while the student model is referred to as <span class="ltx_text ltx_font_bold">LASER3</span>. A student model is trained for each language covered in Masked Language Modeling objective, after which a multilingual distillation approach is applied by optimizing the cosine loss between the embeddings generated by the teacher and the student. The student architecture has been replaced by a 12-layer transformer instead of a 6-layer BiLSTM of the original LASER. For the Wolof language, the model was trained on 21k bitexts which are pairs of texts in two different languages that are translations of each other and 94k sentences using training distillation in addition to Masked Language Modeling. The experimental results show a clear improvement of LASER3 encoder for Wolof with a score of an xsim error rate of 6.03 (a margin-based similarity score by <cite class="ltx_cite ltx_citemacro_citet">Artetxe and Schwenk [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib10" title="">2019</a>]</cite>) compared to the original LASER encoder which produced a score of 70.65.</p>
</div>
<div class="ltx_para" id="S8.SS10.p2">
<p class="ltx_p">The pre-trained LASER3 sentence encoder, which vectorizes each sentence and all possible combinations of classification models (MPL and CNN) are compared in the results reported in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:laser_sent_enc</span>. Note that all classification models were trained from scratch. For all the evaluations, punctuation are removed and each sentence was converted to lowercase.</p>
</div>
<figure class="ltx_table" id="S8.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>LASER3 with classification models precision, recall and f1-score results on WolBanking77. LASER3+CNN (tuning) denotes LASER3+CNN with CNN hyperparameter tuning. Best models are highlighted in lightgray.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">5k samples</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">All samples</span></th>
<th class="ltx_td ltx_th ltx_th_column"></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span class="ltx_ERROR undefined">\rowcolor</span>lightgray LASER3+MLP</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.43</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.42</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.42</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">LASER3+CNN</th>
<td class="ltx_td ltx_align_center">0.01</td>
<td class="ltx_td ltx_align_center">0.05</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.01</td>
<td class="ltx_td ltx_align_center">0.01</td>
<td class="ltx_td ltx_align_center">0.05</td>
<td class="ltx_td ltx_align_center">0.01</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LASER3+CNN (tuning)</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.32</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S8.SS10.p3">
<p class="ltx_p">Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:laser_sent_enc</span> presents the weighted average results of LASER3 combined with classification models (MLP, CNN). LASER+MLP achieves the best performance with an F1-score of 55% on 5k samples and 42% on the full dataset, compared to LASER+CNN which produced poor results with a low F1-score of 1%. To improve the LASER+CNN model, we tuned the CNN model hyperparameters using the Ray Tune library <cite class="ltx_cite ltx_citemacro_citep">[Liaw et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib49" title="">2018</a>]</cite>. The tuned hyperparameters include the size of the final internal representation before the classification layer, the learning rate (lr) and the batch size.We performed 10 tuning trials, and for each trial, the Ray Library randomly sampled a combination of parameters using the ASHAScheduler, which terminates poorly performing trials early. The best hyperparameters obtained are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.T9" title="Table 9 ‣ 8.6 Hyperparameters ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">9</span></a>. This leads to the improved results reported in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:laser_sent_enc</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS11">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.11 </span>Consent form</h3>
<div class="ltx_para" id="S8.SS11.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.F6" title="Figure 6 ‣ 8.11 Consent form ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">6</span></a> presents the consent form (in French) presented to participants before the recording sessions.</p>
</div>
<figure class="ltx_figure" id="S8.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="275" id="S8.F6.g1" src="figures/form_consent.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Consent form</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Limitations</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p">We can note some limitations in our dataset, such as class imbalance in intents, which may cause the model to favor majority classes at the expense of minority ones. During the ASR evaluation, we noticed spelling errors in rare words, which could be corrected using a language model.</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Ethics Statement</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p">To protect the anonymity of participants, personal data such as speaker names, locations and androidIDs have been removed from the dataset. We notified each participant that the collected audio will be used as a public dataset for research purposes. Each participant read and signed a consent form in which they declared their free and informed consent to participate in the study as part of the data collection described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS11" title="8.11 Consent form ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.11</span></a>. A total amount of $5000 was paid to the linguists for the translation and phonetic transcription of all sentences in Wolof and French.</p>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Wolof language</h2>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p">Wolof is a member of the West Atlantic sub-branch of the Niger-Congo language family. A small minority from various other ethnic groups have adopted Wolof as their first language, while more than half of non-native speakers use Wolof as their second or third language. Together, these three groups of Wolof speakers represent about 90% of the population, making Senegal one of the most linguistically unified nations in West Africa <cite class="ltx_cite ltx_citemacro_citep">[Mbodj, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib53" title="">2014</a>]</cite>. Wolof is also spoken by the majority of the population in Gambia <cite class="ltx_cite ltx_citemacro_citep">[Ndione, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib58" title="">2013</a>]</cite>. Genetically related languages are Pulaar and Serere, which are all languages of the Niger-Congo phylum, and of the Atlantic branch <cite class="ltx_cite ltx_citemacro_citet">Ndione [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib58" title="">2013</a>]</cite> see figure <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S11.F7" title="Figure 7 ‣ 11 Wolof language ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_figure" id="S11.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="289" id="S11.F7.g1" src="figures/senegal-langues-map.jpg" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The main languages of Senegal <cite class="ltx_cite ltx_citemacro_citet">Leclerc [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib47" title="">2023</a>]</cite></figcaption>
</figure>
<div class="ltx_para" id="S11.p2">
<p class="ltx_p">To construct sentences in Wolof, there are several processes such as 8 class markers for words in singular form and 2 class markers for words in plural form. These markers are positioned in front of or right after the word such as: <span class="ltx_text ltx_font_italic">nit ki</span> (to designate a person), <span class="ltx_text ltx_font_italic">nit ñi</span> (to designate several persons). In addition, we also find other construction processes such as consonantal alternation, pre-nasalization and suffixation. For some derivations, there is the simultaneous presence of several of these processes. For example, pre-nasalization can be accompanied by suffixation <cite class="ltx_cite ltx_citemacro_citep">[Ndione, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib58" title="">2013</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S11.p3">
<p class="ltx_p">The conjugation in Wolof is achieved thanks to an invariable lexical base to which are added affixes holding the markers of IPAM (Index; Person Aspect-time; Mode) <cite class="ltx_cite ltx_citemacro_citep">[Perrin, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib66" title="">2005</a>, Ndione, <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib58" title="">2013</a>]</cite>. These inflectional markers highlight both semantic roles and grammatical relations. We mainly distinguish two types of verbs in Wolof, action verbs (expressing an action or a fact carried out or suffered by the subject) and state verbs (giving characteristics to elements, they describe a state, a way of being). There are ten conjugations in Wolof <cite class="ltx_cite ltx_citemacro_citet">Perrin [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib66" title="">2005</a>], Ndione [<a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib58" title="">2013</a>]</cite> such as the perfect, the aorist, the presentative, the emphatic of the verb, the emphatic of the subject, the emphatic of the complement, the negative, the emphatic negative, the imperative and the obligative.</p>
</div>
<div class="ltx_para" id="S11.p4">
<p class="ltx_p">Depending on the grammar, in Wolof, there is generally only one auxiliary, "di", with its variants "d" and "y" which mark the unaccomplished. The variant "d" appears with the time and the negation markers (<span class="ltx_text ltx_font_italic">doon, daa, daan, dee, du</span>).</p>
</div>
</section>
<section class="ltx_section" id="S12">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">12 </span>Datasheets for WolBanking77</h2>
<section class="ltx_subsection" id="S12.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.1 </span>Motivation</h3>
<div class="ltx_para" id="S12.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS1.p2">
<p class="ltx_p">This dataset was created with the aim of propelling research on Intent Detection (ID) task in Wolof which is a language spoken in West Africa in order to allow illiterate people to be able to interact with digital systems through the voice channel. We introduce a text dataset and audio dataset including utterances accompanied by their transcription with their corresponding intent in order to be able to simulate a dialogue between the user and the system.</p>
</div>
<div class="ltx_para" id="S12.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</span></p>
</div>
<div class="ltx_para" id="S12.SS1.p4">
<p class="ltx_p">This dataset is created by researchers at Université Cheikh Anta Diop of Dakar and Université Gaston Berger of Saint-Louis in Senegal.</p>
</div>
<div class="ltx_para" id="S12.SS1.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Who funded the creation of the dataset?</span></p>
</div>
<div class="ltx_para" id="S12.SS1.p6">
<p class="ltx_p">Funding was provided by the Partnership for skills in Applied Sciences, Engineering and Technology (PASET) and Regional Scholarship and Innovation Fund (RSIF).</p>
</div>
</section>
<section class="ltx_subsection" id="S12.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.2 </span>Composition</h3>
<div class="ltx_para" id="S12.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p2">
<p class="ltx_p">The dataset includes text and audio data for intent classification. For every sentence, text and annotations are provided.</p>
</div>
<div class="ltx_para" id="S12.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">How many instances are there in total (of each type, if appropriate)?</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p4">
<p class="ltx_p">For the audio version, there are 263 instances covering 10 intents in total. 4 hours and 17 minutes of audios from spoken sentences. For the text version, there are 9,791 instances covering 77 intents in total.</p>
</div>
<div class="ltx_para" id="S12.SS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p6">
<p class="ltx_p">The dataset contain all possible instances.</p>
</div>
<div class="ltx_para" id="S12.SS2.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">What data does each instance consist of? “Raw” data (e.g., unprocessed text or images)or features? In either case, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p8">
<p class="ltx_p">Each instance consists of the text associated with intent label. The audio data contains transcriptions of the corresponding audio files.</p>
</div>
<div class="ltx_para" id="S12.SS2.p9">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is there a label or target associated with each instance? If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p10">
<p class="ltx_p">Yes, there is a label intent class for each instance in the dataset.</p>
</div>
<div class="ltx_para" id="S12.SS2.p11">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p12">
<p class="ltx_p">Everything is included. There is no missing data.</p>
</div>
<div class="ltx_para" id="S12.SS2.p13">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p14">
<p class="ltx_p">Each instance in WolBanking77 is relatively independent.</p>
</div>
<div class="ltx_para" id="S12.SS2.p15">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Are there recommended data splits (e.g., training, development/validation,testing)?
If so, please provide a description of these splits, explaining the rationale behind them.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p16">
<p class="ltx_p">There are 80% of the data for training and 20% for validation. The intent categories were considered during the split by stratifying the data according to the intent target. This separation guarantees us to have all the intents both in the training set and in the test set.</p>
</div>
<div class="ltx_para" id="S12.SS2.p17">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p18">
<p class="ltx_p">The audio recordings were made in a controlled environment but may contain background noise.</p>
</div>
<div class="ltx_para" id="S12.SS2.p19">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p20">
<p class="ltx_p">WolBanking77 is self-contained.</p>
</div>
<div class="ltx_para" id="S12.SS2.p21">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals’ non-public communications)?If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p22">
<p class="ltx_p">No.</p>
</div>
<div class="ltx_para" id="S12.SS2.p23">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p24">
<p class="ltx_p">No.</p>
</div>
<div class="ltx_para" id="S12.SS2.p25">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p26">
<p class="ltx_p">The dataset identify subpopulations by age, gender and origin. The subpopulations are identified when participants fill the identification form by giving there age, gender and origin before starting a new recording session.</p>
</div>
<div class="ltx_para" id="S12.SS2.p27">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p28">
<p class="ltx_p">No it is not possible to identify individuals from the dataset, names are removed from the dataset.</p>
</div>
<div class="ltx_para" id="S12.SS2.p29">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS2.p30">
<p class="ltx_p">The dataset contain regions of origin showing that different voices and accents have been recorded to cover as more ethnic as possible and to make models robust to different accents.</p>
</div>
</section>
<section class="ltx_subsection" id="S12.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.3 </span>Collection Process</h3>
<div class="ltx_para" id="S12.SS3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p2">
<p class="ltx_p">Dataset collection process has been reported in section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3" title="3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S12.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p4">
<p class="ltx_p">Data has been recorded using Lig-Aikuma Android software <cite class="ltx_cite ltx_citemacro_citep">[Blachon et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib12" title="">2016</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S12.SS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p6">
<p class="ltx_p">Wolbanking77 is not sampled from a larger set.</p>
</div>
<div class="ltx_para" id="S12.SS3.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p8">
<p class="ltx_p">The data was collected with the help of students and professional translators. Data collection cost is around $5000.</p>
</div>
<div class="ltx_para" id="S12.SS3.p9">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p10">
<p class="ltx_p">Our data collection started in March 2024. The contents of our dataset are independent of the time of collection.</p>
</div>
<div class="ltx_para" id="S12.SS3.p11">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Were any ethical review processes conducted (e.g., by an institutional review board)?
If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p12">
<p class="ltx_p">Not applicable.</p>
</div>
<div class="ltx_para" id="S12.SS3.p13">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p14">
<p class="ltx_p">The text data is a translated version of Banking77 and the audio version from MINDS14.</p>
</div>
<div class="ltx_para" id="S12.SS3.p15">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p16">
<p class="ltx_p">A consent form shown in section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS11" title="8.11 Consent form ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.11</span></a> has been provided to each individuals.</p>
</div>
<div class="ltx_para" id="S12.SS3.p17">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p18">
<p class="ltx_p">Yes, see previous question.</p>
</div>
<div class="ltx_para" id="S12.SS3.p19">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p20">
<p class="ltx_p">Yes, see <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S8.SS11" title="8.11 Consent form ‣ 8 Technical Appendices and Supplementary Material ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">8.11</span></a>.</p>
</div>
<div class="ltx_para" id="S12.SS3.p21">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.</span></p>
</div>
<div class="ltx_para" id="S12.SS3.p22">
<p class="ltx_p">No.</p>
</div>
</section>
<section class="ltx_subsection" id="S12.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.4 </span>Preprocessing/cleaning/labeling</h3>
<div class="ltx_para" id="S12.SS4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.</span></p>
</div>
<div class="ltx_para" id="S12.SS4.p2">
<p class="ltx_p">Dataset construction and cleaning has been reported in section <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#S3" title="3 Data collection ‣ WolBanking77: Wolof Banking Speech Intent Classification Dataset"><span class="ltx_text ltx_ref_tag">3</span></a>. Text tokenization and removal of duplicated sentences has been done for NLP task.</p>
</div>
<div class="ltx_para" id="S12.SS4.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data.</span></p>
</div>
<div class="ltx_para" id="S12.SS4.p4">
<p class="ltx_p">No. The dataset does not contain all the raw data and metadata.</p>
</div>
<div class="ltx_para" id="S12.SS4.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.</span></p>
</div>
<div class="ltx_para" id="S12.SS4.p6">
<p class="ltx_p">Python programming language has been used to clean the data. All source codes are available on <a class="ltx_ref ltx_href" href="https://github.com/abdoukarim/wolbanking77" title="">Github</a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S12.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.5 </span>Uses</h3>
<div class="ltx_para" id="S12.SS5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Has the dataset been used for any tasks already? If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS5.p2">
<p class="ltx_p">Not yet.</p>
</div>
<div class="ltx_para" id="S12.SS5.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.</span></p>
</div>
<div class="ltx_para" id="S12.SS5.p4">
<p class="ltx_p">No yet.</p>
</div>
<div class="ltx_para" id="S12.SS5.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">What (other) tasks could the dataset be used for?</span></p>
</div>
<div class="ltx_para" id="S12.SS5.p6">
<p class="ltx_p">The dataset can be used for anything related to intent classification as well as to train speech and text models.</p>
</div>
<div class="ltx_para" id="S12.SS5.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?</span></p>
</div>
<div class="ltx_para" id="S12.SS5.p8">
<p class="ltx_p">Not applicable.</p>
</div>
<div class="ltx_para" id="S12.SS5.p9">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Are there tasks for which the dataset should not be used? If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS5.p10">
<p class="ltx_p">Not applicable.</p>
</div>
</section>
<section class="ltx_subsection" id="S12.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.6 </span>Distribution</h3>
<div class="ltx_para" id="S12.SS6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</span></p>
</div>
<div class="ltx_para" id="S12.SS6.p2">
<p class="ltx_p">The dataset is distributed publicly on <a class="ltx_ref ltx_href" href="https://www.kaggle.com/datasets/abdoukarimkandji/wolbanking77" title="">Kaggle</a>.</p>
</div>
<div class="ltx_para" id="S12.SS6.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?</span></p>
</div>
<div class="ltx_para" id="S12.SS6.p4">
<p class="ltx_p">The dataset is distributed on <a class="ltx_ref ltx_href" href="https://www.kaggle.com/datasets/abdoukarimkandji/wolbanking77" title="">Kaggle</a> with DOI <cite class="ltx_cite ltx_citemacro_citep">[KANDJI et al., <a class="ltx_ref" href="https://arxiv.org/html/2509.19271v3#bib.bib43" title="">2025</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S12.SS6.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">When will the dataset be distributed?</span></p>
</div>
<div class="ltx_para" id="S12.SS6.p6">
<p class="ltx_p">The dataset is published on <a class="ltx_ref ltx_href" href="https://www.kaggle.com/datasets/abdoukarimkandji/wolbanking77" title="">Kaggle</a>.</p>
</div>
<div class="ltx_para" id="S12.SS6.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</span></p>
</div>
<div class="ltx_para" id="S12.SS6.p8">
<p class="ltx_p">WolBanking77 is distributed under the CC BY 4.0 <span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_href" href="https://creativecommons.org/licenses/by/4.0/legalcode.en" title="">https://creativecommons.org/licenses/by/4.0/legalcode.en</a></span></span></span> license. This license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, even for commercial purposes.</p>
</div>
<div class="ltx_para" id="S12.SS6.p9">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.</span></p>
</div>
<div class="ltx_para" id="S12.SS6.p10">
<p class="ltx_p">No.</p>
</div>
<div class="ltx_para" id="S12.SS6.p11">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.</span></p>
</div>
<div class="ltx_para" id="S12.SS6.p12">
<p class="ltx_p">No.</p>
</div>
</section>
<section class="ltx_subsection" id="S12.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.7 </span>Maintenance</h3>
<div class="ltx_para" id="S12.SS7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Who is supporting/hosting/maintaining the dataset?</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p2">
<p class="ltx_p">Maintenance will be supported by the authors.</p>
</div>
<div class="ltx_para" id="S12.SS7.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">How can the owner/curator/manager of the dataset be contacted (e.g., email address)?</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p4">
<p class="ltx_p">Authors can be contacted by their email address.</p>
</div>
<div class="ltx_para" id="S12.SS7.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Is there an erratum? If so, please provide a link or other access point.</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p6">
<p class="ltx_p">Any updates will be shared on <a class="ltx_ref ltx_href" href="https://www.kaggle.com/datasets/abdoukarimkandji/wolbanking77" title="">Kaggle</a>.</p>
</div>
<div class="ltx_para" id="S12.SS7.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g.,mailing list,GitHub)?</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p8">
<p class="ltx_p">If necessary, any updates will be released on <a class="ltx_ref ltx_href" href="https://www.kaggle.com/datasets/abdoukarimkandji/wolbanking77" title="">Kaggle</a>.</p>
</div>
<div class="ltx_para" id="S12.SS7.p9">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p10">
<p class="ltx_p">N/A.</p>
</div>
<div class="ltx_para" id="S12.SS7.p11">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p12">
<p class="ltx_p">Older versions of the dataset will be kept.</p>
</div>
<div class="ltx_para" id="S12.SS7.p13">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.</span></p>
</div>
<div class="ltx_para" id="S12.SS7.p14">
<p class="ltx_p">Yes. Please contact the authors of this paper for building upon this dataset.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 24 19:22:16 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
